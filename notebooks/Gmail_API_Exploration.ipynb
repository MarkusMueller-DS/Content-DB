{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gmail Newsletter Exploration\n",
    "\n",
    "### Goal\n",
    "- explore the features of the Gmail api \n",
    "- gather relevant newsletter mails form the mailbox\n",
    "- scrape newsletter based on thier Design so that i can standardize the database article mentioned in the mails\n",
    "\n",
    "### DB-Layout\n",
    "- id\n",
    "- url\n",
    "- content name\n",
    "- type of content\n",
    "- full text or snippet of content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# imports\n",
    "from google.oauth2.credentials import Credentials   # handel auth\n",
    "from googleapiclient.discovery import build         # used to make the queries\n",
    "\n",
    "import os                  # handle paths\n",
    "import base64                   # decrypt mail data\n",
    "from bs4 import BeautifulSoup   # parse data after decryption\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "relevant paramters for list()\n",
    "- maxResult: default 100 / up to 500\n",
    "- q: only returns messages matching a query \"from:lon@dataelixir.com\"     \n",
    "- pageToken: \"retrive a specific page of result in a list\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Define SCOPES\n",
    "# relevant for auth\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "\n",
    "pathToToken = '/Users/markusmuller/python/projects/gmail-newsletter-db/token.json'\n",
    "\n",
    "creds = Credentials.from_authorized_user_file(pathToToken, SCOPES)\n",
    "\n",
    "# connect to Gmail api\n",
    "service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "# request a list of all the messages\n",
    "result = service.users().messages().list(userId='markus.mueller.ds@gmail.com', y).execute()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(type(result))\n",
    "# frirst 10 result of \n",
    "print(result)\n",
    "print('Next page Token: ',  result['nextPageToken'])\n",
    "print('length: ', len(result['messages']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n",
      "{'messages': [{'id': '17bf36ad1f8ffa14', 'threadId': '17bf36ad1f8ffa14'}, {'id': '17bf2d743a2a4a64', 'threadId': '17bf2d743a2a4a64'}, {'id': '17bf2ba112b2990e', 'threadId': '17bf2ba112b2990e'}, {'id': '17bf2b9a28a4e1cf', 'threadId': '17bf2b9a28a4e1cf'}, {'id': '17bed324d52f36ee', 'threadId': '17bed324d52f36ee'}, {'id': '17beb2ea7a06eb5e', 'threadId': '17beb2ea7a06eb5e'}, {'id': '17be8bcdaa338b35', 'threadId': '17be8bcdaa338b35'}, {'id': '17be5f07abd13e84', 'threadId': '17be5f07abd13e84'}, {'id': '17be5ac70eaf3f1d', 'threadId': '17be5ac70eaf3f1d'}, {'id': '17be368851c51489', 'threadId': '17be368851c51489'}], 'nextPageToken': '17088860671368477437', 'resultSizeEstimate': 11}\n",
      "Next page Token:  17088860671368477437\n",
      "length:  10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can use the ids to get the content of the mails"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "msg_ids = result.get('messages')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "for msg in msg_ids:\n",
    "    print(msg['id'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17bf36ad1f8ffa14\n",
      "17bf2d743a2a4a64\n",
      "17bf2ba112b2990e\n",
      "17bf2b9a28a4e1cf\n",
      "17bed324d52f36ee\n",
      "17beb2ea7a06eb5e\n",
      "17be8bcdaa338b35\n",
      "17be5f07abd13e84\n",
      "17be5ac70eaf3f1d\n",
      "17be368851c51489\n",
      "17be067784b98401\n",
      "17bdf62d721058f5\n",
      "17bdafc60ea3954c\n",
      "17bd51b3bf685077\n",
      "17bd4d4f6a8e5bd7\n",
      "17bd41ed7e3689c9\n",
      "17bd157766588699\n",
      "17bd053251a031b3\n",
      "17bcf5e615f87ecd\n",
      "17bcef38c960fc3b\n",
      "17bca0bc853a32f3\n",
      "17bc92883487c59a\n",
      "17bc6bf1817afc13\n",
      "17bc4a99883075e1\n",
      "17bc47ca455dea43\n",
      "17bc1e3e4c595966\n",
      "17bc17764ecf3708\n",
      "17bbf853dfebce27\n",
      "17bba2e6ee21ba8a\n",
      "17bb7432c5ae73c8\n",
      "17bb6870bb68b786\n",
      "17bb0770b57f10ee\n",
      "17bad571fe5322f8\n",
      "17bab51d2e5a93ff\n",
      "17bab5189de5ff8a\n",
      "17bab32fc35f68e2\n",
      "17ba5c0f63e5d7a6\n",
      "17ba598e729bf32b\n",
      "17ba349206733d4b\n",
      "17ba27ef25924242\n",
      "17ba0a53c538f36a\n",
      "17b9de70877d3d32\n",
      "17b9dd5d1c159068\n",
      "17b9d97f81a5b755\n",
      "17b9d3e197b14d4f\n",
      "17b8c4228e41efa3\n",
      "17b8c0b2c93af7d1\n",
      "17b890045309533f\n",
      "17b880171729f007\n",
      "17b87457879bb57f\n",
      "17b86ab90b36ee35\n",
      "17b829a3d641cc67\n",
      "17b7f2b2004c9889\n",
      "17b7c8c29c863548\n",
      "17b7c536cd4056ff\n",
      "17b79c95aed7e0a6\n",
      "17b797edd35042ff\n",
      "17b78787b99689b8\n",
      "17b77a030916434b\n",
      "17b7213fcd2d544f\n",
      "17b6f44da0eae264\n",
      "17b68bd606a5edde\n",
      "17b68035bfd5b604\n",
      "17b653999e95ddc1\n",
      "17b635db549f04ac\n",
      "17b6338d79acdd5e\n",
      "17b63061251c287e\n",
      "17b5e94d4de8f91f\n",
      "17b5da7ddeb42a02\n",
      "17b5ae5db53b1ef3\n",
      "17b58c81ef295ea4\n",
      "17b587ef70b09641\n",
      "17b55bca4211a8d8\n",
      "17b555d6233cb712\n",
      "17b546b40afe5421\n",
      "17b5390c85ea26ff\n",
      "17b4e02fc2048dcc\n",
      "17b44b0945caed6b\n",
      "17b43f177da6552a\n",
      "17b40feb8e5b8a03\n",
      "17b40a9332e41ec9\n",
      "17b40785fb74b467\n",
      "17b3f2c5e51bffa7\n",
      "17b39dc773a967cd\n",
      "17b39a24678a7cc1\n",
      "17b348465b5af49d\n",
      "17b31a781c138459\n",
      "17b31263df428414\n",
      "17b305e5cbc7d3a9\n",
      "17b29f5b10ba35ef\n",
      "17b207e46e1ea6cb\n",
      "17b1fd83cfc3564e\n",
      "17b1c6a10d71b231\n",
      "17b1bd34d3fd61fe\n",
      "17b1b7156152ed98\n",
      "17b1b1fda0eaded4\n",
      "17b1a7b722cc5175\n",
      "17b15c89d641e3a1\n",
      "17b14e8515e8eae5\n",
      "17b11b5f94c8db32\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# get the data for one mail\n",
    "id_ = '17bed324d52f36ee'\n",
    "txt = service.users().messages().get(userId='me', id=id_).execute()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "message object https://developers.google.com/gmail/api/reference/rest/v1/users.messages#Message:\n",
    "- id: ID of the message\n",
    "- threadId: The ID of the thread the message belongs to. \n",
    "- labelIds: List of IDs of labels applied to this message.\n",
    "- snippet: A short part of the message text.\n",
    "- historyId: The ID of the last history record that modified this message.\n",
    "- internalDate: The internal message creation timestamp (epoch ms)\n",
    "- payload: The parsed email structure in the message parts. (here is the relevant data)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(txt.keys())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['id', 'threadId', 'labelIds', 'snippet', 'payload', 'sizeEstimate', 'historyId', 'internalDate'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "payload = txt['payload']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next cell saves the output in an external file os I can view the content "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "%%capture cap --no-stderr\n",
    "print(payload)\n",
    "with open('test.txt', 'w') as f:\n",
    "    f.write(str(cap))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "payload.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['partId', 'mimeType', 'filename', 'headers', 'body', 'parts'])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(payload['headers'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'name': 'Delivered-To', 'value': 'markus.mueller.ds@gmail.com'}, {'name': 'Received', 'value': 'by 2002:a05:6e04:139c:0:0:0:0 with SMTP id be28csp2135701imb;        Wed, 15 Sep 2021 23:01:42 -0700 (PDT)'}, {'name': 'X-Google-Smtp-Source', 'value': 'ABdhPJyxdMCapZIZ5Y2skh15iwb4zjdvYJLLsM4QvWJA0SJ0kRLSxC91nVNXVGAVCOPtoTAajtQ2'}, {'name': 'X-Received', 'value': 'by 2002:a05:6902:110d:: with SMTP id o13mr4912149ybu.530.1631772101846;        Wed, 15 Sep 2021 23:01:41 -0700 (PDT)'}, {'name': 'ARC-Seal', 'value': 'i=1; a=rsa-sha256; t=1631772101; cv=none;        d=google.com; s=arc-20160816;        b=BQI6VxIPo295XbyRQYuNmSHaNs3p/jg82bkgLzhoIwVuSh9IfN+IQdEmXONgWqVhbJ         A8gtVP29JeiD2nEpEd+j/jQoiHlPowC2/1v+JIn3XcVIWKvu/Yzade4fEnGRjNOiFfMC         QNjRNBYEmW0GTwEMtEFYRZWjKf0btWjR+igqfKaPhv/4CekxQU7IdnC/mewIJeTYDsX1         h0SWLsDTwqVy5XHJUy8xszprwtvJCWqSmfpLK7r+EL1V3PuBzc0EzUunYoyjTOA3I9Tk         9t68bAN+Gc3Vo9IWsYLod+gJj+kpM550alSh+QQgA5CqbtbN9pilltmlE4+tyJkES/WO         qKzg=='}, {'name': 'ARC-Message-Signature', 'value': 'i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;        h=mime-version:list-unsubscribe-post:list-unsubscribe:list-id         :feedback-id:message-id:date:to:reply-to:from:subject:dkim-signature         :dkim-signature;        bh=1165KYCxl689EdDErQVR013tDTXC8+nqSQDKVu22k70=;        b=gLg4T9rILDOP5nfLSlHfV1sfozgETFUICSpUz5h8xBem/mpuOSffzgssD4jRrQ1lbW         k1qUjhbw8rd5cqltQsvZuCTuxx2LQ2vJ3b2EmiB/eH2IWDy5Hhh/uIQomGynzEHokeIK         XRmEqiWMVKAnyGd9eiKGFXQVmrPTxVZpYsY0XaKw/pOKz1nDIYwvle8kzczc3KttpP50         ofAIRnkThuLXMDNvx6IY3lLj2/BTtGiRIKX8HQb+Ipkz5ulx4+pyefrITZLgrC3Gh4EK         DV3I73GUm+HhhzL1SfgcqvbIx/nU7Vl8UmGTEdSx1ckVCw2SkpjlWqWvr63cQSS779xD         W66Q=='}, {'name': 'ARC-Authentication-Results', 'value': 'i=1; mx.google.com;       dkim=pass header.i=@mcc.mcsv.net header.s=k1 header.b=Q4c+SPzI;       dkim=pass header.i=@kdnuggets.com header.s=k2 header.b=\"QYE7xr+/\";       spf=pass (google.com: domain of bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net designates 198.2.181.224 as permitted sender) smtp.mailfrom=bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net;       dmarc=pass (p=QUARANTINE sp=QUARANTINE dis=NONE) header.from=kdnuggets.com'}, {'name': 'Return-Path', 'value': '<bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net>'}, {'name': 'Received', 'value': 'from mail224.suw18.rsgsv.net (mail224.suw18.rsgsv.net. [198.2.181.224])        by mx.google.com with ESMTPS id k190si2449219ybk.101.2021.09.15.23.01.41        for <Markus.mueller.ds@gmail.com>        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);        Wed, 15 Sep 2021 23:01:41 -0700 (PDT)'}, {'name': 'Received-SPF', 'value': 'pass (google.com: domain of bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net designates 198.2.181.224 as permitted sender) client-ip=198.2.181.224;'}, {'name': 'Authentication-Results', 'value': 'mx.google.com;       dkim=pass header.i=@mcc.mcsv.net header.s=k1 header.b=Q4c+SPzI;       dkim=pass header.i=@kdnuggets.com header.s=k2 header.b=\"QYE7xr+/\";       spf=pass (google.com: domain of bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net designates 198.2.181.224 as permitted sender) smtp.mailfrom=bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net;       dmarc=pass (p=QUARANTINE sp=QUARANTINE dis=NONE) header.from=kdnuggets.com'}, {'name': 'DKIM-Signature', 'value': 'v=1; a=rsa-sha256; c=relaxed/relaxed; d=mcc.mcsv.net; s=k1; t=1631771998; bh=1165KYCxl689EdDErQVR013tDTXC8+nqSQDKVu22k70=; h=Subject:From:Reply-To:To:Date:Message-ID:Feedback-ID:List-ID:\\t List-Unsubscribe:List-Unsubscribe-Post:Content-Type:MIME-Version; b=Q4c+SPzILD9VS+ynzRc332wsTNNqe8Tdf2W6WuYlU6WTc2M6QoCB+L4alfMZAi58t\\t haAV3wX884IZTCBsgDTkvv+/In1wPIWYfCGOn/q58/+/vhQviNLEHlvJOYLavjkazx\\t HHFeIrcowHnZHSKzxZ6WiuO1B8/scfw9P+tFAC61GJPH2e7rV4iK9y4o2N0Q2bRx0O\\t FwAubnXp6jtSudnsai7DYX4NCtuoSd0BG1uNYbnIJtuEZDI5RI9r2lkQmqfu00FcMl\\t 7zOmFygmkS4GSDmNIp6txk5m+0TkzJ3x0XhOgClOvonh5Smp1CJQNqd+Lc5+1az73m\\t QEj+WPiKofukg=='}, {'name': 'DKIM-Signature', 'value': 'v=1; a=rsa-sha256; c=relaxed/relaxed; d=kdnuggets.com; s=k2; t=1631771998; i=editor1@kdnuggets.com; bh=1165KYCxl689EdDErQVR013tDTXC8+nqSQDKVu22k70=; h=Subject:From:Reply-To:To:Date:Message-ID:List-ID:List-Unsubscribe:\\t List-Unsubscribe-Post:Content-Type:MIME-Version; b=QYE7xr+/SiZjPsZYNw6Iv+gdvoN6vLH02a9nGD7cySH2qa2c/hQitkaLJeePHMR3y\\t G+g0UZNTqx3yUK8cRqThOaduU0WKTxwP34ATprNZ+5Nu7WMUEYqzCdr1j/XKzQo6tO\\t U7owV3d8LXUZAuAvOH+xGOgeLhE9r4msHtoi0LSjrDzAD5ZM0fDpwg8kFI7HizIYoh\\t MCJxp2t6eRfBRu04xY7O1mg2tbO4NLU786yDDaIvxQd8Qbm3P692Z9o7XaEydf8Jf3\\t 3LftpxEewy0c45FHQu+e6EuoRRnjFii7IdDPoaOSLwXOEcrDCp3zgmzEQx8IFAdENn\\t iI899wSL5+BwA=='}, {'name': 'Received', 'value': 'from localhost (localhost [127.0.0.1]) by mail224.suw18.rsgsv.net (Mailchimp) with ESMTP id 4H95yk6zFDzp5KRs for <Markus.mueller.ds@gmail.com>; Thu, 16 Sep 2021 05:59:58 +0000 (GMT)'}, {'name': 'Subject', 'value': 'Intro to Data Labeling for Machine Learning'}, {'name': 'From', 'value': 'KDnuggets <editor1@kdnuggets.com>'}, {'name': 'Reply-To', 'value': 'KDnuggets <editor1@kdnuggets.com>'}, {'name': 'To', 'value': '<Markus.mueller.ds@gmail.com>'}, {'name': 'Date', 'value': 'Thu, 16 Sep 2021 05:59:57 +0000'}, {'name': 'Message-ID', 'value': '<4f2891ebb155b23f120ece0bd.bfdfd73c17.20210916055947.79acd481a2.01c3fbcb@mail224.suw18.rsgsv.net>'}, {'name': 'X-Mailer', 'value': 'MailChimp Mailer - **CID79acd481a2bfdfd73c17**'}, {'name': 'X-Campaign', 'value': 'mailchimp4f2891ebb155b23f120ece0bd.79acd481a2'}, {'name': 'X-campaignid', 'value': 'mailchimp4f2891ebb155b23f120ece0bd.79acd481a2'}, {'name': 'X-Report-Abuse', 'value': 'Please report abuse for this campaign here: https://mailchimp.com/contact/abuse/?u=4f2891ebb155b23f120ece0bd&id=79acd481a2&e=bfdfd73c17'}, {'name': 'X-MC-User', 'value': '4f2891ebb155b23f120ece0bd'}, {'name': 'Feedback-ID', 'value': '50554633:50554633.321818:us12:mc'}, {'name': 'List-ID', 'value': '4f2891ebb155b23f120ece0bdmc list <4f2891ebb155b23f120ece0bd.169725.list-id.mcsv.net>'}, {'name': 'X-Accounttype', 'value': 'pr'}, {'name': 'List-Unsubscribe', 'value': '<https://kdnuggets.us12.list-manage.com/unsubscribe?u=4f2891ebb155b23f120ece0bd&id=b2fa8716d2&e=bfdfd73c17&c=79acd481a2>, <mailto:unsubscribe-mc.us12_4f2891ebb155b23f120ece0bd.79acd481a2-bfdfd73c17@mailin.mcsv.net?subject=unsubscribe>'}, {'name': 'List-Unsubscribe-Post', 'value': 'List-Unsubscribe=One-Click'}, {'name': 'Content-Type', 'value': 'multipart/alternative; boundary=\"_----------=_MCPart_2127704198\"'}, {'name': 'MIME-Version', 'value': '1.0'}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "for item in payload['headers']:\n",
    "    print(item)\n",
    "    print('-'*30)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'name': 'Delivered-To', 'value': 'markus.mueller.ds@gmail.com'}\n",
      "------------------------------\n",
      "{'name': 'Received', 'value': 'by 2002:a05:6e04:139c:0:0:0:0 with SMTP id be28csp2135701imb;        Wed, 15 Sep 2021 23:01:42 -0700 (PDT)'}\n",
      "------------------------------\n",
      "{'name': 'X-Google-Smtp-Source', 'value': 'ABdhPJyxdMCapZIZ5Y2skh15iwb4zjdvYJLLsM4QvWJA0SJ0kRLSxC91nVNXVGAVCOPtoTAajtQ2'}\n",
      "------------------------------\n",
      "{'name': 'X-Received', 'value': 'by 2002:a05:6902:110d:: with SMTP id o13mr4912149ybu.530.1631772101846;        Wed, 15 Sep 2021 23:01:41 -0700 (PDT)'}\n",
      "------------------------------\n",
      "{'name': 'ARC-Seal', 'value': 'i=1; a=rsa-sha256; t=1631772101; cv=none;        d=google.com; s=arc-20160816;        b=BQI6VxIPo295XbyRQYuNmSHaNs3p/jg82bkgLzhoIwVuSh9IfN+IQdEmXONgWqVhbJ         A8gtVP29JeiD2nEpEd+j/jQoiHlPowC2/1v+JIn3XcVIWKvu/Yzade4fEnGRjNOiFfMC         QNjRNBYEmW0GTwEMtEFYRZWjKf0btWjR+igqfKaPhv/4CekxQU7IdnC/mewIJeTYDsX1         h0SWLsDTwqVy5XHJUy8xszprwtvJCWqSmfpLK7r+EL1V3PuBzc0EzUunYoyjTOA3I9Tk         9t68bAN+Gc3Vo9IWsYLod+gJj+kpM550alSh+QQgA5CqbtbN9pilltmlE4+tyJkES/WO         qKzg=='}\n",
      "------------------------------\n",
      "{'name': 'ARC-Message-Signature', 'value': 'i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;        h=mime-version:list-unsubscribe-post:list-unsubscribe:list-id         :feedback-id:message-id:date:to:reply-to:from:subject:dkim-signature         :dkim-signature;        bh=1165KYCxl689EdDErQVR013tDTXC8+nqSQDKVu22k70=;        b=gLg4T9rILDOP5nfLSlHfV1sfozgETFUICSpUz5h8xBem/mpuOSffzgssD4jRrQ1lbW         k1qUjhbw8rd5cqltQsvZuCTuxx2LQ2vJ3b2EmiB/eH2IWDy5Hhh/uIQomGynzEHokeIK         XRmEqiWMVKAnyGd9eiKGFXQVmrPTxVZpYsY0XaKw/pOKz1nDIYwvle8kzczc3KttpP50         ofAIRnkThuLXMDNvx6IY3lLj2/BTtGiRIKX8HQb+Ipkz5ulx4+pyefrITZLgrC3Gh4EK         DV3I73GUm+HhhzL1SfgcqvbIx/nU7Vl8UmGTEdSx1ckVCw2SkpjlWqWvr63cQSS779xD         W66Q=='}\n",
      "------------------------------\n",
      "{'name': 'ARC-Authentication-Results', 'value': 'i=1; mx.google.com;       dkim=pass header.i=@mcc.mcsv.net header.s=k1 header.b=Q4c+SPzI;       dkim=pass header.i=@kdnuggets.com header.s=k2 header.b=\"QYE7xr+/\";       spf=pass (google.com: domain of bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net designates 198.2.181.224 as permitted sender) smtp.mailfrom=bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net;       dmarc=pass (p=QUARANTINE sp=QUARANTINE dis=NONE) header.from=kdnuggets.com'}\n",
      "------------------------------\n",
      "{'name': 'Return-Path', 'value': '<bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net>'}\n",
      "------------------------------\n",
      "{'name': 'Received', 'value': 'from mail224.suw18.rsgsv.net (mail224.suw18.rsgsv.net. [198.2.181.224])        by mx.google.com with ESMTPS id k190si2449219ybk.101.2021.09.15.23.01.41        for <Markus.mueller.ds@gmail.com>        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);        Wed, 15 Sep 2021 23:01:41 -0700 (PDT)'}\n",
      "------------------------------\n",
      "{'name': 'Received-SPF', 'value': 'pass (google.com: domain of bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net designates 198.2.181.224 as permitted sender) client-ip=198.2.181.224;'}\n",
      "------------------------------\n",
      "{'name': 'Authentication-Results', 'value': 'mx.google.com;       dkim=pass header.i=@mcc.mcsv.net header.s=k1 header.b=Q4c+SPzI;       dkim=pass header.i=@kdnuggets.com header.s=k2 header.b=\"QYE7xr+/\";       spf=pass (google.com: domain of bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net designates 198.2.181.224 as permitted sender) smtp.mailfrom=bounce-mc.us12_50554633.321818-bfdfd73c17@mail224.suw18.rsgsv.net;       dmarc=pass (p=QUARANTINE sp=QUARANTINE dis=NONE) header.from=kdnuggets.com'}\n",
      "------------------------------\n",
      "{'name': 'DKIM-Signature', 'value': 'v=1; a=rsa-sha256; c=relaxed/relaxed; d=mcc.mcsv.net; s=k1; t=1631771998; bh=1165KYCxl689EdDErQVR013tDTXC8+nqSQDKVu22k70=; h=Subject:From:Reply-To:To:Date:Message-ID:Feedback-ID:List-ID:\\t List-Unsubscribe:List-Unsubscribe-Post:Content-Type:MIME-Version; b=Q4c+SPzILD9VS+ynzRc332wsTNNqe8Tdf2W6WuYlU6WTc2M6QoCB+L4alfMZAi58t\\t haAV3wX884IZTCBsgDTkvv+/In1wPIWYfCGOn/q58/+/vhQviNLEHlvJOYLavjkazx\\t HHFeIrcowHnZHSKzxZ6WiuO1B8/scfw9P+tFAC61GJPH2e7rV4iK9y4o2N0Q2bRx0O\\t FwAubnXp6jtSudnsai7DYX4NCtuoSd0BG1uNYbnIJtuEZDI5RI9r2lkQmqfu00FcMl\\t 7zOmFygmkS4GSDmNIp6txk5m+0TkzJ3x0XhOgClOvonh5Smp1CJQNqd+Lc5+1az73m\\t QEj+WPiKofukg=='}\n",
      "------------------------------\n",
      "{'name': 'DKIM-Signature', 'value': 'v=1; a=rsa-sha256; c=relaxed/relaxed; d=kdnuggets.com; s=k2; t=1631771998; i=editor1@kdnuggets.com; bh=1165KYCxl689EdDErQVR013tDTXC8+nqSQDKVu22k70=; h=Subject:From:Reply-To:To:Date:Message-ID:List-ID:List-Unsubscribe:\\t List-Unsubscribe-Post:Content-Type:MIME-Version; b=QYE7xr+/SiZjPsZYNw6Iv+gdvoN6vLH02a9nGD7cySH2qa2c/hQitkaLJeePHMR3y\\t G+g0UZNTqx3yUK8cRqThOaduU0WKTxwP34ATprNZ+5Nu7WMUEYqzCdr1j/XKzQo6tO\\t U7owV3d8LXUZAuAvOH+xGOgeLhE9r4msHtoi0LSjrDzAD5ZM0fDpwg8kFI7HizIYoh\\t MCJxp2t6eRfBRu04xY7O1mg2tbO4NLU786yDDaIvxQd8Qbm3P692Z9o7XaEydf8Jf3\\t 3LftpxEewy0c45FHQu+e6EuoRRnjFii7IdDPoaOSLwXOEcrDCp3zgmzEQx8IFAdENn\\t iI899wSL5+BwA=='}\n",
      "------------------------------\n",
      "{'name': 'Received', 'value': 'from localhost (localhost [127.0.0.1]) by mail224.suw18.rsgsv.net (Mailchimp) with ESMTP id 4H95yk6zFDzp5KRs for <Markus.mueller.ds@gmail.com>; Thu, 16 Sep 2021 05:59:58 +0000 (GMT)'}\n",
      "------------------------------\n",
      "{'name': 'Subject', 'value': 'Intro to Data Labeling for Machine Learning'}\n",
      "------------------------------\n",
      "{'name': 'From', 'value': 'KDnuggets <editor1@kdnuggets.com>'}\n",
      "------------------------------\n",
      "{'name': 'Reply-To', 'value': 'KDnuggets <editor1@kdnuggets.com>'}\n",
      "------------------------------\n",
      "{'name': 'To', 'value': '<Markus.mueller.ds@gmail.com>'}\n",
      "------------------------------\n",
      "{'name': 'Date', 'value': 'Thu, 16 Sep 2021 05:59:57 +0000'}\n",
      "------------------------------\n",
      "{'name': 'Message-ID', 'value': '<4f2891ebb155b23f120ece0bd.bfdfd73c17.20210916055947.79acd481a2.01c3fbcb@mail224.suw18.rsgsv.net>'}\n",
      "------------------------------\n",
      "{'name': 'X-Mailer', 'value': 'MailChimp Mailer - **CID79acd481a2bfdfd73c17**'}\n",
      "------------------------------\n",
      "{'name': 'X-Campaign', 'value': 'mailchimp4f2891ebb155b23f120ece0bd.79acd481a2'}\n",
      "------------------------------\n",
      "{'name': 'X-campaignid', 'value': 'mailchimp4f2891ebb155b23f120ece0bd.79acd481a2'}\n",
      "------------------------------\n",
      "{'name': 'X-Report-Abuse', 'value': 'Please report abuse for this campaign here: https://mailchimp.com/contact/abuse/?u=4f2891ebb155b23f120ece0bd&id=79acd481a2&e=bfdfd73c17'}\n",
      "------------------------------\n",
      "{'name': 'X-MC-User', 'value': '4f2891ebb155b23f120ece0bd'}\n",
      "------------------------------\n",
      "{'name': 'Feedback-ID', 'value': '50554633:50554633.321818:us12:mc'}\n",
      "------------------------------\n",
      "{'name': 'List-ID', 'value': '4f2891ebb155b23f120ece0bdmc list <4f2891ebb155b23f120ece0bd.169725.list-id.mcsv.net>'}\n",
      "------------------------------\n",
      "{'name': 'X-Accounttype', 'value': 'pr'}\n",
      "------------------------------\n",
      "{'name': 'List-Unsubscribe', 'value': '<https://kdnuggets.us12.list-manage.com/unsubscribe?u=4f2891ebb155b23f120ece0bd&id=b2fa8716d2&e=bfdfd73c17&c=79acd481a2>, <mailto:unsubscribe-mc.us12_4f2891ebb155b23f120ece0bd.79acd481a2-bfdfd73c17@mailin.mcsv.net?subject=unsubscribe>'}\n",
      "------------------------------\n",
      "{'name': 'List-Unsubscribe-Post', 'value': 'List-Unsubscribe=One-Click'}\n",
      "------------------------------\n",
      "{'name': 'Content-Type', 'value': 'multipart/alternative; boundary=\"_----------=_MCPart_2127704198\"'}\n",
      "------------------------------\n",
      "{'name': 'MIME-Version', 'value': '1.0'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print('recived: ',  payload['headers'][1].get('value'))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "recived:  by 2002:a05:6e04:139c:0:0:0:0 with SMTP id be28csp2135701imb;        Wed, 15 Sep 2021 23:01:42 -0700 (PDT)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# payload['headers'] is a list of dicts\n",
    "for d in payload['headers']:\n",
    "    if d['name'] == 'Date':\n",
    "        date = d['value']\n",
    "    if d['name'] == 'From':\n",
    "        sender = d['value']\n",
    "    if d['name'] == 'Subject':\n",
    "        subject = d['value']\n",
    "    \n",
    "print('date: ', date)\n",
    "print('from: ', sender)\n",
    "print('subject: ', subject)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "date:  Thu, 16 Sep 2021 05:59:57 +0000\n",
      "from:  KDnuggets <editor1@kdnuggets.com>\n",
      "subject:  Intro to Data Labeling for Machine Learning\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "parts = payload['parts'][0]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# get data of the messages\n",
    "data = parts['body']['data']\n",
    "data = data.replace(\"-\",\"+\").replace(\"_\",\"/\")\n",
    "\n",
    "# data is encoded\n",
    "decoded_data = base64.b64decode(data)\n",
    "\n",
    "# parse obtained lxml with BeautifulSoup \n",
    "soup = BeautifulSoup(decoded_data , \"lxml\")\n",
    "body = soup.body()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "type(body)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "body"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<p>https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=6226337bb4&amp;e=bfdfd73c17\n",
       " \n",
       " Once upon a time, there was a company that dreamed of changing the world with machine learning. The brilliant data team labeled away day and night but it was never enough to data. The models just weren't performing correctly. They longed for the blissful days of data analysis and innovation.\n",
       " \n",
       " Sound familiar? You might be ready to outsource your data labeling.\n",
       " \n",
       " Your dream of changing the world can come true with a little help from some strangers across the globe. If this is your first time working with outsourced teams, it might seem daunting. This free guide (https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=b2ae45d703&amp;e=bfdfd73c17) will help you confidently start that part of your innovation journey.\n",
       " • Tips for successfully outsourcing AI and ML projects and ensuring quality datasets\n",
       " • Common data labeling terminology, technology, and best practices\n",
       " • Questions you should ask a prospective data labeling service provider\n",
       " Start Your Journey (https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=9d2e1e3bd2&amp;e=bfdfd73c17)\n",
       " \n",
       " P.S. Have questions? Contact CloudFactory anytime here (https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=589ad1925d&amp;e=bfdfd73c17) . You might also enjoy this free guide (https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=df9adcfbb5&amp;e=bfdfd73c17) to accelerating the AI lifecycle with humans in the loop.\n",
       " \n",
       " This email was sent to Markus.mueller.ds@gmail.com (mailto:Markus.mueller.ds@gmail.com)\n",
       " why did I get this? (https://kdnuggets.us12.list-manage.com/about?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=79acd481a2)     unsubscribe from this list (https://kdnuggets.us12.list-manage.com/unsubscribe?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=79acd481a2)     update subscription preferences (https://kdnuggets.us12.list-manage.com/profile?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=79acd481a2)\n",
       " KDnuggets . Reservoir Rd . Brookline, MA 02467 . USA</p>]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "body_str = str(body)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "body_str"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"[<p>https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=6226337bb4&amp;e=bfdfd73c17\\r\\n\\r\\nOnce upon a time, there was a company that dreamed of changing the world with machine learning. The brilliant data team labeled away day and night but it was never enough to data. The models just weren't performing correctly. They longed for the blissful days of data analysis and innovation.\\r\\n\\r\\nSound familiar? You might be ready to outsource your data labeling.\\r\\n\\r\\nYour dream of changing the world can come true with a little help from some strangers across the globe. If this is your first time working with outsourced teams, it might seem daunting. This free guide (https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=b2ae45d703&amp;e=bfdfd73c17) will help you confidently start that part of your innovation journey.\\r\\n• Tips for successfully outsourcing AI and ML projects and ensuring quality datasets\\r\\n• Common data labeling terminology, technology, and best practices\\r\\n• Questions you should ask a prospective data labeling service provider\\r\\nStart Your Journey (https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=9d2e1e3bd2&amp;e=bfdfd73c17)\\r\\n\\r\\nP.S. Have questions? Contact CloudFactory anytime here (https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=589ad1925d&amp;e=bfdfd73c17) . You might also enjoy this free guide (https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&amp;id=df9adcfbb5&amp;e=bfdfd73c17) to accelerating the AI lifecycle with humans in the loop.\\r\\n\\r\\nThis email was sent to Markus.mueller.ds@gmail.com (mailto:Markus.mueller.ds@gmail.com)\\r\\nwhy did I get this? (https://kdnuggets.us12.list-manage.com/about?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=79acd481a2)     unsubscribe from this list (https://kdnuggets.us12.list-manage.com/unsubscribe?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=79acd481a2)     update subscription preferences (https://kdnuggets.us12.list-manage.com/profile?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=79acd481a2)\\r\\nKDnuggets . Reservoir Rd . Brookline, MA 02467 . USA</p>]\""
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test saving and loading query result as pkl-file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# save raw data in a pickle file\n",
    "filename = 'test_pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(txt,outfile)\n",
    "outfile.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# reload pickled data\n",
    "infile = open(filename,'rb')\n",
    "new_txt = pickle.load(infile)\n",
    "infile.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "new_txt['payload']['parts'][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'partId': '0',\n",
       " 'mimeType': 'text/plain',\n",
       " 'filename': '',\n",
       " 'headers': [{'name': 'Content-Type',\n",
       "   'value': 'text/plain; charset=\"utf-8\"; format=\"fixed\"'},\n",
       "  {'name': 'Content-Transfer-Encoding', 'value': 'quoted-printable'}],\n",
       " 'body': {'size': 2173,\n",
       "  'data': 'aHR0cHM6Ly9rZG51Z2dldHMudXMxMi5saXN0LW1hbmFnZS5jb20vdHJhY2svY2xpY2s_dT00ZjI4OTFlYmIxNTViMjNmMTIwZWNlMGJkJmlkPTYyMjYzMzdiYjQmZT1iZmRmZDczYzE3DQoNCk9uY2UgdXBvbiBhIHRpbWUsIHRoZXJlIHdhcyBhIGNvbXBhbnkgdGhhdCBkcmVhbWVkIG9mIGNoYW5naW5nIHRoZSB3b3JsZCB3aXRoIG1hY2hpbmUgbGVhcm5pbmcuIFRoZSBicmlsbGlhbnQgZGF0YSB0ZWFtIGxhYmVsZWQgYXdheSBkYXkgYW5kIG5pZ2h0IGJ1dCBpdCB3YXMgbmV2ZXIgZW5vdWdoIHRvIGRhdGEuIFRoZSBtb2RlbHMganVzdCB3ZXJlbid0IHBlcmZvcm1pbmcgY29ycmVjdGx5LiBUaGV5IGxvbmdlZCBmb3IgdGhlIGJsaXNzZnVsIGRheXMgb2YgZGF0YSBhbmFseXNpcyBhbmQgaW5ub3ZhdGlvbi4NCg0KU291bmQgZmFtaWxpYXI_IFlvdSBtaWdodCBiZSByZWFkeSB0byBvdXRzb3VyY2UgeW91ciBkYXRhIGxhYmVsaW5nLg0KDQpZb3VyIGRyZWFtIG9mIGNoYW5naW5nIHRoZSB3b3JsZCBjYW4gY29tZSB0cnVlIHdpdGggYSBsaXR0bGUgaGVscCBmcm9tIHNvbWUgc3RyYW5nZXJzIGFjcm9zcyB0aGUgZ2xvYmUuIElmIHRoaXMgaXMgeW91ciBmaXJzdCB0aW1lIHdvcmtpbmcgd2l0aCBvdXRzb3VyY2VkIHRlYW1zLCBpdCBtaWdodCBzZWVtIGRhdW50aW5nLiBUaGlzIGZyZWUgZ3VpZGUgKGh0dHBzOi8va2RudWdnZXRzLnVzMTIubGlzdC1tYW5hZ2UuY29tL3RyYWNrL2NsaWNrP3U9NGYyODkxZWJiMTU1YjIzZjEyMGVjZTBiZCZpZD1iMmFlNDVkNzAzJmU9YmZkZmQ3M2MxNykgd2lsbCBoZWxwIHlvdSBjb25maWRlbnRseSBzdGFydCB0aGF0IHBhcnQgb2YgeW91ciBpbm5vdmF0aW9uIGpvdXJuZXkuDQrigKIgVGlwcyBmb3Igc3VjY2Vzc2Z1bGx5IG91dHNvdXJjaW5nIEFJIGFuZCBNTCBwcm9qZWN0cyBhbmQgZW5zdXJpbmcgcXVhbGl0eSBkYXRhc2V0cw0K4oCiIENvbW1vbiBkYXRhIGxhYmVsaW5nIHRlcm1pbm9sb2d5LCB0ZWNobm9sb2d5LCBhbmQgYmVzdCBwcmFjdGljZXMNCuKAoiBRdWVzdGlvbnMgeW91IHNob3VsZCBhc2sgYSBwcm9zcGVjdGl2ZSBkYXRhIGxhYmVsaW5nIHNlcnZpY2UgcHJvdmlkZXINClN0YXJ0IFlvdXIgSm91cm5leSAoaHR0cHM6Ly9rZG51Z2dldHMudXMxMi5saXN0LW1hbmFnZS5jb20vdHJhY2svY2xpY2s_dT00ZjI4OTFlYmIxNTViMjNmMTIwZWNlMGJkJmlkPTlkMmUxZTNiZDImZT1iZmRmZDczYzE3KQ0KDQpQLlMuIEhhdmUgcXVlc3Rpb25zPyBDb250YWN0IENsb3VkRmFjdG9yeSBhbnl0aW1lIGhlcmUgKGh0dHBzOi8va2RudWdnZXRzLnVzMTIubGlzdC1tYW5hZ2UuY29tL3RyYWNrL2NsaWNrP3U9NGYyODkxZWJiMTU1YjIzZjEyMGVjZTBiZCZpZD01ODlhZDE5MjVkJmU9YmZkZmQ3M2MxNykgLiBZb3UgbWlnaHQgYWxzbyBlbmpveSB0aGlzIGZyZWUgZ3VpZGUgKGh0dHBzOi8va2RudWdnZXRzLnVzMTIubGlzdC1tYW5hZ2UuY29tL3RyYWNrL2NsaWNrP3U9NGYyODkxZWJiMTU1YjIzZjEyMGVjZTBiZCZpZD1kZjlhZGNmYmI1JmU9YmZkZmQ3M2MxNykgdG8gYWNjZWxlcmF0aW5nIHRoZSBBSSBsaWZlY3ljbGUgd2l0aCBodW1hbnMgaW4gdGhlIGxvb3AuDQoNClRoaXMgZW1haWwgd2FzIHNlbnQgdG8gTWFya3VzLm11ZWxsZXIuZHNAZ21haWwuY29tIChtYWlsdG86TWFya3VzLm11ZWxsZXIuZHNAZ21haWwuY29tKQ0Kd2h5IGRpZCBJIGdldCB0aGlzPyAoaHR0cHM6Ly9rZG51Z2dldHMudXMxMi5saXN0LW1hbmFnZS5jb20vYWJvdXQ_dT00ZjI4OTFlYmIxNTViMjNmMTIwZWNlMGJkJmlkPWIyZmE4NzE2ZDImZT1iZmRmZDczYzE3JmM9NzlhY2Q0ODFhMikgICAgIHVuc3Vic2NyaWJlIGZyb20gdGhpcyBsaXN0IChodHRwczovL2tkbnVnZ2V0cy51czEyLmxpc3QtbWFuYWdlLmNvbS91bnN1YnNjcmliZT91PTRmMjg5MWViYjE1NWIyM2YxMjBlY2UwYmQmaWQ9YjJmYTg3MTZkMiZlPWJmZGZkNzNjMTcmYz03OWFjZDQ4MWEyKSAgICAgdXBkYXRlIHN1YnNjcmlwdGlvbiBwcmVmZXJlbmNlcyAoaHR0cHM6Ly9rZG51Z2dldHMudXMxMi5saXN0LW1hbmFnZS5jb20vcHJvZmlsZT91PTRmMjg5MWViYjE1NWIyM2YxMjBlY2UwYmQmaWQ9YjJmYTg3MTZkMiZlPWJmZGZkNzNjMTcmYz03OWFjZDQ4MWEyKQ0KS0RudWdnZXRzIC4gUmVzZXJ2b2lyIFJkIC4gQnJvb2tsaW5lLCBNQSAwMjQ2NyAuIFVTQQ=='}}"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check id query"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "filename = '/Users/markusmuller/python/projects/gmail-newsletter-db/scripts/id_query_17.09.2021'\n",
    "infile = open(filename,'rb')\n",
    "id_list = pickle.load(infile)\n",
    "infile.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "id_list "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'messages': [{'id': '17bf36ad1f8ffa14', 'threadId': '17bf36ad1f8ffa14'},\n",
       "  {'id': '17bf2d743a2a4a64', 'threadId': '17bf2d743a2a4a64'},\n",
       "  {'id': '17bf2ba112b2990e', 'threadId': '17bf2ba112b2990e'},\n",
       "  {'id': '17bf2b9a28a4e1cf', 'threadId': '17bf2b9a28a4e1cf'},\n",
       "  {'id': '17bed324d52f36ee', 'threadId': '17bed324d52f36ee'},\n",
       "  {'id': '17beb2ea7a06eb5e', 'threadId': '17beb2ea7a06eb5e'},\n",
       "  {'id': '17be8bcdaa338b35', 'threadId': '17be8bcdaa338b35'},\n",
       "  {'id': '17be5f07abd13e84', 'threadId': '17be5f07abd13e84'},\n",
       "  {'id': '17be5ac70eaf3f1d', 'threadId': '17be5ac70eaf3f1d'},\n",
       "  {'id': '17be368851c51489', 'threadId': '17be368851c51489'},\n",
       "  {'id': '17be067784b98401', 'threadId': '17be067784b98401'},\n",
       "  {'id': '17bdf62d721058f5', 'threadId': '17bdf62d721058f5'},\n",
       "  {'id': '17bdafc60ea3954c', 'threadId': '17bdafc60ea3954c'},\n",
       "  {'id': '17bd51b3bf685077', 'threadId': '17bd51b3bf685077'},\n",
       "  {'id': '17bd4d4f6a8e5bd7', 'threadId': '17bd4d4f6a8e5bd7'},\n",
       "  {'id': '17bd41ed7e3689c9', 'threadId': '17bd41ed7e3689c9'},\n",
       "  {'id': '17bd157766588699', 'threadId': '17bd157766588699'},\n",
       "  {'id': '17bd053251a031b3', 'threadId': '17bd053251a031b3'},\n",
       "  {'id': '17bcf5e615f87ecd', 'threadId': '17bcf5e615f87ecd'},\n",
       "  {'id': '17bcef38c960fc3b', 'threadId': '17bcef38c960fc3b'},\n",
       "  {'id': '17bca0bc853a32f3', 'threadId': '17bca0bc853a32f3'},\n",
       "  {'id': '17bc92883487c59a', 'threadId': '17bc92883487c59a'},\n",
       "  {'id': '17bc6bf1817afc13', 'threadId': '17bc6bf1817afc13'},\n",
       "  {'id': '17bc4a99883075e1', 'threadId': '17bc4a99883075e1'},\n",
       "  {'id': '17bc47ca455dea43', 'threadId': '17bc47ca455dea43'},\n",
       "  {'id': '17bc1e3e4c595966', 'threadId': '17bc1e3e4c595966'},\n",
       "  {'id': '17bc17764ecf3708', 'threadId': '17bc17764ecf3708'},\n",
       "  {'id': '17bbf853dfebce27', 'threadId': '17bbf853dfebce27'},\n",
       "  {'id': '17bba2e6ee21ba8a', 'threadId': '17bba2e6ee21ba8a'},\n",
       "  {'id': '17bb7432c5ae73c8', 'threadId': '17bb7432c5ae73c8'},\n",
       "  {'id': '17bb6870bb68b786', 'threadId': '17bb6870bb68b786'},\n",
       "  {'id': '17bb0770b57f10ee', 'threadId': '17bb0770b57f10ee'},\n",
       "  {'id': '17bad571fe5322f8', 'threadId': '17bad571fe5322f8'},\n",
       "  {'id': '17bab51d2e5a93ff', 'threadId': '17bab51d2e5a93ff'},\n",
       "  {'id': '17bab5189de5ff8a', 'threadId': '17bab5189de5ff8a'},\n",
       "  {'id': '17bab32fc35f68e2', 'threadId': '17bab32fc35f68e2'},\n",
       "  {'id': '17ba5c0f63e5d7a6', 'threadId': '17ba5c0f63e5d7a6'},\n",
       "  {'id': '17ba598e729bf32b', 'threadId': '17ba598e729bf32b'},\n",
       "  {'id': '17ba349206733d4b', 'threadId': '17ba349206733d4b'},\n",
       "  {'id': '17ba27ef25924242', 'threadId': '17ba27ef25924242'},\n",
       "  {'id': '17ba0a53c538f36a', 'threadId': '17ba0a53c538f36a'},\n",
       "  {'id': '17b9de70877d3d32', 'threadId': '17b9de70877d3d32'},\n",
       "  {'id': '17b9dd5d1c159068', 'threadId': '17b9dd5d1c159068'},\n",
       "  {'id': '17b9d97f81a5b755', 'threadId': '17b9d97f81a5b755'},\n",
       "  {'id': '17b9d3e197b14d4f', 'threadId': '17b9d3e197b14d4f'},\n",
       "  {'id': '17b8c4228e41efa3', 'threadId': '17b8c4228e41efa3'},\n",
       "  {'id': '17b8c0b2c93af7d1', 'threadId': '17b8c0b2c93af7d1'},\n",
       "  {'id': '17b890045309533f', 'threadId': '17b890045309533f'},\n",
       "  {'id': '17b880171729f007', 'threadId': '17b880171729f007'},\n",
       "  {'id': '17b87457879bb57f', 'threadId': '17b87457879bb57f'},\n",
       "  {'id': '17b86ab90b36ee35', 'threadId': '17b86ab90b36ee35'},\n",
       "  {'id': '17b829a3d641cc67', 'threadId': '17b829a3d641cc67'},\n",
       "  {'id': '17b7f2b2004c9889', 'threadId': '17b7f2b2004c9889'},\n",
       "  {'id': '17b7c8c29c863548', 'threadId': '17b7c8c29c863548'},\n",
       "  {'id': '17b7c536cd4056ff', 'threadId': '17b7c536cd4056ff'},\n",
       "  {'id': '17b79c95aed7e0a6', 'threadId': '17b79c95aed7e0a6'},\n",
       "  {'id': '17b797edd35042ff', 'threadId': '17b797edd35042ff'},\n",
       "  {'id': '17b78787b99689b8', 'threadId': '17b78787b99689b8'},\n",
       "  {'id': '17b77a030916434b', 'threadId': '17b77a030916434b'},\n",
       "  {'id': '17b7213fcd2d544f', 'threadId': '17b7213fcd2d544f'},\n",
       "  {'id': '17b6f44da0eae264', 'threadId': '17b6f44da0eae264'},\n",
       "  {'id': '17b68bd606a5edde', 'threadId': '17b68bd606a5edde'},\n",
       "  {'id': '17b68035bfd5b604', 'threadId': '17b68035bfd5b604'},\n",
       "  {'id': '17b653999e95ddc1', 'threadId': '17b653999e95ddc1'},\n",
       "  {'id': '17b635db549f04ac', 'threadId': '17b635db549f04ac'},\n",
       "  {'id': '17b6338d79acdd5e', 'threadId': '17b6338d79acdd5e'},\n",
       "  {'id': '17b63061251c287e', 'threadId': '17b63061251c287e'},\n",
       "  {'id': '17b5e94d4de8f91f', 'threadId': '17b5e94d4de8f91f'},\n",
       "  {'id': '17b5da7ddeb42a02', 'threadId': '17b5da7ddeb42a02'},\n",
       "  {'id': '17b5ae5db53b1ef3', 'threadId': '17b5ae5db53b1ef3'},\n",
       "  {'id': '17b58c81ef295ea4', 'threadId': '17b58c81ef295ea4'},\n",
       "  {'id': '17b587ef70b09641', 'threadId': '17b587ef70b09641'},\n",
       "  {'id': '17b55bca4211a8d8', 'threadId': '17b55bca4211a8d8'},\n",
       "  {'id': '17b555d6233cb712', 'threadId': '17b555d6233cb712'},\n",
       "  {'id': '17b546b40afe5421', 'threadId': '17b546b40afe5421'},\n",
       "  {'id': '17b5390c85ea26ff', 'threadId': '17b5390c85ea26ff'},\n",
       "  {'id': '17b4e02fc2048dcc', 'threadId': '17b4e02fc2048dcc'},\n",
       "  {'id': '17b44b0945caed6b', 'threadId': '17b44b0945caed6b'},\n",
       "  {'id': '17b43f177da6552a', 'threadId': '17b43f177da6552a'},\n",
       "  {'id': '17b40feb8e5b8a03', 'threadId': '17b40feb8e5b8a03'},\n",
       "  {'id': '17b40a9332e41ec9', 'threadId': '17b40a9332e41ec9'},\n",
       "  {'id': '17b40785fb74b467', 'threadId': '17b40785fb74b467'},\n",
       "  {'id': '17b3f2c5e51bffa7', 'threadId': '17b3f2c5e51bffa7'},\n",
       "  {'id': '17b39dc773a967cd', 'threadId': '17b39dc773a967cd'},\n",
       "  {'id': '17b39a24678a7cc1', 'threadId': '17b39a24678a7cc1'},\n",
       "  {'id': '17b348465b5af49d', 'threadId': '17b348465b5af49d'},\n",
       "  {'id': '17b31a781c138459', 'threadId': '17b31a781c138459'},\n",
       "  {'id': '17b31263df428414', 'threadId': '17b31263df428414'},\n",
       "  {'id': '17b305e5cbc7d3a9', 'threadId': '17b305e5cbc7d3a9'},\n",
       "  {'id': '17b29f5b10ba35ef', 'threadId': '17b29f5b10ba35ef'},\n",
       "  {'id': '17b207e46e1ea6cb', 'threadId': '17b207e46e1ea6cb'},\n",
       "  {'id': '17b1fd83cfc3564e', 'threadId': '17b1fd83cfc3564e'},\n",
       "  {'id': '17b1c6a10d71b231', 'threadId': '17b1c6a10d71b231'},\n",
       "  {'id': '17b1bd34d3fd61fe', 'threadId': '17b1bd34d3fd61fe'},\n",
       "  {'id': '17b1b7156152ed98', 'threadId': '17b1b7156152ed98'},\n",
       "  {'id': '17b1b1fda0eaded4', 'threadId': '17b1b1fda0eaded4'},\n",
       "  {'id': '17b1a7b722cc5175', 'threadId': '17b1a7b722cc5175'},\n",
       "  {'id': '17b15c89d641e3a1', 'threadId': '17b15c89d641e3a1'},\n",
       "  {'id': '17b14e8515e8eae5', 'threadId': '17b14e8515e8eae5'},\n",
       "  {'id': '17b11b5f94c8db32', 'threadId': '17b11b5f94c8db32'},\n",
       "  {'id': '17b1077078112e18', 'threadId': '17b1077078112e18'},\n",
       "  {'id': '17b1036fbf83d6e5', 'threadId': '17b1036fbf83d6e5'},\n",
       "  {'id': '17b0da3bf3520350', 'threadId': '17b0da3bf3520350'},\n",
       "  {'id': '17b0d42f4df1a3d3', 'threadId': '17b0d42f4df1a3d3'},\n",
       "  {'id': '17b0c525ca170e0d', 'threadId': '17b0c525ca170e0d'},\n",
       "  {'id': '17b0b46c098ede10', 'threadId': '17b0b46c098ede10'},\n",
       "  {'id': '17b069551c51f4f2', 'threadId': '17b069551c51f4f2'},\n",
       "  {'id': '17b023ac17504eaf', 'threadId': '17b023ac17504eaf'},\n",
       "  {'id': '17b0131a29d55b1e', 'threadId': '17b0131a29d55b1e'},\n",
       "  {'id': '17afc86157d65e29', 'threadId': '17afc86157d65e29'},\n",
       "  {'id': '17afc3e0c305615a', 'threadId': '17afc3e0c305615a'},\n",
       "  {'id': '17af95eb4debf776', 'threadId': '17af95eb4debf776'},\n",
       "  {'id': '17af7a6dd21de3a3', 'threadId': '17af7a6dd21de3a3'},\n",
       "  {'id': '17af712f0ca4a55a', 'threadId': '17af712f0ca4a55a'},\n",
       "  {'id': '17af604c55a21738', 'threadId': '17af604c55a21738'},\n",
       "  {'id': '17af2c79684e310f', 'threadId': '17af2c79684e310f'},\n",
       "  {'id': '17af1bd04be3ae58', 'threadId': '17af1bd04be3ae58'},\n",
       "  {'id': '17af0dc83593c6b5', 'threadId': '17af0dc83593c6b5'},\n",
       "  {'id': '17aeeb3d7712ca46', 'threadId': '17aeeb3d7712ca46'},\n",
       "  {'id': '17aec658bb5267b4', 'threadId': '17aec658bb5267b4'},\n",
       "  {'id': '17aebea3ce085769', 'threadId': '17aebea3ce085769'},\n",
       "  {'id': '17aea40a07a68b40', 'threadId': '17aea40a07a68b40'},\n",
       "  {'id': '17ae99850928621a', 'threadId': '17ae99850928621a'},\n",
       "  {'id': '17ae926d95acbd82', 'threadId': '17ae926d95acbd82'},\n",
       "  {'id': '17ae87ad387a80e7', 'threadId': '17ae87ad387a80e7'},\n",
       "  {'id': '17ae715a8530ca19', 'threadId': '17ae715a8530ca19'},\n",
       "  {'id': '17ae1d5b1a2c2c42', 'threadId': '17ae1d5b1a2c2c42'},\n",
       "  {'id': '17ad878f5ef1abce', 'threadId': '17ad878f5ef1abce'},\n",
       "  {'id': '17ad7cb976ae437c', 'threadId': '17ad7cb976ae437c'},\n",
       "  {'id': '17ad4fdbf03f2481', 'threadId': '17ad4fdbf03f2481'},\n",
       "  {'id': '17ad380fa82e3e0c', 'threadId': '17ad380fa82e3e0c'},\n",
       "  {'id': '17ad328c9d4fcf5a', 'threadId': '17ad328c9d4fcf5a'},\n",
       "  {'id': '17ad2cf8d559034a', 'threadId': '17ad2cf8d559034a'},\n",
       "  {'id': '17ad26a2d8d44672', 'threadId': '17ad26a2d8d44672'},\n",
       "  {'id': '17acd51283deaa2c', 'threadId': '17acd51283deaa2c'},\n",
       "  {'id': '17acb3e4839b55ef', 'threadId': '17acb3e4839b55ef'},\n",
       "  {'id': '17aca7132df42dd6', 'threadId': '17aca7132df42dd6'},\n",
       "  {'id': '17ac85485ec12684', 'threadId': '17ac85485ec12684'},\n",
       "  {'id': '17ac58d41a2f5aad', 'threadId': '17ac58d41a2f5aad'},\n",
       "  {'id': '17ac576ac5dedc8b', 'threadId': '17ac576ac5dedc8b'},\n",
       "  {'id': '17ac361038e272fc', 'threadId': '17ac361038e272fc'},\n",
       "  {'id': '17ac304d7e773317', 'threadId': '17ac304d7e773317'},\n",
       "  {'id': '17abdff620a5e5dc', 'threadId': '17abdff620a5e5dc'},\n",
       "  {'id': '17ab413d64336846', 'threadId': '17ab413d64336846'},\n",
       "  {'id': '17ab3bf62f2638f0', 'threadId': '17ab3bf62f2638f0'},\n",
       "  {'id': '17ab02a4209d6ca7', 'threadId': '17ab02a4209d6ca7'},\n",
       "  {'id': '17aaec3622a15cfe', 'threadId': '17aaec3622a15cfe'},\n",
       "  {'id': '17aae6b168c06f71', 'threadId': '17aae6b168c06f71'},\n",
       "  {'id': '17aad392e55280b5', 'threadId': '17aad392e55280b5'},\n",
       "  {'id': '17aac0a9601d031a', 'threadId': '17aac0a9601d031a'},\n",
       "  {'id': '17aa96f288711fa3', 'threadId': '17aa96f288711fa3'},\n",
       "  {'id': '17aa6bbbce6b5df5', 'threadId': '17aa6bbbce6b5df5'},\n",
       "  {'id': '17aa5623a8a3a737', 'threadId': '17aa5623a8a3a737'},\n",
       "  {'id': '17aa43fec793adfb', 'threadId': '17aa43fec793adfb'},\n",
       "  {'id': '17aa41ef8ce608f1', 'threadId': '17aa41ef8ce608f1'},\n",
       "  {'id': '17aa1c51622fdc9b', 'threadId': '17aa1c51622fdc9b'},\n",
       "  {'id': '17aa1876803db2be', 'threadId': '17aa1876803db2be'},\n",
       "  {'id': '17aa12076fad1de4', 'threadId': '17aa12076fad1de4'},\n",
       "  {'id': '17aa0012d1aee940', 'threadId': '17aa0012d1aee940'},\n",
       "  {'id': '17a9f1c7c6879682', 'threadId': '17a9f1c7c6879682'},\n",
       "  {'id': '17a9ef84e71ecb99', 'threadId': '17a9ef84e71ecb99'},\n",
       "  {'id': '17a994fd6cda1991', 'threadId': '17a994fd6cda1991'},\n",
       "  {'id': '17a904aa36e36212', 'threadId': '17a904aa36e36212'},\n",
       "  {'id': '17a8faaa4a717626', 'threadId': '17a8faaa4a717626'},\n",
       "  {'id': '17a8b60bf5d43efa', 'threadId': '17a8b60bf5d43efa'},\n",
       "  {'id': '17a8ab77a3e8ffc3', 'threadId': '17a8ab77a3e8ffc3'},\n",
       "  {'id': '17a8a5550899807e', 'threadId': '17a8a5550899807e'},\n",
       "  {'id': '17a898fda4bba645', 'threadId': '17a898fda4bba645'},\n",
       "  {'id': '17a852a57f65cc7d', 'threadId': '17a852a57f65cc7d'},\n",
       "  {'id': '17a815850c840b11', 'threadId': '17a815850c840b11'},\n",
       "  {'id': '17a807247ffadc24', 'threadId': '17a807247ffadc24'},\n",
       "  {'id': '17a8033fe419cb17', 'threadId': '17a8033fe419cb17'},\n",
       "  {'id': '17a7d724893265fb', 'threadId': '17a7d724893265fb'},\n",
       "  {'id': '17a7d600c108c8f0', 'threadId': '17a7d600c108c8f0'},\n",
       "  {'id': '17a7d0f6e10992b1', 'threadId': '17a7d0f6e10992b1'},\n",
       "  {'id': '17a7cfb40287381f', 'threadId': '17a7cfb40287381f'},\n",
       "  {'id': '17a7bb107ad40891', 'threadId': '17a7bb107ad40891'},\n",
       "  {'id': '17a7b149cb1e4789', 'threadId': '17a7b149cb1e4789'},\n",
       "  {'id': '17a7759e5dce74ae', 'threadId': '17a7759e5dce74ae'},\n",
       "  {'id': '17a75419491ce6d0', 'threadId': '17a75419491ce6d0'},\n",
       "  {'id': '17a6bfbe28279338', 'threadId': '17a6bfbe28279338'},\n",
       "  {'id': '17a6b9d15f1762d0', 'threadId': '17a6b9d15f1762d0'},\n",
       "  {'id': '17a694a5c4e5fb5f', 'threadId': '17a694a5c4e5fb5f'},\n",
       "  {'id': '17a66acb30f2dc60', 'threadId': '17a66acb30f2dc60'},\n",
       "  {'id': '17a65cd969b20624', 'threadId': '17a65cd969b20624'},\n",
       "  {'id': '17a63808ee28036a', 'threadId': '17a63808ee28036a'},\n",
       "  {'id': '17a628501b82c7c6', 'threadId': '17a628501b82c7c6'},\n",
       "  {'id': '17a611dd5ffe1510', 'threadId': '17a611dd5ffe1510'},\n",
       "  {'id': '17a5ec288bd7e712', 'threadId': '17a5ec288bd7e712'},\n",
       "  {'id': '17a5e6e9b7fd5228', 'threadId': '17a5e6e9b7fd5228'},\n",
       "  {'id': '17a5c6e9c8bb8b22', 'threadId': '17a5c6e9c8bb8b22'},\n",
       "  {'id': '17a5c339eadb2671', 'threadId': '17a5c327c44fd628'},\n",
       "  {'id': '17a5c327c44fd628', 'threadId': '17a5c327c44fd628'},\n",
       "  {'id': '17a5bf336f424d3c', 'threadId': '17a5bf336f424d3c'},\n",
       "  {'id': '17a5967ecf68fa0b', 'threadId': '17a5967ecf68fa0b'},\n",
       "  {'id': '17a59576995e80ec', 'threadId': '17a59576995e80ec'},\n",
       "  {'id': '17a59134d3e5da96', 'threadId': '17a59134d3e5da96'},\n",
       "  {'id': '17a57ca92cd80433', 'threadId': '17a57ca92cd80433'},\n",
       "  {'id': '17a5737efede3b1d', 'threadId': '17a5737efede3b1d'},\n",
       "  {'id': '17a5353b2c02d25f', 'threadId': '17a51e0c2828dd68'},\n",
       "  {'id': '17a531ea19dd7209', 'threadId': '17a531ea19dd7209'},\n",
       "  {'id': '17a51e0c2828dd68', 'threadId': '17a51e0c2828dd68'},\n",
       "  {'id': '17a51b86a8599f25', 'threadId': '17a51b86a8599f25'},\n",
       "  {'id': '17a481b3ed809821', 'threadId': '17a481b3ed809821'},\n",
       "  {'id': '17a47551508b7a31', 'threadId': '17a47551508b7a31'},\n",
       "  {'id': '17a44b9d07773549', 'threadId': '17a44b9d07773549'},\n",
       "  {'id': '17a429fc0071547e', 'threadId': '17a429fc0071547e'},\n",
       "  {'id': '17a423c5802fbcd7', 'threadId': '17a423c5802fbcd7'},\n",
       "  {'id': '17a3f88820f5b354', 'threadId': '17a3f88820f5b354'},\n",
       "  {'id': '17a3d416d348d6cb', 'threadId': '17a3d416d348d6cb'},\n",
       "  {'id': '17a3aa5c2296e19a', 'threadId': '17a3aa5c2296e19a'},\n",
       "  {'id': '17a39b5ec166e308', 'threadId': '17a39b5ec166e308'},\n",
       "  {'id': '17a397386dffb1b7', 'threadId': '17a397386dffb1b7'},\n",
       "  {'id': '17a3866c8fcbb8a4', 'threadId': '17a3866c8fcbb8a4'},\n",
       "  {'id': '17a37f8f6eaa26bd', 'threadId': '17a37f8f6eaa26bd'},\n",
       "  {'id': '17a3559e5b60fa29', 'threadId': '17a3559e5b60fa29'},\n",
       "  {'id': '17a35439158157eb', 'threadId': '17a35439158157eb'},\n",
       "  {'id': '17a3520a5bd69d31', 'threadId': '17a3520a5bd69d31'},\n",
       "  {'id': '17a324f2fc8c8258', 'threadId': '17a324f2fc8c8258'},\n",
       "  {'id': '17a2f551fe8ea5b0', 'threadId': '17a2f551fe8ea5b0'},\n",
       "  {'id': '17a2f1e074d6bcf7', 'threadId': '17a2f1e074d6bcf7'},\n",
       "  {'id': '17a2a2eb9a3a139c', 'threadId': '17a2a2eb9a3a139c'},\n",
       "  {'id': '17a2a009bad9a48a', 'threadId': '17a28fa183235277'},\n",
       "  {'id': '17a29c3876355f45', 'threadId': '17a28fa183235277'},\n",
       "  {'id': '17a28fa183235277', 'threadId': '17a28fa183235277'},\n",
       "  {'id': '17a25d4706048ae3', 'threadId': '17a239ccf6c693cf'},\n",
       "  {'id': '17a24d8dc8a645df', 'threadId': '17a239ccf6c693cf'},\n",
       "  {'id': '17a2415ce216726f', 'threadId': '17a2415ce216726f'},\n",
       "  {'id': '17a239ccf6c693cf', 'threadId': '17a239ccf6c693cf'},\n",
       "  {'id': '17a23892b59ac495', 'threadId': '17a23892b59ac495'},\n",
       "  {'id': '17a1fe49cf5d0127', 'threadId': '17a1fe49cf5d0127'},\n",
       "  {'id': '17a1e92a2862b82f', 'threadId': '17a1e92a2862b82f'},\n",
       "  {'id': '17a1e301769d065a', 'threadId': '17a1e301769d065a'},\n",
       "  {'id': '17a1bce2e54678f6', 'threadId': '17a1bce2e54678f6'},\n",
       "  {'id': '17a1b7be8376604d', 'threadId': '17a1b7be8376604d'},\n",
       "  {'id': '17a1b3e6933b515f', 'threadId': '17a1b3e6933b515f'},\n",
       "  {'id': '17a1b0ed93a45366', 'threadId': '17a1b0ed93a45366'},\n",
       "  {'id': '17a19a19ce1e6a7a', 'threadId': '17a19a19ce1e6a7a'},\n",
       "  {'id': '17a169657d573ccd', 'threadId': '17a169657d573ccd'},\n",
       "  {'id': '17a141167009c4bd', 'threadId': '17a141167009c4bd'},\n",
       "  {'id': '17a114cf0610feb2', 'threadId': '17a114cf0610feb2'},\n",
       "  {'id': '17a1140508afa602', 'threadId': '17a1140508afa602'},\n",
       "  {'id': '17a10de34368be51', 'threadId': '17a10de34368be51'},\n",
       "  {'id': '17a0eeedc1b57a17', 'threadId': '17a0eeedc1b57a17'},\n",
       "  {'id': '17a0e9c191021d4f', 'threadId': '17a0e9c191021d4f'},\n",
       "  {'id': '17a09c02c271af0a', 'threadId': '17a09c02c271af0a'},\n",
       "  {'id': '17a002f33f1a7430', 'threadId': '17a002f33f1a7430'},\n",
       "  {'id': '179ff89baf09a441', 'threadId': '179ff89baf09a441'},\n",
       "  {'id': '179fb4cdedf3902f', 'threadId': '179fb4cdedf3902f'},\n",
       "  {'id': '179fa866d4627b24', 'threadId': '179fa866d4627b24'},\n",
       "  {'id': '179fa8577b084cc7', 'threadId': '179fa8577b084cc7'},\n",
       "  {'id': '179f76f60d3abca4', 'threadId': '179f76f60d3abca4'},\n",
       "  {'id': '179f265d179c179d', 'threadId': '179f265d179c179d'},\n",
       "  {'id': '179f01307ae36460', 'threadId': '179f01307ae36460'},\n",
       "  {'id': '179efdafe179a284', 'threadId': '179efdafe179a284'},\n",
       "  {'id': '179ed402b676d2eb', 'threadId': '179ed402b676d2eb'},\n",
       "  {'id': '179ed2dd01ef201f', 'threadId': '179ed2dd01ef201f'},\n",
       "  {'id': '179ecf15d74ad661', 'threadId': '179ecf15d74ad661'},\n",
       "  {'id': '179e765d238734ca', 'threadId': '179e765d238734ca'},\n",
       "  {'id': '179e71e26107380c', 'threadId': '179e71e26107380c'},\n",
       "  {'id': '179e71d0d5fe868f', 'threadId': '179e71d0d5fe868f'},\n",
       "  {'id': '179e2489a219f99d', 'threadId': '179e2489a219f99d'},\n",
       "  {'id': '179dbb8dad3af036', 'threadId': '179dbb8dad3af036'},\n",
       "  {'id': '179db7453981d983', 'threadId': '179db7453981d983'},\n",
       "  {'id': '179d8529a004ffb5', 'threadId': '179d8529a004ffb5'},\n",
       "  {'id': '179d83bea26e4bc8', 'threadId': '179d83bea26e4bc8'},\n",
       "  {'id': '179d7b00bb75ff47', 'threadId': '179d7b00bb75ff47'},\n",
       "  {'id': '179d67941a1b5091', 'threadId': '179d67941a1b5091'},\n",
       "  {'id': '179d62dc094ecf43', 'threadId': '179d62dc094ecf43'},\n",
       "  {'id': '179d60b4d444f2dc', 'threadId': '179d60b4d444f2dc'},\n",
       "  {'id': '179d362e72604cd9', 'threadId': '179d362e72604cd9'},\n",
       "  {'id': '179d153b6b352a8b', 'threadId': '179d153b6b352a8b'},\n",
       "  {'id': '179cf31bff5b2da7', 'threadId': '179cf31bff5b2da7'},\n",
       "  {'id': '179ce8d2e45485a1', 'threadId': '179ce8d2e45485a1'},\n",
       "  {'id': '179cd34e0f66eb48', 'threadId': '179cd34e0f66eb48'},\n",
       "  {'id': '179ccdc7f046e9c4', 'threadId': '179ccdc7f046e9c4'},\n",
       "  {'id': '179c932bc96e54e2', 'threadId': '179c932bc96e54e2'},\n",
       "  {'id': '179c8db93379f94a', 'threadId': '179c8db93379f94a'},\n",
       "  {'id': '179c2db8f94ef707', 'threadId': '179c2db8f94ef707'},\n",
       "  {'id': '179c186c136ad9f3', 'threadId': '179c186c136ad9f3'},\n",
       "  {'id': '179be3c4dab016cb', 'threadId': '179be3c4dab016cb'},\n",
       "  {'id': '179b8158fbf48b8d', 'threadId': '179b8158fbf48b8d'},\n",
       "  {'id': '179b75e5156b1ac8', 'threadId': '179b75e5156b1ac8'},\n",
       "  {'id': '179b3515220ccf40', 'threadId': '179b3515220ccf40'},\n",
       "  {'id': '179b26d38a0a9a57', 'threadId': '179b26d38a0a9a57'},\n",
       "  {'id': '179b237eefa25e17', 'threadId': '179b237eefa25e17'},\n",
       "  {'id': '179af53a720b929e', 'threadId': '179af53a720b929e'},\n",
       "  {'id': '179ae2ffd01b09d6', 'threadId': '179ae2ffd01b09d6'},\n",
       "  {'id': '179ace24d98ead56', 'threadId': '179ace24d98ead56'},\n",
       "  {'id': '179aa676ef7614c1', 'threadId': '179aa676ef7614c1'},\n",
       "  {'id': '179a83063a14c645', 'threadId': '179a83063a14c645'},\n",
       "  {'id': '179a7b3b4920e4a5', 'threadId': '179a7b3b4920e4a5'},\n",
       "  {'id': '179a5277fe6a7a25', 'threadId': '179a5277fe6a7a25'},\n",
       "  {'id': '179a516e885fa159', 'threadId': '179a516e885fa159'},\n",
       "  {'id': '179a4ced1a8f0937', 'threadId': '179a4ced1a8f0937'},\n",
       "  {'id': '179a37cef1879695', 'threadId': '179a37cef1879695'},\n",
       "  {'id': '179a2c228fac4c6c', 'threadId': '179a2c228fac4c6c'},\n",
       "  {'id': '179a25511ee8e6c5', 'threadId': '179a25511ee8e6c5'},\n",
       "  {'id': '1799e0a90a5954c7', 'threadId': '1799e0a90a5954c7'},\n",
       "  {'id': '17995212e8ecd542', 'threadId': '17995212e8ecd542'},\n",
       "  {'id': '17993c9e777e77c3', 'threadId': '17993c9e777e77c3'},\n",
       "  {'id': '179934edf7f393db', 'threadId': '179934edf7f393db'},\n",
       "  {'id': '1798fb33cc964147', 'threadId': '1798fb33cc964147'},\n",
       "  {'id': '1798e60a3e6fb753', 'threadId': '1798e60a3e6fb753'},\n",
       "  {'id': '1798dfe0e276da7c', 'threadId': '1798dfe0e276da7c'},\n",
       "  {'id': '1798b4717ca62ae4', 'threadId': '1798b4717ca62ae4'},\n",
       "  {'id': '17988d3714d08c97', 'threadId': '17988d3714d08c97'},\n",
       "  {'id': '1798412f121d0ec1', 'threadId': '1798412f121d0ec1'},\n",
       "  {'id': '17983b55c16d3189', 'threadId': '17983b55c16d3189'},\n",
       "  {'id': '179811ab8164c725', 'threadId': '179811ab8164c725'},\n",
       "  {'id': '179810a1738dff8b', 'threadId': '179810a1738dff8b'},\n",
       "  {'id': '17980c9e765908b0', 'threadId': '17980c9e765908b0'},\n",
       "  {'id': '17980a6b51bee10a', 'threadId': '17980a6b51bee10a'},\n",
       "  {'id': '1797f83c95698ac6', 'threadId': '1797f83c95698ac6'},\n",
       "  {'id': '1797eb57ffd4b94b', 'threadId': '1797eb57ffd4b94b'},\n",
       "  {'id': '1797e8a6eea7a2c6', 'threadId': '1797e8a6eea7a2c6'},\n",
       "  {'id': '1797bf5ba181142a', 'threadId': '1797bf5ba181142a'},\n",
       "  {'id': '1796fd137aec6c33', 'threadId': '1796fd137aec6c33'},\n",
       "  {'id': '1796f4a3135b9517', 'threadId': '1796f4a3135b9517'},\n",
       "  {'id': '1796bf6fefee9a82', 'threadId': '1796bf6fefee9a82'},\n",
       "  {'id': '1796a53945f33679', 'threadId': '1796a53945f33679'},\n",
       "  {'id': '17969f13f21cd62e', 'threadId': '17969f13f21cd62e'},\n",
       "  {'id': '179673b8d73db627', 'threadId': '179673b8d73db627'},\n",
       "  {'id': '179623fd3ddf8976', 'threadId': '179623fd3ddf8976'},\n",
       "  {'id': '17962305c8538b8b', 'threadId': '17962305c8538b8b'},\n",
       "  {'id': '1796006c02d9a47a', 'threadId': '1796006c02d9a47a'},\n",
       "  {'id': '1795fcfc56f17e94', 'threadId': '1795fcfc56f17e94'},\n",
       "  {'id': '1795d0cbedfc2319', 'threadId': '1795d0cbedfc2319'},\n",
       "  {'id': '1795cfa9e48ef68b', 'threadId': '1795cfa9e48ef68b'},\n",
       "  {'id': '1795c8df56e7b605', 'threadId': '1795c8df56e7b605'},\n",
       "  {'id': '1795b6dab7f268eb', 'threadId': '1795b6dab7f268eb'},\n",
       "  {'id': '1795a3de0e88fd42', 'threadId': '1795a3de0e88fd42'},\n",
       "  {'id': '17955f107bf8050e', 'threadId': '17955f107bf8050e'},\n",
       "  {'id': '1794b8248b64285a', 'threadId': '1794b8248b64285a'},\n",
       "  {'id': '1794b3474b4806f2', 'threadId': '1794b3474b4806f2'},\n",
       "  {'id': '17947ed230009f43', 'threadId': '17947ed230009f43'},\n",
       "  {'id': '179464711efec226', 'threadId': '179464711efec226'},\n",
       "  {'id': '17945e90b7d8c627', 'threadId': '17945e90b7d8c627'},\n",
       "  {'id': '1794357490dbd048', 'threadId': '1794357490dbd048'},\n",
       "  {'id': '179432b56ea910f2', 'threadId': '179432b56ea910f2'},\n",
       "  {'id': '1794103e86b42839', 'threadId': '1794103e86b42839'},\n",
       "  {'id': '17940b9904f0d07a', 'threadId': '17940b9904f0d07a'},\n",
       "  {'id': '1793e4a17fe80f1a', 'threadId': '1793e4a17fe80f1a'},\n",
       "  {'id': '1793d0b20052d8c0', 'threadId': '1793d0b20052d8c0'},\n",
       "  {'id': '1793bcf94685bb49', 'threadId': '1793bcf94685bb49'},\n",
       "  {'id': '1793900aa26ac307', 'threadId': '1793900aa26ac307'},\n",
       "  {'id': '17938f3c1f932335', 'threadId': '17938f3c1f932335'},\n",
       "  {'id': '179387902d9f4ed7', 'threadId': '179387902d9f4ed7'},\n",
       "  {'id': '17936717395173ad', 'threadId': '17936717395173ad'},\n",
       "  {'id': '17931e40387a9207', 'threadId': '17931e40387a9207'},\n",
       "  {'id': '17927b4c9b8f12c1', 'threadId': '17927b4c9b8f12c1'},\n",
       "  {'id': '179273a51500f89e', 'threadId': '179273a51500f89e'},\n",
       "  {'id': '1792551941d7cfd5', 'threadId': '1792551941d7cfd5'},\n",
       "  {'id': '179244f31709f88c', 'threadId': '179244f31709f88c'},\n",
       "  {'id': '17923a65bc4ba772', 'threadId': '17923a65bc4ba772'},\n",
       "  {'id': '1792363c1a9e5f29', 'threadId': '1792363c1a9e5f29'},\n",
       "  {'id': '1792363583fc1929', 'threadId': '1792363583fc1929'},\n",
       "  {'id': '179223c5e2abe2f7', 'threadId': '179223c5e2abe2f7'},\n",
       "  {'id': '1792213a90b510ac', 'threadId': '1792213a90b510ac'},\n",
       "  {'id': '1791fd229a830153', 'threadId': '1791fd229a830153'},\n",
       "  {'id': '1791f219b7614987', 'threadId': '1791f219b7614987'},\n",
       "  {'id': '1791a0a72cba4d18', 'threadId': '1791a0a72cba4d18'},\n",
       "  {'id': '17918fe97a4a5124', 'threadId': '17918fe97a4a5124'},\n",
       "  {'id': '17917babfd30830b', 'threadId': '17917babfd30830b'},\n",
       "  {'id': '17914f425eacbca7', 'threadId': '17914f425eacbca7'},\n",
       "  {'id': '17914c6a35da017f', 'threadId': '17914c6a35da017f'},\n",
       "  {'id': '1791415e5954cbea', 'threadId': '1791415e5954cbea'},\n",
       "  {'id': '17913bc19042a009', 'threadId': '17913bc19042a009'},\n",
       "  {'id': '179126e6d0b67440', 'threadId': '179126e6d0b67440'},\n",
       "  {'id': '1790f201c337e4bc', 'threadId': '1790f201c337e4bc'},\n",
       "  {'id': '1790ed6c55dd5b7c', 'threadId': '1790ed6c55dd5b7c'},\n",
       "  {'id': '1790dd847a69156a', 'threadId': '1790dd847a69156a'},\n",
       "  {'id': '17909622cc1f5086', 'threadId': '17909622cc1f5086'},\n",
       "  {'id': '17908ac1f625e8f4', 'threadId': '17908ac1f625e8f4'},\n",
       "  {'id': '17903a9b6ebacf23', 'threadId': '17903a9b6ebacf23'},\n",
       "  {'id': '17903249177c633d', 'threadId': '17903249177c633d'},\n",
       "  {'id': '17900d163ade40ec', 'threadId': '17900d163ade40ec'},\n",
       "  {'id': '17900a87ae0c6c3a', 'threadId': '17900a87ae0c6c3a'},\n",
       "  {'id': '178fe9adaf6c4ecd', 'threadId': '178fe9adaf6c4ecd'},\n",
       "  {'id': '178fdd4d4108f93e', 'threadId': '178fdd4d4108f93e'},\n",
       "  {'id': '178faff0aa822d8b', 'threadId': '178faff0aa822d8b'},\n",
       "  {'id': '178fac959e7b602e', 'threadId': '178fac959e7b602e'},\n",
       "  {'id': '178f9afa123fbd72', 'threadId': '178f9afa123fbd72'},\n",
       "  {'id': '178f8b456df5ebf6', 'threadId': '178f8b456df5ebf6'},\n",
       "  {'id': '178f8a6130530480', 'threadId': '178f8a6130530480'},\n",
       "  {'id': '178f643601a41b07', 'threadId': '178f643601a41b07'},\n",
       "  {'id': '178f418415bc5370', 'threadId': '178f418415bc5370'},\n",
       "  {'id': '178f3ba9910ee5a6', 'threadId': '178f3ba9910ee5a6'},\n",
       "  {'id': '178f0e7891aad5e5', 'threadId': '178f0e7891aad5e5'},\n",
       "  {'id': '178f0d821fe56023', 'threadId': '178f0d821fe56023'},\n",
       "  {'id': '178f09bcfd9b4a7f', 'threadId': '178f09bcfd9b4a7f'},\n",
       "  {'id': '178df6141ef04234', 'threadId': '178df6141ef04234'},\n",
       "  {'id': '178df17be7d506f8', 'threadId': '178df17be7d506f8'},\n",
       "  {'id': '178da9a0cba746df', 'threadId': '178da9a0cba746df'},\n",
       "  {'id': '178da8e7c7ddee7e', 'threadId': '178da8e7c7ddee7e'},\n",
       "  {'id': '178d9ec9264928ae', 'threadId': '178d9ec9264928ae'},\n",
       "  {'id': '178d7087788a8128', 'threadId': '178d7087788a8128'},\n",
       "  {'id': '178d216cd606700b', 'threadId': '178d216cd606700b'},\n",
       "  {'id': '178cfc031cc8e267', 'threadId': '178cfc031cc8e267'},\n",
       "  {'id': '178cf890adc3faf2', 'threadId': '178cf890adc3faf2'},\n",
       "  {'id': '178ccdc672e35180', 'threadId': '178ccdc672e35180'},\n",
       "  {'id': '178ccc896e97adee', 'threadId': '178ccc896e97adee'},\n",
       "  {'id': '178cc77bd592db7d', 'threadId': '178cc77bd592db7d'},\n",
       "  {'id': '178bb8f4aa2f167a', 'threadId': '178bb8f4aa2f167a'},\n",
       "  {'id': '178ba95c813f0ddf', 'threadId': '178ba95c813f0ddf'},\n",
       "  {'id': '178b834762fd8db9', 'threadId': '178b834762fd8db9'},\n",
       "  {'id': '178b7020a4bf5d23', 'threadId': '178b7020a4bf5d23'},\n",
       "  {'id': '178b6b9ab2d748ca', 'threadId': '178b6b9ab2d748ca'},\n",
       "  {'id': '178b64abaa891f4d', 'threadId': '178b64abaa891f4d'},\n",
       "  {'id': '178b5717c5656150', 'threadId': '178b5717c5656150'},\n",
       "  {'id': '178b2b0ec89b81f6', 'threadId': '178b2b0ec89b81f6'},\n",
       "  {'id': '178b26a841b572ff', 'threadId': '178b23cbddb6a500'},\n",
       "  {'id': '178b23cbddb6a500', 'threadId': '178b23cbddb6a500'},\n",
       "  {'id': '178b15f275de26da', 'threadId': '178b15f275de26da'},\n",
       "  {'id': '178b0d4231cf8381', 'threadId': '178b0d4231cf8381'},\n",
       "  {'id': '178b0c304256b046', 'threadId': '178b0c304256b046'},\n",
       "  {'id': '178ae065662e7242', 'threadId': '178ae065662e7242'},\n",
       "  {'id': '178ad96320828e38', 'threadId': '178ad87940728bdb'},\n",
       "  {'id': '178ad87940728bdb', 'threadId': '178ad87940728bdb'},\n",
       "  {'id': '178abd90a49672d6', 'threadId': '178abd90a49672d6'},\n",
       "  {'id': '178ab9017928e3a1', 'threadId': '178ab9017928e3a1'},\n",
       "  {'id': '178ab61ff814cb16', 'threadId': '178ab61ff814cb16'},\n",
       "  {'id': '178a8cf8eba2b02b', 'threadId': '178a8cf8eba2b02b'},\n",
       "  {'id': '178a8bbed7d937f7', 'threadId': '178a8bbed7d937f7'},\n",
       "  {'id': '178a864b36a4c7b0', 'threadId': '178a864b36a4c7b0'},\n",
       "  {'id': '178a10c224a3f31b', 'threadId': '178a10c224a3f31b'},\n",
       "  {'id': '1789dcadd7dbf7ca', 'threadId': '1789dcadd7dbf7ca'},\n",
       "  {'id': '178977839506be2e', 'threadId': '178977839506be2e'},\n",
       "  {'id': '17897035e31091f6', 'threadId': '17897035e31091f6'},\n",
       "  {'id': '178933b3bddf10b0', 'threadId': '178933b3bddf10b0'},\n",
       "  {'id': '178923e4f2ab51c5', 'threadId': '178923e4f2ab51c5'},\n",
       "  {'id': '17891997b96085c8', 'threadId': '17891997b96085c8'},\n",
       "  {'id': '1788ea478113cc00', 'threadId': '1788ea478113cc00'},\n",
       "  {'id': '1788c3c3ee108927', 'threadId': '1788c3c3ee108927'},\n",
       "  {'id': '1788a200c5ebb9a3', 'threadId': '1788a200c5ebb9a3'},\n",
       "  {'id': '17889ad0ddc5abc7', 'threadId': '17889ad0ddc5abc7'},\n",
       "  {'id': '17889aa4e00b0af2', 'threadId': '178889eee5c33e2a'},\n",
       "  {'id': '178889eee5c33e2a', 'threadId': '178889eee5c33e2a'},\n",
       "  {'id': '178886e451cdab08', 'threadId': '178886e451cdab08'},\n",
       "  {'id': '1788784746a701f1', 'threadId': '1788784746a701f1'},\n",
       "  {'id': '17884c213a4673b0', 'threadId': '17884c213a4673b0'},\n",
       "  {'id': '17884b2bcc3d06f6', 'threadId': '17884b2bcc3d06f6'},\n",
       "  {'id': '17884addfd23d1c8', 'threadId': '17884addfd23d1c8'},\n",
       "  {'id': '1788227d3a713b88', 'threadId': '1788227d3a713b88'},\n",
       "  {'id': '1787f96f12529fa3', 'threadId': '1787f96f12529fa3'},\n",
       "  {'id': '1787d7f22a6af74c', 'threadId': '1787d7f22a6af74c'},\n",
       "  {'id': '1787d0384068c877', 'threadId': '1787d0384068c877'},\n",
       "  {'id': '17879be839a69635', 'threadId': '17879be839a69635'},\n",
       "  {'id': '178781d62a4d6a65', 'threadId': '178781d62a4d6a65'},\n",
       "  {'id': '17877a8d066dded1', 'threadId': '17877a8d066dded1'},\n",
       "  {'id': '17877a87f2cd66cc', 'threadId': '17877a87f2cd66cc'},\n",
       "  {'id': '178735c81a142782', 'threadId': '178735c81a142782'},\n",
       "  {'id': '17872f6d194c90c4', 'threadId': '17872f6d194c90c4'},\n",
       "  {'id': '1786f9b4ab8a2aa1', 'threadId': '1786f9b4ab8a2aa1'},\n",
       "  {'id': '1786dc6890941562', 'threadId': '1786dc6890941562'},\n",
       "  {'id': '1786a960eb84c4a5', 'threadId': '1786a960eb84c4a5'},\n",
       "  {'id': '17869798be93ca82', 'threadId': '17869798be93ca82'},\n",
       "  {'id': '17868d8121113eac', 'threadId': '17868d8121113eac'},\n",
       "  {'id': '17868b7b71291fb5', 'threadId': '17868b7b71291fb5'},\n",
       "  {'id': '178687c36b487313', 'threadId': '178687c36b487313'},\n",
       "  {'id': '17865cacb80a4e6e', 'threadId': '17865cacb80a4e6e'},\n",
       "  {'id': '178647209836f5bb', 'threadId': '178647209836f5bb'},\n",
       "  {'id': '1786451373d94fed', 'threadId': '1786451373d94fed'},\n",
       "  {'id': '17863413bc124e05', 'threadId': '17863413bc124e05'},\n",
       "  {'id': '17860b5716bec4f1', 'threadId': '17860b5716bec4f1'},\n",
       "  {'id': '17860a496215ad12', 'threadId': '17860a496215ad12'},\n",
       "  {'id': '178600a904b4d926', 'threadId': '178600a904b4d926'},\n",
       "  {'id': '1785e66cc1e0191c', 'threadId': '1785e66cc1e0191c'},\n",
       "  {'id': '1785a8d5a35fb431', 'threadId': '1785a8d5a35fb431'},\n",
       "  {'id': '178592e11ef81caf', 'threadId': '178592e11ef81caf'},\n",
       "  {'id': '17855f23a9fce9d1', 'threadId': '17855f23a9fce9d1'},\n",
       "  {'id': '17855b8d111dde40', 'threadId': '17855b8d111dde40'},\n",
       "  {'id': '1784f30b81e69df1', 'threadId': '1784f30b81e69df1'},\n",
       "  {'id': '1784ee5ac7370556', 'threadId': '1784ee5ac7370556'},\n",
       "  {'id': '1784ba5e81c8158a', 'threadId': '1784ba5e81c8158a'},\n",
       "  {'id': '1784a25b3ba92ba3', 'threadId': '1784a25b3ba92ba3'},\n",
       "  {'id': '178483a0d68a2174', 'threadId': '178483a0d68a2174'},\n",
       "  {'id': '1784685f01f6ad4c', 'threadId': '1784685f01f6ad4c'},\n",
       "  {'id': '17845014e0e4bb2b', 'threadId': '17845014e0e4bb2b'},\n",
       "  {'id': '17844623442d9d37', 'threadId': '17844623442d9d37'},\n",
       "  {'id': '17840b87c1339322', 'threadId': '17840b87c1339322'},\n",
       "  {'id': '178405a07123b926', 'threadId': '178405a07123b926'},\n",
       "  {'id': '1783caa1e9dcafc3', 'threadId': '1783caa1e9dcafc3'},\n",
       "  {'id': '1783c98465d1e278', 'threadId': '1783c98465d1e278'},\n",
       "  {'id': '1783bcef8ff7c3c6', 'threadId': '1783bcef8ff7c3c6'},\n",
       "  {'id': '1783a5e99de7e402', 'threadId': '1783a5e99de7e402'},\n",
       "  {'id': '1782b32c420500e5', 'threadId': '1782b32c420500e5'},\n",
       "  {'id': '1782b1def19eee63', 'threadId': '1782b1def19eee63'},\n",
       "  {'id': '178282871ab983bc', 'threadId': '178282871ab983bc'},\n",
       "  {'id': '1782754526678fa2', 'threadId': '1782754526678fa2'},\n",
       "  {'id': '17826c2c09644b87', 'threadId': '17826c2c09644b87'},\n",
       "  {'id': '178227d07d46962c', 'threadId': '178227d07d46962c'},\n",
       "  {'id': '17821036dc7dabdc', 'threadId': '17821036dc7dabdc'},\n",
       "  {'id': '17820f7fbfbc375a', 'threadId': '17820f7fbfbc375a'},\n",
       "  {'id': '1782084c8cd65901', 'threadId': '1782084c8cd65901'},\n",
       "  {'id': '1781bc644f161d89', 'threadId': '1781bc644f161d89'},\n",
       "  {'id': '1781b81abf5162ec', 'threadId': '1781b81abf5162ec'},\n",
       "  {'id': '17818c6e7e30ab41', 'threadId': '17818c6e7e30ab41'},\n",
       "  {'id': '17818c40e0b34160', 'threadId': '17818c40e0b34160'},\n",
       "  {'id': '178189cb56c4f4ce', 'threadId': '178189cb56c4f4ce'}],\n",
       " 'nextPageToken': '12881640945484414016',\n",
       " 'resultSizeEstimate': 511}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next request we can use 'nextPageToken' to get the next 500 mails"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# should be 500 as we set max Result to 500\n",
    "# to get more \n",
    "len(id_list['messages'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "filename = '/Users/markusmuller/python/projects/gmail-newsletter-db/scripts/id_query_17.09.2021_q2'\n",
    "infile = open(filename,'rb')\n",
    "id_list_2 = pickle.load(infile)\n",
    "infile.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "id_list_2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'messages': [{'id': '17816701fa1d9c13', 'threadId': '17816701fa1d9c13'},\n",
       "  {'id': '178131161b8442c3', 'threadId': '178131161b8442c3'},\n",
       "  {'id': '1781126f55307a59', 'threadId': '1781126f55307a59'},\n",
       "  {'id': '178076a41dea0508', 'threadId': '178076a41dea0508'},\n",
       "  {'id': '178070e67f1b7a01', 'threadId': '178070e67f1b7a01'},\n",
       "  {'id': '17802f10b90c9cc0', 'threadId': '17802f10b90c9cc0'},\n",
       "  {'id': '177fe7088ffa657d', 'threadId': '177fe7088ffa657d'},\n",
       "  {'id': '177fd57b08afc2c3', 'threadId': '177fd57b08afc2c3'},\n",
       "  {'id': '177fd22793af68d6', 'threadId': '177fd22793af68d6'},\n",
       "  {'id': '177fcb336f882499', 'threadId': '177fcb336f882499'},\n",
       "  {'id': '177f7f9b3adf06c1', 'threadId': '177f7f9b3adf06c1'},\n",
       "  {'id': '177f4a8bb7de2c33', 'threadId': '177f4a8bb7de2c33'},\n",
       "  {'id': '177f490742fa4610', 'threadId': '177f490742fa4610'},\n",
       "  {'id': '177f48f055fcf4f8', 'threadId': '177f48f055fcf4f8'},\n",
       "  {'id': '177f42cf322b3b7a', 'threadId': '177f42cf322b3b7a'},\n",
       "  {'id': '177f41fabe152c86', 'threadId': '177f41fabe152c86'},\n",
       "  {'id': '177f3b2ae29c744b', 'threadId': '177f3b2ae29c744b'},\n",
       "  {'id': '177f35060bb3cc59', 'threadId': '177f35060bb3cc59'},\n",
       "  {'id': '177f263154fe692a', 'threadId': '177f263154fe692a'},\n",
       "  {'id': '177ef67352b9db1b', 'threadId': '177ef67352b9db1b'},\n",
       "  {'id': '177ee729e8ff28f7', 'threadId': '177ee729e8ff28f7'},\n",
       "  {'id': '177eb1465fed317d', 'threadId': '177eb1465fed317d'},\n",
       "  {'id': '177e81d5244dc681', 'threadId': '177e81d5244dc681'},\n",
       "  {'id': '177e34ae07f4b051', 'threadId': '177e34ae07f4b051'},\n",
       "  {'id': '177e31287d40f5eb', 'threadId': '177e31287d40f5eb'},\n",
       "  {'id': '177de699feca957d', 'threadId': '177de699feca957d'},\n",
       "  {'id': '177dd9258f962d16', 'threadId': '177dd9258f962d16'},\n",
       "  {'id': '177da606a84d3dde', 'threadId': '177da606a84d3dde'},\n",
       "  {'id': '177da49eedc719e7', 'threadId': '177da49eedc719e7'},\n",
       "  {'id': '177d9edd30399a29', 'threadId': '177d9edd30399a29'},\n",
       "  {'id': '177d9822acd06577', 'threadId': '177d9822acd06577'},\n",
       "  {'id': '177d8defc8153368', 'threadId': '177d8defc8153368'},\n",
       "  {'id': '177d39fb6e1f2d30', 'threadId': '177d39fb6e1f2d30'},\n",
       "  {'id': '177d3685af309fce', 'threadId': '177d3685af309fce'},\n",
       "  {'id': '177d09c2b82f06ef', 'threadId': '177d09c2b82f06ef'},\n",
       "  {'id': '177d094a9f4d69c8', 'threadId': '177d094a9f4d69c8'},\n",
       "  {'id': '177d084b6cbc96c5', 'threadId': '177d084b6cbc96c5'},\n",
       "  {'id': '177d0208709e8d77', 'threadId': '177d0208709e8d77'},\n",
       "  {'id': '177ce995cf929b9e', 'threadId': '177ce995cf929b9e'},\n",
       "  {'id': '177ce68237476560', 'threadId': '177ce68237476560'},\n",
       "  {'id': '177ce347bf18e056', 'threadId': '177ce347bf18e056'},\n",
       "  {'id': '177c91b9d81d2cdc', 'threadId': '177c91b9d81d2cdc'},\n",
       "  {'id': '177bf185752db2d8', 'threadId': '177bf185752db2d8'},\n",
       "  {'id': '177bf16de4fa5f3c', 'threadId': '177bf16de4fa5f3c'},\n",
       "  {'id': '177ba8c143ea7a72', 'threadId': '177ba8c143ea7a72'},\n",
       "  {'id': '177b9969ce3d5dfd', 'threadId': '177b9969ce3d5dfd'},\n",
       "  {'id': '177b652ebb1699b9', 'threadId': '177b652ebb1699b9'},\n",
       "  {'id': '177b5710ed4e36ca', 'threadId': '177b5710ed4e36ca'},\n",
       "  {'id': '177b4ea1c676ae4a', 'threadId': '177b4ea1c676ae4a'},\n",
       "  {'id': '177b45f2a8bd9b90', 'threadId': '177b45f2a8bd9b90'},\n",
       "  {'id': '177b0df22af8da34', 'threadId': '177b0df22af8da34'},\n",
       "  {'id': '177aff7ba05c9715', 'threadId': '177aff7ba05c9715'},\n",
       "  {'id': '177acbd04fe96653', 'threadId': '177acbd04fe96653'},\n",
       "  {'id': '177ac777c9d622f7', 'threadId': '177ac777c9d622f7'},\n",
       "  {'id': '177ac681df5d96e8', 'threadId': '177ac681df5d96e8'},\n",
       "  {'id': '177ab76014d0b611', 'threadId': '177ab76014d0b611'},\n",
       "  {'id': '177aa1ea4416f74e', 'threadId': '177aa1ea4416f74e'},\n",
       "  {'id': '1779b34b7be93254', 'threadId': '1779b34b7be93254'},\n",
       "  {'id': '177970e827a7a9dd', 'threadId': '177970e827a7a9dd'},\n",
       "  {'id': '1779654cf5ca831d', 'threadId': '1779654cf5ca831d'},\n",
       "  {'id': '17795ee8b28f719f', 'threadId': '17795ee8b28f719f'},\n",
       "  {'id': '17792457a5f50f5b', 'threadId': '17792457a5f50f5b'},\n",
       "  {'id': '17791230a395ad14', 'threadId': '17791230a395ad14'},\n",
       "  {'id': '1778baf6fcc66234', 'threadId': '1778baf6fcc66234'},\n",
       "  {'id': '1778b624badf26e0', 'threadId': '1778b624badf26e0'},\n",
       "  {'id': '17788870607d7a7d', 'threadId': '17788870607d7a7d'},\n",
       "  {'id': '177886a8183ecd51', 'threadId': '177886a8183ecd51'},\n",
       "  {'id': '177882f35fdd7dab', 'threadId': '177882f35fdd7dab'},\n",
       "  {'id': '17786247effe1d3f', 'threadId': '17786247effe1d3f'},\n",
       "  {'id': '17780dfb6d4d4b4f', 'threadId': '17780dfb6d4d4b4f'},\n",
       "  {'id': '1777cf3ea384b17c', 'threadId': '1777cf3ea384b17c'},\n",
       "  {'id': '177776d40891949c', 'threadId': '177776d40891949c'},\n",
       "  {'id': '177770223e9ab00e', 'threadId': '177770223e9ab00e'},\n",
       "  {'id': '177725928a47b09b', 'threadId': '177725928a47b09b'},\n",
       "  {'id': '1777174438d80a1c', 'threadId': '1777174438d80a1c'},\n",
       "  {'id': '1776e38110fddfd2', 'threadId': '1776e38110fddfd2'},\n",
       "  {'id': '1776c4b29cdb9836', 'threadId': '1776c4b29cdb9836'},\n",
       "  {'id': '17767ac5baf63530', 'threadId': '17767ac5baf63530'},\n",
       "  {'id': '1776735335118de3', 'threadId': '1776735335118de3'},\n",
       "  {'id': '1776487fb1e92047', 'threadId': '1776487fb1e92047'},\n",
       "  {'id': '1776461b8995a1f6', 'threadId': '1776461b8995a1f6'},\n",
       "  {'id': '177642bf647caeb7', 'threadId': '177642bf647caeb7'},\n",
       "  {'id': '17762334622064d0', 'threadId': '17762334622064d0'},\n",
       "  {'id': '17759bd709795923', 'threadId': '17759bd709795923'},\n",
       "  {'id': '17758d383c013cba', 'threadId': '17758d383c013cba'},\n",
       "  {'id': '1775436783f138ed', 'threadId': '1775436783f138ed'},\n",
       "  {'id': '17753205aeafcf04', 'threadId': '17753205aeafcf04'},\n",
       "  {'id': '17752bbef8a6f556', 'threadId': '17752bbef8a6f556'},\n",
       "  {'id': '1774e3f0bd56150f', 'threadId': '1774e3f0bd56150f'},\n",
       "  {'id': '1774e2e6c9fe035e', 'threadId': '1774e2e6c9fe035e'},\n",
       "  {'id': '1774a31f6a3c2b4b', 'threadId': '1774a31f6a3c2b4b'},\n",
       "  {'id': '177495168ff7281d', 'threadId': '177495168ff7281d'},\n",
       "  {'id': '17748541ed786dda', 'threadId': '17748541ed786dda'},\n",
       "  {'id': '17748409e00266f1', 'threadId': '17748409e00266f1'},\n",
       "  {'id': '1774444a02454d67', 'threadId': '1774444a02454d67'},\n",
       "  {'id': '177434d938813500', 'threadId': '177434d938813500'},\n",
       "  {'id': '17740709836941ec', 'threadId': '17740709836941ec'},\n",
       "  {'id': '1774051c00b12ce6', 'threadId': '1774051c00b12ce6'},\n",
       "  {'id': '1774019f0ab19064', 'threadId': '1774019f0ab19064'},\n",
       "  {'id': '1773eca6f6124ac9', 'threadId': '1773eca6f6124ac9'},\n",
       "  {'id': '1773e07419feab21', 'threadId': '1773e07419feab21'},\n",
       "  {'id': '1773a45e374fd739', 'threadId': '1773a45e374fd739'},\n",
       "  {'id': '17738ee20260a5ce', 'threadId': '17738ee20260a5ce'},\n",
       "  {'id': '17735fb1801e86e6', 'threadId': '17735fb1801e86e6'},\n",
       "  {'id': '17734b954dcc8727', 'threadId': '17734b954dcc8727'},\n",
       "  {'id': '1772edcd088cc52d', 'threadId': '1772edcd088cc52d'},\n",
       "  {'id': '1772ec196fb2df68', 'threadId': '1772ec196fb2df68'},\n",
       "  {'id': '1772aceb9a68e592', 'threadId': '1772aceb9a68e592'},\n",
       "  {'id': '1772952c6b776b34', 'threadId': '1772952c6b776b34'},\n",
       "  {'id': '17726258da9af269', 'threadId': '17726258da9af269'},\n",
       "  {'id': '177249aa3cf6e400', 'threadId': '177249aa3cf6e400'},\n",
       "  {'id': '17721d7ff72a361a', 'threadId': '17721d7ff72a361a'},\n",
       "  {'id': '17720525828ab56b', 'threadId': '17720525828ab56b'},\n",
       "  {'id': '1771f05b905465a8', 'threadId': '1771f05b905465a8'},\n",
       "  {'id': '1771c60e65e0c867', 'threadId': '1771c60e65e0c867'},\n",
       "  {'id': '1771c45fe0fade05', 'threadId': '1771c45fe0fade05'},\n",
       "  {'id': '1771be1e1cad981a', 'threadId': '1771be1e1cad981a'},\n",
       "  {'id': '1771bdc4bd3ea63a', 'threadId': '1771bdc4bd3ea63a'},\n",
       "  {'id': '17714c6c569381ca', 'threadId': '17714c6c569381ca'},\n",
       "  {'id': '17710a027b62f2db', 'threadId': '17710a027b62f2db'},\n",
       "  {'id': '1770f90b1712f05e', 'threadId': '1770f90b1712f05e'},\n",
       "  {'id': '1770aff1773a2abc', 'threadId': '1770aff1773a2abc'},\n",
       "  {'id': '1770aa3bf157364e', 'threadId': '1770aa3bf157364e'},\n",
       "  {'id': '1770497baed44b86', 'threadId': '1770497baed44b86'},\n",
       "  {'id': '1770218ec2a402fd', 'threadId': '1770218ec2a402fd'},\n",
       "  {'id': '17701b048ccce03b', 'threadId': '17701b048ccce03b'},\n",
       "  {'id': '17700280b365d5c6', 'threadId': '17700280b365d5c6'},\n",
       "  {'id': '176fb4faa3b7f9f4', 'threadId': '176fb4faa3b7f9f4'},\n",
       "  {'id': '176fa957371ed81b', 'threadId': '176fa957371ed81b'},\n",
       "  {'id': '176f8984a40ab6c7', 'threadId': '176f8984a40ab6c7'},\n",
       "  {'id': '176f83ab03bd2ad7', 'threadId': '176f83ab03bd2ad7'},\n",
       "  {'id': '176f62df74cc48da', 'threadId': '176f62df74cc48da'},\n",
       "  {'id': '176f277fde21d1a3', 'threadId': '176f277fde21d1a3'},\n",
       "  {'id': '176ec9012a56c6ce', 'threadId': '176ec9012a56c6ce'},\n",
       "  {'id': '176e7acc616eba08', 'threadId': '176e7acc616eba08'},\n",
       "  {'id': '176e6ae61bbdae09', 'threadId': '176e6ae61bbdae09'},\n",
       "  {'id': '176e3299819cfca6', 'threadId': '176e3299819cfca6'},\n",
       "  {'id': '176de0c70c8b29fe', 'threadId': '176de0c70c8b29fe'},\n",
       "  {'id': '176dc237dd7b426e', 'threadId': '176dc237dd7b426e'},\n",
       "  {'id': '176d81b133152948', 'threadId': '176d81b133152948'},\n",
       "  {'id': '176d75ba4e8888bf', 'threadId': '176d75ba4e8888bf'},\n",
       "  {'id': '176d6f9f0d4e7e6b', 'threadId': '176d6f9f0d4e7e6b'},\n",
       "  {'id': '176d40df316eaa4c', 'threadId': '176d40df316eaa4c'},\n",
       "  {'id': '176d3e7a97b0bf68', 'threadId': '176d3e7a97b0bf68'},\n",
       "  {'id': '176d318d320c8025', 'threadId': '176d318d320c8025'},\n",
       "  {'id': '176c87458959ce37', 'threadId': '176c87458959ce37'},\n",
       "  {'id': '176c48568996b647', 'threadId': '176c48568996b647'},\n",
       "  {'id': '176c298069924820', 'threadId': '176c298069924820'},\n",
       "  {'id': '176bc027d763890b', 'threadId': '176bc027d763890b'},\n",
       "  {'id': '176ba238aef67348', 'threadId': '176ba238aef67348'},\n",
       "  {'id': '176ba01b4ad27234', 'threadId': '176ba01b4ad27234'},\n",
       "  {'id': '176b59a32c0bc0a7', 'threadId': '176b59a32c0bc0a7'},\n",
       "  {'id': '176b469be6579dff', 'threadId': '176b469be6579dff'},\n",
       "  {'id': '176b3e1d9b39d585', 'threadId': '176b3e1d9b39d585'},\n",
       "  {'id': '176b0009bcd55e0e', 'threadId': '176b0009bcd55e0e'},\n",
       "  {'id': '176af1ee03cd62be', 'threadId': '176af1ee03cd62be'},\n",
       "  {'id': '176af1df03dde1a5', 'threadId': '176af1df03dde1a5'},\n",
       "  {'id': '176aef445357c8b7', 'threadId': '176aef445357c8b7'},\n",
       "  {'id': '176aa2451c649ad5', 'threadId': '176aa2451c649ad5'},\n",
       "  {'id': '176a89b7048734ad', 'threadId': '176a89b7048734ad'},\n",
       "  {'id': '176a44f02ab194ed', 'threadId': '176a44f02ab194ed'},\n",
       "  {'id': '1769eb3f315e9fde', 'threadId': '1769eb3f315e9fde'},\n",
       "  {'id': '1769e98c65012e01', 'threadId': '1769e98c65012e01'},\n",
       "  {'id': '176991a804284034', 'threadId': '176991a804284034'},\n",
       "  {'id': '17695f366867b2a8', 'threadId': '17695f366867b2a8'},\n",
       "  {'id': '1769425cb38cb776', 'threadId': '1769425cb38cb776'},\n",
       "  {'id': '1768f262a450ec89', 'threadId': '1768f262a450ec89'},\n",
       "  {'id': '1768d9f19b324170', 'threadId': '1768d9f19b324170'},\n",
       "  {'id': '1768c236d0ef6dbd', 'threadId': '1768c236d0ef6dbd'},\n",
       "  {'id': '1768bf58fb6b31c5', 'threadId': '1768bf58fb6b31c5'},\n",
       "  {'id': '17687722a2d03a77', 'threadId': '17687722a2d03a77'},\n",
       "  {'id': '17680412e751a86e', 'threadId': '17680412e751a86e'},\n",
       "  {'id': '1767ab7c89c81b58', 'threadId': '1767ab7c89c81b58'},\n",
       "  {'id': '1767a90b2fdf76e1', 'threadId': '1767a90b2fdf76e1'},\n",
       "  {'id': '1767a32d6a0fb2d9', 'threadId': '1767a32d6a0fb2d9'},\n",
       "  {'id': '176751cb53eed43c', 'threadId': '176751cb53eed43c'},\n",
       "  {'id': '1767243220c96e6d', 'threadId': '1767243220c96e6d'},\n",
       "  {'id': '176715d9e3ede8ba', 'threadId': '176715d9e3ede8ba'},\n",
       "  {'id': '17670ceec2837123', 'threadId': '17670ceec2837123'},\n",
       "  {'id': '1766ffb6d47a04ee', 'threadId': '1766ffb6d47a04ee'},\n",
       "  {'id': '1766bb6359eb7df2', 'threadId': '1766bb6359eb7df2'},\n",
       "  {'id': '1766b0b6fbf19051', 'threadId': '1766b0b6fbf19051'},\n",
       "  {'id': '176681d5dc1eaaf4', 'threadId': '176681d5dc1eaaf4'},\n",
       "  {'id': '17667ea4d30f53de', 'threadId': '17667ea4d30f53de'},\n",
       "  {'id': '17667e45e41b9fae', 'threadId': '17667e45e41b9fae'},\n",
       "  {'id': '17665d1d7b06c0df', 'threadId': '17665d1d7b06c0df'},\n",
       "  {'id': '17665a30b52053ee', 'threadId': '17665a30b52053ee'},\n",
       "  {'id': '17660ba0d96a7b98', 'threadId': '17660ba0d96a7b98'},\n",
       "  {'id': '17656b3da33c471a', 'threadId': '17656b3da33c471a'},\n",
       "  {'id': '1765665127ad61b5', 'threadId': '1765665127ad61b5'},\n",
       "  {'id': '17650fd3237126d2', 'threadId': '17650fd3237126d2'},\n",
       "  {'id': '1764dda65b2043ad', 'threadId': '1764dda65b2043ad'},\n",
       "  {'id': '1764ceaf9a856a4b', 'threadId': '1764ceaf9a856a4b'},\n",
       "  {'id': '1764c40740e6f01c', 'threadId': '1764c40740e6f01c'},\n",
       "  {'id': '1764bd2d075243d3', 'threadId': '1764bd2d075243d3'},\n",
       "  {'id': '1764bc7af62b719e', 'threadId': '1764bc7af62b719e'},\n",
       "  {'id': '1764b4f292082509', 'threadId': '1764b4f292082509'},\n",
       "  {'id': '176499302a4b700a', 'threadId': '176499302a4b700a'},\n",
       "  {'id': '1764988958f1d139', 'threadId': '1764988958f1d139'},\n",
       "  {'id': '176473ee7fa0e26e', 'threadId': '176473ee7fa0e26e'},\n",
       "  {'id': '17646e815285908e', 'threadId': '17646e815285908e'},\n",
       "  {'id': '17646dea9d1edc88', 'threadId': '17646dea9d1edc88'},\n",
       "  {'id': '1764417b66d94c5b', 'threadId': '1764417b66d94c5b'},\n",
       "  {'id': '17643ee7d3d0ccae', 'threadId': '17643ee7d3d0ccae'},\n",
       "  {'id': '17643e054504c818', 'threadId': '17643e054504c818'},\n",
       "  {'id': '17642c000f64e684', 'threadId': '17642c000f64e684'},\n",
       "  {'id': '17641b3c43336edf', 'threadId': '17641b3c43336edf'},\n",
       "  {'id': '1763ecd193b71700', 'threadId': '1763ecd193b71700'},\n",
       "  {'id': '1763c9204123734c', 'threadId': '1763c9204123734c'},\n",
       "  {'id': '176329f33b194651', 'threadId': '176329f33b194651'},\n",
       "  {'id': '1763269db16b930d', 'threadId': '1763269db16b930d'},\n",
       "  {'id': '1762de228a8b3e7e', 'threadId': '1762de228a8b3e7e'},\n",
       "  {'id': '17629cdcf2de2130', 'threadId': '17629cdcf2de2130'},\n",
       "  {'id': '17627dd412a63e48', 'threadId': '17627dd412a63e48'},\n",
       "  {'id': '17627b0263e541de', 'threadId': '17627b0263e541de'},\n",
       "  {'id': '176254cc2d13d0ca', 'threadId': '176254cc2d13d0ca'},\n",
       "  {'id': '17623a95a0ec5259', 'threadId': '17623a95a0ec5259'},\n",
       "  {'id': '17622e76816d6aca', 'threadId': '17622e76816d6aca'},\n",
       "  {'id': '1761fd160c754362', 'threadId': '1761fd160c754362'},\n",
       "  {'id': '1761f8bc5ef1d16d', 'threadId': '1761f8bc5ef1d16d'},\n",
       "  {'id': '1761f76f72da044e', 'threadId': '1761f76f72da044e'},\n",
       "  {'id': '1761f3c4a1d8541c', 'threadId': '1761f3c4a1d8541c'},\n",
       "  {'id': '1761d8b07b5e9254', 'threadId': '1761d8b07b5e9254'},\n",
       "  {'id': '1761a69626c92e5b', 'threadId': '1761a69626c92e5b'},\n",
       "  {'id': '1761a611f96e44f2', 'threadId': '1761a611f96e44f2'},\n",
       "  {'id': '17619c69a4615869', 'threadId': '17619c69a4615869'},\n",
       "  {'id': '176189929222107c', 'threadId': '176189929222107c'},\n",
       "  {'id': '1760e951fb81c48d', 'threadId': '1760e951fb81c48d'},\n",
       "  {'id': '1760e4d17b1b8ca4', 'threadId': '1760e4d17b1b8ca4'},\n",
       "  {'id': '1760af74130f062a', 'threadId': '1760af74130f062a'},\n",
       "  {'id': '1760a536ba0e07a0', 'threadId': '1760a536ba0e07a0'},\n",
       "  {'id': '17608d5a2ca59ee3', 'threadId': '17608d5a2ca59ee3'},\n",
       "  {'id': '17605c4f59bcb86d', 'threadId': '17605c4f59bcb86d'},\n",
       "  {'id': '176022e6c59815fb', 'threadId': '176022e6c59815fb'},\n",
       "  {'id': '17601b016d73bef5', 'threadId': '17601b016d73bef5'},\n",
       "  {'id': '175fc62bebcf9806', 'threadId': '175fc62bebcf9806'},\n",
       "  {'id': '175fbc6e4a4780b6', 'threadId': '175fbc6e4a4780b6'},\n",
       "  {'id': '175fbb2c22633dd8', 'threadId': '175fbb2c22633dd8'},\n",
       "  {'id': '175fb6a4a1a122a0', 'threadId': '175fb6a4a1a122a0'},\n",
       "  {'id': '175fab1cb97d26b2', 'threadId': '175fab1cb97d26b2'},\n",
       "  {'id': '175f6567f3895677', 'threadId': '175f6567f3895677'},\n",
       "  {'id': '175f5aae4bd9362d', 'threadId': '175f5aae4bd9362d'},\n",
       "  {'id': '175f4be5464e2812', 'threadId': '175f4be5464e2812'},\n",
       "  {'id': '175f48df9fcc09df', 'threadId': '175f48df9fcc09df'},\n",
       "  {'id': '175ea8c6682c2bad', 'threadId': '175ea8c6682c2bad'},\n",
       "  {'id': '175ea4004470af07', 'threadId': '175ea4004470af07'},\n",
       "  {'id': '175e66c23b1ecfad', 'threadId': '175e66c23b1ecfad'},\n",
       "  {'id': '175e50a850fac0ab', 'threadId': '175e50a850fac0ab'},\n",
       "  {'id': '175e4b2e3b66f6c8', 'threadId': '175e4b2e3b66f6c8'},\n",
       "  {'id': '175e1b4d3e3aba2d', 'threadId': '175e1b4d3e3aba2d'},\n",
       "  {'id': '175dd5f540673694', 'threadId': '175dd5f540673694'},\n",
       "  {'id': '175db062c2b8cc65', 'threadId': '175db062c2b8cc65'},\n",
       "  {'id': '175d822ef9f7c067', 'threadId': '175d822ef9f7c067'},\n",
       "  {'id': '175d7b7d63aef3f7', 'threadId': '175d7b7d63aef3f7'},\n",
       "  {'id': '175d791587d680ee', 'threadId': '175d791587d680ee'},\n",
       "  {'id': '175d75dc67893fcd', 'threadId': '175d75dc67893fcd'},\n",
       "  {'id': '175d5d3bf8aa9683', 'threadId': '175d5d3bf8aa9683'},\n",
       "  {'id': '175d5a5edfde246a', 'threadId': '175d5a5edfde246a'},\n",
       "  {'id': '175d140652c407a2', 'threadId': '175d140652c407a2'},\n",
       "  {'id': '175c6ce9c642d806', 'threadId': '175c6ce9c642d806'},\n",
       "  {'id': '175c67beac1d1f30', 'threadId': '175c67beac1d1f30'},\n",
       "  {'id': '175c63b56db41310', 'threadId': '175c63b56db41310'},\n",
       "  {'id': '175c1516ef2a93bf', 'threadId': '175c1516ef2a93bf'},\n",
       "  {'id': '175bdaa2c7ca004b', 'threadId': '175bdaa2c7ca004b'},\n",
       "  {'id': '175bd5fb1c5b3dab', 'threadId': '175bd5fb1c5b3dab'},\n",
       "  {'id': '175b9379b959e838', 'threadId': '175b9379b959e838'},\n",
       "  {'id': '175b796e103b8010', 'threadId': '175b796e103b8010'},\n",
       "  {'id': '175b6fba99e39ab9', 'threadId': '175b6fba99e39ab9'},\n",
       "  {'id': '175b6a447147a0b8', 'threadId': '175b6a447147a0b8'},\n",
       "  {'id': '175b428a70ac5695', 'threadId': '175b428a70ac5695'},\n",
       "  {'id': '175b3aba075785a4', 'threadId': '175b3aba075785a4'},\n",
       "  {'id': '175b371ca4ecc04e', 'threadId': '175b371ca4ecc04e'},\n",
       "  {'id': '175b3510d9beb9e5', 'threadId': '175b3510d9beb9e5'},\n",
       "  {'id': '175b1868571e4920', 'threadId': '175b1868571e4920'},\n",
       "  {'id': '175ad6f8adc0cfea', 'threadId': '175ad6f8adc0cfea'},\n",
       "  {'id': '175a2cc817834fca', 'threadId': '175a2cc817834fca'},\n",
       "  {'id': '1759ccb11842ceca', 'threadId': '1759ccb11842ceca'},\n",
       "  {'id': '1759cc2132cad442', 'threadId': '1759cc2132cad442'},\n",
       "  {'id': '1759cbcf3405c3ee', 'threadId': '1759cbcf3405c3ee'},\n",
       "  {'id': '175999ae4b0df7be', 'threadId': '175999ae4b0df7be'},\n",
       "  {'id': '17597eb3b97c9e02', 'threadId': '17597eb3b97c9e02'},\n",
       "  {'id': '175953b0cec6ceaa', 'threadId': '175953b0cec6ceaa'},\n",
       "  {'id': '17593e95170ca051', 'threadId': '17593e95170ca051'},\n",
       "  {'id': '17593787170609f5', 'threadId': '17593787170609f5'},\n",
       "  {'id': '175935a2ab23b419', 'threadId': '175935a2ab23b419'},\n",
       "  {'id': '17590232bf86f501', 'threadId': '17590232bf86f501'},\n",
       "  {'id': '1758fa9c11c160bf', 'threadId': '1758fa9c11c160bf'},\n",
       "  {'id': '1758f4439f3d3bff', 'threadId': '1758f4439f3d3bff'},\n",
       "  {'id': '1758f12c63411cae', 'threadId': '1758f12c63411cae'},\n",
       "  {'id': '1758dbd2cd27bf58', 'threadId': '1758dbd2cd27bf58'},\n",
       "  {'id': '1758d5e59e5249c8', 'threadId': '1758d5e59e5249c8'},\n",
       "  {'id': '1758053cc29e18b5', 'threadId': '1758053cc29e18b5'},\n",
       "  {'id': '1757e3b3e6ab2f3d', 'threadId': '1757e3b3e6ab2f3d'},\n",
       "  {'id': '1757de6e364d0f11', 'threadId': '1757de6e364d0f11'},\n",
       "  {'id': '1757b7dc1f7cbed1', 'threadId': '1757b7dc1f7cbed1'},\n",
       "  {'id': '175758f44702155b', 'threadId': '175758f44702155b'},\n",
       "  {'id': '175736268b558f9f', 'threadId': '175736268b558f9f'},\n",
       "  {'id': '17570a5e3c625c22', 'threadId': '17570a5e3c625c22'},\n",
       "  {'id': '1756e8ab1b10af1a', 'threadId': '1756e8ab1b10af1a'},\n",
       "  {'id': '1756be49f16e938e', 'threadId': '1756be49f16e938e'},\n",
       "  {'id': '1756bae3baf3aacc', 'threadId': '1756bae3baf3aacc'},\n",
       "  {'id': '1756b1fe763cc66a', 'threadId': '1756b1fe763cc66a'},\n",
       "  {'id': '1756a5d1019fd123', 'threadId': '1756a5d1019fd123'},\n",
       "  {'id': '175691b09657d232', 'threadId': '175691b09657d232'},\n",
       "  {'id': '17565f763c022acd', 'threadId': '17565f763c022acd'},\n",
       "  {'id': '17563f4c719a9778', 'threadId': '17563f4c719a9778'},\n",
       "  {'id': '17559da70ab64f59', 'threadId': '17559da70ab64f59'},\n",
       "  {'id': '17559619a5b2741b', 'threadId': '17559619a5b2741b'},\n",
       "  {'id': '17554e959573329f', 'threadId': '17554e959573329f'},\n",
       "  {'id': '17554b68818a832b', 'threadId': '17554b68818a832b'},\n",
       "  {'id': '1755493e03ade081', 'threadId': '1755493e03ade081'},\n",
       "  {'id': '175543495c216ac0', 'threadId': '175543495c216ac0'},\n",
       "  {'id': '1755182bff2fd1f1', 'threadId': '1755182bff2fd1f1'},\n",
       "  {'id': '175502f4be1b9056', 'threadId': '175502f4be1b9056'},\n",
       "  {'id': '1754fa51da1f544b', 'threadId': '1754fa51da1f544b'},\n",
       "  {'id': '1754f68e64a7aedc', 'threadId': '1754f68e64a7aedc'},\n",
       "  {'id': '1754d04f7cd5f479', 'threadId': '1754d04f7cd5f479'},\n",
       "  {'id': '1754c8af07c6e9cd', 'threadId': '1754c8af07c6e9cd'},\n",
       "  {'id': '1754ae2d9a4c8dcc', 'threadId': '1754ae2d9a4c8dcc'},\n",
       "  {'id': '1754a79b012434b1', 'threadId': '1754a79b012434b1'},\n",
       "  {'id': '17547fdac6046a59', 'threadId': '17547fdac6046a59'},\n",
       "  {'id': '1754784510345e5e', 'threadId': '1754784510345e5e'},\n",
       "  {'id': '1754747aed31892e', 'threadId': '1754747aed31892e'},\n",
       "  {'id': '175464fc0002d5e0', 'threadId': '175464fc0002d5e0'},\n",
       "  {'id': '17535cdf4bb537fa', 'threadId': '17535cdf4bb537fa'},\n",
       "  {'id': '175307469687b896', 'threadId': '175307469687b896'},\n",
       "  {'id': '1752e0b214744445', 'threadId': '1752e0b214744445'},\n",
       "  {'id': '1752d7640f5114c8', 'threadId': '1752d7640f5114c8'},\n",
       "  {'id': '1752b47360d8793c', 'threadId': '1752b47360d8793c'},\n",
       "  {'id': '1752b468154ab3c1', 'threadId': '1752b468154ab3c1'},\n",
       "  {'id': '17528a4e2367fa13', 'threadId': '17528a4e2367fa13'},\n",
       "  {'id': '17523fcfbbd5d856', 'threadId': '17523fcfbbd5d856'},\n",
       "  {'id': '1752379f364c4ccc', 'threadId': '1752379f364c4ccc'},\n",
       "  {'id': '1752243aa2b2e27d', 'threadId': '1752243aa2b2e27d'},\n",
       "  {'id': '1751bd390f44c937', 'threadId': '1751bd390f44c937'},\n",
       "  {'id': '17511d40e1674ec5', 'threadId': '17511d40e1674ec5'},\n",
       "  {'id': '1750f2305ab252fb', 'threadId': '1750f2305ab252fb'},\n",
       "  {'id': '1750c574125f07e8', 'threadId': '1750c574125f07e8'},\n",
       "  {'id': '1750969c4b57f70d', 'threadId': '1750969c4b57f70d'},\n",
       "  {'id': '175073a1f6c5648a', 'threadId': '175073a1f6c5648a'},\n",
       "  {'id': '17504839ee9ce2db', 'threadId': '17504839ee9ce2db'},\n",
       "  {'id': '175028999f42cec4', 'threadId': '175028999f42cec4'},\n",
       "  {'id': '1750225ff6b5a401', 'threadId': '1750225ff6b5a401'},\n",
       "  {'id': '174ff73e491a9783', 'threadId': '174ff73e491a9783'},\n",
       "  {'id': '174ff6b362358a2a', 'threadId': '174ff6b362358a2a'},\n",
       "  {'id': '174feb6b6cdfe557', 'threadId': '174feb6b6cdfe557'},\n",
       "  {'id': '174fe3748041d326', 'threadId': '174fe3748041d326'},\n",
       "  {'id': '174fcb9e2bd83732', 'threadId': '174fcb9e2bd83732'},\n",
       "  {'id': '174f8673682815c4', 'threadId': '174f8673682815c4'},\n",
       "  {'id': '174f7cf0ebc5f5c6', 'threadId': '174f7cf0ebc5f5c6'},\n",
       "  {'id': '174f7a30c238caa6', 'threadId': '174f7a30c238caa6'},\n",
       "  {'id': '174f3a1667db7044', 'threadId': '174f3a1667db7044'},\n",
       "  {'id': '174edb47d200bf19', 'threadId': '174edb47d200bf19'},\n",
       "  {'id': '174ed4548a75fc41', 'threadId': '174ed4548a75fc41'},\n",
       "  {'id': '174e8c33228d916d', 'threadId': '174e8c33228d916d'},\n",
       "  {'id': '174e8af311192d76', 'threadId': '174e8af311192d76'},\n",
       "  {'id': '174e82003db32919', 'threadId': '174e82003db32919'},\n",
       "  {'id': '174e5941eb87a084', 'threadId': '174e5941eb87a084'},\n",
       "  {'id': '174e410648165a52', 'threadId': '174e410648165a52'},\n",
       "  {'id': '174e3438c2339851', 'threadId': '174e3438c2339851'},\n",
       "  {'id': '174e2f8bb8403fda', 'threadId': '174e2f8bb8403fda'},\n",
       "  {'id': '174e092c19b9d72e', 'threadId': '174e092c19b9d72e'},\n",
       "  {'id': '174df682b20bab2b', 'threadId': '174df682b20bab2b'},\n",
       "  {'id': '174de951f7b643e6', 'threadId': '174de951f7b643e6'},\n",
       "  {'id': '174de13ed748e625', 'threadId': '174de13ed748e625'},\n",
       "  {'id': '174ddd24f00ea713', 'threadId': '174ddd24f00ea713'},\n",
       "  {'id': '174db9d40dd58705', 'threadId': '174db9d40dd58705'},\n",
       "  {'id': '174db6b9fd81fef7', 'threadId': '174db6b9fd81fef7'},\n",
       "  {'id': '174db6212f7e16be', 'threadId': '174db6212f7e16be'},\n",
       "  {'id': '174da2d7580f4abf', 'threadId': '174da2d7580f4abf'},\n",
       "  {'id': '174d97f57630c0ed', 'threadId': '174d97f57630c0ed'},\n",
       "  {'id': '174d8ac4c5aebbe4', 'threadId': '174d8ac4c5aebbe4'},\n",
       "  {'id': '174d4a217abb2396', 'threadId': '174d4a217abb2396'},\n",
       "  {'id': '174d385d768ba977', 'threadId': '174d385d768ba977'},\n",
       "  {'id': '174ce5f8a8e16b71', 'threadId': '174ce5f8a8e16b71'},\n",
       "  {'id': '174ca4d820c5aecf', 'threadId': '174ca4d820c5aecf'},\n",
       "  {'id': '174c9392db2febcd', 'threadId': '174c9392db2febcd'},\n",
       "  {'id': '174c6601b271550d', 'threadId': '174c6601b271550d'},\n",
       "  {'id': '174c4acf98766713', 'threadId': '174c4acf98766713'},\n",
       "  {'id': '174c4aa5cf22cc24', 'threadId': '174c4aa5cf22cc24'},\n",
       "  {'id': '174c44a4309e009e', 'threadId': '174c44a4309e009e'},\n",
       "  {'id': '174c4128553dff6b', 'threadId': '174c4128553dff6b'},\n",
       "  {'id': '174c187a6416c4ba', 'threadId': '174c187a6416c4ba'},\n",
       "  {'id': '174bfaa294077443', 'threadId': '174bfaa294077443'},\n",
       "  {'id': '174beec4c186a620', 'threadId': '174beec4c186a620'},\n",
       "  {'id': '174bc6dcc752a1c7', 'threadId': '174bc6dcc752a1c7'},\n",
       "  {'id': '174ba7f141187ced', 'threadId': '174ba7f141187ced'},\n",
       "  {'id': '174ba0cd073b36d6', 'threadId': '174ba0cd073b36d6'},\n",
       "  {'id': '174b9c6969c28f76', 'threadId': '174b9c6969c28f76'},\n",
       "  {'id': '174b79299d67cffb', 'threadId': '174b79299d67cffb'},\n",
       "  {'id': '174b754a2a445250', 'threadId': '174b754a2a445250'},\n",
       "  {'id': '174b61e14cd7c4a6', 'threadId': '174b61e14cd7c4a6'},\n",
       "  {'id': '174b5b4b907f0331', 'threadId': '174b5b4b907f0331'},\n",
       "  {'id': '174b49f6b3b44a84', 'threadId': '174b49f6b3b44a84'},\n",
       "  {'id': '174afb15a2014444', 'threadId': '174afb15a2014444'},\n",
       "  {'id': '174af79bc0875757', 'threadId': '174af79bc0875757'},\n",
       "  {'id': '174ab9e45e45ee4d', 'threadId': '174ab9e45e45ee4d'},\n",
       "  {'id': '174aa53ae02ce883', 'threadId': '174aa53ae02ce883'},\n",
       "  {'id': '174a59c156c04276', 'threadId': '174a59c156c04276'},\n",
       "  {'id': '174a19403ba632c1', 'threadId': '174a19403ba632c1'},\n",
       "  {'id': '174a0607313cfdaa', 'threadId': '174a0607313cfdaa'},\n",
       "  {'id': '174a0073d20b6f50', 'threadId': '174a0073d20b6f50'},\n",
       "  {'id': '1749d7cf6b8c85e4', 'threadId': '1749d7cf6b8c85e4'},\n",
       "  {'id': '1749b17e3e715a8c', 'threadId': '1749b17e3e715a8c'},\n",
       "  {'id': '1749ae0214df52dd', 'threadId': '1749ae0214df52dd'},\n",
       "  {'id': '17498e7f9b2b362d', 'threadId': '17498e7f9b2b362d'},\n",
       "  {'id': '17498643701ddf37', 'threadId': '17498643701ddf37'},\n",
       "  {'id': '17495b9a4dfae52e', 'threadId': '17495b9a4dfae52e'},\n",
       "  {'id': '174938e947b329d2', 'threadId': '174938e947b329d2'},\n",
       "  {'id': '174934835ae6d972', 'threadId': '174934835ae6d972'},\n",
       "  {'id': '1749213b7c3a4a80', 'threadId': '1749213b7c3a4a80'},\n",
       "  {'id': '1748b6c8d982f476', 'threadId': '1748b6c8d982f476'},\n",
       "  {'id': '1748646db4dcdd4e', 'threadId': '1748646db4dcdd4e'},\n",
       "  {'id': '174811fb2905e607', 'threadId': '174811fb2905e607'},\n",
       "  {'id': '1747c9bba9666eee', 'threadId': '1747c9bba9666eee'},\n",
       "  {'id': '1747c61fef59417d', 'threadId': '1747c61fef59417d'},\n",
       "  {'id': '1747bfa309c9516e', 'threadId': '1747bfa309c9516e'},\n",
       "  {'id': '174796ea1d3dbc06', 'threadId': '174796ea1d3dbc06'},\n",
       "  {'id': '1747708bcec50275', 'threadId': '1747708bcec50275'},\n",
       "  {'id': '17476d2ebd5b0342', 'threadId': '17476d2ebd5b0342'},\n",
       "  {'id': '17474da7e6e898d4', 'threadId': '17474da7e6e898d4'},\n",
       "  {'id': '17474526143dd377', 'threadId': '17474526143dd377'},\n",
       "  {'id': '174727cc20f4e971', 'threadId': '174727cc20f4e971'},\n",
       "  {'id': '174721cf917b40d0', 'threadId': '174721cf917b40d0'},\n",
       "  {'id': '17471acd94f65a57', 'threadId': '17471acd94f65a57'},\n",
       "  {'id': '1746f3b0f9c9d482', 'threadId': '1746f3b0f9c9d482'},\n",
       "  {'id': '1746f3911d103e3e', 'threadId': '1746f3911d103e3e'},\n",
       "  {'id': '1746f13310d7c635', 'threadId': '1746f13310d7c635'},\n",
       "  {'id': '1746e054ccc6a72c', 'threadId': '1746e054ccc6a72c'},\n",
       "  {'id': '1746de6c3e2bd8aa', 'threadId': '1746de6c3e2bd8aa'},\n",
       "  {'id': '1746cbb9e7be4d01', 'threadId': '1746cbb9e7be4d01'},\n",
       "  {'id': '1746c866848accb3', 'threadId': '1746c866848accb3'},\n",
       "  {'id': '1746c83a65e27eea', 'threadId': '1746c83a65e27eea'},\n",
       "  {'id': '17467be79a732320', 'threadId': '17467be79a732320'},\n",
       "  {'id': '17467961f2502512', 'threadId': '17467961f2502512'},\n",
       "  {'id': '1745e26c83e2d8f2', 'threadId': '1745e26c83e2d8f2'},\n",
       "  {'id': '1745861e0dc79023', 'threadId': '1745861e0dc79023'},\n",
       "  {'id': '17455623b21a1756', 'threadId': '17455623b21a1756'},\n",
       "  {'id': '1745316aade844c7', 'threadId': '1745316aade844c7'},\n",
       "  {'id': '174506575f69f37d', 'threadId': '174506575f69f37d'},\n",
       "  {'id': '1744f41d3361c116', 'threadId': '1744f41d3361c116'},\n",
       "  {'id': '1744b2fb87483da1', 'threadId': '1744b2fb87483da1'},\n",
       "  {'id': '1744a2aedc23febc', 'threadId': '1744a2aedc23febc'},\n",
       "  {'id': '17449f87d0dc45f6', 'threadId': '17449f87d0dc45f6'},\n",
       "  {'id': '17448d2436a69a85', 'threadId': '17448d2436a69a85'},\n",
       "  {'id': '17439755b93ba3a0', 'threadId': '17439755b93ba3a0'},\n",
       "  {'id': '174343f9fd9a8dd6', 'threadId': '174343f9fd9a8dd6'},\n",
       "  {'id': '17431559d23fbbf7', 'threadId': '17431559d23fbbf7'},\n",
       "  {'id': '1742ef7438780d7d', 'threadId': '1742ef7438780d7d'},\n",
       "  {'id': '1742cfd34c9996f4', 'threadId': '1742cfd34c9996f4'},\n",
       "  {'id': '1742c3448da4e672', 'threadId': '1742c3448da4e672'},\n",
       "  {'id': '1742a0c68b3800fc', 'threadId': '1742a0c68b3800fc'},\n",
       "  {'id': '17428a1a8c6dc485', 'threadId': '17428a1a8c6dc485'},\n",
       "  {'id': '174274bc5c9c1bd6', 'threadId': '174274bc5c9c1bd6'},\n",
       "  {'id': '1742723cab3af39b', 'threadId': '1742723cab3af39b'},\n",
       "  {'id': '17425eb6cdaa4e50', 'threadId': '17425eb6cdaa4e50'},\n",
       "  {'id': '1740fe6ecd0626ec', 'threadId': '1740fe6ecd0626ec'},\n",
       "  {'id': '1740d493e50b286a', 'threadId': '1740d493e50b286a'},\n",
       "  {'id': '1740bd8ee85a48aa', 'threadId': '1740bd8ee85a48aa'},\n",
       "  {'id': '1740af892d1dd764', 'threadId': '1740af892d1dd764'},\n",
       "  {'id': '174091c379330d04', 'threadId': '174091c379330d04'},\n",
       "  {'id': '174070dfcb6fe301', 'threadId': '174070dfcb6fe301'},\n",
       "  {'id': '174062be7ac8d47f', 'threadId': '174062be7ac8d47f'},\n",
       "  {'id': '17405c230ea94ae4', 'threadId': '17405c230ea94ae4'},\n",
       "  {'id': '174032527e8d2754', 'threadId': '174032527e8d2754'},\n",
       "  {'id': '17403121038f7994', 'threadId': '17403121038f7994'},\n",
       "  {'id': '17402cdaf99e28b5', 'threadId': '17402cdaf99e28b5'},\n",
       "  {'id': '17401df2e502bacd', 'threadId': '17401df2e502bacd'},\n",
       "  {'id': '173f15ab3e6d17a7', 'threadId': '173f15ab3e6d17a7'},\n",
       "  {'id': '173ed4fa2343af8e', 'threadId': '173ed4fa2343af8e'},\n",
       "  {'id': '173ed4d9c253790e', 'threadId': '173ed4d9c253790e'},\n",
       "  {'id': '173ec714b8954f65', 'threadId': '173ec714b8954f65'},\n",
       "  {'id': '173e93c9148d463c', 'threadId': '173e93c9148d463c'},\n",
       "  {'id': '173e6d6cc87eadf8', 'threadId': '173e6d6cc87eadf8'},\n",
       "  {'id': '173e3ffe9d111de7', 'threadId': '173e3ffe9d111de7'},\n",
       "  {'id': '173e28f5fd67c236', 'threadId': '173e28f5fd67c236'},\n",
       "  {'id': '173df07fce889d7d', 'threadId': '173df07fce889d7d'},\n",
       "  {'id': '173df07fcc9114f6', 'threadId': '173df07fcc9114f6'},\n",
       "  {'id': '173ddd326ea41819', 'threadId': '173ddd326ea41819'},\n",
       "  {'id': '173dc8912ab66ee2', 'threadId': '173dc8912ab66ee2'},\n",
       "  {'id': '173ce2b571fa7be7', 'threadId': '173ce2b571fa7be7'},\n",
       "  {'id': '173c9dfa2d4621e7', 'threadId': '173c9dfa2d4621e7'},\n",
       "  {'id': '173c7f88c29c352c', 'threadId': '173c7f88c29c352c'},\n",
       "  {'id': '173c7a88cbcab36a', 'threadId': '173c7a88cbcab36a'},\n",
       "  {'id': '173c52ffd0287724', 'threadId': '173c52ffd0287724'},\n",
       "  {'id': '173c46039a62242c', 'threadId': '173c46039a62242c'},\n",
       "  {'id': '173c000320e7aa70', 'threadId': '173c000320e7aa70'},\n",
       "  {'id': '173be303b10db157', 'threadId': '173be303b10db157'},\n",
       "  {'id': '173bdabaf5078f6b', 'threadId': '173bdabaf5078f6b'},\n",
       "  {'id': '173bafdfad3504a1', 'threadId': '173bafdfad3504a1'},\n",
       "  {'id': '173ba3409033d73f', 'threadId': '173ba3409033d73f'},\n",
       "  {'id': '173b9c76247b8012', 'threadId': '173b9c76247b8012'},\n",
       "  {'id': '173aecc7bcdc8d91', 'threadId': '173aecc7bcdc8d91'},\n",
       "  {'id': '173a944dbb320396', 'threadId': '173a944dbb320396'},\n",
       "  {'id': '173a534700617532', 'threadId': '173a534700617532'},\n",
       "  {'id': '173a4e40833290f4', 'threadId': '173a4e40833290f4'},\n",
       "  {'id': '173a3e0d3f4f5d81', 'threadId': '173a3e0d3f4f5d81'},\n",
       "  {'id': '173a123ae9ea3175', 'threadId': '173a123ae9ea3175'},\n",
       "  {'id': '1739bf23f7db2f8a', 'threadId': '1739bf23f7db2f8a'},\n",
       "  {'id': '1739a1eb5647c814', 'threadId': '1739a1eb5647c814'}],\n",
       " 'nextPageToken': '12641347343635093789',\n",
       " 'resultSizeEstimate': 511}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "filename = '/Users/markusmuller/python/projects/gmail-newsletter-db/scripts/id_query_17.09.2021_q3'\n",
    "infile = open(filename,'rb')\n",
    "id_list_3 = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "id_list_3"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'messages': [{'id': '17399a307f77cab8', 'threadId': '17399a307f77cab8'},\n",
       "  {'id': '173974f11ed7790a', 'threadId': '173974f11ed7790a'},\n",
       "  {'id': '17395b9d4a8ea12c', 'threadId': '17395b9d4a8ea12c'},\n",
       "  {'id': '1738d5b273ab1830', 'threadId': '1738d5b273ab1830'},\n",
       "  {'id': '1738533b8f2273bd', 'threadId': '1738533b8f2273bd'},\n",
       "  {'id': '1738136480f2a6a9', 'threadId': '1738136480f2a6a9'},\n",
       "  {'id': '1738034e8080a86d', 'threadId': '1738034e8080a86d'},\n",
       "  {'id': '1737d164df70df8b', 'threadId': '1737d164df70df8b'},\n",
       "  {'id': '1737ac6506ec325c', 'threadId': '1737ac6506ec325c'},\n",
       "  {'id': '17377c63fea4b846', 'threadId': '17377c63fea4b846'},\n",
       "  {'id': '1737649a940c39af', 'threadId': '1737649a940c39af'},\n",
       "  {'id': '173737a8746577a9', 'threadId': '173737a8746577a9'},\n",
       "  {'id': '17372e140111bdd8', 'threadId': '17372e140111bdd8'},\n",
       "  {'id': '17371ae0375b20b8', 'threadId': '17371ae0375b20b8'},\n",
       "  {'id': '1737099fe7305539', 'threadId': '1737099fe7305539'},\n",
       "  {'id': '173706721b68dcef', 'threadId': '173706721b68dcef'},\n",
       "  {'id': '1736b73cc9a39324', 'threadId': '1736b73cc9a39324'},\n",
       "  {'id': '1736b4a70a3a05c9', 'threadId': '1736b4a70a3a05c9'},\n",
       "  {'id': '173695112c82d5d7', 'threadId': '173695112c82d5d7'},\n",
       "  {'id': '173623c1ab056d16', 'threadId': '173623c1ab056d16'},\n",
       "  {'id': '1736134e69aec8df', 'threadId': '1736134e69aec8df'},\n",
       "  {'id': '1735f726146f215e', 'threadId': '1735f726146f215e'},\n",
       "  {'id': '1735bf68112e8ac9', 'threadId': '1735bf68112e8ac9'},\n",
       "  {'id': '1735bf3823136bae', 'threadId': '1735bf3823136bae'},\n",
       "  {'id': '1735bbf31ed991e2', 'threadId': '1735bbf31ed991e2'},\n",
       "  {'id': '173590ae4cc96178', 'threadId': '173590ae4cc96178'},\n",
       "  {'id': '1734f2af6170651d', 'threadId': '1734f2af6170651d'},\n",
       "  {'id': '1734f0374077ef48', 'threadId': '1734f0374077ef48'},\n",
       "  {'id': '1734ed444136c9a6', 'threadId': '1734ed444136c9a6'},\n",
       "  {'id': '1732af6cdbc23a8b', 'threadId': '1732af6cdbc23a8b'},\n",
       "  {'id': '1732af64ab3f6938', 'threadId': '1732af64ab3f6938'},\n",
       "  {'id': '1732af5a854e860a', 'threadId': '1732af5a854e860a'},\n",
       "  {'id': '1732998e16123caf', 'threadId': '1732998e16123caf'},\n",
       "  {'id': '173246f1ae71931e', 'threadId': '173246f1ae71931e'},\n",
       "  {'id': '173109991a528d09', 'threadId': '173109991a528d09'},\n",
       "  {'id': '173099ad0e87adc4', 'threadId': '173099ad0e87adc4'},\n",
       "  {'id': '173047499776bf48', 'threadId': '173047499776bf48'},\n",
       "  {'id': '172ff4e1904d78ea', 'threadId': '172ff4e1904d78ea'},\n",
       "  {'id': '172efdaf46f5ba09', 'threadId': '172efdaf46f5ba09'},\n",
       "  {'id': '172ece6c7379ddfe', 'threadId': '172ece6c7379ddfe'},\n",
       "  {'id': '172eab4dcd3c4997', 'threadId': '172eab4dcd3c4997'},\n",
       "  {'id': '172e58e413e2ff31', 'threadId': '172e58e413e2ff31'},\n",
       "  {'id': '172e3ef71b47dd5f', 'threadId': '172e3ef71b47dd5f'},\n",
       "  {'id': '172e067f0fde26b1', 'threadId': '172e067f0fde26b1'},\n",
       "  {'id': '172db550a8dfe6c0', 'threadId': '172db550a8dfe6c0'},\n",
       "  {'id': '172db506c6b92391', 'threadId': '172db506c6b92391'},\n",
       "  {'id': '172db4382d4830d4', 'threadId': '172db4382d4830d4'},\n",
       "  {'id': '172d5dac884c0cfa', 'threadId': '172d5dac884c0cfa'},\n",
       "  {'id': '172d5d99e84daf5b', 'threadId': '172d5d99e84daf5b'},\n",
       "  {'id': '172d5d83bfab0245', 'threadId': '172d5d83bfab0245'}],\n",
       " 'resultSizeEstimate': 50}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "as expected there is no nextPageToken, so we reached the end of our mailbox and we have no extracted every id "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Newsletter: KDnuggets\n",
    "- when you subscribe to KDnuggets you will get mails from two differrent mail addresses.\n",
    "- one for events and news (editor1@kdnuggets.com) and one with top stories (mattmayo@kdnuggets.com)\n",
    "- i want both but the first one is polluted with mails about events so i have to filter them later"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os                  # handle paths\n",
    "import base64                   # decrypt mail data\n",
    "from bs4 import BeautifulSoup   # parse data after decryption\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "search_str = \"from:KDnuggets\" \n",
    "filename = \"KDnuggets_ids_\"\n",
    "date = datetime.date.today().strftime(\"%m.%d.%Y\")\n",
    "filename = filename + date + \".pkl\"\n",
    "print(filename)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KDnuggets_ids_09.30.2021.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# paths\n",
    "pathToToken = '/Users/markusmuller/python/projects/content-db/gmail/token.json'\n",
    "data_path   = '/Users/markusmuller/python/projects/content-db/gmail/data'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "\n",
    "creds = Credentials.from_authorized_user_file(pathToToken, SCOPES)\n",
    "\n",
    "# connect to Gmail api\n",
    "service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "# request a list of all the messages\n",
    "result = service.users().messages().list(userId='markus.mueller.ds@gmail.com', q=search_str, maxResults=500).execute()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "result"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'messages': [{'id': '17c313783d7e09f1', 'threadId': '17c313783d7e09f1'},\n",
       "  {'id': '17c30c98e39c3de7', 'threadId': '17c30c98e39c3de7'},\n",
       "  {'id': '17c2b7d2843852d9', 'threadId': '17c2b7d2843852d9'},\n",
       "  {'id': '17c25e3aa627293e', 'threadId': '17c25e3aa627293e'},\n",
       "  {'id': '17c2311129d967a5', 'threadId': '17c2311129d967a5'},\n",
       "  {'id': '17c1c3c531e08e3e', 'threadId': '17c1c3c531e08e3e'},\n",
       "  {'id': '17c16e39b6a5d42e', 'threadId': '17c16e39b6a5d42e'},\n",
       "  {'id': '17c11c78f016205d', 'threadId': '17c11c78f016205d'},\n",
       "  {'id': '17c0d04c01474fa8', 'threadId': '17c0d04c01474fa8'},\n",
       "  {'id': '17c0c9b99548525d', 'threadId': '17c0c9b99548525d'},\n",
       "  {'id': '17c0796a03484489', 'threadId': '17c0796a03484489'},\n",
       "  {'id': '17c023c69e0bc049', 'threadId': '17c023c69e0bc049'},\n",
       "  {'id': '17bf839a7969c51f', 'threadId': '17bf839a7969c51f'},\n",
       "  {'id': '17bf2d743a2a4a64', 'threadId': '17bf2d743a2a4a64'},\n",
       "  {'id': '17bed324d52f36ee', 'threadId': '17bed324d52f36ee'},\n",
       "  {'id': '17be8bcdaa338b35', 'threadId': '17be8bcdaa338b35'},\n",
       "  {'id': '17be368851c51489', 'threadId': '17be368851c51489'},\n",
       "  {'id': '17bdafc60ea3954c', 'threadId': '17bdafc60ea3954c'},\n",
       "  {'id': '17bd41ed7e3689c9', 'threadId': '17bd41ed7e3689c9'},\n",
       "  {'id': '17bcef38c960fc3b', 'threadId': '17bcef38c960fc3b'},\n",
       "  {'id': '17bc92883487c59a', 'threadId': '17bc92883487c59a'},\n",
       "  {'id': '17bc4a99883075e1', 'threadId': '17bc4a99883075e1'},\n",
       "  {'id': '17bc47ca455dea43', 'threadId': '17bc47ca455dea43'},\n",
       "  {'id': '17bbf853dfebce27', 'threadId': '17bbf853dfebce27'},\n",
       "  {'id': '17bba2e6ee21ba8a', 'threadId': '17bba2e6ee21ba8a'},\n",
       "  {'id': '17bab5189de5ff8a', 'threadId': '17bab5189de5ff8a'},\n",
       "  {'id': '17ba5c0f63e5d7a6', 'threadId': '17ba5c0f63e5d7a6'},\n",
       "  {'id': '17ba598e729bf32b', 'threadId': '17ba598e729bf32b'},\n",
       "  {'id': '17ba0a53c538f36a', 'threadId': '17ba0a53c538f36a'},\n",
       "  {'id': '17b9d3e197b14d4f', 'threadId': '17b9d3e197b14d4f'},\n",
       "  {'id': '17b8c0b2c93af7d1', 'threadId': '17b8c0b2c93af7d1'},\n",
       "  {'id': '17b86ab90b36ee35', 'threadId': '17b86ab90b36ee35'},\n",
       "  {'id': '17b7c8c29c863548', 'threadId': '17b7c8c29c863548'},\n",
       "  {'id': '17b7c536cd4056ff', 'threadId': '17b7c536cd4056ff'},\n",
       "  {'id': '17b77a030916434b', 'threadId': '17b77a030916434b'},\n",
       "  {'id': '17b7213fcd2d544f', 'threadId': '17b7213fcd2d544f'},\n",
       "  {'id': '17b6f44da0eae264', 'threadId': '17b6f44da0eae264'},\n",
       "  {'id': '17b68035bfd5b604', 'threadId': '17b68035bfd5b604'},\n",
       "  {'id': '17b63061251c287e', 'threadId': '17b63061251c287e'},\n",
       "  {'id': '17b5da7ddeb42a02', 'threadId': '17b5da7ddeb42a02'},\n",
       "  {'id': '17b58c81ef295ea4', 'threadId': '17b58c81ef295ea4'},\n",
       "  {'id': '17b587ef70b09641', 'threadId': '17b587ef70b09641'},\n",
       "  {'id': '17b5390c85ea26ff', 'threadId': '17b5390c85ea26ff'},\n",
       "  {'id': '17b4e02fc2048dcc', 'threadId': '17b4e02fc2048dcc'},\n",
       "  {'id': '17b43f177da6552a', 'threadId': '17b43f177da6552a'},\n",
       "  {'id': '17b40785fb74b467', 'threadId': '17b40785fb74b467'},\n",
       "  {'id': '17b39dc773a967cd', 'threadId': '17b39dc773a967cd'},\n",
       "  {'id': '17b348465b5af49d', 'threadId': '17b348465b5af49d'},\n",
       "  {'id': '17b29f5b10ba35ef', 'threadId': '17b29f5b10ba35ef'},\n",
       "  {'id': '17b1fd83cfc3564e', 'threadId': '17b1fd83cfc3564e'},\n",
       "  {'id': '17b1a7b722cc5175', 'threadId': '17b1a7b722cc5175'},\n",
       "  {'id': '17b14e8515e8eae5', 'threadId': '17b14e8515e8eae5'},\n",
       "  {'id': '17b1077078112e18', 'threadId': '17b1077078112e18'},\n",
       "  {'id': '17b1036fbf83d6e5', 'threadId': '17b1036fbf83d6e5'},\n",
       "  {'id': '17b0b46c098ede10', 'threadId': '17b0b46c098ede10'},\n",
       "  {'id': '17b0131a29d55b1e', 'threadId': '17b0131a29d55b1e'},\n",
       "  {'id': '17afc3e0c305615a', 'threadId': '17afc3e0c305615a'},\n",
       "  {'id': '17af604c55a21738', 'threadId': '17af604c55a21738'},\n",
       "  {'id': '17af0dc83593c6b5', 'threadId': '17af0dc83593c6b5'},\n",
       "  {'id': '17aec658bb5267b4', 'threadId': '17aec658bb5267b4'},\n",
       "  {'id': '17aebea3ce085769', 'threadId': '17aebea3ce085769'},\n",
       "  {'id': '17ae715a8530ca19', 'threadId': '17ae715a8530ca19'},\n",
       "  {'id': '17ae1d5b1a2c2c42', 'threadId': '17ae1d5b1a2c2c42'},\n",
       "  {'id': '17ad7cb976ae437c', 'threadId': '17ad7cb976ae437c'},\n",
       "  {'id': '17ad26a2d8d44672', 'threadId': '17ad26a2d8d44672'},\n",
       "  {'id': '17acd51283deaa2c', 'threadId': '17acd51283deaa2c'},\n",
       "  {'id': '17ac85485ec12684', 'threadId': '17ac85485ec12684'},\n",
       "  {'id': '17ac361038e272fc', 'threadId': '17ac361038e272fc'},\n",
       "  {'id': '17ac304d7e773317', 'threadId': '17ac304d7e773317'},\n",
       "  {'id': '17abdff620a5e5dc', 'threadId': '17abdff620a5e5dc'},\n",
       "  {'id': '17ab3bf62f2638f0', 'threadId': '17ab3bf62f2638f0'},\n",
       "  {'id': '17aae6b168c06f71', 'threadId': '17aae6b168c06f71'},\n",
       "  {'id': '17aa96f288711fa3', 'threadId': '17aa96f288711fa3'},\n",
       "  {'id': '17aa43fec793adfb', 'threadId': '17aa43fec793adfb'},\n",
       "  {'id': '17aa41ef8ce608f1', 'threadId': '17aa41ef8ce608f1'},\n",
       "  {'id': '17a9f1c7c6879682', 'threadId': '17a9f1c7c6879682'},\n",
       "  {'id': '17a9ef84e71ecb99', 'threadId': '17a9ef84e71ecb99'},\n",
       "  {'id': '17a994fd6cda1991', 'threadId': '17a994fd6cda1991'},\n",
       "  {'id': '17a8faaa4a717626', 'threadId': '17a8faaa4a717626'},\n",
       "  {'id': '17a8a5550899807e', 'threadId': '17a8a5550899807e'},\n",
       "  {'id': '17a807247ffadc24', 'threadId': '17a807247ffadc24'},\n",
       "  {'id': '17a8033fe419cb17', 'threadId': '17a8033fe419cb17'},\n",
       "  {'id': '17a7b149cb1e4789', 'threadId': '17a7b149cb1e4789'},\n",
       "  {'id': '17a75419491ce6d0', 'threadId': '17a75419491ce6d0'},\n",
       "  {'id': '17a6b9d15f1762d0', 'threadId': '17a6b9d15f1762d0'},\n",
       "  {'id': '17a65cd969b20624', 'threadId': '17a65cd969b20624'},\n",
       "  {'id': '17a611dd5ffe1510', 'threadId': '17a611dd5ffe1510'},\n",
       "  {'id': '17a5c6e9c8bb8b22', 'threadId': '17a5c6e9c8bb8b22'},\n",
       "  {'id': '17a5bf336f424d3c', 'threadId': '17a5bf336f424d3c'},\n",
       "  {'id': '17a5737efede3b1d', 'threadId': '17a5737efede3b1d'},\n",
       "  {'id': '17a51b86a8599f25', 'threadId': '17a51b86a8599f25'},\n",
       "  {'id': '17a47551508b7a31', 'threadId': '17a47551508b7a31'},\n",
       "  {'id': '17a423c5802fbcd7', 'threadId': '17a423c5802fbcd7'},\n",
       "  {'id': '17a3d416d348d6cb', 'threadId': '17a3d416d348d6cb'},\n",
       "  {'id': '17a3866c8fcbb8a4', 'threadId': '17a3866c8fcbb8a4'},\n",
       "  {'id': '17a37f8f6eaa26bd', 'threadId': '17a37f8f6eaa26bd'},\n",
       "  {'id': '17a324f2fc8c8258', 'threadId': '17a324f2fc8c8258'},\n",
       "  {'id': '17a23892b59ac495', 'threadId': '17a23892b59ac495'},\n",
       "  {'id': '17a1e301769d065a', 'threadId': '17a1e301769d065a'},\n",
       "  {'id': '17a19a19ce1e6a7a', 'threadId': '17a19a19ce1e6a7a'},\n",
       "  {'id': '17a141167009c4bd', 'threadId': '17a141167009c4bd'},\n",
       "  {'id': '17a0eeedc1b57a17', 'threadId': '17a0eeedc1b57a17'},\n",
       "  {'id': '17a09c02c271af0a', 'threadId': '17a09c02c271af0a'},\n",
       "  {'id': '179ff89baf09a441', 'threadId': '179ff89baf09a441'},\n",
       "  {'id': '179fa866d4627b24', 'threadId': '179fa866d4627b24'},\n",
       "  {'id': '179f01307ae36460', 'threadId': '179f01307ae36460'},\n",
       "  {'id': '179efdafe179a284', 'threadId': '179efdafe179a284'},\n",
       "  {'id': '179e71d0d5fe868f', 'threadId': '179e71d0d5fe868f'},\n",
       "  {'id': '179e2489a219f99d', 'threadId': '179e2489a219f99d'},\n",
       "  {'id': '179db7453981d983', 'threadId': '179db7453981d983'},\n",
       "  {'id': '179d60b4d444f2dc', 'threadId': '179d60b4d444f2dc'},\n",
       "  {'id': '179d153b6b352a8b', 'threadId': '179d153b6b352a8b'},\n",
       "  {'id': '179c186c136ad9f3', 'threadId': '179c186c136ad9f3'},\n",
       "  {'id': '179be3c4dab016cb', 'threadId': '179be3c4dab016cb'},\n",
       "  {'id': '179b75e5156b1ac8', 'threadId': '179b75e5156b1ac8'},\n",
       "  {'id': '179b237eefa25e17', 'threadId': '179b237eefa25e17'},\n",
       "  {'id': '179ace24d98ead56', 'threadId': '179ace24d98ead56'},\n",
       "  {'id': '179a83063a14c645', 'threadId': '179a83063a14c645'},\n",
       "  {'id': '179a7b3b4920e4a5', 'threadId': '179a7b3b4920e4a5'},\n",
       "  {'id': '179a2c228fac4c6c', 'threadId': '179a2c228fac4c6c'},\n",
       "  {'id': '179a25511ee8e6c5', 'threadId': '179a25511ee8e6c5'},\n",
       "  {'id': '1799e0a90a5954c7', 'threadId': '1799e0a90a5954c7'},\n",
       "  {'id': '179934edf7f393db', 'threadId': '179934edf7f393db'},\n",
       "  {'id': '1798dfe0e276da7c', 'threadId': '1798dfe0e276da7c'},\n",
       "  {'id': '1798412f121d0ec1', 'threadId': '1798412f121d0ec1'},\n",
       "  {'id': '17983b55c16d3189', 'threadId': '17983b55c16d3189'},\n",
       "  {'id': '1797eb57ffd4b94b', 'threadId': '1797eb57ffd4b94b'},\n",
       "  {'id': '1797e8a6eea7a2c6', 'threadId': '1797e8a6eea7a2c6'},\n",
       "  {'id': '1796f4a3135b9517', 'threadId': '1796f4a3135b9517'},\n",
       "  {'id': '17969f13f21cd62e', 'threadId': '17969f13f21cd62e'},\n",
       "  {'id': '1796006c02d9a47a', 'threadId': '1796006c02d9a47a'},\n",
       "  {'id': '1795fcfc56f17e94', 'threadId': '1795fcfc56f17e94'},\n",
       "  {'id': '1795a3de0e88fd42', 'threadId': '1795a3de0e88fd42'},\n",
       "  {'id': '17955f107bf8050e', 'threadId': '17955f107bf8050e'},\n",
       "  {'id': '1794b3474b4806f2', 'threadId': '1794b3474b4806f2'},\n",
       "  {'id': '17945e90b7d8c627', 'threadId': '17945e90b7d8c627'},\n",
       "  {'id': '1794103e86b42839', 'threadId': '1794103e86b42839'},\n",
       "  {'id': '17940b9904f0d07a', 'threadId': '17940b9904f0d07a'},\n",
       "  {'id': '1793bcf94685bb49', 'threadId': '1793bcf94685bb49'},\n",
       "  {'id': '17936717395173ad', 'threadId': '17936717395173ad'},\n",
       "  {'id': '17931e40387a9207', 'threadId': '17931e40387a9207'},\n",
       "  {'id': '179273a51500f89e', 'threadId': '179273a51500f89e'},\n",
       "  {'id': '1792213a90b510ac', 'threadId': '1792213a90b510ac'},\n",
       "  {'id': '17917babfd30830b', 'threadId': '17917babfd30830b'},\n",
       "  {'id': '179126e6d0b67440', 'threadId': '179126e6d0b67440'},\n",
       "  {'id': '1790dd847a69156a', 'threadId': '1790dd847a69156a'},\n",
       "  {'id': '17909622cc1f5086', 'threadId': '17909622cc1f5086'},\n",
       "  {'id': '17903249177c633d', 'threadId': '17903249177c633d'},\n",
       "  {'id': '178fdd4d4108f93e', 'threadId': '178fdd4d4108f93e'},\n",
       "  {'id': '178f8b456df5ebf6', 'threadId': '178f8b456df5ebf6'},\n",
       "  {'id': '178f8a6130530480', 'threadId': '178f8a6130530480'},\n",
       "  {'id': '178f418415bc5370', 'threadId': '178f418415bc5370'},\n",
       "  {'id': '178f3ba9910ee5a6', 'threadId': '178f3ba9910ee5a6'},\n",
       "  {'id': '178df17be7d506f8', 'threadId': '178df17be7d506f8'},\n",
       "  {'id': '178d9ec9264928ae', 'threadId': '178d9ec9264928ae'},\n",
       "  {'id': '178cfc031cc8e267', 'threadId': '178cfc031cc8e267'},\n",
       "  {'id': '178cf890adc3faf2', 'threadId': '178cf890adc3faf2'},\n",
       "  {'id': '178ba95c813f0ddf', 'threadId': '178ba95c813f0ddf'},\n",
       "  {'id': '178b5717c5656150', 'threadId': '178b5717c5656150'},\n",
       "  {'id': '178b15f275de26da', 'threadId': '178b15f275de26da'},\n",
       "  {'id': '178b0c304256b046', 'threadId': '178b0c304256b046'},\n",
       "  {'id': '178abd90a49672d6', 'threadId': '178abd90a49672d6'},\n",
       "  {'id': '178ab9017928e3a1', 'threadId': '178ab9017928e3a1'},\n",
       "  {'id': '178ab61ff814cb16', 'threadId': '178ab61ff814cb16'},\n",
       "  {'id': '178a10c224a3f31b', 'threadId': '178a10c224a3f31b'},\n",
       "  {'id': '17897035e31091f6', 'threadId': '17897035e31091f6'},\n",
       "  {'id': '17891997b96085c8', 'threadId': '17891997b96085c8'},\n",
       "  {'id': '1788c3c3ee108927', 'threadId': '1788c3c3ee108927'},\n",
       "  {'id': '1788784746a701f1', 'threadId': '1788784746a701f1'},\n",
       "  {'id': '1788227d3a713b88', 'threadId': '1788227d3a713b88'},\n",
       "  {'id': '1787d7f22a6af74c', 'threadId': '1787d7f22a6af74c'},\n",
       "  {'id': '1787d0384068c877', 'threadId': '1787d0384068c877'},\n",
       "  {'id': '178781d62a4d6a65', 'threadId': '178781d62a4d6a65'},\n",
       "  {'id': '17872f6d194c90c4', 'threadId': '17872f6d194c90c4'},\n",
       "  {'id': '1786dc6890941562', 'threadId': '1786dc6890941562'},\n",
       "  {'id': '17868b7b71291fb5', 'threadId': '17868b7b71291fb5'},\n",
       "  {'id': '178687c36b487313', 'threadId': '178687c36b487313'},\n",
       "  {'id': '178647209836f5bb', 'threadId': '178647209836f5bb'},\n",
       "  {'id': '17863413bc124e05', 'threadId': '17863413bc124e05'},\n",
       "  {'id': '1785e66cc1e0191c', 'threadId': '1785e66cc1e0191c'},\n",
       "  {'id': '178592e11ef81caf', 'threadId': '178592e11ef81caf'},\n",
       "  {'id': '1784ee5ac7370556', 'threadId': '1784ee5ac7370556'},\n",
       "  {'id': '1784a25b3ba92ba3', 'threadId': '1784a25b3ba92ba3'},\n",
       "  {'id': '17844623442d9d37', 'threadId': '17844623442d9d37'},\n",
       "  {'id': '17840b87c1339322', 'threadId': '17840b87c1339322'},\n",
       "  {'id': '178405a07123b926', 'threadId': '178405a07123b926'},\n",
       "  {'id': '1783a5e99de7e402', 'threadId': '1783a5e99de7e402'},\n",
       "  {'id': '1782b1def19eee63', 'threadId': '1782b1def19eee63'},\n",
       "  {'id': '17821036dc7dabdc', 'threadId': '17821036dc7dabdc'},\n",
       "  {'id': '1782084c8cd65901', 'threadId': '1782084c8cd65901'},\n",
       "  {'id': '1781bc644f161d89', 'threadId': '1781bc644f161d89'},\n",
       "  {'id': '1781b81abf5162ec', 'threadId': '1781b81abf5162ec'},\n",
       "  {'id': '17816701fa1d9c13', 'threadId': '17816701fa1d9c13'},\n",
       "  {'id': '1781126f55307a59', 'threadId': '1781126f55307a59'},\n",
       "  {'id': '178076a41dea0508', 'threadId': '178076a41dea0508'},\n",
       "  {'id': '177fd57b08afc2c3', 'threadId': '177fd57b08afc2c3'},\n",
       "  {'id': '177f7f9b3adf06c1', 'threadId': '177f7f9b3adf06c1'},\n",
       "  {'id': '177f263154fe692a', 'threadId': '177f263154fe692a'},\n",
       "  {'id': '177e81d5244dc681', 'threadId': '177e81d5244dc681'},\n",
       "  {'id': '177e31287d40f5eb', 'threadId': '177e31287d40f5eb'},\n",
       "  {'id': '177dd9258f962d16', 'threadId': '177dd9258f962d16'},\n",
       "  {'id': '177d39fb6e1f2d30', 'threadId': '177d39fb6e1f2d30'},\n",
       "  {'id': '177d3685af309fce', 'threadId': '177d3685af309fce'},\n",
       "  {'id': '177ce995cf929b9e', 'threadId': '177ce995cf929b9e'},\n",
       "  {'id': '177ce347bf18e056', 'threadId': '177ce347bf18e056'},\n",
       "  {'id': '177c91b9d81d2cdc', 'threadId': '177c91b9d81d2cdc'},\n",
       "  {'id': '177bf185752db2d8', 'threadId': '177bf185752db2d8'},\n",
       "  {'id': '177b9969ce3d5dfd', 'threadId': '177b9969ce3d5dfd'},\n",
       "  {'id': '177b45f2a8bd9b90', 'threadId': '177b45f2a8bd9b90'},\n",
       "  {'id': '177aff7ba05c9715', 'threadId': '177aff7ba05c9715'},\n",
       "  {'id': '177ab76014d0b611', 'threadId': '177ab76014d0b611'},\n",
       "  {'id': '17795ee8b28f719f', 'threadId': '17795ee8b28f719f'},\n",
       "  {'id': '1778baf6fcc66234', 'threadId': '1778baf6fcc66234'},\n",
       "  {'id': '1778b624badf26e0', 'threadId': '1778b624badf26e0'},\n",
       "  {'id': '17786247effe1d3f', 'threadId': '17786247effe1d3f'},\n",
       "  {'id': '17780dfb6d4d4b4f', 'threadId': '17780dfb6d4d4b4f'},\n",
       "  {'id': '177776d40891949c', 'threadId': '177776d40891949c'},\n",
       "  {'id': '1777174438d80a1c', 'threadId': '1777174438d80a1c'},\n",
       "  {'id': '17767ac5baf63530', 'threadId': '17767ac5baf63530'},\n",
       "  {'id': '1776735335118de3', 'threadId': '1776735335118de3'},\n",
       "  {'id': '17762334622064d0', 'threadId': '17762334622064d0'},\n",
       "  {'id': '17752bbef8a6f556', 'threadId': '17752bbef8a6f556'},\n",
       "  {'id': '1774e3f0bd56150f', 'threadId': '1774e3f0bd56150f'},\n",
       "  {'id': '17748541ed786dda', 'threadId': '17748541ed786dda'},\n",
       "  {'id': '1774444a02454d67', 'threadId': '1774444a02454d67'},\n",
       "  {'id': '177434d938813500', 'threadId': '177434d938813500'},\n",
       "  {'id': '1773eca6f6124ac9', 'threadId': '1773eca6f6124ac9'},\n",
       "  {'id': '1773e07419feab21', 'threadId': '1773e07419feab21'},\n",
       "  {'id': '17738ee20260a5ce', 'threadId': '17738ee20260a5ce'},\n",
       "  {'id': '17735fb1801e86e6', 'threadId': '17735fb1801e86e6'},\n",
       "  {'id': '1772ec196fb2df68', 'threadId': '1772ec196fb2df68'},\n",
       "  {'id': '1772952c6b776b34', 'threadId': '1772952c6b776b34'},\n",
       "  {'id': '177249aa3cf6e400', 'threadId': '177249aa3cf6e400'},\n",
       "  {'id': '17720525828ab56b', 'threadId': '17720525828ab56b'},\n",
       "  {'id': '1771f05b905465a8', 'threadId': '1771f05b905465a8'},\n",
       "  {'id': '17714c6c569381ca', 'threadId': '17714c6c569381ca'},\n",
       "  {'id': '1770aa3bf157364e', 'threadId': '1770aa3bf157364e'},\n",
       "  {'id': '176fb4faa3b7f9f4', 'threadId': '176fb4faa3b7f9f4'},\n",
       "  {'id': '176f62df74cc48da', 'threadId': '176f62df74cc48da'},\n",
       "  {'id': '176e7acc616eba08', 'threadId': '176e7acc616eba08'},\n",
       "  {'id': '176e3299819cfca6', 'threadId': '176e3299819cfca6'},\n",
       "  {'id': '176d75ba4e8888bf', 'threadId': '176d75ba4e8888bf'},\n",
       "  {'id': '176d6f9f0d4e7e6b', 'threadId': '176d6f9f0d4e7e6b'},\n",
       "  {'id': '176a89b7048734ad', 'threadId': '176a89b7048734ad'},\n",
       "  {'id': '1769e98c65012e01', 'threadId': '1769e98c65012e01'},\n",
       "  {'id': '1768f262a450ec89', 'threadId': '1768f262a450ec89'},\n",
       "  {'id': '1767a90b2fdf76e1', 'threadId': '1767a90b2fdf76e1'},\n",
       "  {'id': '176751cb53eed43c', 'threadId': '176751cb53eed43c'},\n",
       "  {'id': '1766ffb6d47a04ee', 'threadId': '1766ffb6d47a04ee'},\n",
       "  {'id': '1766b0b6fbf19051', 'threadId': '1766b0b6fbf19051'},\n",
       "  {'id': '17665a30b52053ee', 'threadId': '17665a30b52053ee'},\n",
       "  {'id': '17660ba0d96a7b98', 'threadId': '17660ba0d96a7b98'},\n",
       "  {'id': '1765665127ad61b5', 'threadId': '1765665127ad61b5'},\n",
       "  {'id': '1764c40740e6f01c', 'threadId': '1764c40740e6f01c'},\n",
       "  {'id': '1764bd2d075243d3', 'threadId': '1764bd2d075243d3'},\n",
       "  {'id': '176473ee7fa0e26e', 'threadId': '176473ee7fa0e26e'},\n",
       "  {'id': '17646dea9d1edc88', 'threadId': '17646dea9d1edc88'},\n",
       "  {'id': '17641b3c43336edf', 'threadId': '17641b3c43336edf'},\n",
       "  {'id': '1763c9204123734c', 'threadId': '1763c9204123734c'},\n",
       "  {'id': '1763269db16b930d', 'threadId': '1763269db16b930d'},\n",
       "  {'id': '17627dd412a63e48', 'threadId': '17627dd412a63e48'},\n",
       "  {'id': '17623a95a0ec5259', 'threadId': '17623a95a0ec5259'},\n",
       "  {'id': '17622e76816d6aca', 'threadId': '17622e76816d6aca'},\n",
       "  {'id': '1761d8b07b5e9254', 'threadId': '1761d8b07b5e9254'},\n",
       "  {'id': '176189929222107c', 'threadId': '176189929222107c'},\n",
       "  {'id': '1760e4d17b1b8ca4', 'threadId': '1760e4d17b1b8ca4'},\n",
       "  {'id': '175f48df9fcc09df', 'threadId': '175f48df9fcc09df'},\n",
       "  {'id': '175ea4004470af07', 'threadId': '175ea4004470af07'},\n",
       "  {'id': '175e50a850fac0ab', 'threadId': '175e50a850fac0ab'},\n",
       "  {'id': '175db062c2b8cc65', 'threadId': '175db062c2b8cc65'},\n",
       "  {'id': '175d5a5edfde246a', 'threadId': '175d5a5edfde246a'},\n",
       "  {'id': '175c63b56db41310', 'threadId': '175c63b56db41310'},\n",
       "  {'id': '175b6fba99e39ab9', 'threadId': '175b6fba99e39ab9'},\n",
       "  {'id': '175b6a447147a0b8', 'threadId': '175b6a447147a0b8'},\n",
       "  {'id': '175b1868571e4920', 'threadId': '175b1868571e4920'},\n",
       "  {'id': '1759ccb11842ceca', 'threadId': '1759ccb11842ceca'},\n",
       "  {'id': '17597eb3b97c9e02', 'threadId': '17597eb3b97c9e02'},\n",
       "  {'id': '17593787170609f5', 'threadId': '17593787170609f5'},\n",
       "  {'id': '175935a2ab23b419', 'threadId': '175935a2ab23b419'},\n",
       "  {'id': '1758d5e59e5249c8', 'threadId': '1758d5e59e5249c8'},\n",
       "  {'id': '1757de6e364d0f11', 'threadId': '1757de6e364d0f11'},\n",
       "  {'id': '175736268b558f9f', 'threadId': '175736268b558f9f'},\n",
       "  {'id': '1756e8ab1b10af1a', 'threadId': '1756e8ab1b10af1a'},\n",
       "  {'id': '175691b09657d232', 'threadId': '175691b09657d232'},\n",
       "  {'id': '17563f4c719a9778', 'threadId': '17563f4c719a9778'},\n",
       "  {'id': '17559da70ab64f59', 'threadId': '17559da70ab64f59'},\n",
       "  {'id': '1755493e03ade081', 'threadId': '1755493e03ade081'},\n",
       "  {'id': '1754fa51da1f544b', 'threadId': '1754fa51da1f544b'},\n",
       "  {'id': '1754f68e64a7aedc', 'threadId': '1754f68e64a7aedc'},\n",
       "  {'id': '1754ae2d9a4c8dcc', 'threadId': '1754ae2d9a4c8dcc'},\n",
       "  {'id': '1754a79b012434b1', 'threadId': '1754a79b012434b1'},\n",
       "  {'id': '17535cdf4bb537fa', 'threadId': '17535cdf4bb537fa'},\n",
       "  {'id': '1752b468154ab3c1', 'threadId': '1752b468154ab3c1'},\n",
       "  {'id': '1751bd390f44c937', 'threadId': '1751bd390f44c937'},\n",
       "  {'id': '17511d40e1674ec5', 'threadId': '17511d40e1674ec5'},\n",
       "  {'id': '175073a1f6c5648a', 'threadId': '175073a1f6c5648a'},\n",
       "  {'id': '175028999f42cec4', 'threadId': '175028999f42cec4'},\n",
       "  {'id': '1750225ff6b5a401', 'threadId': '1750225ff6b5a401'},\n",
       "  {'id': '174fcb9e2bd83732', 'threadId': '174fcb9e2bd83732'},\n",
       "  {'id': '174f7cf0ebc5f5c6', 'threadId': '174f7cf0ebc5f5c6'},\n",
       "  {'id': '174edb47d200bf19', 'threadId': '174edb47d200bf19'},\n",
       "  {'id': '174e3438c2339851', 'threadId': '174e3438c2339851'},\n",
       "  {'id': '174de951f7b643e6', 'threadId': '174de951f7b643e6'},\n",
       "  {'id': '174de13ed748e625', 'threadId': '174de13ed748e625'},\n",
       "  {'id': '174d4a217abb2396', 'threadId': '174d4a217abb2396'},\n",
       "  {'id': '174ca4d820c5aecf', 'threadId': '174ca4d820c5aecf'},\n",
       "  {'id': '174c44a4309e009e', 'threadId': '174c44a4309e009e'},\n",
       "  {'id': '174bfaa294077443', 'threadId': '174bfaa294077443'},\n",
       "  {'id': '174ba7f141187ced', 'threadId': '174ba7f141187ced'},\n",
       "  {'id': '174ba0cd073b36d6', 'threadId': '174ba0cd073b36d6'},\n",
       "  {'id': '174b5b4b907f0331', 'threadId': '174b5b4b907f0331'},\n",
       "  {'id': '174afb15a2014444', 'threadId': '174afb15a2014444'},\n",
       "  {'id': '174ab9e45e45ee4d', 'threadId': '174ab9e45e45ee4d'},\n",
       "  {'id': '174a59c156c04276', 'threadId': '174a59c156c04276'},\n",
       "  {'id': '174a19403ba632c1', 'threadId': '174a19403ba632c1'},\n",
       "  {'id': '1749b17e3e715a8c', 'threadId': '1749b17e3e715a8c'},\n",
       "  {'id': '1747708bcec50275', 'threadId': '1747708bcec50275'},\n",
       "  {'id': '174727cc20f4e971', 'threadId': '174727cc20f4e971'},\n",
       "  {'id': '174721cf917b40d0', 'threadId': '174721cf917b40d0'},\n",
       "  {'id': '1746cbb9e7be4d01', 'threadId': '1746cbb9e7be4d01'},\n",
       "  {'id': '1746c83a65e27eea', 'threadId': '1746c83a65e27eea'},\n",
       "  {'id': '17467961f2502512', 'threadId': '17467961f2502512'},\n",
       "  {'id': '1745e26c83e2d8f2', 'threadId': '1745e26c83e2d8f2'},\n",
       "  {'id': '1745316aade844c7', 'threadId': '1745316aade844c7'},\n",
       "  {'id': '17448d2436a69a85', 'threadId': '17448d2436a69a85'},\n",
       "  {'id': '17439755b93ba3a0', 'threadId': '17439755b93ba3a0'},\n",
       "  {'id': '1742ef7438780d7d', 'threadId': '1742ef7438780d7d'},\n",
       "  {'id': '1742a0c68b3800fc', 'threadId': '1742a0c68b3800fc'},\n",
       "  {'id': '1740af892d1dd764', 'threadId': '1740af892d1dd764'},\n",
       "  {'id': '174062be7ac8d47f', 'threadId': '174062be7ac8d47f'},\n",
       "  {'id': '17405c230ea94ae4', 'threadId': '17405c230ea94ae4'},\n",
       "  {'id': '173f15ab3e6d17a7', 'threadId': '173f15ab3e6d17a7'},\n",
       "  {'id': '173e6d6cc87eadf8', 'threadId': '173e6d6cc87eadf8'},\n",
       "  {'id': '173e28f5fd67c236', 'threadId': '173e28f5fd67c236'},\n",
       "  {'id': '173dc8912ab66ee2', 'threadId': '173dc8912ab66ee2'},\n",
       "  {'id': '173ce2b571fa7be7', 'threadId': '173ce2b571fa7be7'},\n",
       "  {'id': '173c7f88c29c352c', 'threadId': '173c7f88c29c352c'},\n",
       "  {'id': '173be303b10db157', 'threadId': '173be303b10db157'},\n",
       "  {'id': '173bdabaf5078f6b', 'threadId': '173bdabaf5078f6b'},\n",
       "  {'id': '173a944dbb320396', 'threadId': '173a944dbb320396'},\n",
       "  {'id': '1739a1eb5647c814', 'threadId': '1739a1eb5647c814'},\n",
       "  {'id': '17399a307f77cab8', 'threadId': '17399a307f77cab8'},\n",
       "  {'id': '1738533b8f2273bd', 'threadId': '1738533b8f2273bd'},\n",
       "  {'id': '1738136480f2a6a9', 'threadId': '1738136480f2a6a9'},\n",
       "  {'id': '1737ac6506ec325c', 'threadId': '1737ac6506ec325c'},\n",
       "  {'id': '1737649a940c39af', 'threadId': '1737649a940c39af'},\n",
       "  {'id': '173706721b68dcef', 'threadId': '173706721b68dcef'},\n",
       "  {'id': '1736b4a70a3a05c9', 'threadId': '1736b4a70a3a05c9'},\n",
       "  {'id': '1736134e69aec8df', 'threadId': '1736134e69aec8df'}],\n",
       " 'resultSizeEstimate': 349}"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# save id as pkl\n",
    "with open(os.path.join(data_path, filename), 'wb') as f:\n",
    "    pickle.dump(result, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# for loop to get id \n",
    "for id in result['messages']:\n",
    "    print(id['id'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17c313783d7e09f1\n",
      "17c30c98e39c3de7\n",
      "17c2b7d2843852d9\n",
      "17c25e3aa627293e\n",
      "17c2311129d967a5\n",
      "17c1c3c531e08e3e\n",
      "17c16e39b6a5d42e\n",
      "17c11c78f016205d\n",
      "17c0d04c01474fa8\n",
      "17c0c9b99548525d\n",
      "17c0796a03484489\n",
      "17c023c69e0bc049\n",
      "17bf839a7969c51f\n",
      "17bf2d743a2a4a64\n",
      "17bed324d52f36ee\n",
      "17be8bcdaa338b35\n",
      "17be368851c51489\n",
      "17bdafc60ea3954c\n",
      "17bd41ed7e3689c9\n",
      "17bcef38c960fc3b\n",
      "17bc92883487c59a\n",
      "17bc4a99883075e1\n",
      "17bc47ca455dea43\n",
      "17bbf853dfebce27\n",
      "17bba2e6ee21ba8a\n",
      "17bab5189de5ff8a\n",
      "17ba5c0f63e5d7a6\n",
      "17ba598e729bf32b\n",
      "17ba0a53c538f36a\n",
      "17b9d3e197b14d4f\n",
      "17b8c0b2c93af7d1\n",
      "17b86ab90b36ee35\n",
      "17b7c8c29c863548\n",
      "17b7c536cd4056ff\n",
      "17b77a030916434b\n",
      "17b7213fcd2d544f\n",
      "17b6f44da0eae264\n",
      "17b68035bfd5b604\n",
      "17b63061251c287e\n",
      "17b5da7ddeb42a02\n",
      "17b58c81ef295ea4\n",
      "17b587ef70b09641\n",
      "17b5390c85ea26ff\n",
      "17b4e02fc2048dcc\n",
      "17b43f177da6552a\n",
      "17b40785fb74b467\n",
      "17b39dc773a967cd\n",
      "17b348465b5af49d\n",
      "17b29f5b10ba35ef\n",
      "17b1fd83cfc3564e\n",
      "17b1a7b722cc5175\n",
      "17b14e8515e8eae5\n",
      "17b1077078112e18\n",
      "17b1036fbf83d6e5\n",
      "17b0b46c098ede10\n",
      "17b0131a29d55b1e\n",
      "17afc3e0c305615a\n",
      "17af604c55a21738\n",
      "17af0dc83593c6b5\n",
      "17aec658bb5267b4\n",
      "17aebea3ce085769\n",
      "17ae715a8530ca19\n",
      "17ae1d5b1a2c2c42\n",
      "17ad7cb976ae437c\n",
      "17ad26a2d8d44672\n",
      "17acd51283deaa2c\n",
      "17ac85485ec12684\n",
      "17ac361038e272fc\n",
      "17ac304d7e773317\n",
      "17abdff620a5e5dc\n",
      "17ab3bf62f2638f0\n",
      "17aae6b168c06f71\n",
      "17aa96f288711fa3\n",
      "17aa43fec793adfb\n",
      "17aa41ef8ce608f1\n",
      "17a9f1c7c6879682\n",
      "17a9ef84e71ecb99\n",
      "17a994fd6cda1991\n",
      "17a8faaa4a717626\n",
      "17a8a5550899807e\n",
      "17a807247ffadc24\n",
      "17a8033fe419cb17\n",
      "17a7b149cb1e4789\n",
      "17a75419491ce6d0\n",
      "17a6b9d15f1762d0\n",
      "17a65cd969b20624\n",
      "17a611dd5ffe1510\n",
      "17a5c6e9c8bb8b22\n",
      "17a5bf336f424d3c\n",
      "17a5737efede3b1d\n",
      "17a51b86a8599f25\n",
      "17a47551508b7a31\n",
      "17a423c5802fbcd7\n",
      "17a3d416d348d6cb\n",
      "17a3866c8fcbb8a4\n",
      "17a37f8f6eaa26bd\n",
      "17a324f2fc8c8258\n",
      "17a23892b59ac495\n",
      "17a1e301769d065a\n",
      "17a19a19ce1e6a7a\n",
      "17a141167009c4bd\n",
      "17a0eeedc1b57a17\n",
      "17a09c02c271af0a\n",
      "179ff89baf09a441\n",
      "179fa866d4627b24\n",
      "179f01307ae36460\n",
      "179efdafe179a284\n",
      "179e71d0d5fe868f\n",
      "179e2489a219f99d\n",
      "179db7453981d983\n",
      "179d60b4d444f2dc\n",
      "179d153b6b352a8b\n",
      "179c186c136ad9f3\n",
      "179be3c4dab016cb\n",
      "179b75e5156b1ac8\n",
      "179b237eefa25e17\n",
      "179ace24d98ead56\n",
      "179a83063a14c645\n",
      "179a7b3b4920e4a5\n",
      "179a2c228fac4c6c\n",
      "179a25511ee8e6c5\n",
      "1799e0a90a5954c7\n",
      "179934edf7f393db\n",
      "1798dfe0e276da7c\n",
      "1798412f121d0ec1\n",
      "17983b55c16d3189\n",
      "1797eb57ffd4b94b\n",
      "1797e8a6eea7a2c6\n",
      "1796f4a3135b9517\n",
      "17969f13f21cd62e\n",
      "1796006c02d9a47a\n",
      "1795fcfc56f17e94\n",
      "1795a3de0e88fd42\n",
      "17955f107bf8050e\n",
      "1794b3474b4806f2\n",
      "17945e90b7d8c627\n",
      "1794103e86b42839\n",
      "17940b9904f0d07a\n",
      "1793bcf94685bb49\n",
      "17936717395173ad\n",
      "17931e40387a9207\n",
      "179273a51500f89e\n",
      "1792213a90b510ac\n",
      "17917babfd30830b\n",
      "179126e6d0b67440\n",
      "1790dd847a69156a\n",
      "17909622cc1f5086\n",
      "17903249177c633d\n",
      "178fdd4d4108f93e\n",
      "178f8b456df5ebf6\n",
      "178f8a6130530480\n",
      "178f418415bc5370\n",
      "178f3ba9910ee5a6\n",
      "178df17be7d506f8\n",
      "178d9ec9264928ae\n",
      "178cfc031cc8e267\n",
      "178cf890adc3faf2\n",
      "178ba95c813f0ddf\n",
      "178b5717c5656150\n",
      "178b15f275de26da\n",
      "178b0c304256b046\n",
      "178abd90a49672d6\n",
      "178ab9017928e3a1\n",
      "178ab61ff814cb16\n",
      "178a10c224a3f31b\n",
      "17897035e31091f6\n",
      "17891997b96085c8\n",
      "1788c3c3ee108927\n",
      "1788784746a701f1\n",
      "1788227d3a713b88\n",
      "1787d7f22a6af74c\n",
      "1787d0384068c877\n",
      "178781d62a4d6a65\n",
      "17872f6d194c90c4\n",
      "1786dc6890941562\n",
      "17868b7b71291fb5\n",
      "178687c36b487313\n",
      "178647209836f5bb\n",
      "17863413bc124e05\n",
      "1785e66cc1e0191c\n",
      "178592e11ef81caf\n",
      "1784ee5ac7370556\n",
      "1784a25b3ba92ba3\n",
      "17844623442d9d37\n",
      "17840b87c1339322\n",
      "178405a07123b926\n",
      "1783a5e99de7e402\n",
      "1782b1def19eee63\n",
      "17821036dc7dabdc\n",
      "1782084c8cd65901\n",
      "1781bc644f161d89\n",
      "1781b81abf5162ec\n",
      "17816701fa1d9c13\n",
      "1781126f55307a59\n",
      "178076a41dea0508\n",
      "177fd57b08afc2c3\n",
      "177f7f9b3adf06c1\n",
      "177f263154fe692a\n",
      "177e81d5244dc681\n",
      "177e31287d40f5eb\n",
      "177dd9258f962d16\n",
      "177d39fb6e1f2d30\n",
      "177d3685af309fce\n",
      "177ce995cf929b9e\n",
      "177ce347bf18e056\n",
      "177c91b9d81d2cdc\n",
      "177bf185752db2d8\n",
      "177b9969ce3d5dfd\n",
      "177b45f2a8bd9b90\n",
      "177aff7ba05c9715\n",
      "177ab76014d0b611\n",
      "17795ee8b28f719f\n",
      "1778baf6fcc66234\n",
      "1778b624badf26e0\n",
      "17786247effe1d3f\n",
      "17780dfb6d4d4b4f\n",
      "177776d40891949c\n",
      "1777174438d80a1c\n",
      "17767ac5baf63530\n",
      "1776735335118de3\n",
      "17762334622064d0\n",
      "17752bbef8a6f556\n",
      "1774e3f0bd56150f\n",
      "17748541ed786dda\n",
      "1774444a02454d67\n",
      "177434d938813500\n",
      "1773eca6f6124ac9\n",
      "1773e07419feab21\n",
      "17738ee20260a5ce\n",
      "17735fb1801e86e6\n",
      "1772ec196fb2df68\n",
      "1772952c6b776b34\n",
      "177249aa3cf6e400\n",
      "17720525828ab56b\n",
      "1771f05b905465a8\n",
      "17714c6c569381ca\n",
      "1770aa3bf157364e\n",
      "176fb4faa3b7f9f4\n",
      "176f62df74cc48da\n",
      "176e7acc616eba08\n",
      "176e3299819cfca6\n",
      "176d75ba4e8888bf\n",
      "176d6f9f0d4e7e6b\n",
      "176a89b7048734ad\n",
      "1769e98c65012e01\n",
      "1768f262a450ec89\n",
      "1767a90b2fdf76e1\n",
      "176751cb53eed43c\n",
      "1766ffb6d47a04ee\n",
      "1766b0b6fbf19051\n",
      "17665a30b52053ee\n",
      "17660ba0d96a7b98\n",
      "1765665127ad61b5\n",
      "1764c40740e6f01c\n",
      "1764bd2d075243d3\n",
      "176473ee7fa0e26e\n",
      "17646dea9d1edc88\n",
      "17641b3c43336edf\n",
      "1763c9204123734c\n",
      "1763269db16b930d\n",
      "17627dd412a63e48\n",
      "17623a95a0ec5259\n",
      "17622e76816d6aca\n",
      "1761d8b07b5e9254\n",
      "176189929222107c\n",
      "1760e4d17b1b8ca4\n",
      "175f48df9fcc09df\n",
      "175ea4004470af07\n",
      "175e50a850fac0ab\n",
      "175db062c2b8cc65\n",
      "175d5a5edfde246a\n",
      "175c63b56db41310\n",
      "175b6fba99e39ab9\n",
      "175b6a447147a0b8\n",
      "175b1868571e4920\n",
      "1759ccb11842ceca\n",
      "17597eb3b97c9e02\n",
      "17593787170609f5\n",
      "175935a2ab23b419\n",
      "1758d5e59e5249c8\n",
      "1757de6e364d0f11\n",
      "175736268b558f9f\n",
      "1756e8ab1b10af1a\n",
      "175691b09657d232\n",
      "17563f4c719a9778\n",
      "17559da70ab64f59\n",
      "1755493e03ade081\n",
      "1754fa51da1f544b\n",
      "1754f68e64a7aedc\n",
      "1754ae2d9a4c8dcc\n",
      "1754a79b012434b1\n",
      "17535cdf4bb537fa\n",
      "1752b468154ab3c1\n",
      "1751bd390f44c937\n",
      "17511d40e1674ec5\n",
      "175073a1f6c5648a\n",
      "175028999f42cec4\n",
      "1750225ff6b5a401\n",
      "174fcb9e2bd83732\n",
      "174f7cf0ebc5f5c6\n",
      "174edb47d200bf19\n",
      "174e3438c2339851\n",
      "174de951f7b643e6\n",
      "174de13ed748e625\n",
      "174d4a217abb2396\n",
      "174ca4d820c5aecf\n",
      "174c44a4309e009e\n",
      "174bfaa294077443\n",
      "174ba7f141187ced\n",
      "174ba0cd073b36d6\n",
      "174b5b4b907f0331\n",
      "174afb15a2014444\n",
      "174ab9e45e45ee4d\n",
      "174a59c156c04276\n",
      "174a19403ba632c1\n",
      "1749b17e3e715a8c\n",
      "1747708bcec50275\n",
      "174727cc20f4e971\n",
      "174721cf917b40d0\n",
      "1746cbb9e7be4d01\n",
      "1746c83a65e27eea\n",
      "17467961f2502512\n",
      "1745e26c83e2d8f2\n",
      "1745316aade844c7\n",
      "17448d2436a69a85\n",
      "17439755b93ba3a0\n",
      "1742ef7438780d7d\n",
      "1742a0c68b3800fc\n",
      "1740af892d1dd764\n",
      "174062be7ac8d47f\n",
      "17405c230ea94ae4\n",
      "173f15ab3e6d17a7\n",
      "173e6d6cc87eadf8\n",
      "173e28f5fd67c236\n",
      "173dc8912ab66ee2\n",
      "173ce2b571fa7be7\n",
      "173c7f88c29c352c\n",
      "173be303b10db157\n",
      "173bdabaf5078f6b\n",
      "173a944dbb320396\n",
      "1739a1eb5647c814\n",
      "17399a307f77cab8\n",
      "1738533b8f2273bd\n",
      "1738136480f2a6a9\n",
      "1737ac6506ec325c\n",
      "1737649a940c39af\n",
      "173706721b68dcef\n",
      "1736b4a70a3a05c9\n",
      "1736134e69aec8df\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# get data\n",
    "txt = service.users().messages().get(userId='me', id='1736134e69aec8df').execute()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# payload['headers'] is a list of dicts\n",
    "payload = txt['payload']\n",
    "for d in payload['headers']:\n",
    "    if d['name'] == 'Date':\n",
    "        date = d['value']\n",
    "    if d['name'] == 'From':\n",
    "        sender = d['value']\n",
    "    if d['name'] == 'Subject':\n",
    "        subject = d['value']\n",
    "    \n",
    "print('date: ', date)\n",
    "print('from: ', sender)\n",
    "print('subject: ', subject)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "date:  Sat, 18 Jul 2020 09:14:49 +0000\n",
      "from:  Matthew Mayo <mattmayo@kdnuggets.com>\n",
      "subject:  The Bitter Lesson of Machine Learning; Free MIT Courses on Calculus: The Key to Understanding Deep Learning\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "now that we know that we got every id let us get the email data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "list_mail = []\n",
    "# create a counter know the position if the code breaks\n",
    "counter = 0\n",
    "# for loop to get id \n",
    "for id in result['messages']:\n",
    "    mail = service.users().messages().get(userId='me', id=id['id']).execute()\n",
    "    list_mail.append(mail)\n",
    "    counter = counter + 1\n",
    "    print(counter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "thats workes now I have every mail from KDNuggets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "len(list_mail)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "filename = \"KDnuggets_mails_\"\n",
    "date = datetime.date.today().strftime(\"%m.%d.%Y\")\n",
    "filename = filename + date + \".pkl\"\n",
    "print(filename)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KDnuggets_mails_09.30.2021.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# save data\n",
    "with open(os.path.join(data_path, filename), 'wb') as f:\n",
    "    pickle.dump(list_mail, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create parser for KDnuggest\n",
    "- filter out unrelevant mails\n",
    "    - i cant use the title since there is no indication if a mail relevant or not\n",
    "    - but i can use snipptes if it is relevant it will state the following in the snippet:\n",
    "        - \"News\" or \"Top Stories\"\n",
    "1. build a pandas dataframe with the relevant data\n",
    "2. filter out mails from mattmayo@kdnuggets.com"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "list_mail[0].keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['id', 'threadId', 'labelIds', 'snippet', 'payload', 'sizeEstimate', 'historyId', 'internalDate'])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "list_mail[0]['id']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'17c313783d7e09f1'"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "list_mail[0]['snippet']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Data Management 101 on Databricks How do organizations avoid a data management mess and uplevel their process to more efficiently serve downstream analytics, data science and machine learning? Get the'"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "list_mail[5]['snippet']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Also: The Machine &amp; Deep Learning Compendium Open Book; Easy SQL in Native Python KDnuggets KDnuggets™ Top Stories, Sep 25, 2021 Please consider submitting an original blog to KDnuggets! If it is'"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "# create DF with relevant data\n",
    "id_list = []\n",
    "date_list = []\n",
    "from_list = []\n",
    "subject_list = []\n",
    "snippet_list = []\n",
    "content_list = []\n",
    "\n",
    "for counter, id_ in enumerate(result['messages']):\n",
    "    id_ = id_['id']\n",
    "\n",
    "    payload = list_mail[counter]['payload']\n",
    "    for d in payload['headers']:\n",
    "        if d['name'] == 'Date':\n",
    "            date = d['value']\n",
    "        if d['name'] == 'From':\n",
    "            sender = d['value']\n",
    "        if d['name'] == 'Subject':\n",
    "            subject = d['value']\n",
    "\n",
    "    # there are some mails that dont have a parts section\n",
    "    if \"parts\" in list_mail[counter]['payload'].keys():\n",
    "        parts = payload.get('parts')[0]\n",
    "        data = parts['body']['data']\n",
    "        data = data.replace(\"-\",\"+\").replace(\"_\",\"/\")\n",
    "        decoded_data = base64.b64decode(data)\n",
    "        soup = BeautifulSoup(decoded_data , \"lxml\")\n",
    "        body = soup.body()\n",
    "        body_str = str(body)\n",
    "    else:\n",
    "        data = payload['body']['data']\n",
    "        data = data.replace(\"-\",\"+\").replace(\"_\",\"/\")\n",
    "        decoded_data = base64.b64decode(data)\n",
    "        soup = BeautifulSoup(decoded_data , \"lxml\")\n",
    "        body = soup.body()\n",
    "        body_str = str(body)\n",
    "\n",
    "\n",
    "    id_list.append(id_)\n",
    "    date_list.append(date)\n",
    "    from_list.append(sender)\n",
    "    subject_list.append(subject)\n",
    "    snippet_list.append(list_mail[counter]['snippet'])\n",
    "    content_list.append(body_str)\n",
    "\n",
    "    \n",
    "#print('id: ', list_mail[0]['id'])\n",
    "#print('date: ', date)\n",
    "#print('from: ', sender)\n",
    "#print('subject: ', subject)\n",
    "#print('snippet: ', list_mail[0]['snippet'])\n",
    "#print('content: ', body_str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "len(snippet_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "print(date_list[348])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sat, 18 Jul 2020 09:14:49 +0000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "print(content_list[348])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[<p>Also: A Layman's Guide to Data Science. Part 3: Data Science Workflow; Exploratory Data Analysis on Steroids\n",
      "\n",
      "https://www.kdnuggets.com/index.html\n",
      "\n",
      "KDnuggets™ Top Stories, Jul 18, 2020 (https://www.kdnuggets.com/news/top-stories.html)\n",
      "\n",
      "\n",
      "** Featured Story\n",
      "------------------------------------------------------------\n",
      "* The Bitter Lesson of Machine Learning (https://www.kdnuggets.com/2020/07/bitter-lesson-machine-learning.html)\n",
      "\n",
      "https://www.kdnuggets.com/2020/07/bitter-lesson-machine-learning.html\n",
      "\n",
      "\n",
      "** Most Popular Last Week\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. A Layman's Guide to Data Science. Part 3: Data Science Workflow (https://www.kdnuggets.com/2020/07/laymans-guide-data-science-workflow.html)\n",
      "2. Free MIT Courses on Calculus: The Key to Understanding Deep Learning (https://www.kdnuggets.com/2020/07/free-mit-courses-calculus-key-deep-learning.html)\n",
      "3. Exploratory Data Analysis on Steroids (https://www.kdnuggets.com/2020/07/exploratory-data-analysis-steroids.html)\n",
      "4. How Much Math do you need in Data Science? (https://www.kdnuggets.com/2020/06/math-data-science.html)\n",
      "5. Speed up your Numpy and Pandas with NumExpr Package (https://www.kdnuggets.com/2020/07/speed-up-numpy-pandas-numexpr-package.html)\n",
      "\n",
      "\n",
      "** Most Shared Last Week\n",
      "------------------------------------------------------------\n",
      "1. Free MIT Courses on Calculus: The Key to Understanding Deep Learning (https://www.kdnuggets.com/2020/07/free-mit-courses-calculus-key-deep-learning.html)\n",
      "2. A Layman's Guide to Data Science. Part 3: Data Science Workflow (https://www.kdnuggets.com/2020/07/laymans-guide-data-science-workflow.html)\n",
      "3. A Complete Guide To Survival Analysis In Python, part 1 (https://www.kdnuggets.com/2020/07/complete-guide-survival-analysis-python-part1.html)\n",
      "4. PyTorch for Deep Learning: The Free eBook (https://www.kdnuggets.com/2020/07/pytorch-deep-learning-free-ebook.html)\n",
      "5. 5 Things You Dont Know About PyCaret (https://www.kdnuggets.com/2020/07/5-things-pycaret.html)\n",
      "\n",
      "Software (https://www.kdnuggets.com/software/index.html) | News (https://www.kdnuggets.com/news/index.html) | Jobs (https://www.kdnuggets.com/jobs/index.html) | Datasets (https://www.kdnuggets.com/datasets/index.html) | Courses (https://www.kdnuggets.com/courses/index.html) | Education (https://www.kdnuggets.com/education/index.html) | Meetings (https://www.kdnuggets.com/meetings/index.html) | Webcasts (https://www.kdnuggets.com/webcasts/index.html)\n",
      "\n",
      "https://twitter.com/kdnuggets    https://facebook.com/kdnuggets   https://www.linkedin.com/groups/54257/\n",
      "\n",
      "Copyright © 2020 KDnuggets\n",
      "\n",
      "About KDnuggets (https://www.kdnuggets.com/about/index.html)  |  Contact us (https://www.kdnuggets.com/contact)\n",
      "\n",
      "This email was sent to Markus.mueller.ds@gmail.com (mailto:Markus.mueller.ds@gmail.com)\n",
      "why did I get this? (https://kdnuggets.us12.list-manage.com/about?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=25d21eab15)     unsubscribe from this list (https://kdnuggets.us12.list-manage.com/unsubscribe?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=25d21eab15)     update subscription preferences (https://kdnuggets.us12.list-manage.com/profile?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17)\n",
      "KDnuggets . Reservoir Rd . Brookline, MA 02467 . USA</p>]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"id\"] = id_list\n",
    "df[\"date\"] = date_list\n",
    "df[\"from\"] = from_list\n",
    "df[\"subject\"] = subject_list\n",
    "df[\"snippet\"] = snippet_list\n",
    "df[\"content\"] = content_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>subject</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17c313783d7e09f1</td>\n",
       "      <td>Wed, 29 Sep 2021 11:00:05 +0000</td>\n",
       "      <td>KDnuggets &lt;editor1@kdnuggets.com&gt;</td>\n",
       "      <td>Learn how Databricks streamlines the data mana...</td>\n",
       "      <td>Data Management 101 on Databricks How do organ...</td>\n",
       "      <td>[&lt;p&gt;https://kdnuggets.us12.list-manage.com/tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17c30c98e39c3de7</td>\n",
       "      <td>Wed, 29 Sep 2021 09:00:46 +0000</td>\n",
       "      <td>KDnuggets &lt;editor1@kdnuggets.com&gt;</td>\n",
       "      <td>Nine Tools I Wish I Mastered Before My PhD in ...</td>\n",
       "      <td>KDnuggets News 21:n37, Sep 29, 2021 issue (rea...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 21:n37, Sep 29, 2021 issue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17c2b7d2843852d9</td>\n",
       "      <td>Tue, 28 Sep 2021 08:19:29 +0000</td>\n",
       "      <td>KDnuggets &lt;editor1@kdnuggets.com&gt;</td>\n",
       "      <td>[Download] 3 Perils of Dashboard-Only Decision...</td>\n",
       "      <td>The only path to full data trust is transactio...</td>\n",
       "      <td>[&lt;p&gt;The only path to full data trust is transa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17c25e3aa627293e</td>\n",
       "      <td>Mon, 27 Sep 2021 06:13:47 +0000</td>\n",
       "      <td>KDnuggets &lt;editor1@kdnuggets.com&gt;</td>\n",
       "      <td>How to use AI for better ROI and Insights</td>\n",
       "      <td>AI in Retail Marketing Introduce AI to Level-U...</td>\n",
       "      <td>[&lt;p&gt;Company Logo\\r\\nAI in Retail Marketing (ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17c2311129d967a5</td>\n",
       "      <td>Sun, 26 Sep 2021 17:04:29 +0000</td>\n",
       "      <td>KDnuggets &lt;editor1@kdnuggets.com&gt;</td>\n",
       "      <td>Language, Vision and Deep Learning Models - Fr...</td>\n",
       "      <td>Hi, Over at Abacus.AI we are super excited to ...</td>\n",
       "      <td>[&lt;p&gt;Hi,\\r\\n\\r\\nOver at Abacus.AI we are super ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1737ac6506ec325c</td>\n",
       "      <td>Thu, 23 Jul 2020 08:24:25 +0000</td>\n",
       "      <td>Matthew Mayo &lt;mattmayo@kdnuggets.com&gt;</td>\n",
       "      <td>Unlocking the Promise of AI and Machine Learning</td>\n",
       "      <td>Company Logo Unlocking the Promise of Artifici...</td>\n",
       "      <td>[&lt;p&gt;https://www.datarobot.com/resource/unlocki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1737649a940c39af</td>\n",
       "      <td>Wed, 22 Jul 2020 11:29:56 +0000</td>\n",
       "      <td>Matthew Mayo &lt;mattmayo@kdnuggets.com&gt;</td>\n",
       "      <td>Data Science MOOCs are too Superficial; The Bi...</td>\n",
       "      <td>KDnuggets News 20:n28, Jul 22, 2020 issue (rea...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 20:n28, Jul 22, 2020 issue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>173706721b68dcef</td>\n",
       "      <td>Tue, 21 Jul 2020 08:04:29 +0000</td>\n",
       "      <td>Matthew Mayo &lt;mattmayo@kdnuggets.com&gt;</td>\n",
       "      <td>Don't miss your copy of Learning Spark, 2nd Ed...</td>\n",
       "      <td>Download Now Apache Spark™ has become the de-f...</td>\n",
       "      <td>[&lt;p&gt;https://databricks.com/p/ebook/learning-sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1736b4a70a3a05c9</td>\n",
       "      <td>Mon, 20 Jul 2020 08:14:41 +0000</td>\n",
       "      <td>KDnuggets &lt;editor1@kdnuggets.com&gt;</td>\n",
       "      <td>Subsurface - the cloud data lake conference - ...</td>\n",
       "      <td>July 30, 2020 REGISTER NOW Join Us Live at the...</td>\n",
       "      <td>[&lt;p&gt;wave\\r\\n\\r\\nJuly 30, 2020\\r\\n\\r\\n\\r\\nREGIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1736134e69aec8df</td>\n",
       "      <td>Sat, 18 Jul 2020 09:14:49 +0000</td>\n",
       "      <td>Matthew Mayo &lt;mattmayo@kdnuggets.com&gt;</td>\n",
       "      <td>The Bitter Lesson of Machine Learning; Free MI...</td>\n",
       "      <td>Also: A Layman&amp;#39;s Guide to Data Science. Pa...</td>\n",
       "      <td>[&lt;p&gt;Also: A Layman's Guide to Data Science. Pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                             date  \\\n",
       "0    17c313783d7e09f1  Wed, 29 Sep 2021 11:00:05 +0000   \n",
       "1    17c30c98e39c3de7  Wed, 29 Sep 2021 09:00:46 +0000   \n",
       "2    17c2b7d2843852d9  Tue, 28 Sep 2021 08:19:29 +0000   \n",
       "3    17c25e3aa627293e  Mon, 27 Sep 2021 06:13:47 +0000   \n",
       "4    17c2311129d967a5  Sun, 26 Sep 2021 17:04:29 +0000   \n",
       "..                ...                              ...   \n",
       "344  1737ac6506ec325c  Thu, 23 Jul 2020 08:24:25 +0000   \n",
       "345  1737649a940c39af  Wed, 22 Jul 2020 11:29:56 +0000   \n",
       "346  173706721b68dcef  Tue, 21 Jul 2020 08:04:29 +0000   \n",
       "347  1736b4a70a3a05c9  Mon, 20 Jul 2020 08:14:41 +0000   \n",
       "348  1736134e69aec8df  Sat, 18 Jul 2020 09:14:49 +0000   \n",
       "\n",
       "                                      from  \\\n",
       "0        KDnuggets <editor1@kdnuggets.com>   \n",
       "1        KDnuggets <editor1@kdnuggets.com>   \n",
       "2        KDnuggets <editor1@kdnuggets.com>   \n",
       "3        KDnuggets <editor1@kdnuggets.com>   \n",
       "4        KDnuggets <editor1@kdnuggets.com>   \n",
       "..                                     ...   \n",
       "344  Matthew Mayo <mattmayo@kdnuggets.com>   \n",
       "345  Matthew Mayo <mattmayo@kdnuggets.com>   \n",
       "346  Matthew Mayo <mattmayo@kdnuggets.com>   \n",
       "347      KDnuggets <editor1@kdnuggets.com>   \n",
       "348  Matthew Mayo <mattmayo@kdnuggets.com>   \n",
       "\n",
       "                                               subject  \\\n",
       "0    Learn how Databricks streamlines the data mana...   \n",
       "1    Nine Tools I Wish I Mastered Before My PhD in ...   \n",
       "2    [Download] 3 Perils of Dashboard-Only Decision...   \n",
       "3            How to use AI for better ROI and Insights   \n",
       "4    Language, Vision and Deep Learning Models - Fr...   \n",
       "..                                                 ...   \n",
       "344   Unlocking the Promise of AI and Machine Learning   \n",
       "345  Data Science MOOCs are too Superficial; The Bi...   \n",
       "346  Don't miss your copy of Learning Spark, 2nd Ed...   \n",
       "347  Subsurface - the cloud data lake conference - ...   \n",
       "348  The Bitter Lesson of Machine Learning; Free MI...   \n",
       "\n",
       "                                               snippet  \\\n",
       "0    Data Management 101 on Databricks How do organ...   \n",
       "1    KDnuggets News 21:n37, Sep 29, 2021 issue (rea...   \n",
       "2    The only path to full data trust is transactio...   \n",
       "3    AI in Retail Marketing Introduce AI to Level-U...   \n",
       "4    Hi, Over at Abacus.AI we are super excited to ...   \n",
       "..                                                 ...   \n",
       "344  Company Logo Unlocking the Promise of Artifici...   \n",
       "345  KDnuggets News 20:n28, Jul 22, 2020 issue (rea...   \n",
       "346  Download Now Apache Spark™ has become the de-f...   \n",
       "347  July 30, 2020 REGISTER NOW Join Us Live at the...   \n",
       "348  Also: A Layman&#39;s Guide to Data Science. Pa...   \n",
       "\n",
       "                                               content  \n",
       "0    [<p>https://kdnuggets.us12.list-manage.com/tra...  \n",
       "1    [<p>KDnuggets News 21:n37, Sep 29, 2021 issue ...  \n",
       "2    [<p>The only path to full data trust is transa...  \n",
       "3    [<p>Company Logo\\r\\nAI in Retail Marketing (ht...  \n",
       "4    [<p>Hi,\\r\\n\\r\\nOver at Abacus.AI we are super ...  \n",
       "..                                                 ...  \n",
       "344  [<p>https://www.datarobot.com/resource/unlocki...  \n",
       "345  [<p>KDnuggets News 20:n28, Jul 22, 2020 issue ...  \n",
       "346  [<p>https://databricks.com/p/ebook/learning-sp...  \n",
       "347  [<p>wave\\r\\n\\r\\nJuly 30, 2020\\r\\n\\r\\n\\r\\nREGIS...  \n",
       "348  [<p>Also: A Layman's Guide to Data Science. Pa...  \n",
       "\n",
       "[349 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "file_ = os.path.join(data_path, \"KDnuggets_mails_09.30.2021.csv\")\n",
    "df.to_csv(file_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter out unrelevant emails.\n",
    "- mails from mattmayo@kdnuggets.com are relevant for the most parts but there are also unrelevant mails\n",
    "    - mails should have \"KDnuggets News\" or \"KDnuggets Top Stories\"   \n",
    "    - \"KDnuggets\" Top Stories always start with \"Also:\" in the snippet \n",
    "- mails form editor1@kdnuggets.com are somethimes relevant:\n",
    "    - when they have \"KDnuggets News\" in snippet folder\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# load saved csv\n",
    "data_path   = '/Users/markusmuller/python/projects/content-db/gmail/data'\n",
    "df = pd.read_csv(os.path.join(data_path, \"KDnuggets_mails_09.30.2021.csv\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for i in range(10):\n",
    "    print(df[\"snippet\"][:10][i])\n",
    "    print('-----')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Management 101 on Databricks How do organizations avoid a data management mess and uplevel their process to more efficiently serve downstream analytics, data science and machine learning? Get the\n",
      "-----\n",
      "KDnuggets News 21:n37, Sep 29, 2021 issue (read online) KDnuggets KDnuggets™ News 21:n37, Sep 29 Features | Products | Tutorials | Opinions | Tops | Jobs | Submit a blog | Image of the week Whether you\n",
      "-----\n",
      "The only path to full data trust is transaction-level intelligence - here&#39;s why. x2_Three-Reasons-to-Think-Beyond-the-Dashboard Business leaders are asked to make massively impactful decisions\n",
      "-----\n",
      "AI in Retail Marketing Introduce AI to Level-Up Customer Engagement and Marketing Spend Beyond Traditional BI Download now Hi there, Retailers that adopt AI stand to gain a critical competitive\n",
      "-----\n",
      "Hi, Over at Abacus.AI we are super excited to build you a language, vision or deep learning model for FREE, as part of our free POC (Proof-of-concept) offer. We would use our end to end deep-learning\n",
      "-----\n",
      "Also: The Machine &amp; Deep Learning Compendium Open Book; Easy SQL in Native Python KDnuggets KDnuggets™ Top Stories, Sep 25, 2021 Please consider submitting an original blog to KDnuggets! If it is\n",
      "-----\n",
      "Limited Time Offer! Free Dataset Worth $1350 Download Free Dataset Research shows that speech recognition is not nearly as accurate in understanding nonnative accents as they are in understanding white\n",
      "-----\n",
      "Adoption, use, challenges, architectures, and best practices for unified platforms for modern analytics x2_TDWI-Best-Practices-Report-Unified-Platforms-for-Modern-Analytics The demand for analytics\n",
      "-----\n",
      "KDnuggets News 21:n36, Sep 22, 2021 issue (read online) KDnuggets KDnuggets™ News 21:n36, Sep 22 Features | Products | Tutorials | Opinions | Tops | Jobs Submit a blog | Image of the week This week on\n",
      "-----\n",
      "Dremio Logo Deploying your AWS Data Lake? Are you getting the most value for your time and investment? DOWNLOAD All types of enterprises invest in data lakes as a cost-effective, centralized repository\n",
      "-----\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df['from']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0          KDnuggets <editor1@kdnuggets.com>\n",
       "1          KDnuggets <editor1@kdnuggets.com>\n",
       "2          KDnuggets <editor1@kdnuggets.com>\n",
       "3          KDnuggets <editor1@kdnuggets.com>\n",
       "4          KDnuggets <editor1@kdnuggets.com>\n",
       "                       ...                  \n",
       "344    Matthew Mayo <mattmayo@kdnuggets.com>\n",
       "345    Matthew Mayo <mattmayo@kdnuggets.com>\n",
       "346    Matthew Mayo <mattmayo@kdnuggets.com>\n",
       "347        KDnuggets <editor1@kdnuggets.com>\n",
       "348    Matthew Mayo <mattmayo@kdnuggets.com>\n",
       "Name: from, Length: 349, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df[[\"from\", \"mail\"]] = df['from'].str.split('<', 1, expand=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# remove > from str\n",
    "df['mail'] = df['mail'].str.replace('>', '',)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df_mattmayo = df[df['mail'] == \"mattmayo@kdnuggets.com\"]\n",
    "df_mattmayo"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>subject</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>17c1c3c531e08e3e</td>\n",
       "      <td>Sat, 25 Sep 2021 09:14:32 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Nine Machine Learning Tools to Master; Data Sc...</td>\n",
       "      <td>Also: The Machine &amp;amp; Deep Learning Compendi...</td>\n",
       "      <td>[&lt;p&gt;Also: The Machine &amp;amp; Deep Learning Comp...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17c0d04c01474fa8</td>\n",
       "      <td>Wed, 22 Sep 2021 10:19:35 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>The Machine &amp; Deep Learning Compendium Open Bo...</td>\n",
       "      <td>KDnuggets News 21:n36, Sep 22, 2021 issue (rea...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 21:n36, Sep 22, 2021 issue ...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>17bf839a7969c51f</td>\n",
       "      <td>Sat, 18 Sep 2021 09:24:28 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Easy SQL in Native Python; Read Excel Files wi...</td>\n",
       "      <td>Also: How to Create Stunning Web Apps for your...</td>\n",
       "      <td>[&lt;p&gt;Also: How to Create Stunning Web Apps for ...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>17bd41ed7e3689c9</td>\n",
       "      <td>Sat, 11 Sep 2021 09:09:36 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Create Stunning Web Apps for your Data Science...</td>\n",
       "      <td>Also: The Top Industries Hiring Data Scientist...</td>\n",
       "      <td>[&lt;p&gt;Also: The Top Industries Hiring Data Scien...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>17bc4a99883075e1</td>\n",
       "      <td>Wed,  8 Sep 2021 09:04:53 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Do You Read Excel Files with Python? There is ...</td>\n",
       "      <td>KDnuggets News 21:n34, Sep 8, 2021 issue (read...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 21:n34, Sep 8, 2021 issue (...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>342</td>\n",
       "      <td>1738533b8f2273bd</td>\n",
       "      <td>Sat, 25 Jul 2020 09:00:09 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Data Science MOOCs are too Superficial; The Bi...</td>\n",
       "      <td>Also: Free MIT Courses on Calculus: The Key to...</td>\n",
       "      <td>[&lt;p&gt;Also: Free MIT Courses on Calculus: The Ke...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>344</td>\n",
       "      <td>1737ac6506ec325c</td>\n",
       "      <td>Thu, 23 Jul 2020 08:24:25 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Unlocking the Promise of AI and Machine Learning</td>\n",
       "      <td>Company Logo Unlocking the Promise of Artifici...</td>\n",
       "      <td>[&lt;p&gt;https://www.datarobot.com/resource/unlocki...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>345</td>\n",
       "      <td>1737649a940c39af</td>\n",
       "      <td>Wed, 22 Jul 2020 11:29:56 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Data Science MOOCs are too Superficial; The Bi...</td>\n",
       "      <td>KDnuggets News 20:n28, Jul 22, 2020 issue (rea...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 20:n28, Jul 22, 2020 issue ...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>346</td>\n",
       "      <td>173706721b68dcef</td>\n",
       "      <td>Tue, 21 Jul 2020 08:04:29 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Don't miss your copy of Learning Spark, 2nd Ed...</td>\n",
       "      <td>Download Now Apache Spark™ has become the de-f...</td>\n",
       "      <td>[&lt;p&gt;https://databricks.com/p/ebook/learning-sp...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>348</td>\n",
       "      <td>1736134e69aec8df</td>\n",
       "      <td>Sat, 18 Jul 2020 09:14:49 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>The Bitter Lesson of Machine Learning; Free MI...</td>\n",
       "      <td>Also: A Layman&amp;#39;s Guide to Data Science. Pa...</td>\n",
       "      <td>[&lt;p&gt;Also: A Layman's Guide to Data Science. Pa...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                id                             date  \\\n",
       "5             5  17c1c3c531e08e3e  Sat, 25 Sep 2021 09:14:32 +0000   \n",
       "8             8  17c0d04c01474fa8  Wed, 22 Sep 2021 10:19:35 +0000   \n",
       "12           12  17bf839a7969c51f  Sat, 18 Sep 2021 09:24:28 +0000   \n",
       "18           18  17bd41ed7e3689c9  Sat, 11 Sep 2021 09:09:36 +0000   \n",
       "21           21  17bc4a99883075e1  Wed,  8 Sep 2021 09:04:53 +0000   \n",
       "..          ...               ...                              ...   \n",
       "342         342  1738533b8f2273bd  Sat, 25 Jul 2020 09:00:09 +0000   \n",
       "344         344  1737ac6506ec325c  Thu, 23 Jul 2020 08:24:25 +0000   \n",
       "345         345  1737649a940c39af  Wed, 22 Jul 2020 11:29:56 +0000   \n",
       "346         346  173706721b68dcef  Tue, 21 Jul 2020 08:04:29 +0000   \n",
       "348         348  1736134e69aec8df  Sat, 18 Jul 2020 09:14:49 +0000   \n",
       "\n",
       "              from                                            subject  \\\n",
       "5    Matthew Mayo   Nine Machine Learning Tools to Master; Data Sc...   \n",
       "8    Matthew Mayo   The Machine & Deep Learning Compendium Open Bo...   \n",
       "12   Matthew Mayo   Easy SQL in Native Python; Read Excel Files wi...   \n",
       "18   Matthew Mayo   Create Stunning Web Apps for your Data Science...   \n",
       "21   Matthew Mayo   Do You Read Excel Files with Python? There is ...   \n",
       "..             ...                                                ...   \n",
       "342  Matthew Mayo   Data Science MOOCs are too Superficial; The Bi...   \n",
       "344  Matthew Mayo    Unlocking the Promise of AI and Machine Learning   \n",
       "345  Matthew Mayo   Data Science MOOCs are too Superficial; The Bi...   \n",
       "346  Matthew Mayo   Don't miss your copy of Learning Spark, 2nd Ed...   \n",
       "348  Matthew Mayo   The Bitter Lesson of Machine Learning; Free MI...   \n",
       "\n",
       "                                               snippet  \\\n",
       "5    Also: The Machine &amp; Deep Learning Compendi...   \n",
       "8    KDnuggets News 21:n36, Sep 22, 2021 issue (rea...   \n",
       "12   Also: How to Create Stunning Web Apps for your...   \n",
       "18   Also: The Top Industries Hiring Data Scientist...   \n",
       "21   KDnuggets News 21:n34, Sep 8, 2021 issue (read...   \n",
       "..                                                 ...   \n",
       "342  Also: Free MIT Courses on Calculus: The Key to...   \n",
       "344  Company Logo Unlocking the Promise of Artifici...   \n",
       "345  KDnuggets News 20:n28, Jul 22, 2020 issue (rea...   \n",
       "346  Download Now Apache Spark™ has become the de-f...   \n",
       "348  Also: A Layman&#39;s Guide to Data Science. Pa...   \n",
       "\n",
       "                                               content                    mail  \n",
       "5    [<p>Also: The Machine &amp; Deep Learning Comp...  mattmayo@kdnuggets.com  \n",
       "8    [<p>KDnuggets News 21:n36, Sep 22, 2021 issue ...  mattmayo@kdnuggets.com  \n",
       "12   [<p>Also: How to Create Stunning Web Apps for ...  mattmayo@kdnuggets.com  \n",
       "18   [<p>Also: The Top Industries Hiring Data Scien...  mattmayo@kdnuggets.com  \n",
       "21   [<p>KDnuggets News 21:n34, Sep 8, 2021 issue (...  mattmayo@kdnuggets.com  \n",
       "..                                                 ...                     ...  \n",
       "342  [<p>Also: Free MIT Courses on Calculus: The Ke...  mattmayo@kdnuggets.com  \n",
       "344  [<p>https://www.datarobot.com/resource/unlocki...  mattmayo@kdnuggets.com  \n",
       "345  [<p>KDnuggets News 20:n28, Jul 22, 2020 issue ...  mattmayo@kdnuggets.com  \n",
       "346  [<p>https://databricks.com/p/ebook/learning-sp...  mattmayo@kdnuggets.com  \n",
       "348  [<p>Also: A Layman's Guide to Data Science. Pa...  mattmayo@kdnuggets.com  \n",
       "\n",
       "[105 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "mask_mayo = df_mattmayo['snippet'].str.contains('KDnuggets Top Stories|Also|KDnuggets News', case=False, regex=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df_mattmayo[mask_mayo].info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 57 entries, 5 to 348\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  57 non-null     int64 \n",
      " 1   id          57 non-null     object\n",
      " 2   date        57 non-null     object\n",
      " 3   from        57 non-null     object\n",
      " 4   subject     57 non-null     object\n",
      " 5   snippet     57 non-null     object\n",
      " 6   content     57 non-null     object\n",
      " 7   mail        57 non-null     object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 4.0+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df_mattmayo_filtered = df_mattmayo[mask_mayo]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df_editor1 = df[df['mail'] == \"editor1@kdnuggets.com\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "mask = df_editor1['snippet'].str.contains('KDnuggets News', case=False, regex=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "df_editor1_filtered = df_editor1[mask]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "frames = [df_mattmayo_filtered, df_editor1_filtered]\n",
    "df_final = pd.concat(frames)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df_final"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>subject</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>17c1c3c531e08e3e</td>\n",
       "      <td>Sat, 25 Sep 2021 09:14:32 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Nine Machine Learning Tools to Master; Data Sc...</td>\n",
       "      <td>Also: The Machine &amp;amp; Deep Learning Compendi...</td>\n",
       "      <td>[&lt;p&gt;Also: The Machine &amp;amp; Deep Learning Comp...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17c0d04c01474fa8</td>\n",
       "      <td>Wed, 22 Sep 2021 10:19:35 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>The Machine &amp; Deep Learning Compendium Open Bo...</td>\n",
       "      <td>KDnuggets News 21:n36, Sep 22, 2021 issue (rea...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 21:n36, Sep 22, 2021 issue ...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>17bf839a7969c51f</td>\n",
       "      <td>Sat, 18 Sep 2021 09:24:28 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Easy SQL in Native Python; Read Excel Files wi...</td>\n",
       "      <td>Also: How to Create Stunning Web Apps for your...</td>\n",
       "      <td>[&lt;p&gt;Also: How to Create Stunning Web Apps for ...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>17bd41ed7e3689c9</td>\n",
       "      <td>Sat, 11 Sep 2021 09:09:36 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Create Stunning Web Apps for your Data Science...</td>\n",
       "      <td>Also: The Top Industries Hiring Data Scientist...</td>\n",
       "      <td>[&lt;p&gt;Also: The Top Industries Hiring Data Scien...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>17bc4a99883075e1</td>\n",
       "      <td>Wed,  8 Sep 2021 09:04:53 +0000</td>\n",
       "      <td>Matthew Mayo</td>\n",
       "      <td>Do You Read Excel Files with Python? There is ...</td>\n",
       "      <td>KDnuggets News 21:n34, Sep 8, 2021 issue (read...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 21:n34, Sep 8, 2021 issue (...</td>\n",
       "      <td>mattmayo@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>317</td>\n",
       "      <td>174727cc20f4e971</td>\n",
       "      <td>Wed,  9 Sep 2020 10:50:02 +0000</td>\n",
       "      <td>KDnuggets</td>\n",
       "      <td>Top Online Data Science Masters Degrees; Moder...</td>\n",
       "      <td>KDnuggets News 20:n34, Sep 9, 2020 issue (read...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 20:n34, Sep 9, 2020 issue (...</td>\n",
       "      <td>editor1@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>327</td>\n",
       "      <td>1742a0c68b3800fc</td>\n",
       "      <td>Wed, 26 Aug 2020 09:14:30 +0000</td>\n",
       "      <td>KDnuggets</td>\n",
       "      <td>If I had to start learning Data Science again,...</td>\n",
       "      <td>KDnuggets News 20:n33, Aug 26, 2020 issue (rea...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 20:n33, Aug 26, 2020 issue ...</td>\n",
       "      <td>editor1@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>329</td>\n",
       "      <td>174062be7ac8d47f</td>\n",
       "      <td>Wed, 19 Aug 2020 10:01:12 +0000</td>\n",
       "      <td>KDnuggets</td>\n",
       "      <td>The List of Top 10 Data Science Lists; Data Sc...</td>\n",
       "      <td>KDnuggets News 20:n32, Aug 19, 2020 issue (rea...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 20:n32, Aug 19, 2020 issue ...</td>\n",
       "      <td>editor1@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>333</td>\n",
       "      <td>173e28f5fd67c236</td>\n",
       "      <td>Wed, 12 Aug 2020 12:02:25 +0000</td>\n",
       "      <td>KDnuggets</td>\n",
       "      <td>Data Science Skills: Have vs Want - New Poll; ...</td>\n",
       "      <td>KDnuggets News 20:n31, Aug 12, 2020 issue (rea...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 20:n31, Aug 12, 2020 issue ...</td>\n",
       "      <td>editor1@kdnuggets.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>340</td>\n",
       "      <td>1739a1eb5647c814</td>\n",
       "      <td>Wed, 29 Jul 2020 10:29:27 +0000</td>\n",
       "      <td>Gregory Piatetsky</td>\n",
       "      <td>Easy Guide To Data Preprocessing In Python; Bu...</td>\n",
       "      <td>KDnuggets News 20:n29, Jul 29, 2020 issue (rea...</td>\n",
       "      <td>[&lt;p&gt;KDnuggets News 20:n29, Jul 29, 2020 issue ...</td>\n",
       "      <td>editor1@kdnuggets.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                id                             date  \\\n",
       "5             5  17c1c3c531e08e3e  Sat, 25 Sep 2021 09:14:32 +0000   \n",
       "8             8  17c0d04c01474fa8  Wed, 22 Sep 2021 10:19:35 +0000   \n",
       "12           12  17bf839a7969c51f  Sat, 18 Sep 2021 09:24:28 +0000   \n",
       "18           18  17bd41ed7e3689c9  Sat, 11 Sep 2021 09:09:36 +0000   \n",
       "21           21  17bc4a99883075e1  Wed,  8 Sep 2021 09:04:53 +0000   \n",
       "..          ...               ...                              ...   \n",
       "317         317  174727cc20f4e971  Wed,  9 Sep 2020 10:50:02 +0000   \n",
       "327         327  1742a0c68b3800fc  Wed, 26 Aug 2020 09:14:30 +0000   \n",
       "329         329  174062be7ac8d47f  Wed, 19 Aug 2020 10:01:12 +0000   \n",
       "333         333  173e28f5fd67c236  Wed, 12 Aug 2020 12:02:25 +0000   \n",
       "340         340  1739a1eb5647c814  Wed, 29 Jul 2020 10:29:27 +0000   \n",
       "\n",
       "                   from                                            subject  \\\n",
       "5         Matthew Mayo   Nine Machine Learning Tools to Master; Data Sc...   \n",
       "8         Matthew Mayo   The Machine & Deep Learning Compendium Open Bo...   \n",
       "12        Matthew Mayo   Easy SQL in Native Python; Read Excel Files wi...   \n",
       "18        Matthew Mayo   Create Stunning Web Apps for your Data Science...   \n",
       "21        Matthew Mayo   Do You Read Excel Files with Python? There is ...   \n",
       "..                  ...                                                ...   \n",
       "317          KDnuggets   Top Online Data Science Masters Degrees; Moder...   \n",
       "327          KDnuggets   If I had to start learning Data Science again,...   \n",
       "329          KDnuggets   The List of Top 10 Data Science Lists; Data Sc...   \n",
       "333          KDnuggets   Data Science Skills: Have vs Want - New Poll; ...   \n",
       "340  Gregory Piatetsky   Easy Guide To Data Preprocessing In Python; Bu...   \n",
       "\n",
       "                                               snippet  \\\n",
       "5    Also: The Machine &amp; Deep Learning Compendi...   \n",
       "8    KDnuggets News 21:n36, Sep 22, 2021 issue (rea...   \n",
       "12   Also: How to Create Stunning Web Apps for your...   \n",
       "18   Also: The Top Industries Hiring Data Scientist...   \n",
       "21   KDnuggets News 21:n34, Sep 8, 2021 issue (read...   \n",
       "..                                                 ...   \n",
       "317  KDnuggets News 20:n34, Sep 9, 2020 issue (read...   \n",
       "327  KDnuggets News 20:n33, Aug 26, 2020 issue (rea...   \n",
       "329  KDnuggets News 20:n32, Aug 19, 2020 issue (rea...   \n",
       "333  KDnuggets News 20:n31, Aug 12, 2020 issue (rea...   \n",
       "340  KDnuggets News 20:n29, Jul 29, 2020 issue (rea...   \n",
       "\n",
       "                                               content                    mail  \n",
       "5    [<p>Also: The Machine &amp; Deep Learning Comp...  mattmayo@kdnuggets.com  \n",
       "8    [<p>KDnuggets News 21:n36, Sep 22, 2021 issue ...  mattmayo@kdnuggets.com  \n",
       "12   [<p>Also: How to Create Stunning Web Apps for ...  mattmayo@kdnuggets.com  \n",
       "18   [<p>Also: The Top Industries Hiring Data Scien...  mattmayo@kdnuggets.com  \n",
       "21   [<p>KDnuggets News 21:n34, Sep 8, 2021 issue (...  mattmayo@kdnuggets.com  \n",
       "..                                                 ...                     ...  \n",
       "317  [<p>KDnuggets News 20:n34, Sep 9, 2020 issue (...   editor1@kdnuggets.com  \n",
       "327  [<p>KDnuggets News 20:n33, Aug 26, 2020 issue ...   editor1@kdnuggets.com  \n",
       "329  [<p>KDnuggets News 20:n32, Aug 19, 2020 issue ...   editor1@kdnuggets.com  \n",
       "333  [<p>KDnuggets News 20:n31, Aug 12, 2020 issue ...   editor1@kdnuggets.com  \n",
       "340  [<p>KDnuggets News 20:n29, Jul 29, 2020 issue ...   editor1@kdnuggets.com  \n",
       "\n",
       "[95 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "txt = df_final['content'][5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "txt"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[<p>Also: The Machine &amp; Deep Learning Compendium Open Book; Easy SQL in Native Python\\r\\n\\r\\nhttps://www.kdnuggets.com/index.html\\r\\n\\r\\nKDnuggets™ Top Stories, Sep 25, 2021 (https://www.kdnuggets.com/news/top-stories.html)\\r\\n\\r\\nPlease consider submitting an original blog to KDnuggets!\\r\\nIf it is among top blogs of the month, it will win KDnuggets Top Blogs Rewards. See full details here (https://www.kdnuggets.com/2021/04/kdnuggets-top-authors-reward-program.html) .\\r\\n\\r\\n\\r\\n** Featured Story\\r\\n------------------------------------------------------------\\r\\n* Nine Tools I Wish I Mastered Before My PhD in Machine Learning (https://www.kdnuggets.com/2021/09/nine-tools-mastered-before-phd-machine-learning.html) , by Aliaksei Mikhailiuk\\r\\n\\r\\nhttps://www.kdnuggets.com/2021/09/nine-tools-mastered-before-phd-machine-learning.html\\r\\n\\r\\n\\r\\n** Most Popular Last Week\\r\\n------------------------------------------------------------\\r\\n\\r\\n1. Data Scientists Without Data Engineering Skills Will Face the Harsh Truth (https://www.kdnuggets.com/2021/09/data-scientists-data-engineering-skills.html) , by Soner Yildirim\\r\\n2. The Machine &amp; Deep Learning Compendium Open Book (https://www.kdnuggets.com/2021/09/machine-deep-learning-open-book.html) , by Ori Cohen\\r\\n3. Easy SQL in Native Python (https://www.kdnuggets.com/2021/09/easy-sql-native-python.html) , by Matthew Mayo\\r\\n4. The Prefect Way to Automate &amp; Orchestrate Data Pipelines (https://www.kdnuggets.com/2021/09/prefect-way-automate-orchestrate-data-pipelines.html) , by Thuwarakesh Murallie\\r\\n5. A Data Science Portfolio That Will Land You The Job (https://www.kdnuggets.com/2021/09/data-science-portfolio-job.html) , by Natassha Selvaraj\\r\\n\\r\\n\\r\\n** Most Shared Last Week\\r\\n------------------------------------------------------------\\r\\n1. The Machine &amp; Deep Learning Compendium Open Book (https://www.kdnuggets.com/2021/09/machine-deep-learning-open-book.html) , by Ori Cohen\\r\\n2. Data Scientists Without Data Engineering Skills Will Face the Harsh Truth (https://www.kdnuggets.com/2021/09/data-scientists-data-engineering-skills.html) , by Soner Yildirim\\r\\n3. An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google Colab (https://www.kdnuggets.com/2021/09/intro-reinforcement-learning-openai-gym-rllib-colab.html) , by Galarnyk &amp; Mika\\r\\n4. The Prefect Way to Automate &amp; Orchestrate Data Pipelines (https://www.kdnuggets.com/2021/09/prefect-way-automate-orchestrate-data-pipelines.html) , by Thuwarakesh Murallie\\r\\n5. Easy SQL in Native Python (https://www.kdnuggets.com/2021/09/easy-sql-native-python.html) , by Matthew Mayo\\r\\n\\r\\nSoftware (https://www.kdnuggets.com/software/index.html) | News (https://www.kdnuggets.com/news/index.html) | Jobs (https://www.kdnuggets.com/jobs/index.html) | Datasets (https://www.kdnuggets.com/datasets/index.html) | Courses (https://www.kdnuggets.com/courses/index.html) | Education (https://www.kdnuggets.com/education/index.html) | Meetings (https://www.kdnuggets.com/meetings/index.html) | Webcasts (https://www.kdnuggets.com/webcasts/index.html)\\r\\n\\r\\nhttps://twitter.com/kdnuggets    https://facebook.com/kdnuggets   https://www.linkedin.com/groups/54257/\\r\\n\\r\\nCopyright © 2021 KDnuggets\\r\\n\\r\\nAbout KDnuggets (https://www.kdnuggets.com/about/index.html)  |  Contact us (https://www.kdnuggets.com/contact)\\r\\n\\r\\nThis email was sent to Markus.mueller.ds@gmail.com (mailto:Markus.mueller.ds@gmail.com)\\r\\nwhy did I get this? (https://kdnuggets.us12.list-manage.com/about?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=06def11439)     unsubscribe from this list (https://kdnuggets.us12.list-manage.com/unsubscribe?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=06def11439)     update subscription preferences (https://kdnuggets.us12.list-manage.com/profile?u=4f2891ebb155b23f120ece0bd&amp;id=b2fa8716d2&amp;e=bfdfd73c17&amp;c=06def11439)\\r\\nKDnuggets . Reservoir Rd . Brookline, MA 02467 . USA</p>]'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# get evey link in string\n",
    "# regex from: https://stackoverflow.com/questions/6038061/regular-expression-to-find-urls-within-a-string\n",
    "urls = []\n",
    "for url in re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', txt):\n",
    "    urls.append(url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "urls"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://www.kdnuggets.com/index.html',\n",
       " 'https://www.kdnuggets.com/news/top-stories.html',\n",
       " 'https://www.kdnuggets.com/2021/04/kdnuggets-top-authors-reward-program.html',\n",
       " 'https://www.kdnuggets.com/2021/09/nine-tools-mastered-before-phd-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/09/nine-tools-mastered-before-phd-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-scientists-data-engineering-skills.html',\n",
       " 'https://www.kdnuggets.com/2021/09/machine-deep-learning-open-book.html',\n",
       " 'https://www.kdnuggets.com/2021/09/easy-sql-native-python.html',\n",
       " 'https://www.kdnuggets.com/2021/09/prefect-way-automate-orchestrate-data-pipelines.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-science-portfolio-job.html',\n",
       " 'https://www.kdnuggets.com/2021/09/machine-deep-learning-open-book.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-scientists-data-engineering-skills.html',\n",
       " 'https://www.kdnuggets.com/2021/09/intro-reinforcement-learning-openai-gym-rllib-colab.html',\n",
       " 'https://www.kdnuggets.com/2021/09/prefect-way-automate-orchestrate-data-pipelines.html',\n",
       " 'https://www.kdnuggets.com/2021/09/easy-sql-native-python.html',\n",
       " 'https://www.kdnuggets.com/software/index.html',\n",
       " 'https://www.kdnuggets.com/news/index.html',\n",
       " 'https://www.kdnuggets.com/jobs/index.html',\n",
       " 'https://www.kdnuggets.com/datasets/index.html',\n",
       " 'https://www.kdnuggets.com/courses/index.html',\n",
       " 'https://www.kdnuggets.com/education/index.html',\n",
       " 'https://www.kdnuggets.com/meetings/index.html',\n",
       " 'https://www.kdnuggets.com/webcasts/index.html',\n",
       " 'https://twitter.com/kdnuggets',\n",
       " 'https://facebook.com/kdnuggets',\n",
       " 'https://www.linkedin.com/groups/54257/',\n",
       " 'https://www.kdnuggets.com/about/index.html',\n",
       " 'https://www.kdnuggets.com/contact',\n",
       " 'Markus.mueller.ds',\n",
       " 'gmail.com',\n",
       " 'Markus.mueller.ds',\n",
       " 'gmail.com',\n",
       " 'https://kdnuggets.us12.list-manage.com/about?u=4f2891ebb155b23f120ece0bd&amp',\n",
       " 'https://kdnuggets.us12.list-manage.com/unsubscribe?u=4f2891ebb155b23f120ece0bd&amp',\n",
       " 'https://kdnuggets.us12.list-manage.com/profile?u=4f2891ebb155b23f120ece0bd&amp']"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "urls = []\n",
    "for text in df_final['content'].values:\n",
    "    for url in re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', text):\n",
    "        urls.append(url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "len(urls)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4118"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# list with invalid urls\n",
    "invalid_urls = ['https://www.kdnuggets.com/index.html',\n",
    " 'https://www.kdnuggets.com/news/top-stories.html',\n",
    " 'https://www.kdnuggets.com/2021/04/kdnuggets-top-authors-reward-program.html',\n",
    " 'https://www.kdnuggets.com/software/index.html',\n",
    " 'https://www.kdnuggets.com/news/index.html',\n",
    " 'https://www.kdnuggets.com/jobs/index.html',\n",
    " 'https://www.kdnuggets.com/datasets/index.html',\n",
    " 'https://www.kdnuggets.com/courses/index.html',\n",
    " 'https://www.kdnuggets.com/education/index.html',\n",
    " 'https://www.kdnuggets.com/meetings/index.html',\n",
    " 'https://www.kdnuggets.com/webcasts/index.html',\n",
    " 'https://twitter.com/kdnuggets',\n",
    " 'https://facebook.com/kdnuggets',\n",
    " 'https://www.linkedin.com/groups/54257/',\n",
    " 'https://www.kdnuggets.com/about/index.html',\n",
    " 'https://www.kdnuggets.com/contact',\n",
    " 'https://www.kdnuggets.com/contact.html',\n",
    " 'https://www.kdnuggets.com/news/submissions.html',\n",
    " 'kdnuggets.com/jobs',\n",
    " 'Markus.mueller.ds',\n",
    " 'gmail.com',\n",
    " 'Markus.mueller.ds',\n",
    " 'gmail.com',\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# not really a sohisticated process but gets the job done\n",
    "valid_urls = []\n",
    "for url in urls:\n",
    "    if url in invalid_urls:\n",
    "        continue\n",
    "    elif url.startswith(\"https://kdnuggets.us12.list-manage.com/about\"):\n",
    "        continue\n",
    "    elif url.startswith(\"https://kdnuggets.us12.list-manage.com/unsubscribe\"):\n",
    "        continue\n",
    "    elif url.startswith(\"https://kdnuggets.us12.list-manage.com/profile\"):\n",
    "        continue\n",
    "    elif url.startswith(\"https://kdnuggets.us12.list-manage.com/track/click?\"):\n",
    "        continue\n",
    "    elif not url.startswith(\"https:\"):\n",
    "        continue\n",
    "    elif url.startswith(\"https://www.kdnuggets.com/2021/n\"):\n",
    "        continue\n",
    "    elif url.startswith(\"https://www.kdnuggets.com/2020/n\"):\n",
    "        continue \n",
    "    elif url.find(\"top-news-week\") != -1:\n",
    "        continue\n",
    "    elif url.find(\"top-tweets\") != -1:\n",
    "        continue\n",
    "    else: valid_urls.append(url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "len(valid_urls)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1459"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "len(set(valid_urls))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "valid_urls = list(set(valid_urls))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sanity Checks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# checking if links from last mail in mail box are in links\n",
    "'https://www.kdnuggets.com/2020/07/5-fantastic-nlp-books.html' in valid_urls"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "'https://www.kdnuggets.com/2021/09/nine-tools-mastered-before-phd-machine-learning.html' in valid_urls"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "valid_urls"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://www.kdnuggets.com/2021/08/visplore-data-understanding-interactive-exploration.html',\n",
       " 'https://www.kdnuggets.com/2020/07/pytorch-deep-learning-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2021/06/interactive-plots-directly-pandas.html',\n",
       " 'https://www.kdnuggets.com/2021/01/sas-viya-faster-trusted-decisions-cloud.html',\n",
       " 'https://www.kdnuggets.com/2021/07/building-machine-learning-pipelines-snowflake-dask.html',\n",
       " 'https://www.kdnuggets.com/2021/04/microsoft-research-trains-neural-networks-understand-read.html',\n",
       " 'https://www.kdnuggets.com/2021/06/create-deploy-sentiment-analysis-app-api.html',\n",
       " 'https://www.kdnuggets.com/2020/12/informs-machine-learning-roots.html',\n",
       " 'https://www.kdnuggets.com/2020/09/solving-linear-regression.html',\n",
       " 'https://www.kdnuggets.com/2020/12/crack-sql-interviews.html',\n",
       " 'https://www.kdnuggets.com/2021/04/automated-text-classification-evalml.html',\n",
       " 'https://www.kdnuggets.com/2021/06/high-performance-deep-learning-part2.html',\n",
       " 'https://www.kdnuggets.com/2021/03/top-youtube-machine-learning-channels.html',\n",
       " 'https://www.kdnuggets.com/2020/12/data-science-machine-learning-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2021/06/analytics-engineering-everywhere.html',\n",
       " 'https://www.kdnuggets.com/2021/07/python-data-structures-compared.html',\n",
       " 'https://www.kdnuggets.com/2021/07/roidna-aws-webinar-data-driven-esg-sustainability-decisions.html',\n",
       " 'https://www.kdnuggets.com/2021/06/nomad-data-matters.html',\n",
       " 'https://www.kdnuggets.com/2021/02/6-data-science-certificates.html',\n",
       " 'https://www.kdnuggets.com/2021/04/covid-do-all-our-models.html',\n",
       " 'https://www.kdnuggets.com/2021/04/automated-anomaly-detection-pycaret.html',\n",
       " 'https://www.kdnuggets.com/2021/04/e-commerce-data-analysis-sales-strategy-python.html',\n",
       " 'https://www.kdnuggets.com/2020/10/ai-learn-human-values.html',\n",
       " 'https://www.kdnuggets.com/2020/10/getting-data-science-job-harder.html',\n",
       " 'https://www.kdnuggets.com/2021/05/generate-meaningful-sentences-t5-transformer.html',\n",
       " 'https://www.kdnuggets.com/2021/06/applied-language-technology.html',\n",
       " 'https://www.kdnuggets.com/2020/12/essential-math-data-science-probability-density-probability-mass-functions.html',\n",
       " 'https://www.kdnuggets.com/2020/07/monitoring-apache-spark-better-ui.html',\n",
       " 'https://www.kdnuggets.com/2021/05/soda-io-managing-data-quality-sql-scale.html',\n",
       " 'https://www.kdnuggets.com/2021/05/streamsets-dataops-summit-2021-cfp.html',\n",
       " 'https://www.kdnuggets.com/2020/09/data-scientist-not-just-tiny-hands.html',\n",
       " 'https://www.kdnuggets.com/2021/09/antifragility-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/01/data-science-learning-journey.html',\n",
       " 'https://www.kdnuggets.com/2021/07/top-stories-2021-jun.html',\n",
       " 'https://www.kdnuggets.com/2021/08/querying-granular-demographic-dataset.html',\n",
       " 'https://www.kdnuggets.com/2020/10/understanding-transformers-data-science-way.html',\n",
       " 'https://www.kdnuggets.com/2020/07/better-blog-post-analysis-google-analytics-r.html',\n",
       " 'https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html',\n",
       " 'https://www.kdnuggets.com/2021/03/begin-nlp-journey.html',\n",
       " 'https://www.kdnuggets.com/2021/08/common-data-science-interview-questions-answers.html',\n",
       " 'https://www.kdnuggets.com/2020/10/deploying-streamlit-apps-streamlit-sharing.html',\n",
       " 'https://www.kdnuggets.com/2021/08/5-data-science-career-mistakes-avoid.html',\n",
       " 'https://www.kdnuggets.com/2021/07/brief-introduction-concept-data.html',\n",
       " 'https://www.kdnuggets.com/2021/09/top-18-low-code-no-code-machine-learning-platforms.html',\n",
       " 'https://www.kdnuggets.com/2021/03/simple-way-time-code-python.html',\n",
       " 'https://www.kdnuggets.com/2021/04/models-data-science-teams-chess-checkers.html',\n",
       " 'https://www.kdnuggets.com/2021/08/expert-nlp-insights-music.html',\n",
       " 'https://www.kdnuggets.com/2021/07/full-cross-validation-learning-curves-time-series.html',\n",
       " 'https://www.kdnuggets.com/2020/10/fastcore-underrated-python-library.html',\n",
       " 'https://www.kdnuggets.com/2021/03/worlddata-ai-kdnuggets-partner.html',\n",
       " 'https://www.kdnuggets.com/2021/09/roidna-aws-webinar-consumer-insights-data.html',\n",
       " 'https://www.kdnuggets.com/2021/05/6-side-hustles-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/03/11-essential-code-blocks-exploratory-data-analysis.html',\n",
       " 'https://www.kdnuggets.com/2020/08/samsung-semiconductor-innovation-prevent-pandemic.html',\n",
       " 'https://www.kdnuggets.com/2020/12/sqream-massive-data-video-challenge.html',\n",
       " 'https://www.kdnuggets.com/2021/03/evaluating-object-detection-models-using-mean-average-precision.html',\n",
       " 'https://www.kdnuggets.com/2021/07/overview-albumentations-open-source-library-advanced-image-augmentations.html',\n",
       " 'https://www.kdnuggets.com/2021/01/mastering-tensorflow-variables-5-easy-steps.html',\n",
       " 'https://www.kdnuggets.com/2021/06/bigquery-snowflake-comparison-data-warehouse-giants.html',\n",
       " 'https://www.kdnuggets.com/2021/04/time-series-forecasting-predict-weather.html',\n",
       " 'https://www.kdnuggets.com/2020/12/facebook-open-sources-rebel-new-reinforcement-learning-agent.html',\n",
       " 'https://www.kdnuggets.com/2021/01/deep-learning-pioneer-geoff-hinton-research-future-ai.html',\n",
       " 'https://www.kdnuggets.com/2020/11/5-things-doing-wrong-pycaret.html',\n",
       " 'https://www.kdnuggets.com/2021/07/10-machine-learning-model-training-mistakes.html',\n",
       " 'https://www.kdnuggets.com/2021/07/high-performance-deep-learning-part3.html',\n",
       " 'https://www.kdnuggets.com/2021/05/essential-math-data-science-basis-change-basis.html',\n",
       " 'https://www.kdnuggets.com/2021/04/dijkstra-principle-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/imerit2-bias-variance-unpredictability.html',\n",
       " 'https://www.kdnuggets.com/2021/04/secret-analysing-large-complex-datasets-constraint.html',\n",
       " 'https://www.kdnuggets.com/2021/08/top-stories-2021-jul.html',\n",
       " 'https://www.kdnuggets.com/2021/09/speeding-neural-network-training-multiple-gpus-dask.html',\n",
       " 'https://www.kdnuggets.com/2021/03/10-amazing-machine-learning-projects-2020.html',\n",
       " 'https://www.kdnuggets.com/2021/06/10-mistakes-avoid-data-science-beginner.html',\n",
       " 'https://www.kdnuggets.com/2021/08/train-bert-model-scratch.html',\n",
       " 'https://www.kdnuggets.com/2021/01/google-trillion-parameter-switch-transformer-model.html',\n",
       " 'https://www.kdnuggets.com/2020/09/geographical-plots-python.html',\n",
       " 'https://www.kdnuggets.com/2020/12/predictions-ai-machine-learning-data-science-research.html',\n",
       " 'https://www.kdnuggets.com/2021/04/cogitotech-6-mistakes-avoid-training-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/04/best-machine-learning-frameworks-extensions-tensorflow.html',\n",
       " 'https://www.kdnuggets.com/2020/11/dalex-explain-tensorflow-model.html',\n",
       " 'https://www.kdnuggets.com/2021/08/ai21-jurassic1-language-models.html',\n",
       " 'https://www.kdnuggets.com/2021/03/learning-from-machine-learning-mistakes.html',\n",
       " 'https://www.kdnuggets.com/2021/06/determined-ai-speed-up-deep-learning-language-model.html',\n",
       " 'https://www.kdnuggets.com/2020/10/data-science-cloud-dask.html',\n",
       " 'https://www.kdnuggets.com/2020/09/autograd-best-machine-learning-library-not-using.html',\n",
       " 'https://www.kdnuggets.com/2021/06/nij-recidivism-forecasting-challenge.html',\n",
       " 'https://www.kdnuggets.com/2021/04/build-impressive-data-science-resume.html',\n",
       " 'https://www.kdnuggets.com/2021/03/top-10-python-libraries-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/09/springboard-difference-data-engineers-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/08/demystifying-ai-prejudices.html',\n",
       " 'https://www.kdnuggets.com/2021/05/vaex-pandas-1000x-faster.html',\n",
       " 'https://www.kdnuggets.com/2021/08/learned-women-data-science-conferences.html',\n",
       " 'https://www.kdnuggets.com/2021/06/shortage-data-science-jobs-5-years.html',\n",
       " 'https://www.kdnuggets.com/2021/04/awesome-tricks-best-practices-kaggle.html',\n",
       " 'https://www.kdnuggets.com/2021/07/best-sota-nlp-course-free.html',\n",
       " 'https://www.kdnuggets.com/2020/12/industry-2021-predictions-ai-data-science-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/05/vc-pitch-deck-open-source-elt-platform.html',\n",
       " 'https://www.kdnuggets.com/2020/11/data-professionals-add-variation-resumes.html',\n",
       " 'https://www.kdnuggets.com/2020/12/mlops-why-required-what-is.html',\n",
       " 'https://www.kdnuggets.com/2021/06/machine-learning-model-interpretation.html',\n",
       " 'https://www.kdnuggets.com/2021/04/easy-automl-python.html',\n",
       " 'https://www.kdnuggets.com/2020/08/nlp-model-forge.html',\n",
       " 'https://www.kdnuggets.com/2021/07/wht-simpler-fast-fourier-transform-fft.html',\n",
       " 'https://www.kdnuggets.com/2021/06/fine-tune-bert-transformer-spacy.html',\n",
       " 'https://www.kdnuggets.com/2020/09/data-scientist-data-problem-wrong.html',\n",
       " 'https://www.kdnuggets.com/2021/01/data-engineering-troublesome.html',\n",
       " 'https://www.kdnuggets.com/2021/07/abstraction-data-science-not-great-combination.html',\n",
       " 'https://www.kdnuggets.com/2020/10/ethics-ai-qa-farzindar.html',\n",
       " 'https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html',\n",
       " 'https://www.kdnuggets.com/2021/03/pandas-big-data-better-options.html',\n",
       " 'https://www.kdnuggets.com/2021/05/dont-need-data-engineers-need-better-tools-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/03/top-stories-2021-feb.html',\n",
       " 'https://www.kdnuggets.com/2021/03/8-women-ai-striving-humanize-world.html',\n",
       " 'https://www.kdnuggets.com/2021/04/how-organize-your-data-science-project-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/04/time-series-using-sql.html',\n",
       " 'https://www.kdnuggets.com/2021/09/a-breakdown-deep-learning-frameworks.html',\n",
       " 'https://www.kdnuggets.com/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html',\n",
       " 'https://www.kdnuggets.com/2021/06/pycaret-101-introduction-beginners.html',\n",
       " 'https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/04/top-3-statistical-paradoxes-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/07/github-copilot-open-source-alternatives-code-generation.html',\n",
       " 'https://www.kdnuggets.com/2020/11/microsoft-google-open-sourced-frameworks-scaling-deep-learning-training.html',\n",
       " 'https://www.kdnuggets.com/2021/01/deepmind-muzero-important-deep-learning-systems.html',\n",
       " 'https://www.kdnuggets.com/2021/07/11-important-probability-distributions-explained.html',\n",
       " 'https://www.kdnuggets.com/2021/09/8-deep-learning-project-ideas-beginners.html',\n",
       " 'https://www.kdnuggets.com/2021/07/python-tips-snippets-data-processing.html',\n",
       " 'https://www.kdnuggets.com/2020/07/ethical-social-issues-natural-language-processing.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-scientists-compete-global-job-market.html',\n",
       " 'https://www.kdnuggets.com/2020/12/14-data-science-projects-improve-skills.html',\n",
       " 'https://www.kdnuggets.com/2020/09/performance-machine-learning-model.html',\n",
       " 'https://www.kdnuggets.com/2021/02/deploy-flask-api-kubernetes-connect-micro-services.html',\n",
       " 'https://www.kdnuggets.com/2021/03/cmu-ms-business-analytics.html',\n",
       " 'https://www.kdnuggets.com/2020/12/manning-deep-learning-design-patterns.html',\n",
       " 'https://www.kdnuggets.com/2021/04/data-science-101-normalization-standardization-regularization.html',\n",
       " 'https://www.kdnuggets.com/2020/11/mastering-tensorflow-tensors-5-easy-steps.html',\n",
       " 'https://www.kdnuggets.com/2021/02/multidimensional-multi-sensor-time-series-data-analysis-framework.html',\n",
       " 'https://www.kdnuggets.com/2021/09/sas-popular-certifications-data-analytics-skills.html',\n",
       " 'https://www.kdnuggets.com/2021/04/data-science-predict-prevent-real-world-problems.html',\n",
       " 'https://www.kdnuggets.com/2020/12/resampling-imbalanced-data-limits.html',\n",
       " 'https://www.kdnuggets.com/2020/11/essential-math-data-science-integrals-area-under-curve.html',\n",
       " 'https://www.kdnuggets.com/2021/07/7-open-source-libraries-deep-learning-graphs.html',\n",
       " 'https://www.kdnuggets.com/2020/10/imbalanced-data-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/09/international-alternatives-kaggle-data-science-competitions.html',\n",
       " 'https://www.kdnuggets.com/2021/03/kdnuggets-survey-data-community-job-satisfaction.html',\n",
       " 'https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html',\n",
       " 'https://www.kdnuggets.com/2021/06/7-data-security-best-practices-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-visualization-feature-selection.html',\n",
       " 'https://www.kdnuggets.com/2021/06/pandas-vs-sql.html',\n",
       " 'https://www.kdnuggets.com/2020/12/5-free-books-learn-statistics-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/most-demand-skills-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/02/dannet-triggers-deep-cnn-revolution.html',\n",
       " 'https://www.kdnuggets.com/2021/07/kafka-open-source-data-pipeline-processing-real-time-data.html',\n",
       " 'https://www.kdnuggets.com/2021/02/gilbert-people-skills-analytical-thinkers.html',\n",
       " 'https://www.kdnuggets.com/2020/09/implementing-deep-learning-library-scratch-python.html',\n",
       " 'https://www.kdnuggets.com/2020/11/top-stories-2020-oct.html',\n",
       " 'https://www.kdnuggets.com/2021/07/sas-building-tech-skills.html',\n",
       " 'https://www.kdnuggets.com/2021/05/ai-books-read-2021.html',\n",
       " 'https://www.kdnuggets.com/2020/11/best-data-science-certification-never-heard.html',\n",
       " 'https://www.kdnuggets.com/2020/09/potential-predictive-analytics-labor-industries.html',\n",
       " 'https://www.kdnuggets.com/2021/04/gradient-boosted-trees-conceptual-explanation.html',\n",
       " 'https://www.kdnuggets.com/2020/08/data-science-skills-superpower.html',\n",
       " 'https://www.kdnuggets.com/2020/08/microsoft-dowhy-framework-causal-inference.html',\n",
       " 'https://www.kdnuggets.com/2021/03/informs-virtual-career-fair.html',\n",
       " 'https://www.kdnuggets.com/2021/04/essential-math-data-science-linear-transformation-matrices.html',\n",
       " 'https://www.kdnuggets.com/2021/07/accept-null-hypothesis-wrong-intuitive-explanation.html',\n",
       " 'https://www.kdnuggets.com/2020/08/accelerated-computer-vision-free-course-amazon.html',\n",
       " 'https://www.kdnuggets.com/2020/10/data-protection-techniques-guarantee-privacy.html',\n",
       " 'https://www.kdnuggets.com/2021/04/8-most-common-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-storytelling.html',\n",
       " 'https://www.kdnuggets.com/2021/01/graph-theory-why-care.html',\n",
       " 'https://www.kdnuggets.com/2021/09/openai-codex-challenges.html',\n",
       " 'https://www.kdnuggets.com/2021/05/dataops-5-things-need-know.html',\n",
       " 'https://www.kdnuggets.com/2021/01/cloud-data-warehouse-future-data-storage.html',\n",
       " 'https://www.kdnuggets.com/2021/07/top-blogs-rewards-jun.html',\n",
       " 'https://www.kdnuggets.com/2020/08/data-science-machine-learning-capability-python.html',\n",
       " 'https://www.kdnuggets.com/2021/07/deep-learning-gpu-accelerate-data-science-data-analytics.html',\n",
       " 'https://www.kdnuggets.com/2021/01/unsupervised-learning-predictive-maintenance-auto-encoders.html',\n",
       " 'https://www.kdnuggets.com/2021/09/datacated-expo-oct-5.html',\n",
       " 'https://www.kdnuggets.com/2021/06/ai-with-feature-store.html',\n",
       " 'https://www.kdnuggets.com/2021/01/best-python-ide-code-editors.html',\n",
       " 'https://www.kdnuggets.com/2020/10/feature-ranking-recursive-feature-elimination-scikit-learn.html',\n",
       " 'https://www.kdnuggets.com/2020/10/5-must-read-data-science-papers.html',\n",
       " 'https://www.kdnuggets.com/2020/10/advice-aspiring-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/06/train-joint-entities-relation-extraction-classifier-bert-spacy.html',\n",
       " 'https://www.kdnuggets.com/2021/01/greatlearning-data-science-analytics-career-trends-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/04/ab-testing-7-common-questions-answers-data-science-interviews-2.html',\n",
       " 'https://www.kdnuggets.com/2021/01/comprehensive-guide-normal-distribution.html',\n",
       " 'https://www.kdnuggets.com/2021/02/northwestern-ms-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/06/managing-reusable-python-code-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/06/generate-automated-pdf-documents-python.html',\n",
       " 'https://www.kdnuggets.com/2021/08/bemyapp-florida-hacks-ibm.html',\n",
       " 'https://www.kdnuggets.com/2020/07/immuta-scale-sensitive-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/consider-being-data-engineer-instead-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/05/okera-airside-security-data-governance.html',\n",
       " 'https://www.kdnuggets.com/2021/07/5-mistakes-data-science-career.html',\n",
       " 'https://www.kdnuggets.com/2021/03/bayesian-hyperparameter-optimization-tune-sklearn-pycaret.html',\n",
       " 'https://www.kdnuggets.com/2021/04/coursera-career-growing-field-google-data-analytics-certificate.html',\n",
       " 'https://www.kdnuggets.com/2021/02/hugging-face-transformer-basics.html',\n",
       " 'https://www.kdnuggets.com/2021/03/overview-mlops.html',\n",
       " 'https://www.kdnuggets.com/2020/12/immuta-future-cloud-now.html',\n",
       " 'https://www.kdnuggets.com/2020/11/computer-vision-scale-dask-pytorch.html',\n",
       " 'https://www.kdnuggets.com/2020/11/jmp-effective-disease-outbreak-alert-system.html',\n",
       " 'https://www.kdnuggets.com/2021/03/forget-telling-stories-help-people-navigate.html',\n",
       " 'https://www.kdnuggets.com/2021/04/sas-viya-cloud-microsoft.html',\n",
       " 'https://www.kdnuggets.com/2021/06/coiled-workflow-orchestration-prefect.html',\n",
       " 'https://www.kdnuggets.com/2021/07/google-advice-learning-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/whats-etl.html',\n",
       " 'https://www.kdnuggets.com/2020/10/text-mining-r-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2021/07/streamlit-tips-tricks-hacks-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/07/build-image-classifier-in-few-lines-of-code-with-flash.html',\n",
       " 'https://www.kdnuggets.com/2021/01/mlops-model-monitoring-101.html',\n",
       " 'https://www.kdnuggets.com/2020/07/understanding-neural-networks-think.html',\n",
       " 'https://www.kdnuggets.com/2020/10/fast-gradient-boosting-catboost.html',\n",
       " 'https://www.kdnuggets.com/2020/09/what-argentine-writer-hungarian-mathematician-machine-learning-overfitting.html',\n",
       " 'https://www.kdnuggets.com/2021/08/development-testing-etl-pipelines-aws-locally.html',\n",
       " 'https://www.kdnuggets.com/2021/09/best-resources-learn-natural-language-processing-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/06/expert-word-has-13-meanings.html',\n",
       " 'https://www.kdnuggets.com/2021/04/coursera2-top-10-data-science-courses.html',\n",
       " 'https://www.kdnuggets.com/2020/07/5-fantastic-nlp-books.html',\n",
       " 'https://www.kdnuggets.com/2021/08/open-source-datasets-computer-vision.html',\n",
       " 'https://www.kdnuggets.com/2021/01/machine-learning-adversarial-attacks.html',\n",
       " 'https://www.kdnuggets.com/2020/08/introduction-federated-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/02/nlp-improve-resume.html',\n",
       " 'https://www.kdnuggets.com/2021/07/distinguish-yourself-hundreds-other-data-science-candidates.html',\n",
       " 'https://www.kdnuggets.com/2021/02/beyond-nash-equilibrium-deepmind-solve-asymmetric-games.html',\n",
       " 'https://www.kdnuggets.com/2021/09/nine-tools-mastered-before-phd-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/08/causal-inference-augmented-analytics-beyond-flatland.html',\n",
       " 'https://www.kdnuggets.com/2020/08/deepmind-three-pillars-building-robust-machine-learning-systems.html',\n",
       " 'https://www.kdnuggets.com/2021/06/troubleshoot-memory-problems-python.html',\n",
       " 'https://www.kdnuggets.com/2020/09/improve-machine-learning-models-accuracy.html',\n",
       " 'https://www.kdnuggets.com/2021/08/learning-path-changed-becoming-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/01/top-10-computer-vision-papers-2020.html',\n",
       " 'https://www.kdnuggets.com/2021/01/data-science-agile-best-practices.html',\n",
       " 'https://www.kdnuggets.com/2020/10/make-sense-reinforcement-learning-agents.html',\n",
       " 'https://www.kdnuggets.com/2020/09/insiders-guide-generative-discriminative-machine-learning-models.html',\n",
       " 'https://www.kdnuggets.com/2021/08/model-drift-machine-learning-big-data.html',\n",
       " 'https://www.kdnuggets.com/2020/11/tabpy-combining-python-tableau.html',\n",
       " 'https://www.kdnuggets.com/2020/11/fraud-eyes-machine.html',\n",
       " 'https://www.kdnuggets.com/2021/06/overcoming-simplicity-illusion-data-migration.html',\n",
       " 'https://www.kdnuggets.com/2020/11/cartoon-thanksgiving-turkey-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/09/machine-learning-leverages-linear-algebra-solve-data-problems.html',\n",
       " 'https://www.kdnuggets.com/2021/08/sql-without-own-database.html',\n",
       " 'https://www.kdnuggets.com/2020/09/kdd-2020-celebrates-recipients-sigkdd-best-paper-awards.html',\n",
       " 'https://www.kdnuggets.com/2021/03/fico-sudoku-decision-engine-solve.html',\n",
       " 'https://www.kdnuggets.com/2020/09/data-science-analytics-job-trends.html',\n",
       " 'https://www.kdnuggets.com/2021/03/know-your-data-much-faster-sweetviz-python-library.html',\n",
       " 'https://www.kdnuggets.com/2021/08/how-become-freelance-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2020/10/guide-preparing-opencv-android.html',\n",
       " 'https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html',\n",
       " 'https://www.kdnuggets.com/2020/10/deploying-secure-scalable-streamlit-apps-aws-docker-swarm-traefik-keycloak.html',\n",
       " 'https://www.kdnuggets.com/2020/09/nist-challenge.html',\n",
       " 'https://www.kdnuggets.com/2020/12/greatlearning-applications-data-science-business-analytics.html',\n",
       " 'https://www.kdnuggets.com/2021/07/swav-method.html',\n",
       " 'https://www.kdnuggets.com/2021/05/machine-translation-nutshell.html',\n",
       " 'https://www.kdnuggets.com/2021/04/feature-engineering-datetime-variables-data-science-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/03/9-skills-become-data-engineer.html',\n",
       " 'https://www.kdnuggets.com/2020/12/mlops-changing-machine-learning-developed.html',\n",
       " 'https://www.kdnuggets.com/2020/07/speed-up-numpy-pandas-numexpr-package.html',\n",
       " 'https://www.kdnuggets.com/2021/05/differentiable-programming-from-scratch.html',\n",
       " 'https://www.kdnuggets.com/2021/01/get-job-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/06/top-10-data-science-projects-beginners.html',\n",
       " 'https://www.kdnuggets.com/2021/02/stunning-visualizations-using-python.html',\n",
       " 'https://www.kdnuggets.com/2020/07/recommender-systems-nutshell.html',\n",
       " 'https://www.kdnuggets.com/2021/02/saving-loading-models-tensorflow.html',\n",
       " 'https://www.kdnuggets.com/2021/02/10-resources-data-science-self-study.html',\n",
       " 'https://www.kdnuggets.com/2020/09/missing-value-imputation-review.html',\n",
       " 'https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-scientists-need-know-code.html',\n",
       " 'https://www.kdnuggets.com/2021/05/what-makes-ai-trustworthy.html',\n",
       " 'https://www.kdnuggets.com/2021/01/github-career-growth-ai-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/02/essential-math-data-science-scalars-vectors.html',\n",
       " 'https://www.kdnuggets.com/2021/01/null-hypothesis-significance-testing-useful.html',\n",
       " 'https://www.kdnuggets.com/2021/07/retrain-machine-learning-model-5-checks-decide-schedule.html',\n",
       " 'https://www.kdnuggets.com/2021/08/visualizing-bias-variance.html',\n",
       " 'https://www.kdnuggets.com/2021/05/super-charge-python-pandas-gpus-saturn-cloud.html',\n",
       " 'https://www.kdnuggets.com/2020/12/remembering-pluribus-facebook-master-difficult-poker-game.html',\n",
       " 'https://www.kdnuggets.com/2021/03/machine-learning-model-monitoring-checklist.html',\n",
       " 'https://www.kdnuggets.com/2020/11/missing-teams-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2020/08/performance-testing-big-data-applications.html',\n",
       " 'https://www.kdnuggets.com/2021/01/top-stories-2020-dec.html',\n",
       " 'https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html',\n",
       " 'https://www.kdnuggets.com/2021/07/pushing-no-code-machine-learning-edge.html',\n",
       " 'https://www.kdnuggets.com/2021/07/learning-path-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/02/build-first-data-science-application.html',\n",
       " 'https://www.kdnuggets.com/2021/03/too-late-learn-ai.html',\n",
       " 'https://www.kdnuggets.com/2021/01/mlops-effective-ai-strategy.html',\n",
       " 'https://www.kdnuggets.com/2020/07/5-big-trends-data-analytics.html',\n",
       " 'https://www.kdnuggets.com/2021/09/-structured-financial-newsfeed-using-python-spacy-and-streamlit.html',\n",
       " 'https://www.kdnuggets.com/2020/11/simple-python-package-comparing-plotting-evaluating-regression-models.html',\n",
       " 'https://www.kdnuggets.com/2021/09/adventures-mlops-github-actions-iterative-ai-label-studio-and-nbdev.html',\n",
       " 'https://www.kdnuggets.com/2021/04/nosql-explained-understanding-key-value-databases.html',\n",
       " 'https://www.kdnuggets.com/2020/11/cellular-automata-stream-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/11/plexpage-multi-domain-summarization.html',\n",
       " 'https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/04/data-profession-job-satisfaction-results.html',\n",
       " 'https://www.kdnuggets.com/2021/09/automl-pipeline-optimization-sandbox.html',\n",
       " 'https://www.kdnuggets.com/2020/09/inside-blackbox-trick-neural-network.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-scientists-data-engineering-skills.html',\n",
       " 'https://www.kdnuggets.com/2020/07/laymans-guide-data-science-workflow.html',\n",
       " 'https://www.kdnuggets.com/2021/01/loglet-analysis-revisiting-covid-19-projections.html',\n",
       " 'https://www.kdnuggets.com/2021/08/data-science-project-infrastructure.html',\n",
       " 'https://www.kdnuggets.com/2020/12/jmp-dark-data-chapter.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-science-not-becoming-extinct-10-years.html',\n",
       " 'https://www.kdnuggets.com/2021/09/domino-300-data-science-leaders.html',\n",
       " 'https://www.kdnuggets.com/2020/12/10-python-skills-beginners.html',\n",
       " 'https://www.kdnuggets.com/2021/02/open-source-automl-python-evalml.html',\n",
       " 'https://www.kdnuggets.com/2020/09/introduction-time-series-analysis-python.html',\n",
       " 'https://www.kdnuggets.com/2021/02/machine-learning-assumptions.html',\n",
       " 'https://www.kdnuggets.com/2021/08/distributed-python-application-ray.html',\n",
       " 'https://www.kdnuggets.com/2021/09/imbalanced-classification-without-re-balancing-data.html',\n",
       " 'https://www.kdnuggets.com/2021/01/cleaner-data-analysis-pandas-pipes.html',\n",
       " 'https://www.kdnuggets.com/2021/02/speed-up-scikit-learn-model-training.html',\n",
       " 'https://www.kdnuggets.com/2020/08/fico-xpress-insight-python-deployment.html',\n",
       " 'https://www.kdnuggets.com/2021/08/linear-algebra-natural-language-processing.html',\n",
       " 'https://www.kdnuggets.com/2021/03/metrics-evaluating-classification-models-part1.html',\n",
       " 'https://www.kdnuggets.com/2021/05/pycaret-write-train-custom-machine-learning-models.html',\n",
       " 'https://www.kdnuggets.com/2020/12/clean-text-data-command-line.html',\n",
       " 'https://www.kdnuggets.com/2021/03/best-machine-learning-frameworks-extensions-scikit-learn.html',\n",
       " 'https://www.kdnuggets.com/2021/03/google-model-search-open-source-framework.html',\n",
       " 'https://www.kdnuggets.com/2020/11/data-scientists-communicating-management.html',\n",
       " 'https://www.kdnuggets.com/2021/01/learn-become-data-scientist-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/05/explainable-boosting-machine.html',\n",
       " 'https://www.kdnuggets.com/2021/06/roidna-external-data-accelerate-business-webinar.html',\n",
       " 'https://www.kdnuggets.com/2021/01/openai-transformer-models-link-language-computer-vision.html',\n",
       " 'https://www.kdnuggets.com/2021/05/sas-live-web-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/07/bitter-lesson-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/04/apply-transformers-any-length-text.html',\n",
       " 'https://www.kdnuggets.com/2021/09/dataset-asking-10-questions.html',\n",
       " 'https://www.kdnuggets.com/2020/11/moving-data-science-machine-learning-engineering.html',\n",
       " 'https://www.kdnuggets.com/2021/03/why-data-scientists-quit-good-jobs.html',\n",
       " 'https://www.kdnuggets.com/2021/05/most-demand-skills-data-engineers-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/04/deepfakes-mainstream-next.html',\n",
       " 'https://www.kdnuggets.com/2020/11/build-football-dataset-web-scraping.html',\n",
       " 'https://www.kdnuggets.com/2021/05/awesome-list-datasets.html',\n",
       " 'https://www.kdnuggets.com/2021/08/csv-files-storage-better-option.html',\n",
       " 'https://www.kdnuggets.com/2021/07/nvidia-gpu-accelerated-libraries.html',\n",
       " 'https://www.kdnuggets.com/2020/12/simple-intuitive-meta-learning-r.html',\n",
       " 'https://www.kdnuggets.com/2021/08/stack-overflow-survey-data-science-highlights.html',\n",
       " 'https://www.kdnuggets.com/2021/08/for-sql-data-people.html',\n",
       " 'https://www.kdnuggets.com/2021/04/datayap-apr-17.html',\n",
       " 'https://www.kdnuggets.com/2021/05/top-programming-languages.html',\n",
       " 'https://www.kdnuggets.com/2021/03/aiaccelerator-ai-industry-innovation.html',\n",
       " 'https://www.kdnuggets.com/2021/06/9-deadly-sins-ml-dataset-selection.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-science-portfolio-job.html',\n",
       " 'https://www.kdnuggets.com/2020/08/copyright-ebook-vocabularies-text-mining-fair-data.html',\n",
       " 'https://www.kdnuggets.com/2021/09/6-cool-python-libraries-recently.html',\n",
       " 'https://www.kdnuggets.com/2020/08/start-learning-data-science-again.html',\n",
       " 'https://www.kdnuggets.com/2020/09/design-experiments-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/deploy-machine-learning-models-to-web.html',\n",
       " 'https://www.kdnuggets.com/2021/06/sas-viya-visual-data-science-free-trial.html',\n",
       " 'https://www.kdnuggets.com/2021/07/understanding-bert-hugging-face.html',\n",
       " 'https://www.kdnuggets.com/2021/06/new-dimension-photos-python.html',\n",
       " 'https://www.kdnuggets.com/2020/11/machine-learning-social-good.html',\n",
       " 'https://www.kdnuggets.com/2020/10/copyright-knowledge-graphs-webinar.html',\n",
       " 'https://www.kdnuggets.com/2021/01/8-new-tools-learned-data-scientist-2020.html',\n",
       " 'https://www.kdnuggets.com/2020/10/optimizing-levenshtein-distance-measuring-text-similarity.html',\n",
       " 'https://www.kdnuggets.com/2020/11/top-5-free-machine-learning-deep-learning-ebooks.html',\n",
       " 'https://www.kdnuggets.com/2021/03/explainable-visual-reasoning-mit-builds-neural-networks-explain-themselves.html',\n",
       " 'https://www.kdnuggets.com/2021/04/top-stories-2021-mar.html',\n",
       " 'https://www.kdnuggets.com/2021/05/octoparse-cloud-web-scraping-big-data.html',\n",
       " 'https://www.kdnuggets.com/2021/06/pinecone-similarity-search-euclid-alexandria-shoe-shopping.html',\n",
       " 'https://www.kdnuggets.com/2021/07/learning-data-science-through-social-media.html',\n",
       " 'https://www.kdnuggets.com/2021/08/most-important-tool-data-engineers.html',\n",
       " 'https://www.kdnuggets.com/2020/07/labelling-data-using-snorkel.html',\n",
       " 'https://www.kdnuggets.com/2020/11/facebook-open-source-frameworks-advance-deep-learning-research.html',\n",
       " 'https://www.kdnuggets.com/2020/10/manning-deep-learning-design-patterns.html',\n",
       " 'https://www.kdnuggets.com/2021/07/colabcode-deploying-machine-learning-models-google-colab.html',\n",
       " 'https://www.kdnuggets.com/2021/06/poll-where-analytics-data-science-ml-applied.html',\n",
       " 'https://www.kdnuggets.com/2020/12/samsung-ai-processor-development.html',\n",
       " 'https://www.kdnuggets.com/2021/08/what-is-noise.html',\n",
       " 'https://www.kdnuggets.com/2021/02/3-ways-understanding-bayes-theorem-improve-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/08/3-data-labeling-synthesizing-augmentation-tools.html',\n",
       " 'https://www.kdnuggets.com/2021/03/dask-pandas-data.html',\n",
       " 'https://www.kdnuggets.com/2021/09/path-full-stack-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/11/data-science-14-self-examination-questions.html',\n",
       " 'https://www.kdnuggets.com/2020/12/introduction-data-engineering.html',\n",
       " 'https://www.kdnuggets.com/2020/10/getting-started-ai-research.html',\n",
       " 'https://www.kdnuggets.com/2021/03/beautiful-decision-tree-visualizations-dtreeviz.html',\n",
       " 'https://www.kdnuggets.com/2020/08/must-read-nlp-deep-learning-articles.html',\n",
       " 'https://www.kdnuggets.com/2021/02/coiled-distributed-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/01/top-5-reasons-why-machine-learning-projects-fail.html',\n",
       " 'https://www.kdnuggets.com/2020/07/apache-spark-dataproc-vs-google-bigquery.html',\n",
       " 'https://www.kdnuggets.com/2020/10/greatlearning-become-data-scientist-guide.html',\n",
       " 'https://www.kdnuggets.com/2020/07/manning-math-architectures-deep-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/08/data-versioning-mean-think-means.html',\n",
       " 'https://www.kdnuggets.com/2021/09/expert-ai-speech-huggingface-facebook.html',\n",
       " 'https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html',\n",
       " 'https://www.kdnuggets.com/2021/03/one-question-makes-data-project-more-valuable.html',\n",
       " 'https://www.kdnuggets.com/2021/02/data-science-interviews-finding-jobs-reaching-gatekeepers-getting-referrals.html',\n",
       " 'https://www.kdnuggets.com/2021/02/ibm-continual-learning-avoid-amnesia-problem-neural-networks.html',\n",
       " 'https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html',\n",
       " 'https://www.kdnuggets.com/2020/12/feature-store-vs-data-warehouse.html',\n",
       " 'https://www.kdnuggets.com/2020/11/pandas-steroids-dask-python-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/10/automation-improving-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/04/data-science-books-start-reading-2021.html',\n",
       " 'https://www.kdnuggets.com/2020/11/emeritus-top-courses-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2020/10/deep-learning-virtual-try-clothes.html',\n",
       " 'https://www.kdnuggets.com/2021/05/pinecone-rise-vector-data.html',\n",
       " 'https://www.kdnuggets.com/2021/04/data-analysis-using-tableau.html',\n",
       " 'https://www.kdnuggets.com/2020/11/machine-learning-less-than-one-example.html',\n",
       " 'https://www.kdnuggets.com/2020/12/5-strategies-enterprise-machine-learning-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/01/machine-learning-real-time.html',\n",
       " 'https://www.kdnuggets.com/2020/10/jhu-analytics-johns-hopkins-ma-global-risk.html',\n",
       " 'https://www.kdnuggets.com/2020/11/acquire-most-wanted-data-science-skills.html',\n",
       " 'https://www.kdnuggets.com/2020/10/goodharts-law-data-science-measure-target.html',\n",
       " 'https://www.kdnuggets.com/2020/07/building-rest-api-tensorflow-serving-part-1.html',\n",
       " 'https://www.kdnuggets.com/2021/01/four-jobs-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/05/hilarious-data-science-humor.html',\n",
       " 'https://www.kdnuggets.com/2021/02/vision-transformers-nlp-efficiency-model-generality.html',\n",
       " 'https://www.kdnuggets.com/2020/07/exploratory-data-analysis-steroids.html',\n",
       " 'https://www.kdnuggets.com/2021/05/deal-with-categorical-data-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/02/critical-comparison-machine-learning-platforms-evolving-market.html',\n",
       " 'https://www.kdnuggets.com/2021/02/deep-learning-not-black-box.html',\n",
       " 'https://www.kdnuggets.com/2020/10/10-days-deep-learning-coders.html',\n",
       " 'https://www.kdnuggets.com/2021/07/northwestern-ms-analytics-partner.html',\n",
       " 'https://www.kdnuggets.com/2020/10/cartoon-cloud-dating.html',\n",
       " 'https://www.kdnuggets.com/2020/12/ai-registers-transparency-ml.html',\n",
       " 'https://www.kdnuggets.com/2021/07/dbt-data-transformation-tutorial.html',\n",
       " 'https://www.kdnuggets.com/2020/07/tour-end-to-end-machine-learning-platforms.html',\n",
       " 'https://www.kdnuggets.com/2021/07/pandas-alternatives-processing-larger-faster-data-python.html',\n",
       " 'https://www.kdnuggets.com/2021/08/5-things-job-data-scientist-easier.html',\n",
       " 'https://www.kdnuggets.com/2021/09/machine-deep-learning-open-book.html',\n",
       " 'https://www.kdnuggets.com/2021/08/sas-ai-real-life.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-scientist-without-stem-degree.html',\n",
       " 'https://www.kdnuggets.com/2021/09/ai-neurons-simulate-brain-neuron.html',\n",
       " 'https://www.kdnuggets.com/2021/07/top-6-data-science-online-courses.html',\n",
       " 'https://www.kdnuggets.com/2020/11/5-tricky-sql-queries-solved.html',\n",
       " 'https://www.kdnuggets.com/2021/09/ebook-learn-data-science-r.html',\n",
       " 'https://www.kdnuggets.com/2020/10/introduction-ai-updated.html',\n",
       " 'https://www.kdnuggets.com/2020/11/hypothesis-vetting-most-important-skill-every-successful-data-scientist-needs.html',\n",
       " 'https://www.kdnuggets.com/2021/06/knowledge-graph-job-search-bert.html',\n",
       " 'https://www.kdnuggets.com/2021/01/deep-learning-based-reverse-image-search.html',\n",
       " 'https://www.kdnuggets.com/2021/05/winning-machine-learning-competition.html',\n",
       " 'https://www.kdnuggets.com/2020/11/perceptilabs-primer-tensorflow.html',\n",
       " 'https://www.kdnuggets.com/2020/10/packt-significance-machine-learning-algorithmic-trading.html',\n",
       " 'https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/08/select-initial-model-data-science-problem.html',\n",
       " 'https://www.kdnuggets.com/2021/03/deepmind-alphafold-protein-folding-problem.html',\n",
       " 'https://www.kdnuggets.com/2020/11/kubernetes-amazon-ecs-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/06/fine-tuning-transformer-model-invoice-recognition.html',\n",
       " 'https://www.kdnuggets.com/2020/07/complete-guide-survival-analysis-python-part1.html',\n",
       " 'https://www.kdnuggets.com/2021/05/top-4-data-extraction-tools.html',\n",
       " 'https://www.kdnuggets.com/2021/06/5-tips-edge-ai-platform.html',\n",
       " 'https://www.kdnuggets.com/2020/09/modern-data-science-skills.html',\n",
       " 'https://www.kdnuggets.com/2021/05/charticulator-microsoft-research-data-visualization-platform.html',\n",
       " 'https://www.kdnuggets.com/2021/05/confidence-intervals-xgboost.html',\n",
       " 'https://www.kdnuggets.com/2021/07/roidnab-ebook-data-smarter-decisions.html',\n",
       " 'https://www.kdnuggets.com/2021/03/statistics-data-scientists-should-know.html',\n",
       " 'https://www.kdnuggets.com/2020/11/tabular-data-huggingface-transformers.html',\n",
       " 'https://www.kdnuggets.com/2021/06/efficiency-deep-learning-part1.html',\n",
       " 'https://www.kdnuggets.com/2020/10/immuta-seven-steps-migrating-sensitive-data-cloud.html',\n",
       " 'https://www.kdnuggets.com/2021/09/top-blogs-rewards-aug.html',\n",
       " 'https://www.kdnuggets.com/2021/09/machine-learning-beneficial-mobile-app-development.html',\n",
       " 'https://www.kdnuggets.com/2021/02/one-question-data-project-10x-valuable.html',\n",
       " 'https://www.kdnuggets.com/2020/12/data-science-volunteering.html',\n",
       " 'https://www.kdnuggets.com/2021/09/introducing-tensorflow-similarity.html',\n",
       " 'https://www.kdnuggets.com/2021/03/reducing-high-cost-training-nlp-models-sru.html',\n",
       " 'https://www.kdnuggets.com/2020/07/first-steps-data-science-project.html',\n",
       " 'https://www.kdnuggets.com/2021/09/sparkbeyond-messy-data-is-beautiful.html',\n",
       " 'https://www.kdnuggets.com/2020/08/implementing-mlops-edge-device.html',\n",
       " 'https://www.kdnuggets.com/2020/09/successful-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/04/multiple-time-series-forecasting-pycaret.html',\n",
       " 'https://www.kdnuggets.com/2021/02/deep-learning-based-real-time-video-processing.html',\n",
       " 'https://www.kdnuggets.com/2020/12/object-oriented-programming-explained-simply-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2020/11/friendly-introduction-graph-neural-networks.html',\n",
       " 'https://www.kdnuggets.com/2020/09/ai-driving-innovation-astronomy.html',\n",
       " 'https://www.kdnuggets.com/2021/05/poll-did-apply-analytics-data-science-machine-learning-2020-2021.html',\n",
       " 'https://www.kdnuggets.com/2020/09/showcasing-benefits-software-optimizations-ai-workloads-intel.html',\n",
       " 'https://www.kdnuggets.com/2021/04/dockerize-any-machine-learning-application.html',\n",
       " 'https://www.kdnuggets.com/2021/06/wpi-polytechnic-make-tech-business-connection.html',\n",
       " 'https://www.kdnuggets.com/2021/09/math-fundamental-importance-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/04/production-ready-machine-learning-nlp-api-fastapi-spacy.html',\n",
       " 'https://www.kdnuggets.com/2021/04/deep-learning-recommendation-models-dlrm-deep-dive.html',\n",
       " 'https://www.kdnuggets.com/2020/09/pycaret-21-new.html',\n",
       " 'https://www.kdnuggets.com/2020/12/20-core-data-science-concepts-beginners.html',\n",
       " 'https://www.kdnuggets.com/2020/07/before-probability-distributions.html',\n",
       " 'https://www.kdnuggets.com/2020/12/mathworks-ai-models-high-frequency-streaming-data.html',\n",
       " 'https://www.kdnuggets.com/2020/10/overcoming-racial-bias-ai.html',\n",
       " 'https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html',\n",
       " 'https://www.kdnuggets.com/2021/02/gpt2-gpt3-openai-showdown.html',\n",
       " 'https://www.kdnuggets.com/2021/06/graph-based-text-similarity-method-named-entity-information-nlp.html',\n",
       " 'https://www.kdnuggets.com/2021/04/adversarial-neural-cryptography.html',\n",
       " 'https://www.kdnuggets.com/2020/08/joke-about.html',\n",
       " 'https://www.kdnuggets.com/2020/10/explain-machine-learning-algorithms-interview.html',\n",
       " 'https://www.kdnuggets.com/2020/08/data-science-meets-devops-mlops-jupyter-git-kubernetes.html',\n",
       " 'https://www.kdnuggets.com/2021/01/data-scientist-dont-start-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/04/data-scientist-machine-learning-engineer-skills.html',\n",
       " 'https://www.kdnuggets.com/2020/10/stop-running-jupyter-notebooks-command-line.html',\n",
       " 'https://www.kdnuggets.com/2021/05/4-tips-dataset-curation-nlp-projects.html',\n",
       " 'https://www.kdnuggets.com/2021/01/support-vector-machine-hand-written-alphabet-r.html',\n",
       " 'https://www.kdnuggets.com/2021/02/feature-store-foundation-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/02/data-science-make-you-happy.html',\n",
       " 'https://www.kdnuggets.com/2020/09/machine-learning-model-deployment.html',\n",
       " 'https://www.kdnuggets.com/2020/09/machine-learning-from-scratch-free-online-textbook.html',\n",
       " 'https://www.kdnuggets.com/2021/09/hypothesis-testing-explained.html',\n",
       " 'https://www.kdnuggets.com/2020/10/building-neural-networks-pytorch-google-colab.html',\n",
       " 'https://www.kdnuggets.com/2021/01/creating-good-meaningful-plots-principles.html',\n",
       " 'https://www.kdnuggets.com/2021/08/learn-data-science-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/03/getting-started-distributed-machine-learning-pytorch-ray.html',\n",
       " 'https://www.kdnuggets.com/2021/01/essential-math-data-science-information-theory.html',\n",
       " 'https://www.kdnuggets.com/2020/08/paw-predictive-analytics-world-berlin-2020-keynote-sessions-announced.html',\n",
       " 'https://www.kdnuggets.com/2021/09/20-machine-learning-projects-hired.html',\n",
       " 'https://www.kdnuggets.com/2021/08/python-data-processing-script-template.html',\n",
       " 'https://www.kdnuggets.com/2020/11/build-data-science-project.html',\n",
       " 'https://www.kdnuggets.com/2020/11/machine-learning-knows-too-much-about-you.html',\n",
       " 'https://www.kdnuggets.com/2020/10/free-mit-intro-computational-thinking-data-science-python.html',\n",
       " 'https://www.kdnuggets.com/2021/09/prefect-way-automate-orchestrate-data-pipelines.html',\n",
       " 'https://www.kdnuggets.com/2020/07/demystifying-statistical-significance.html',\n",
       " 'https://www.kdnuggets.com/2021/08/data-scientist-guide-efficient-coding-python.html',\n",
       " 'https://www.kdnuggets.com/2020/12/greatlearning-ai-modern-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/08/watchful-automated-data-labeling-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/08/build-automl-pycaret.html',\n",
       " 'https://www.kdnuggets.com/2020/07/apache-spark-cluster-docker.html',\n",
       " 'https://www.kdnuggets.com/2020/10/sqream-massive-data.html',\n",
       " 'https://www.kdnuggets.com/2020/08/linguistic-fundamentals-natural-language-processing.html',\n",
       " 'https://www.kdnuggets.com/2020/10/uchicago-machine-learning-cybersecurity-certificate.html',\n",
       " 'https://www.kdnuggets.com/2021/03/multilingual-clip--huggingface-pytorch-lightning.html',\n",
       " 'https://www.kdnuggets.com/2020/09/packt-data-analytics-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/07/data-science-moocs-superficial.html',\n",
       " 'https://www.kdnuggets.com/2021/01/advice-aspiring-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2020/08/spark-python-sql-azure-databricks.html',\n",
       " 'https://www.kdnuggets.com/2020/11/essential-data-science-skills-no-one-talks-about.html',\n",
       " 'https://www.kdnuggets.com/2021/01/jupyterlab-3-here-reasons-upgrade.html',\n",
       " 'https://www.kdnuggets.com/2020/09/siegel2-coursera-machine-learning-training.html',\n",
       " 'https://www.kdnuggets.com/2021/05/soft-skills-data-science-career.html',\n",
       " 'https://www.kdnuggets.com/2021/05/choosing-right-bi-tool-business.html',\n",
       " 'https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html',\n",
       " 'https://www.kdnuggets.com/2021/06/power-mlops-dataops-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/11/greatlearning-data-science-certification.html',\n",
       " 'https://www.kdnuggets.com/2020/09/ai-healthcare-review-innovative-startups.html',\n",
       " 'https://www.kdnuggets.com/2020/09/deepmind-neural-networks-show-imagination.html',\n",
       " 'https://www.kdnuggets.com/2021/08/5-tips-writing-clean-r-code.html',\n",
       " 'https://www.kdnuggets.com/2021/01/ultimate-scikit-learn-machine-learning-cheatsheet.html',\n",
       " 'https://www.kdnuggets.com/2020/11/data-science-online-learning-journey-coursera.html',\n",
       " 'https://www.kdnuggets.com/2020/09/4-tricks-effectively-use-json-python.html',\n",
       " 'https://www.kdnuggets.com/2020/08/getting-started-feature-selection.html',\n",
       " 'https://www.kdnuggets.com/2021/06/in-warehouse-machine-learning-modern-data-science-stack.html',\n",
       " 'https://www.kdnuggets.com/2021/01/top-5-artificial-intelligence-trends-2021.html',\n",
       " 'https://www.kdnuggets.com/2020/08/beyond-turing-test.html',\n",
       " 'https://www.kdnuggets.com/2020/11/predicting-heart-disease-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/09/making-python-programs-blazingly-fast.html',\n",
       " 'https://www.kdnuggets.com/2021/09/build-synthetic-data-pipeline-gretel-apache-airflow.html',\n",
       " 'https://www.kdnuggets.com/2020/07/guide-survival-analysis-python-part-3.html',\n",
       " 'https://www.kdnuggets.com/2020/09/deep-learning-dream-accuracy-interpretability-single-model.html',\n",
       " 'https://www.kdnuggets.com/2021/09/computer-vision-agriculture.html',\n",
       " 'https://www.kdnuggets.com/2021/08/visualization-transforming-exploratory-data-analysis.html',\n",
       " 'https://www.kdnuggets.com/2020/09/data-scientists-should-be-more-end-to-end.html',\n",
       " 'https://www.kdnuggets.com/2021/08/engineer-date-features-python.html',\n",
       " 'https://www.kdnuggets.com/2020/07/r-squared-predictive-capacity-statistical-adequacy.html',\n",
       " 'https://www.kdnuggets.com/2021/05/deploy-dockerized-fastapi-app-google-cloud-platform.html',\n",
       " 'https://www.kdnuggets.com/2020/12/nosql-beginners.html',\n",
       " 'https://www.kdnuggets.com/2021/02/uchicago-machine-learning-cybersecurity.html',\n",
       " 'https://www.kdnuggets.com/2021/02/approaching-almost-any-machine-learning-problem.html',\n",
       " 'https://www.kdnuggets.com/2020/11/greatlearning-data-scientist-machine-learning-engineer.html',\n",
       " 'https://www.kdnuggets.com/2021/01/popular-machine-learning-interview-questions.html',\n",
       " 'https://www.kdnuggets.com/2021/02/data-science-vs-business-intelligence-explained.html',\n",
       " 'https://www.kdnuggets.com/2020/07/data-mining-machine-learning-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2020/07/5-things-pycaret.html',\n",
       " 'https://www.kdnuggets.com/2021/05/building-restful-apis-flask.html',\n",
       " 'https://www.kdnuggets.com/2020/09/creating-powerful-animated-visualizations-tableau.html',\n",
       " 'https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html',\n",
       " 'https://www.kdnuggets.com/2021/08/15-things-data-science-candidates.html',\n",
       " 'https://www.kdnuggets.com/2021/06/streamsets-dataops-platform-summer-public-beta.html',\n",
       " 'https://www.kdnuggets.com/2021/05/disentangling-ai-machine-learning-deep-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/05/top-stories-2021-apr.html',\n",
       " 'https://www.kdnuggets.com/2021/07/design-patterns-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/01/5-tools-effortless-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/02/celebrate-international-womens-day-women-data-science-wids-worldwide-virtual-conference.html',\n",
       " 'https://www.kdnuggets.com/2020/08/transformer-architecture-development-transformer-models.html',\n",
       " 'https://www.kdnuggets.com/2021/08/15-python-snippets-optimize-data-science-pipeline.html',\n",
       " 'https://www.kdnuggets.com/2020/10/machine-learning-omission-business-leadership.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-analysis-scala.html',\n",
       " 'https://www.kdnuggets.com/2020/09/poll-python-ide-editor.html',\n",
       " 'https://www.kdnuggets.com/2021/08/prefect-write-schedule-etl-pipeline-python.html',\n",
       " 'https://www.kdnuggets.com/2021/03/copyright-power-data-visualization.html',\n",
       " 'https://www.kdnuggets.com/2021/08/packed-bert-training-speed-up-natural-language-processing.html',\n",
       " 'https://www.kdnuggets.com/2021/06/from-scratch-permutation-feature-importance-ml-interpretability.html',\n",
       " 'https://www.kdnuggets.com/2021/03/teaching-ai-see-like-human.html',\n",
       " 'https://www.kdnuggets.com/2020/12/top-stories-2020-nov.html',\n",
       " 'https://www.kdnuggets.com/2020/12/future-etl-is-elt.html',\n",
       " 'https://www.kdnuggets.com/2021/07/charu-ai-textbook.html',\n",
       " 'https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html',\n",
       " 'https://www.kdnuggets.com/2021/06/computational-complexity-deep-learning-solution-approaches.html',\n",
       " 'https://www.kdnuggets.com/2021/08/multilabel-document-categorization.html',\n",
       " 'https://www.kdnuggets.com/2021/01/snowflake-saturncloud-faster-data-science-python.html',\n",
       " 'https://www.kdnuggets.com/2020/12/covid-cough-ai-detecting-sounds.html',\n",
       " 'https://www.kdnuggets.com/2020/09/simpsons-paradox.html',\n",
       " 'https://www.kdnuggets.com/2020/09/most-important-data-science-project.html',\n",
       " 'https://www.kdnuggets.com/2021/06/bill-frinks-season-1-data-science-perspectives-webcast.html',\n",
       " 'https://www.kdnuggets.com/2020/11/rise-machine-learning-engineer.html',\n",
       " 'https://www.kdnuggets.com/2021/07/expertai-ai-based-framework-solution-email-management.html',\n",
       " 'https://www.kdnuggets.com/2021/09/paradoxes-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/08/explainable-reproducible-machine-learning-model-development-dalex-neptune.html',\n",
       " 'https://www.kdnuggets.com/2021/03/causal-design-patterns.html',\n",
       " 'https://www.kdnuggets.com/2021/03/sas-banking-talent-development.html',\n",
       " 'https://www.kdnuggets.com/2020/08/immuta-data-pipelines-machine-learning-apache-spark-amazon-sagemaker.html',\n",
       " 'https://www.kdnuggets.com/2020/11/octoparse-coding-free-extract-content.html',\n",
       " 'https://www.kdnuggets.com/2021/03/imerit-data-annotation-workflows-trends.html',\n",
       " 'https://www.kdnuggets.com/2021/07/shareable-data-analyses-using-templates.html',\n",
       " 'https://www.kdnuggets.com/2021/05/next-generation-automl-frameworks.html',\n",
       " 'https://www.kdnuggets.com/2020/11/metis-power-spreadsheets-data-driven-culture.html',\n",
       " 'https://www.kdnuggets.com/2021/07/brutal-truth-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/10/10-best-machine-learning-courses-2020.html',\n",
       " 'https://www.kdnuggets.com/2021/07/data-monetization-101.html',\n",
       " 'https://www.kdnuggets.com/2021/02/adversarial-generation-extreme-samples.html',\n",
       " 'https://www.kdnuggets.com/2020/12/roadmaps-ai-developer-data-scientist-machine-learning-engineer.html',\n",
       " 'https://www.kdnuggets.com/2021/09/smart-ingestion-ontology-driven-ai.html',\n",
       " 'https://www.kdnuggets.com/2021/09/easy-sql-native-python.html',\n",
       " 'https://www.kdnuggets.com/2021/09/text-preprocessing-methods-deep-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/03/build-dag-factory-airflow.html',\n",
       " 'https://www.kdnuggets.com/2021/03/succeed-becoming-freelance-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/08/significance-data-centric-ai.html',\n",
       " 'https://www.kdnuggets.com/2021/03/mongodb-icloud-solutions-2021.html',\n",
       " 'https://www.kdnuggets.com/2020/07/building-content-based-book-recommendation-engine.html',\n",
       " 'https://www.kdnuggets.com/2020/12/matrix-decomposition-decoded.html',\n",
       " 'https://www.kdnuggets.com/2020/11/data-science-history-overview.html',\n",
       " 'https://www.kdnuggets.com/2020/08/accelerated-nlp-free-amazon-machine-learning-university.html',\n",
       " 'https://www.kdnuggets.com/2021/05/data-scientist-data-engineer-data-careers-explained.html',\n",
       " 'https://www.kdnuggets.com/2021/05/applying-pythons-explode-function-pandas-dataframes.html',\n",
       " 'https://www.kdnuggets.com/2020/10/jmp-effective-visualization-techniques.html',\n",
       " 'https://www.kdnuggets.com/2021/06/explainable-ai-xai-explainable-boosting-machines-ebm.html',\n",
       " 'https://www.kdnuggets.com/2021/05/data-practitioner-survey-know-worth.html',\n",
       " 'https://www.kdnuggets.com/2021/03/data-science-curriculum-professionals.html',\n",
       " 'https://www.kdnuggets.com/2021/04/10-real-life-applications-reinforcement-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/08/employers-expecting-data-scientist-role-2020.html',\n",
       " 'https://www.kdnuggets.com/2021/06/top-blogs-rewards-may.html',\n",
       " 'https://www.kdnuggets.com/2021/04/kdd-2021-data-science-conference-virtual.html',\n",
       " 'https://www.kdnuggets.com/2021/09/write-functions-use-dask.html',\n",
       " 'https://www.kdnuggets.com/2020/07/googles-new-machine-learning-certificate.html',\n",
       " 'https://www.kdnuggets.com/2021/06/make-python-code-run-incredibly-fast.html',\n",
       " 'https://www.kdnuggets.com/2021/05/data-validation-machine-learning-imperative.html',\n",
       " 'https://www.kdnuggets.com/2020/11/top-python-libraries-deep-learning-natural-language-processing-computer-vision.html',\n",
       " 'https://www.kdnuggets.com/2021/07/prepare-behavioral-questions-data-science-interviews.html',\n",
       " 'https://www.kdnuggets.com/2021/06/fico-numerics-vs-integrality-close-enough.html',\n",
       " 'https://www.kdnuggets.com/2021/09/python-apis-data-science-project.html',\n",
       " 'https://www.kdnuggets.com/2020/12/r-python-both-prython.html',\n",
       " 'https://www.kdnuggets.com/2021/04/free-stanford-machine-learning-graphs.html',\n",
       " 'https://www.kdnuggets.com/2021/01/six-tips-building-data-science-team-small-company.html',\n",
       " 'https://www.kdnuggets.com/2021/01/best-tool-data-blending-knime.html',\n",
       " 'https://www.kdnuggets.com/2020/10/doe-smart-visualization-platform-prize-challenge.html',\n",
       " 'https://www.kdnuggets.com/2021/04/roidna-location-aws-data-exchange.html',\n",
       " 'https://www.kdnuggets.com/2021/02/microsoft-explores-three-key-mysteries-ensemble-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/08/top-industries-hiring-data-scientists-2021the-top-industries-hiring-data-scientists-in-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/05/guide-become-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/02/inside-architecture-powering-data-quality-management-uber.html',\n",
       " 'https://www.kdnuggets.com/2021/08/coding-ethics-ai-aiops-designing-responsible-ai-systems.html',\n",
       " 'https://www.kdnuggets.com/2020/11/greatlearning-ai-project-ideas-beginners.html',\n",
       " 'https://www.kdnuggets.com/2021/09/introduction-automated-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/07/poll-data-scientists-not-extinct-10-years.html',\n",
       " 'https://www.kdnuggets.com/2020/07/computational-linear-algebra-free-course.html',\n",
       " 'https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html',\n",
       " 'https://www.kdnuggets.com/2021/07/tell-model-trained-enough-data.html',\n",
       " 'https://www.kdnuggets.com/2020/07/depth-useful-self-attention.html',\n",
       " 'https://www.kdnuggets.com/2021/02/fit-lead-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/12/10-python-skills-dont-teach-bootcamp.html',\n",
       " 'https://www.kdnuggets.com/2021/07/facebook-open-sources-chatbot-discuss-any-topic.html',\n",
       " 'https://www.kdnuggets.com/2021/05/what-neural-search.html',\n",
       " 'https://www.kdnuggets.com/2021/04/fludemic-ai-machine-learning-disease.html',\n",
       " 'https://www.kdnuggets.com/2021/01/top-10-technology-trends-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/02/machine-learning-systems-design-free-stanford-course.html',\n",
       " 'https://www.kdnuggets.com/2021/02/powerful-exploratory-data-analysis-sweetviz.html',\n",
       " 'https://www.kdnuggets.com/2020/12/merging-pandas-dataframes-python.html',\n",
       " 'https://www.kdnuggets.com/2021/09/intro-reinforcement-learning-openai-gym-rllib-colab.html',\n",
       " 'https://www.kdnuggets.com/2021/03/top-youtube-channels-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/09/create-stunning-web-apps-data-science-projects.html',\n",
       " 'https://www.kdnuggets.com/2021/08/artificial-intelligence-machine-learning-cybersecurity.html',\n",
       " 'https://www.kdnuggets.com/2021/09/7-differences-between-data-analyst-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2020/12/change-background-video-5-lines-code.html',\n",
       " 'https://www.kdnuggets.com/2020/07/why-put-scikit-learn-browser.html',\n",
       " 'https://www.kdnuggets.com/2020/10/mastering-time-series-analysis-experts.html',\n",
       " 'https://www.kdnuggets.com/2020/11/most-popular-distance-metrics-knn.html',\n",
       " 'https://www.kdnuggets.com/2020/09/online-courses-better-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/09/zero-rapids-minutes-nvidia-gpus-saturn-cloud.html',\n",
       " 'https://www.kdnuggets.com/2021/05/best-python-books-beginner-advanced.html',\n",
       " 'https://www.kdnuggets.com/2021/03/3-mathematical-laws.html',\n",
       " 'https://www.kdnuggets.com/2021/02/machine-learning-model-not-learn.html',\n",
       " 'https://www.kdnuggets.com/2021/08/11-best-data-science-education-platforms.html',\n",
       " 'https://www.kdnuggets.com/2021/07/data-scientists-machine-learning-engineers-luxury-employees.html',\n",
       " 'https://www.kdnuggets.com/2021/09/github-copilot-rise-ai-language-models-programming-automation.html',\n",
       " 'https://www.kdnuggets.com/2021/08/machine-learning-personalization-variety.html',\n",
       " 'https://www.kdnuggets.com/2021/03/overcome-fear-learn-math-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/top-10-must-know-machine-learning-algorithms-data-scientists-1.html',\n",
       " 'https://www.kdnuggets.com/2020/08/announcing-pycaret-2.html',\n",
       " 'https://www.kdnuggets.com/2021/04/getting-started-reinforcement-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/09/fast-automl-flaml-ray-tune.html',\n",
       " 'https://www.kdnuggets.com/2021/01/data-science-offers-doubled-income-2-months.html',\n",
       " 'https://www.kdnuggets.com/2021/02/backcasting-building-accurate-forecasting-model-business.html',\n",
       " 'https://www.kdnuggets.com/2020/12/8-places-data-professionals-find-datasets.html',\n",
       " 'https://www.kdnuggets.com/2020/12/top-stories-2020.html',\n",
       " 'https://www.kdnuggets.com/2020/09/jmp-art-statistics-learning-from-data.html',\n",
       " 'https://www.kdnuggets.com/2021/08/aws-webinar-clinical-trial-biomedical-development-healthcare.html',\n",
       " 'https://www.kdnuggets.com/2021/05/top-data-analytics-trends.html',\n",
       " 'https://www.kdnuggets.com/2021/04/comet-uber-machine-learning-experiments.html',\n",
       " 'https://www.kdnuggets.com/2021/04/top-3-challenges-data-analytics-leaders.html',\n",
       " 'https://www.kdnuggets.com/2021/03/3-more-free-nlp-courses.html',\n",
       " 'https://www.kdnuggets.com/2020/10/behavior-analysis-machine-learning-r-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2021/05/feature-stores-how-avoid-feeling-every-day-is-groundhog-day.html',\n",
       " 'https://www.kdnuggets.com/2021/03/trifacta-wrangle-summit-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/07/high-performance-deep-learning-part4.html',\n",
       " 'https://www.kdnuggets.com/2021/04/song-popular-analyzing-top-songs-spotify.html',\n",
       " 'https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html',\n",
       " 'https://www.kdnuggets.com/2021/06/what-segmentation.html',\n",
       " 'https://www.kdnuggets.com/2021/05/ensemble-methods-explained-plain-english-bagging.html',\n",
       " 'https://www.kdnuggets.com/2020/11/5-useful-machine-learning-tools.html',\n",
       " 'https://www.kdnuggets.com/2021/04/time-series-forecasting-pycaret-regression-module.html',\n",
       " 'https://www.kdnuggets.com/2021/03/introduction-white-box-ai-interpretability.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-scientist-communicate-stakeholders.html',\n",
       " 'https://www.kdnuggets.com/2021/07/memory-machine-learning-code-consuming.html',\n",
       " 'https://www.kdnuggets.com/2021/04/knowledge-graph-conference-leading-researchers-online.html',\n",
       " 'https://www.kdnuggets.com/2021/06/10-python-code-snippets.html',\n",
       " 'https://www.kdnuggets.com/2021/07/towards-responsible-ethical-ai.html',\n",
       " 'https://www.kdnuggets.com/2020/07/numpy-handle-dimensions.html',\n",
       " 'https://www.kdnuggets.com/2021/03/kedro-airflow-orchestrating-pipelines.html',\n",
       " 'https://www.kdnuggets.com/2020/11/streamlit-better-data-apps-new-layout-options.html',\n",
       " 'https://www.kdnuggets.com/2020/10/deepmind-relies-statistical-method-build-fair-machine-learning-models.html',\n",
       " 'https://www.kdnuggets.com/2020/11/serving-tensorflow-models.html',\n",
       " 'https://www.kdnuggets.com/2020/09/causal-inference-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2020/09/immuta-scaling-data-compliance-legal-automation.html',\n",
       " 'https://www.kdnuggets.com/2021/07/geometric-foundations-deep-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/08/data-everywhere-powers-everything.html',\n",
       " 'https://www.kdnuggets.com/2020/11/data-science-without-degree.html',\n",
       " 'https://www.kdnuggets.com/2021/08/2x-data-analytics-consulting-rates-overnight.html',\n",
       " 'https://www.kdnuggets.com/2021/04/make-analysis-used.html',\n",
       " 'https://www.kdnuggets.com/2021/07/github-copilot-ai-pair-programmer.html',\n",
       " 'https://www.kdnuggets.com/2020/11/good-data-analyses-fail.html',\n",
       " 'https://www.kdnuggets.com/2020/12/create-custom-real-time-plots-deep-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/10/ace-data-science-coding-challenge.html',\n",
       " 'https://www.kdnuggets.com/2021/05/deepmind-reimagine-important-algorithms-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/07/single-line-exploratory-data-analysis.html',\n",
       " 'https://www.kdnuggets.com/2021/08/domino-rev3-enterprise-mlops-summit.html',\n",
       " 'https://www.kdnuggets.com/2021/04/zero-shot-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/07/scaling-computer-vision-models-dataflow.html',\n",
       " 'https://www.kdnuggets.com/2021/03/data-validation-data-verification-dictionary-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/03/ramapo-ms-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/05/gurobi-state-mathematical-optimization-report-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/05/how-determine-machine-learning-model-overtrained.html',\n",
       " 'https://www.kdnuggets.com/2021/04/data-careers-not-one-size-fits-all.html',\n",
       " 'https://www.kdnuggets.com/2021/05/great-new-resource-natural-language-processing-research-applications.html',\n",
       " 'https://www.kdnuggets.com/2021/03/15-common-mistakes-python.html',\n",
       " 'https://www.kdnuggets.com/2021/08/django-9-common-applications.html',\n",
       " 'https://www.kdnuggets.com/2021/06/7-open-source-ai-libraries.html',\n",
       " 'https://www.kdnuggets.com/2021/06/single-mistake-wasted-3-years-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-science-cheat-sheet.html',\n",
       " 'https://www.kdnuggets.com/2021/01/working-lambda-layer-keras.html',\n",
       " 'https://www.kdnuggets.com/2020/11/deploy-pytorch-lightning-models-production.html',\n",
       " 'https://www.kdnuggets.com/2021/03/15-habits-learned-from-highly-effective-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/02/cartoon-data-scientist-vs-data-engineer.html',\n",
       " 'https://www.kdnuggets.com/2021/02/overview-synthetic-data-types-generation-methods.html',\n",
       " 'https://www.kdnuggets.com/2021/01/k-means-faster-lower-error-scikit-learn.html',\n",
       " 'https://www.kdnuggets.com/2020/11/adversarial-examples-deep-learning-primer.html',\n",
       " 'https://www.kdnuggets.com/2020/09/online-certificates-ai-data-science-machine-learning-top.html',\n",
       " 'https://www.kdnuggets.com/2021/09/15-must-know-python-string-methods.html',\n",
       " 'https://www.kdnuggets.com/2021/09/solve-machine-learning-problems-real-world.html',\n",
       " 'https://www.kdnuggets.com/2021/06/ethics-fairness-ai.html',\n",
       " 'https://www.kdnuggets.com/2020/10/explaining-explainable-ai.html',\n",
       " 'https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html',\n",
       " 'https://www.kdnuggets.com/2020/11/deep-learning-design-patterns.html',\n",
       " 'https://www.kdnuggets.com/2021/07/northwestern-online-ms-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/01/popular-machine-learning-interview-questions-part2.html',\n",
       " 'https://www.kdnuggets.com/2021/08/essential-features-efficient-data-integration-solution.html',\n",
       " 'https://www.kdnuggets.com/2021/09/9-outstanding-reasons-learn-python-finance.html',\n",
       " 'https://www.kdnuggets.com/2020/09/decide-data-skills-learn.html',\n",
       " 'https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html',\n",
       " 'https://www.kdnuggets.com/2020/11/analyzing-time-series.html',\n",
       " 'https://www.kdnuggets.com/2020/09/statistical-visual-exploratory-data-analysis-one-line-code.html',\n",
       " 'https://www.kdnuggets.com/2021/03/extraction-objects-images-videos-5-lines-code.html',\n",
       " 'https://www.kdnuggets.com/2021/09/altair-future-says-series.html',\n",
       " 'https://www.kdnuggets.com/2021/01/cloud-computing-data-science-ml-trends-2020-2022-battle-giants.html',\n",
       " 'https://www.kdnuggets.com/2020/07/fico-score-matrices.html',\n",
       " 'https://www.kdnuggets.com/2020/08/optimize-cv-data-scientist-career.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-careers-crowd-solutions-architect.html',\n",
       " 'https://www.kdnuggets.com/2021/06/dashboards-interpreting-comparing-machine-learning-models.html',\n",
       " 'https://www.kdnuggets.com/2021/05/simple-static-visualization-often-best-approach.html',\n",
       " 'https://www.kdnuggets.com/2020/11/learn-deep-learning-free-course-yann-lecun.html',\n",
       " 'https://www.kdnuggets.com/2020/10/5-concepts-data-scientist-should-know.html',\n",
       " 'https://www.kdnuggets.com/2020/09/artificial-intelligence-precision-medicine-better-healthcare.html',\n",
       " 'https://www.kdnuggets.com/2020/07/deep-learning-signal-processing.html',\n",
       " 'https://www.kdnuggets.com/2021/08/top-blogs-rewards-jul.html',\n",
       " 'https://www.kdnuggets.com/2020/11/free-mit-intro-computational-thinking-julia.html',\n",
       " 'https://www.kdnuggets.com/2020/11/change-background-image-5-lines-code.html',\n",
       " 'https://www.kdnuggets.com/2020/09/linkedin-pro-ml-architecture-best-practices-building-machine-learning-scale.html',\n",
       " 'https://www.kdnuggets.com/2020/12/implementing-adaboost-algorithm-from-scratch.html',\n",
       " 'https://www.kdnuggets.com/2021/08/gpu-powered-data-science-deep-learning-rapids.html',\n",
       " 'https://www.kdnuggets.com/2021/09/excel-files-python-1000x-faster-way.html',\n",
       " 'https://www.kdnuggets.com/2021/07/transition-data-freelancer-data-entrepreneur-overnight.html',\n",
       " 'https://www.kdnuggets.com/2021/04/ab-testing-data-science-interviews.html',\n",
       " 'https://www.kdnuggets.com/2021/01/microsoft-transformer-networks-answer-questions-minimum-training.html',\n",
       " 'https://www.kdnuggets.com/2021/06/land-data-analytics-job-6-months.html',\n",
       " 'https://www.kdnuggets.com/2020/12/simplilearn-top-9-data-science-courses-online.html',\n",
       " 'https://www.kdnuggets.com/2021/09/roidna-ebook-guide-third-party-data-cloud.html',\n",
       " 'https://www.kdnuggets.com/2021/02/data-observability-building-data-quality-monitors-using-sql.html',\n",
       " 'https://www.kdnuggets.com/2021/01/greatlearning-mtech-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/03/more-data-science-cheatsheets.html',\n",
       " 'https://www.kdnuggets.com/2021/05/data-preparation-sql-cheat-sheet.html',\n",
       " 'https://www.kdnuggets.com/2020/10/nested-cross-validation-python.html',\n",
       " 'https://www.kdnuggets.com/2021/04/imerit-noisy-labels-impact-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html',\n",
       " 'https://www.kdnuggets.com/2021/03/hse-master-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/09/automating-every-aspect-python-project.html',\n",
       " 'https://www.kdnuggets.com/2021/02/understanding-nosql-database-types-column-oriented-databases.html',\n",
       " 'https://www.kdnuggets.com/2021/02/getting-started-5-essential-nlp-libraries.html',\n",
       " 'https://www.kdnuggets.com/2021/04/automated-feature-selection-risks.html',\n",
       " 'https://www.kdnuggets.com/2020/08/5-spark-best-practices-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/10/unspoken-difference-junior-senior-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/05/easy-mlops-pycaret-mlflow.html',\n",
       " 'https://www.kdnuggets.com/2020/07/3-advanced-python-features.html',\n",
       " 'https://www.kdnuggets.com/2020/07/awesome-machine-learning-ai-courses.html',\n",
       " 'https://www.kdnuggets.com/2021/04/continuous-training-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/08/anyscale-2021-state-production-machine-learning-survey.html',\n",
       " 'https://www.kdnuggets.com/2020/10/no-citizen-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/09/weaknesses-machine-learning-models.html',\n",
       " 'https://www.kdnuggets.com/2021/08/modelops-ai-strategy.html',\n",
       " 'https://www.kdnuggets.com/2021/08/detect-overcome-model-drift-mlops.html',\n",
       " 'https://www.kdnuggets.com/2021/02/6-nlp-techniques.html',\n",
       " 'https://www.kdnuggets.com/2020/11/ai-automation-meets-bi.html',\n",
       " 'https://www.kdnuggets.com/2020/07/powerful-csv-processing-kdb.html',\n",
       " 'https://www.kdnuggets.com/2021/08/3-mindset-changes-better-analyst.html',\n",
       " 'https://www.kdnuggets.com/2020/12/data-compression-dimensionality-reduction.html',\n",
       " 'https://www.kdnuggets.com/2021/03/right-questions-answered-using-data.html',\n",
       " 'https://www.kdnuggets.com/2020/07/fuzzy-joins-python-d6tjoin.html',\n",
       " 'https://www.kdnuggets.com/2020/09/effectively-obtain-consumer-insights-data-overload-era.html',\n",
       " 'https://www.kdnuggets.com/2021/09/sparkbeyond-avoid-data-science-projects-fail.html',\n",
       " 'https://www.kdnuggets.com/2021/06/double-income-data-science-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/06/overview-autonlp-hugging-face-example-project.html',\n",
       " 'https://www.kdnuggets.com/2020/11/future-proof-data-science-project.html',\n",
       " 'https://www.kdnuggets.com/2021/07/mlops-engineering-discipline.html',\n",
       " 'https://www.kdnuggets.com/2021/09/corp-agency-virtual-event-big-data-ai-toronto.html',\n",
       " 'https://www.kdnuggets.com/2020/10/flavor-bert-use-qa-task.html',\n",
       " 'https://www.kdnuggets.com/2021/08/mlops-machine-learning-roadmap.html',\n",
       " 'https://www.kdnuggets.com/2021/08/30-machine-learning-questions-answered.html',\n",
       " 'https://www.kdnuggets.com/2021/05/checklist-data-science-progress.html',\n",
       " 'https://www.kdnuggets.com/2020/11/gigantum-containers-will-rule-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/05/topic-modeling-streamlit.html',\n",
       " 'https://www.kdnuggets.com/2021/03/software-engineering-best-practices-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2020/12/kaggle-survey-2020-data-science-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/03/understanding-nosql-database-types-document.html',\n",
       " 'https://www.kdnuggets.com/2020/07/building-rest-api-tensorflow-serving-part-2.html',\n",
       " 'https://www.kdnuggets.com/2021/04/best-podcasts-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/03/ai-dating-algorithms-love.html',\n",
       " 'https://www.kdnuggets.com/2020/12/superwise-ebook-fundamentals-ml-monitoring.html',\n",
       " 'https://www.kdnuggets.com/2020/09/computer-vision-recipes-best-practices-examples.html',\n",
       " 'https://www.kdnuggets.com/2020/07/essential-resources-learn-bayesian-statistics.html',\n",
       " 'https://www.kdnuggets.com/2020/07/manning2-math-programmers.html',\n",
       " 'https://www.kdnuggets.com/2021/01/catalyzex-browser-extension-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/03/fico3-sudoku-decision-engine-solve-candidate-pairs.html',\n",
       " 'https://www.kdnuggets.com/2020/11/machine-learning-model-fail.html',\n",
       " 'https://www.kdnuggets.com/2021/04/learn-neural-networks-natural-language-processing-now.html',\n",
       " 'https://www.kdnuggets.com/2021/08/essential-math-data-science-introduction-systems-linear-equations.html',\n",
       " 'https://www.kdnuggets.com/2021/03/natural-language-processing-pipelines-explained.html',\n",
       " 'https://www.kdnuggets.com/2020/10/most-popular-python-ides-editors.html',\n",
       " 'https://www.kdnuggets.com/2021/02/top-stories-2021-jan.html',\n",
       " 'https://www.kdnuggets.com/2020/07/immuta-automating-security-privacy-controls-data-science-bi.html',\n",
       " 'https://www.kdnuggets.com/2021/07/pycaret-predict-customer-churn-right-way.html',\n",
       " 'https://www.kdnuggets.com/2021/04/etl-cloud-transforming-big-data-analytics-data-warehouse-automation.html',\n",
       " 'https://www.kdnuggets.com/2020/11/six-ethical-quandaries-predictive-policing.html',\n",
       " 'https://www.kdnuggets.com/2020/08/tensorflow-model-regularization-techniques.html',\n",
       " 'https://www.kdnuggets.com/2020/09/best-online-masters-data-science-analytics-online.html',\n",
       " 'https://www.kdnuggets.com/2021/01/data-cleaning-wrangling-sql.html',\n",
       " 'https://www.kdnuggets.com/2020/10/use-docker-anymore.html',\n",
       " 'https://www.kdnuggets.com/2021/06/feature-selection-overview.html',\n",
       " 'https://www.kdnuggets.com/2020/07/formulated-nlp-summit.html',\n",
       " 'https://www.kdnuggets.com/2020/12/fast-intuitive-statistical-modeling-pomegranate.html',\n",
       " 'https://www.kdnuggets.com/2021/06/create-deploy-dashboards-voila-saturn-cloud.html',\n",
       " 'https://www.kdnuggets.com/2021/07/high-performance-deep-learning-part5.html',\n",
       " 'https://www.kdnuggets.com/2021/07/lost-art-decile-analysis.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-scientists-extinct-10-years.html',\n",
       " 'https://www.kdnuggets.com/2021/05/chrome-extensions-machine-learning-engineers-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/06/10-deadly-sins-machine-learning-model-training.html',\n",
       " 'https://www.kdnuggets.com/2020/12/machine-learning-anomaly-detection-conditional-monitoring.html',\n",
       " 'https://www.kdnuggets.com/2021/03/break-model-20-days-tutorial-production-model-analytics.html',\n",
       " 'https://www.kdnuggets.com/2021/04/shapash-machine-learning-models-understandable.html',\n",
       " 'https://www.kdnuggets.com/2020/12/pruning-machine-learning-models-tensorflow.html',\n",
       " 'https://www.kdnuggets.com/2021/02/data-observability-part-2-build-data-quality-monitors-sql.html',\n",
       " 'https://www.kdnuggets.com/2020/09/ebbekernel-data-enrichment-how-it-works.html',\n",
       " 'https://www.kdnuggets.com/2021/03/portfolio-guide-data-science-beginners.html',\n",
       " 'https://www.kdnuggets.com/2020/12/developments-trends-ai-data-science-machine-learning-technology.html',\n",
       " 'https://www.kdnuggets.com/2020/07/10-steps-data-privacy-security-laws.html',\n",
       " 'https://www.kdnuggets.com/2021/05/shaip-budgeting-ai-training-data.html',\n",
       " 'https://www.kdnuggets.com/2021/08/definedcrowd-free-dataset-accent-gap.html',\n",
       " 'https://www.kdnuggets.com/2021/04/7-must-haves-data-science-cv.html',\n",
       " 'https://www.kdnuggets.com/2021/09/five-key-facts-wu-dao-largest-transformer-model.html',\n",
       " 'https://www.kdnuggets.com/2021/09/real-time-histogram-plots-unbounded-data.html',\n",
       " 'https://www.kdnuggets.com/2021/09/2-years-self-teaching-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/11/top-python-libraries-data-science-data-visualization-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/01/11-industrial-ai-trends-dominate-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/04/ab-testing-7-common-questions-answers-data-science-interviews-1.html',\n",
       " 'https://www.kdnuggets.com/2021/02/evaluating-deep-learning-models-confusion-matrix-accuracy-precision-recall.html',\n",
       " 'https://www.kdnuggets.com/2020/11/interpretability-explainability-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/09/python-pcap-certification-roadmap-resources.html',\n",
       " 'https://www.kdnuggets.com/2021/08/agile-data-labeling.html',\n",
       " 'https://www.kdnuggets.com/2020/12/rising-library-beating-pandas-performance.html',\n",
       " 'https://www.kdnuggets.com/2020/08/breaking-privacy-federated-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/03/speed-up-pandas-modin.html',\n",
       " 'https://www.kdnuggets.com/2020/11/building-complete-artificial-neural-network.html',\n",
       " 'https://www.kdnuggets.com/2021/03/women-ai-data-science-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/08/bootstrap-modern-data-stack-terraform.html',\n",
       " 'https://www.kdnuggets.com/2021/06/poll-demand-data-scientists-10-years.html',\n",
       " 'https://www.kdnuggets.com/2021/08/difference-between-data-scientists-ml-engineers.html',\n",
       " 'https://www.kdnuggets.com/2021/07/agi-future-humanity.html',\n",
       " 'https://www.kdnuggets.com/2021/04/interpretable-machine-learning-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2020/11/revisiting-sutton-bitter-lesson-ai.html',\n",
       " 'https://www.kdnuggets.com/2021/08/twitter-understand-pizza-delivery-covid.html',\n",
       " 'https://www.kdnuggets.com/2020/11/topic-modeling-bert.html',\n",
       " 'https://www.kdnuggets.com/2020/08/data-science-keeping-people-safe-covid-19.html',\n",
       " 'https://www.kdnuggets.com/2020/07/free-mit-courses-calculus-key-deep-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/07/200-machine-learning-tools.html',\n",
       " 'https://www.kdnuggets.com/2021/08/mastering-clustering-segmentation-problem.html',\n",
       " 'https://www.kdnuggets.com/2021/08/query-pandas-dataframe.html',\n",
       " 'https://www.kdnuggets.com/2020/12/production-machine-learning-monitoring-outliers-drift-explainers-statistical-performance.html',\n",
       " 'https://www.kdnuggets.com/2020/11/4-stages-data-driven-business.html',\n",
       " 'https://www.kdnuggets.com/2021/03/metrics-evaluating-regression-models-part2.html',\n",
       " 'https://www.kdnuggets.com/2021/07/become-analytics-engineer-90-days.html',\n",
       " 'https://www.kdnuggets.com/2021/02/forecasting-stories-5-story-launch.html',\n",
       " 'https://www.kdnuggets.com/2021/03/rejection-sampling-python.html',\n",
       " 'https://www.kdnuggets.com/2020/11/10-principles-practical-statistical-reasoning.html',\n",
       " 'https://www.kdnuggets.com/2020/08/data-science-tools-illustrated-study-guides.html',\n",
       " 'https://www.kdnuggets.com/2021/02/10-statistical-concepts-data-science-interviews.html',\n",
       " 'https://www.kdnuggets.com/2020/09/mathworks-deep-learning-workflow.html',\n",
       " 'https://www.kdnuggets.com/2021/06/only-jupyter-notebooks-extension-truly-need.html',\n",
       " 'https://www.kdnuggets.com/2021/01/build-data-science-portfolio.html',\n",
       " 'https://www.kdnuggets.com/2021/07/learn-productive-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/01/10-underappreciated-python-packages-machine-learning-practitioners.html',\n",
       " 'https://www.kdnuggets.com/2020/11/neural-network-right-machine-learning-initiative.html',\n",
       " 'https://www.kdnuggets.com/2020/12/journey-from-software-machine-learning-engineer.html',\n",
       " 'https://www.kdnuggets.com/2020/06/math-data-science.html',\n",
       " 'https://www.kdnuggets.com/2020/11/dataframe-manipulation-explained-visualized.html',\n",
       " 'https://www.kdnuggets.com/2020/10/guide-linear-regression-models.html',\n",
       " 'https://www.kdnuggets.com/2020/12/uchicago-career-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/03/coursera-penn-master-computer-science-information-technology.html',\n",
       " 'https://www.kdnuggets.com/2021/08/7-reasons-degree-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/09/top-stories-2021-aug.html',\n",
       " 'https://www.kdnuggets.com/2021/06/facebook-launches-toughest-reinforcement-learning-challenges.html',\n",
       " 'https://www.kdnuggets.com/2020/10/comparing-top-business-intelligence-tools.html',\n",
       " 'https://www.kdnuggets.com/2021/03/land-data-scientist-job.html',\n",
       " 'https://www.kdnuggets.com/2020/11/learn-machine-learning-algorithms-effectively.html',\n",
       " 'https://www.kdnuggets.com/2020/09/learned-2-years-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-engineering-technologies-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/06/5-tasks-automate-python.html',\n",
       " 'https://www.kdnuggets.com/2021/07/sql-syllogisms-explanations.html',\n",
       " 'https://www.kdnuggets.com/2021/05/how-become-online-data-science-tutor.html',\n",
       " 'https://www.kdnuggets.com/2020/10/10-underrated-python-skills.html',\n",
       " 'https://www.kdnuggets.com/2021/09/mlops-modelops-difference.html',\n",
       " 'https://www.kdnuggets.com/2021/07/practical-data-science-experience-career-ready.html',\n",
       " 'https://www.kdnuggets.com/2021/08/introduction-statistical-learning-v2.html',\n",
       " 'https://www.kdnuggets.com/2021/04/one-million-visitors-wow.html',\n",
       " 'https://www.kdnuggets.com/2020/09/flask-app-using-python-heroku.html',\n",
       " 'https://www.kdnuggets.com/2021/02/7-most-recommended-skills-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/03/formulated-natural-language-processing-healthcare.html',\n",
       " 'https://www.kdnuggets.com/2021/03/4-machine-learning-concepts.html',\n",
       " 'https://www.kdnuggets.com/2021/06/top-stories-2021-may.html',\n",
       " 'https://www.kdnuggets.com/2021/08/beacon-north-america.html',\n",
       " 'https://www.kdnuggets.com/2021/06/five-types-thinking-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2020/12/6-things-data-science-employers.html',\n",
       " 'https://www.kdnuggets.com/2021/09/5-awesome-data-visualization-libraries-python.html',\n",
       " 'https://www.kdnuggets.com/2021/06/beginners-guide-debugging-tensorflow-models.html',\n",
       " 'https://www.kdnuggets.com/2020/09/fico-operationalizing-analytics-quiz.html',\n",
       " 'https://www.kdnuggets.com/2021/08/deepmind-trains-agents-play-without-intervention.html',\n",
       " 'https://www.kdnuggets.com/2020/07/wrapping-machine-learning-techniques-ai-jack-library-r.html',\n",
       " 'https://www.kdnuggets.com/2020/08/curious-theory-consciousness-debate-ai.html',\n",
       " 'https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html',\n",
       " 'https://www.kdnuggets.com/2021/02/best-data-science-project-portfolio.html',\n",
       " 'https://www.kdnuggets.com/2021/07/semantic-search-measuring-meaning-jaccard-bert.html',\n",
       " 'https://www.kdnuggets.com/2020/12/mathworks-pt2-ai-models-streaming-data.html',\n",
       " 'https://www.kdnuggets.com/2021/07/mlops-best-practices.html',\n",
       " 'https://www.kdnuggets.com/2021/06/5-data-science-open-source-projects-contribute.html',\n",
       " 'https://www.kdnuggets.com/2021/07/5-lessons-mckinsey-taught-better-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/04/10-statistical-concepts-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/05/supercharge-machine-learning-experiments-pycaret-gradio.html',\n",
       " 'https://www.kdnuggets.com/2021/06/ludwig-update-includes-low-code-machine-learning-capabilities.html',\n",
       " 'https://www.kdnuggets.com/2021/03/dbt-etl-elt-disrupter.html',\n",
       " 'https://www.kdnuggets.com/2020/11/udacity-data-science-programs-beginners.html',\n",
       " 'https://www.kdnuggets.com/2021/04/nlp-index.html',\n",
       " 'https://www.kdnuggets.com/2020/10/perceptilabs-gui-visual-api-tensorflow.html',\n",
       " 'https://www.kdnuggets.com/2021/01/machine-learning-algorithms-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/05/binary-classification-automated-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/05/rebuilding-7-python-projects.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-scientists-mars-software-developers-venus.html',\n",
       " 'https://www.kdnuggets.com/2021/05/machine-learning-research-biggest-ai-labs.html',\n",
       " 'https://www.kdnuggets.com/2021/07/roc-curve-explained.html',\n",
       " 'https://www.kdnuggets.com/2021/02/uchicago-msme-molecular-engineering.html',\n",
       " 'https://www.kdnuggets.com/2021/05/6-business-trends-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/01/learn-data-science-free-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/01/graph-representation-learning-book-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2021/03/automating-machine-learning-model-optimization.html',\n",
       " 'https://www.kdnuggets.com/2020/09/best-free-data-science-ebooks-2020-update.html',\n",
       " 'https://www.kdnuggets.com/2020/09/10-things-know-scikit-learn.html',\n",
       " 'https://www.kdnuggets.com/2020/10/roadmap-computer-vision.html',\n",
       " 'https://www.kdnuggets.com/2021/03/speed-up-scikit-learn-model-training.html',\n",
       " 'https://www.kdnuggets.com/2021/06/best-way-learn-practical-nlp.html',\n",
       " 'https://www.kdnuggets.com/2021/04/build-effective-data-analytics-team-project-ecosystem-success.html',\n",
       " 'https://www.kdnuggets.com/2021/02/adversarial-attacks-explainable-ai.html',\n",
       " 'https://www.kdnuggets.com/2021/06/create-interactive-3d-chart-share.html',\n",
       " 'https://www.kdnuggets.com/2020/12/undersampling-change-base-rates-model-predictions.html',\n",
       " 'https://www.kdnuggets.com/2021/03/data-vault.html',\n",
       " 'https://www.kdnuggets.com/2021/06/hiring-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/05/pandas-faster-pypolars.html',\n",
       " 'https://www.kdnuggets.com/2021/09/visplore-label-time-series-efficiently.html',\n",
       " 'https://www.kdnuggets.com/2020/10/getting-started-pytorch.html',\n",
       " 'https://www.kdnuggets.com/2021/04/model-overtrained.html',\n",
       " 'https://www.kdnuggets.com/2020/12/navigate-road-responsible-ai.html',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting article content from each link \n",
    "- request HTML from each article site\n",
    "- get text content \n",
    "- get short summary of each article "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import requests\n",
    "import bs4 as BeautifulSoup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "data_path_webpages = '/Users/markusmuller/python/projects/content-db/gmail/data/KDnuggets'"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/markusmuller/python/projects/content-db/gmail/data'"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "url = valid_urls[0]\n",
    "url2 = valid_urls[1]\n",
    "page = requests.get(url)\n",
    "page2 = requests.get(url2)\n",
    "\n",
    "soup = BeautifulSoup.BeautifulSoup(page.content, \"html.parser\")\n",
    "soup2 = BeautifulSoup.BeautifulSoup(page2.content, \"html.parser\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "valid_urls[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://www.kdnuggets.com/2021/08/visplore-data-understanding-interactive-exploration.html'"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "there are two different layouts of the sice one with a post-header and one without "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "if soup.find(id=\"post-header\") != None:\n",
    "    print(soup.find(id=\"post-header\").find(id=\"title\"))\n",
    "else: print(soup.find(id=\"title\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<h1 id=\"title\">Speeding up data understanding by interactive exploration</h1>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# summary is in div id post-header, p tag class excerpt\n",
    "# title = soup.find(id=\"post-header\").find(id=\"title\")\n",
    "summary = soup.find(class_='excerpt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "summary.text.strip()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'A key success factor of data science projects is to understand the data well. This blog explains why coding can be inefficient for this and how you can improve.'"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title.text.strip()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1'"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "summary.text.strip()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'In this article, we’ll take an\\xa0interview-driven\\xa0approach by linking some of the most commonly asked interview questions to different components of A/B testing, including selecting ideas for testing, designing A/B tests, evaluating test results, and making ship or no ship decisions.'"
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "soup2.find(id=\"post-\").text.strip()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"By Matthew Mayo, KDnuggets.\\ncomments\\nWhat better option for this week's free eBook than the brand new Manning published Deep Learning with PyTorch, made freely available via PyTorch's website for a limited time (we don't know how limited, so grab it now).\\nWritten by Eli Stevens, Luca Antiga, and Thomas Viehmann, 3 people with serious PyTorch bona fides, Soumith Chintala, co-creator of PyTorch, writes the following in the foreword:\\n\\nWith the publication of Deep Learning with PyTorch, we finally have a definitive treatise on PyTorch. It covers the basics and abstractions in great detail, tearing apart the underpinnings of data structures like tensors and neural networks and making sure you understand their implementation. Additionally, it covers advanced subjects such as JIT and deployment to production (an aspect of PyTorch that no other book currently covers).\\n\\n\\n\\xa0\\nLots of organizations have made the move to PyTorch, and it doesn't seem to be a trend that will stop anytime soon. The project has a large community, and numerous recent APIs such as PyTorch Lightning, fastai, and torchlayers make the library even more flexible and easy to use than ever. A robust ecosystem centered on PyTorch has evolved and rivals that of any other neural network framework out there.\\nWhy PyTorch? From the first chapter of the book:\\n\\nAs Python does for programming, PyTorch provides an excellent introduction to deep learning. At the same time, PyTorch has been proven to be fully qualified for use in professional contexts for real-world, high-profile work. We believe that PyTorch’s clear syntax, streamlined API, and easy debugging make it an excellent choice for introducing deep learning. We highly recommend studying PyTorch for your first deep learning library. Whether it ought to be the last deep learning library you learn is a decision we leave up to you.\\n\\nIf you head over to the PyTorch website you can grab your own PDF copy by filling out the simple form — which only asks what your role is and what it is you are going to build with PyTorch (no email == no spam) — a seemingly reasonable trade-off to get your hands on the book. Once you do, you can see what is covered in the table of contents:\\n\\nIntroduction to Deep Learning and the PyTorch Library\\nPre-trained Networks\\nIt Starts with a Tensor\\nReal-World Data Representation Using Tensors\\nThe Mechanics of Learning\\nUsing a Neural Network to Fit the Data\\nTelling Birds from Airplanes: Learning from Images\\nUsing Convolutions to Generalize\\nUsing PyTorch to Fight Cancer\\nReady, Dataset, Go!\\nTraining a Classification Model to Detect Suspected Tumors\\nMonitoring Metrics: Precision, Recall, and Pretty Pictures\\nUsing Segmentation to Find Suspected Nodules\\nEnd-to-End Nodule Analysis, and Where to Got Next\\nDeploying to Production\\n\\nManning highlights these main points on their website as to what you will find in the book:\\n\\nTraining deep neural networks\\nImplementing modules and loss functions\\nUtilizing pretrained models from PyTorch Hub\\nExploring code samples in Jupyter Notebooks\\n\\nI, for one, am excited to get into this book, and am appreciative of PyTorch's move to make it freely available for a limited time before it is officially released. A great public relations move, but also one which benefits the community of PyTorch researchers and students just the same.\\n\\xa0\\nRelated:\\n\\nDeep Learning: The Free eBook\\nDive Into Deep Learning: The Free eBook\\nMathematics for Machine Learning: The Free eBook\""
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "r = r.replace(u'\\xa0', ' ')\n",
    "print(r)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In this article, we’ll take an interview-driven approach by linking some of the most commonly asked interview questions to different components of A/B testing, including selecting ideas for testing, designing A/B tests, evaluating test results, and making ship or no ship decisions.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "title_list = []\n",
    "summary_list = []\n",
    "content_list = []\n",
    "url_list = []\n",
    "\n",
    "# for loop to get title and summary\n",
    "for c, url in enumerate(valid_urls):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup.BeautifulSoup(page.content, \"html.parser\")\n",
    "    print(c)\n",
    "    \n",
    "    if soup.find(id=\"post-header\") != None:\n",
    "        title = soup.find(id=\"post-header\").find(id=\"title\")\n",
    "        summary = soup.find(id='post-header').find(class_='excerpt')\n",
    "    else: \n",
    "        title = soup.find(id=\"title\")\n",
    "        summary = soup.find(class_=\"excerpt\")\n",
    "\n",
    "    \n",
    "    title = title.text.strip()\n",
    "    summary = summary.text.strip()\n",
    "    \n",
    "    content = soup.find(id=\"post-\").text.strip()\n",
    "\n",
    "    title_list.append(title)\n",
    "    summary_list.append(summary)\n",
    "    content_list.append(content)\n",
    "    url_list.append(url)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-3bec5cc51184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# for loop to get title and summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "title_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Speeding up data understanding by interactive exploration',\n",
       " 'PyTorch for Deep Learning: The Free eBook',\n",
       " 'Get Interactive Plots Directly With Pandas',\n",
       " 'Travel to faster, trusted decisions in the cloud',\n",
       " 'Building Machine Learning Pipelines using Snowflake and Dask',\n",
       " 'Microsoft Research Trains Neural Networks to Understand What They Read',\n",
       " 'How to Create and Deploy a Simple Sentiment Analysis App via API',\n",
       " 'Machine Learning: Cutting Edge Tech with Deep Roots in Other Fields',\n",
       " 'Which methods should be used for solving linear regression?',\n",
       " 'Crack SQL Interviews',\n",
       " 'Automated Text Classification with EvalML',\n",
       " 'High-Performance Deep Learning: How to train smaller, faster, and better models – Part 2',\n",
       " 'Top YouTube Machine Learning Channels',\n",
       " 'Data Science and Machine Learning: The Free eBook',\n",
       " 'Analytics Engineering Everywhere',\n",
       " 'Python Data Structures Compared',\n",
       " 'AWS Webinar: How are data-driven companies using ESG and sustainability data to make actionable decisions?',\n",
       " 'The Data Matters: Choosing the right data to analyze can make or break your analysis',\n",
       " '6 Data Science Certificates To Level Up Your Career',\n",
       " 'What did COVID do to all our models?',\n",
       " 'Automated Anomaly Detection Using PyCaret',\n",
       " 'E-commerce Data Analysis for Sales Strategy Using Python',\n",
       " 'Can AI Learn Human Values?',\n",
       " 'Getting A Data Science Job is Harder Than Ever – How to turn that to your advantage',\n",
       " 'How To Generate Meaningful Sentences Using a T5 Transformer',\n",
       " 'Applied Language Technology: A No-Nonsense Approach',\n",
       " 'Essential Math for Data Science: Probability Density and Probability Mass Functions',\n",
       " 'Monitoring Apache Spark – We’re building a better Spark UI',\n",
       " 'How to get started managing data quality with SQL and scale',\n",
       " 'DataOps Summit 2021 CFP Is Now Open!',\n",
       " 'I’m a Data Scientist, Not Just The Tiny Hands that Crunch your Data',\n",
       " 'Antifragility and Machine Learning',\n",
       " 'My Data Science Learning Journey So Far',\n",
       " 'Top June Stories: 5 Tasks To Automate With Python; Data Scientists Will be Extinct in 10 Years',\n",
       " 'Querying the Most Granular Demographics Dataset',\n",
       " 'Understanding Transformers, the Data Science Way',\n",
       " 'Better Blog Post Analysis with googleAnalyticsR',\n",
       " 'Automate Microsoft Excel and Word Using Python',\n",
       " 'How to Begin Your NLP Journey',\n",
       " 'Most Common Data Science Interview Questions and Answers',\n",
       " 'Deploying Streamlit Apps Using Streamlit Sharing',\n",
       " '5 Data Science Career Mistakes To Avoid',\n",
       " 'A Brief Introduction to the Concept of Data',\n",
       " 'Top 18 Low-Code and No-Code Machine Learning Platforms',\n",
       " 'A Simple Way to Time Code in Python',\n",
       " 'Models of Data Science teams: Chess vs Checkers',\n",
       " 'NLP Insights for the Penguin Café Orchestra',\n",
       " 'Full cross-validation and generating learning curves for time-series models',\n",
       " 'fastcore: An Underrated Python Library',\n",
       " 'Great News for KDnuggets subscribers! You now have access to the WorldData.AI Partners Plan at no cost',\n",
       " 'Amazon Web Services Webinar: Boost customer satisfaction and sales with consumer insights data',\n",
       " '6 side hustles for an aspiring data scientist',\n",
       " '11 Essential Code Blocks for Complete EDA (Exploratory Data Analysis)',\n",
       " 'How Semiconductor Innovation Could Help Prevent The Next Pandemic',\n",
       " 'SQream Announces Massive Data Revolution Video Challenge',\n",
       " 'Evaluating Object Detection Models Using Mean Average Precision',\n",
       " 'Overview of Albumentations: Open-source library for advanced image augmentations',\n",
       " 'Mastering TensorFlow Variables in 5 Easy Steps',\n",
       " 'BigQuery vs Snowflake: A Comparison of Data Warehouse Giants',\n",
       " 'Want To Get Good At Time Series Forecasting? Predict The Weather',\n",
       " 'Facebook Open Sources ReBeL, a New Reinforcement Learning Agent',\n",
       " 'Deep Learning Pioneer Geoff Hinton on his Latest Research and the Future of AI',\n",
       " '5 Things You Are Doing Wrong in PyCaret',\n",
       " '10 Machine Learning Model Training Mistakes',\n",
       " 'High-Performance Deep Learning: How to train smaller, faster, and better models – Part 3',\n",
       " 'Essential Math for Data Science: Basis and Change of Basis',\n",
       " 'Data science is not about data – applying Dijkstra principle to data science',\n",
       " 'The Three Edge Case Culprits: Bias, Variance, and Unpredictability',\n",
       " 'The secret to analysing large, complex datasets quickly and productively?',\n",
       " 'Top July Stories: Data Scientists and ML Engineers Are Luxury Employees',\n",
       " 'Speeding up Neural Network Training With Multiple GPUs and Dask',\n",
       " '10 Amazing Machine Learning Projects of 2020',\n",
       " '10 Mistakes You Should Avoid as a Data Science Beginner',\n",
       " 'How to Train a BERT Model From Scratch',\n",
       " 'Six Times Bigger than GPT-3: Inside Google’s TRILLION Parameter Switch Transformer Model',\n",
       " 'Geographical Plots with Python',\n",
       " 'AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021',\n",
       " '6 Mistakes To Avoid While Training Your Machine Learning Model',\n",
       " 'The Best Machine Learning Frameworks & Extensions for TensorFlow',\n",
       " 'tensorflow + dalex = :) , or how to explain a TensorFlow model',\n",
       " 'Jurassic-1 Language Models and AI21 Studio',\n",
       " 'Learning from machine learning mistakes',\n",
       " 'How to speed up a Deep Learning Language model by almost 50X at half the cost',\n",
       " 'Data Science in the Cloud with Dask',\n",
       " 'Autograd: The Best Machine Learning Library You’re Not Using?',\n",
       " 'Submit Your Algorithm for a Chance to Win Prizes Totaling $700,000+',\n",
       " 'How to Build an Impressive Data Science Resume',\n",
       " 'Top 10 Python Libraries Data Scientists should know in 2021',\n",
       " 'What Is The Real Difference Between Data Engineers and Data Scientists?',\n",
       " 'Demystifying AI: The prejudices of Artificial Intelligence (and human beings)',\n",
       " 'Vaex: Pandas but 1000x faster',\n",
       " 'What I Learned From “Women in Data Science” Conferences',\n",
       " 'Will There Be a Shortage of Data Science Jobs in the Next 5 Years?',\n",
       " 'Awesome Tricks And Best Practices From Kaggle',\n",
       " 'The Best SOTA NLP Course is Free!',\n",
       " 'Industry 2021 Predictions for AI, Analytics, Data Science, Machine Learning',\n",
       " 'How to pitch to VCs, explained: The Deck We Used to Raise Capital For Our Open-Source ELT Platform',\n",
       " 'How Data Professionals Can Add More Variation to Their Resumes',\n",
       " 'MLOps – “Why is it required?” and “What it is”?',\n",
       " 'Machine Learning Model Interpretation',\n",
       " 'Easy AutoML in Python',\n",
       " 'The NLP Model Forge: Generate Model Code On Demand',\n",
       " 'WHT: A Simpler Version of the fast Fourier Transform (FFT) you should know',\n",
       " 'How to Fine-Tune BERT Transformer with spaCy 3',\n",
       " 'Data Scientists think data is their #1 problem. Here’s why they’re wrong.',\n",
       " 'Data Engineering — the Cousin of Data Science, is Troublesome',\n",
       " 'Abstraction and Data Science: Not a great combination',\n",
       " 'The Ethics of AI',\n",
       " 'A Beginner’s Guide to the CLIP Model',\n",
       " 'Are You Still Using Pandas to Process Big Data in 2021? Here are two better options',\n",
       " 'We Don’t Need Data Engineers, We Need Better Tools for Data Scientists',\n",
       " 'Top February Stories: We Don’t Need Data Scientists, We Need Data Engineers; How to create stunning visualizations using python from scratch',\n",
       " '8 Women in AI Who Are Striving to Humanize the World',\n",
       " 'How to organize your data science project in 2021',\n",
       " 'Working With Time Series Using SQL',\n",
       " 'A Breakdown of Deep Learning Frameworks',\n",
       " 'Algorithms for Advanced Hyper-Parameter Optimization/Tuning',\n",
       " 'PyCaret 101: An introduction for beginners',\n",
       " 'The Most Complete Guide to PyTorch for Data Scientists',\n",
       " 'Top 3 Statistical Paradoxes in Data Science',\n",
       " 'GitHub Copilot Open Source Alternatives',\n",
       " 'Microsoft and Google Open Sourced These Frameworks Based on Their Work Scaling Deep Learning Training',\n",
       " 'DeepMind’s MuZero is One of the Most Important Deep Learning Systems Ever Created',\n",
       " '11 Important Probability Distributions Explained',\n",
       " '8 Deep Learning Project Ideas for Beginners',\n",
       " '5 Python Data Processing Tips & Code Snippets',\n",
       " 'Free From Stanford: Ethical and Social Issues in Natural Language Processing',\n",
       " 'How Data Scientists Can Compete in the Global Job Market',\n",
       " '14 Data Science projects to improve your skills',\n",
       " 'How to Evaluate the Performance of Your Machine Learning Model',\n",
       " 'How to Deploy a Flask API in Kubernetes and Connect it with Other Micro-services',\n",
       " 'Read This Before You Apply to a Business Analytics Master’s Program',\n",
       " 'Deep Learning Design Patterns!',\n",
       " 'Data Science 101: Normalization, Standardization, and Regularization',\n",
       " 'Mastering TensorFlow Tensors in 5 Easy Steps',\n",
       " 'Multidimensional multi-sensor time-series data analysis framework',\n",
       " 'Popular Certifications to validate your data and analytics skills',\n",
       " 'Using Data Science to Predict and Prevent Real World Problems',\n",
       " 'Resampling Imbalanced Data and Its Limits',\n",
       " 'Essential Math for Data Science: Integrals And Area Under The Curve',\n",
       " '7 Open Source Libraries for Deep Learning Graphs',\n",
       " 'Dealing with Imbalanced Data in Machine Learning',\n",
       " 'International alternatives to Kaggle for Data Science / Machine Learning competitions',\n",
       " 'Are you satisfied in your job? Take our Data Community Job Satisfaction Survey',\n",
       " 'How to Create Unbiased Machine Learning Models',\n",
       " '7 Data Security Best Practices for 2021',\n",
       " 'This Data Visualization is the First Step for Effective Feature Selection',\n",
       " 'Pandas vs SQL: When Data Scientists Should Use Each Tool',\n",
       " '5 Free Books to Learn Statistics for Data Science',\n",
       " 'The Most In-Demand Skills for Data Scientists in 2021',\n",
       " '2011: DanNet triggers deep CNN revolution',\n",
       " 'How to Use Kafka Connect to Create an Open Source Data Pipeline for Processing Real-Time Data',\n",
       " 'People Skills for Analytical Thinkers',\n",
       " 'Implementing a Deep Learning Library from Scratch in Python',\n",
       " 'Top October Stories: Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science; fastcore: An Underrated Python Library',\n",
       " 'Building Tech Skills in 2021',\n",
       " 'AI Books you should read in 2021',\n",
       " 'The Best Data Science Certification You’ve Never Heard Of',\n",
       " 'The Potential of Predictive Analytics in Labor Industries',\n",
       " 'Gradient Boosted Decision Trees – A Conceptual Explanation',\n",
       " 'These Data Science Skills will be your Superpower',\n",
       " 'Microsoft’s DoWhy is a Cool Framework for Causal Inference',\n",
       " 'Hiring or Looking to Get Hired in Data Science/Analytics? The INFORMS Virtual Career Fair is for You',\n",
       " 'Essential Math for Data Science: Linear Transformation with Matrices',\n",
       " 'Why Saying “We Accept the Null Hypothesis” is Wrong: An Intuitive Explanation',\n",
       " 'Accelerated Computer Vision: A Free Course From Amazon',\n",
       " 'Data Protection Techniques Needed to Guarantee Privacy',\n",
       " 'The 8 Most Common Data Scientists',\n",
       " 'Data storytelling: brains are built for visuals, but hearts turn on stories',\n",
       " 'What is Graph Theory, and Why Should You Care?',\n",
       " 'Behind OpenAI Codex: 5 Fascinating Challenges About Building Codex You Didn’t Know About',\n",
       " 'DataOps: 5 things that you need to know',\n",
       " 'Cloud Data Warehouse is The Future of Data Storage',\n",
       " 'KDnuggets Top Blogs Rewards for June 2021',\n",
       " 'Setting Up Your Data Science & Machine Learning Capability in Python',\n",
       " 'Not Only for Deep Learning: How GPUs Accelerate Data Science & Data Analytics',\n",
       " 'Unsupervised Learning for Predictive Maintenance using Auto-Encoders',\n",
       " 'DATAcated Expo, Oct 5, Live-streamed,Explore new AI / Data Science Tech',\n",
       " 'Beyond Brainless AI with a Feature Store',\n",
       " 'Best Python IDEs and Code Editors You Should Know',\n",
       " 'Feature Ranking with Recursive Feature Elimination in Scikit-Learn',\n",
       " '5 Must-Read Data Science Papers (and How to Use Them)',\n",
       " 'Advice for Aspiring Data Scientists',\n",
       " 'How to Train a Joint Entities and Relation Extraction Classifier using BERT Transformer with spaCy 3',\n",
       " 'Data Science and Analytics Career Trends for 2021',\n",
       " 'A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2',\n",
       " 'Comprehensive Guide to the Normal Distribution',\n",
       " 'Online MS in Data Science from Northwestern',\n",
       " 'Managing Your Reusable Python Code as a Data Scientist',\n",
       " 'How to Generate Automated PDF Documents with Python',\n",
       " 'Florida Hacks with IBM',\n",
       " 'Scale sensitive data science and analytics with confidence',\n",
       " 'Why You Should Consider Being a Data Engineer Instead of a Data Scientist',\n",
       " 'AIRSIDE LIVE Is Where Big Data, Data Security and Data Governance Converge',\n",
       " '5 Mistakes I Wish I Had Avoided in My Data Science Career',\n",
       " 'Bayesian Hyperparameter Optimization with tune-sklearn in PyCaret',\n",
       " 'Start a Career in a Growing Field with Google’s Data Analytics Professional Certificate',\n",
       " 'Hugging Face Transformers Package – What Is It and How To Use It',\n",
       " 'Overview of MLOps',\n",
       " 'The Future of Cloud is Now',\n",
       " 'Computer Vision at Scale With Dask And PyTorch',\n",
       " 'Toward a More Effective Disease Outbreak Alert System: A Symptoms Approach to Biosurveillance [Nov 19 webinar]',\n",
       " 'Forget Telling Stories; Help People Navigate',\n",
       " 'Shaping the new digital age – with SAS and Microsoft',\n",
       " 'Workflow Orchestration with Prefect and Coiled',\n",
       " 'Advice for Learning Data Science from Google’s Director of Research',\n",
       " 'What’s ETL?',\n",
       " 'Text Mining with R: The Free eBook',\n",
       " 'Streamlit Tips, Tricks, and Hacks for Data Scientists',\n",
       " 'How to Build An Image Classifier in Few Lines of Code with Flash',\n",
       " 'MLOps: Model Monitoring 101',\n",
       " 'Understanding How Neural Networks Think']"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "summary_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['A key success factor of data science projects is to understand the data well. This blog explains why coding can be inefficient for this and how you can improve.',\n",
       " \"For this week's free eBook, check out the newly released Deep Learning with PyTorch from Manning, made freely available via PyTorch's website for a limited time. Grab it now!\",\n",
       " 'Telling a story with data is a core function for any Data Scientist, and creating data visualizations that are simultaneously illuminating and appealing can be challenging. This tutorial reviews how to create Plotly and Bokeh plots directly through Pandas plotting syntax, which will help you convert static visualizations into interactive counterparts -- and take your analysis to the next level.',\n",
       " 'Join technology experts, partners and analysts in the industry to see what is taking off in AI, cloud computing and putting models into production for better outcomes and trusted results. Register today!',\n",
       " 'In this post, I want to share some of the tools that I have been exploring recently and show you how I use them and how they helped improve the efficiency of my workflow. The two I will talk about in particular are Snowflake and Dask. Two very different tools but ones that complement each other well especially as part of the ML Lifecycle.',\n",
       " 'The new models make inroads in a new areas of deep learning known as machine reading comprehension.',\n",
       " 'In this article we will create a simple sentiment analysis app using the HuggingFace Transformers library, and deploy it using FastAPI.',\n",
       " 'Join INFORMS community of data, analytics, operations research, and statistics professionals and tackle the future together. With nearly 13,000 members around the world, INFORMS is the largest international association for data science professionals.',\n",
       " 'As a foundational set of algorithms in any machine learning toolbox, linear regression can be solved with a variety of approaches. Here, we discuss. with with code examples, four methods and demonstrate how they should be used.',\n",
       " 'SQL is an essential programming language for data analysis and processing. So, SQL questions are always part of the interview process for data science-related jobs, including data analysts, data scientists, and data engineers. Become familiar with these common patterns seen in SQL interview questions and follow our tips on how to neatly handle each with SQL queries.',\n",
       " 'Learn how EvalML leverages Woodwork, Featuretools and the nlp-primitives library to process text data and create a machine learning model that can detect spam text messages.',\n",
       " 'As your organization begins to consider building advanced deep learning models with efficiency in mind to improve the power delivered through your solutions, the software and hardware tools required for these implementations are foundational to achieving high-performance.',\n",
       " 'These are the top 15 YouTube channels for machine learning as determined by our stated criteria, along with some additional data on the channels to help you decide if they may have some content useful for you.',\n",
       " 'Check out the newest addition to our free eBook collection, Data Science and Machine Learning: Mathematical and Statistical Methods, and start building your statistical learning foundation today.',\n",
       " 'Many new roles have appeared in the data world ever since the rise of the Data Scientist took the spotlight several years ago. Now, there is a new core player ready to take center stage, and we may see in five years, nearly every organization will have an Analytics Engineering team.',\n",
       " \"Let's take a look at 5 different Python data structures and see how they could be used to store data we might be processing in our everyday tasks, as well as the relative memory they use for storage and time they take to create and access.\",\n",
       " 'In this virtual session, on Jul 29 @ 11AM PT, 2PM ET, our panel of experts will uncover how companies across several verticals use ESG data to move beyond the reporting benchmark, deepen business insights, and create competitive differentiation.',\n",
       " 'We started Nomad Data to help data scientists and business analysts quickly find the right commercial datasets to match their specific use case. We catalog use cases of data and use machine learning and AI to match analysis goals with datasets.',\n",
       " 'Anyone looking to obtain a data science certificate to prove their ability in the field will find a range of options exist. We review several valuable certificates to consider that will definitely pump up your resume and portfolio to get you closer to your dream job.',\n",
       " 'An interview with Dean Abbott and John Elder about change management, complexity, interpretability, and the risk of AI taking over humanity.',\n",
       " 'Learn to automate anomaly detection using the open source machine learning library PyCaret.',\n",
       " 'Check out this informative and concise case study applying data analysis using Python to a well-defined e-commerce scenario.',\n",
       " 'OpenAI believes that the path to safe AI requires social sciences.',\n",
       " 'Although many aspiring Data Scientists are finding it is becoming more difficult to land a job than it was in previous years, understanding what has changed in the hiring landscape can be used to to your advantage in matching with the best organization for your goals and interests.',\n",
       " 'Read this article to see how to develop a text generation API using the T5 transformer.',\n",
       " \"Here is a free entry-level applied natural language processing course that can fit into any beginner's roadmap to understanding NLP. Check it out.\",\n",
       " 'In this article, we’ll cover probability mass and probability density function in this sample. You’ll see how to understand and represent these distribution functions and their link with histograms.',\n",
       " 'Data Mechanics is developing a free monitoring UI tool for Apache Spark to replace the Spark UI with a better UX, new metrics, and automated performance recommendations. Preview these high-level feedback features, and consider trying it out to support its first release.',\n",
       " 'Silent data quality issues are the biggest problem facing data teams today, who are flying blind with no systems or processes in place to monitor and detect bad data before it has a downstream impact.',\n",
       " 'Calling all Conductors of Chaos: Tell Us How You Tamed your Data at DataOps Summit 2021 CFP is open through May 31st',\n",
       " 'Not everyone \"gets\" the role of the Data Scientist -- including management. While there can be frustrating aspects of being a data scientist, there are effective ways to go about fixing them.',\n",
       " 'Our intuition for most products, processes, and even some models might be that they either will get worse over time, or if they fail, they will experience an cascade of more failure. But, what if we could intentionally design systems and models to only get better, even as the world around them gets worse?',\n",
       " 'These are some obstacles the author faced in their data science learning journey in the past year, including how much time it took to overcome each obstacle and what it has taught the author.',\n",
       " '5 Tasks To Automate With Python; Data Scientists Will be Extinct in 10 Years: How to Generate Automated PDF Documents with Python; How I Doubled My Income with Data Science and Machine Learning.',\n",
       " 'Having access to broad and detailed population data can potentially offer enormous value to any organization looking to interact with specific demographics. However, access alone is not sufficient without being able to leverage advanced techniques to explore and visualize the data.',\n",
       " 'Read this accessible and conversational article about understanding transformers, the data science way — by asking a lot of questions that is.',\n",
       " \"In this post, we'll walk through using googleAnalyticsR for better blog post analysis, so you can do my better blog post analysis for yourself!\",\n",
       " 'Integrate Excel with Word to generate automated reports seamlessly.',\n",
       " 'In this blog post, learn how to process text using Python.',\n",
       " 'After analyzing 900+ data science interview questions from companies over the past few years, the most common data science interview question categories are reviewed in this guide, each explained with an example.',\n",
       " 'Read this sneak peek into Streamlit’s new deployment platform.',\n",
       " 'Everyone makes mistakes, which can be a good thing when they lead to learning and improvements over time. But, we can also try to first learn from others to expedite our personal growth. To get started, consider these lessons learned the hard way, so you don’t have to.',\n",
       " 'Every aspiring data scientist must know the concept of data and the kind of analysis they can run. This article introduces the concept of data (quantitative and qualitative) and the types of analysis.',\n",
       " 'Machine learning becomes more accessible to companies and individuals when there is less coding involved. Especially if you are just starting your path in ML, then check out these low-code and no-code platforms to help expedite your capabilities in learning and applying AI.',\n",
       " 'Read on to find out how to use a decorator to time your functions.',\n",
       " 'Should we still consider data scientists and data engineers as separate roles? When should a team grow with full-stack data developers? Introducing the Checkers-like data team.',\n",
       " 'We give an example of how to use Expert.ai and Python to investigate favorite music albums.',\n",
       " 'Standard cross-validation on time series data is not possible because the data model is sequential, which does not lend well to splitting the data into statistically useful training and validation sets. However, a new approach called Reconstructive Cross-validation may pave the way toward performing this type of important analysis for predictive models with temporal datasets.',\n",
       " 'A unique python library that extends the python programming language and provides utilities that enhance productivity.',\n",
       " 'Great News for KDnuggets subscribers! You now have access to the WorldData.AI Partners Plan at no cost, including access to some of the premium datasets only available to enterprise members. Connect your data to many of 3.5 Billion WorldData datasets and improve your Data Science and Machine Learning models! Subscribe to KDnuggets to get access.',\n",
       " 'Join this webinar, Sep 27, to learn how to leverage external data to understand market needs and consumer behavior – helping you build a more customer-centric business.',\n",
       " 'As an aspiring data scientist or an employed professional, many opportunities exist for you to offer your skills to a broader audience through side gigs. While the difficulty and risk vary, experiences from applying your data science practice to areas outside your immediate career path can increase your expertise while even increasing your bank account.',\n",
       " 'This article is a practical guide to exploring any data science project and gain valuable insights.',\n",
       " 'Samsung Semiconductor technology has played a particularly essential role in the fight against Covid-19. Samsung technology powers many of the most innovative programs and AI platforms that are helping scientists conduct research and achieve breakthroughs at a speed that would have been impossible just a few years ago.',\n",
       " 'Data professionals are invited to share their massive data challenges from their own unique perspectives. Learn more about the Massive Data Revolution Video Challenge, get a $50 Amazon gift card, and be sure to submit your entry by December 16th.',\n",
       " 'In this article we will see see how precision and recall are used to calculate the Mean Average Precision (mAP).',\n",
       " 'With code snippets on augmentations and integrations with PyTorch and Tensorflow pipelines.',\n",
       " 'Learn how to use TensorFlow Variables, their differences from plain Tensor objects, and when they are preferred over these Tensor objects | Deep Learning with TensorFlow 2.x.',\n",
       " 'In this article we are going to compare the two topmost data warehouses: BigQuery and Snowflake.',\n",
       " 'This article is designed to help the reader understand the components of a time series.',\n",
       " 'The new model tries to recreate the reinforcement learning and search methods used by AlphaZero in imperfect information scenarios.',\n",
       " 'Geoff Hinton has lived at the outer reaches of machine learning research since an aborted attempt at a carpentry career a half century ago. He spoke to Craig Smith about his work In 2020 and what he sees on the horizon for AI.',\n",
       " 'PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few words only. This makes experiments exponentially fast and efficient. Find out 5 ways to improve your usage of the library.',\n",
       " 'These common ML model training mistakes are easy to overlook but costly to redeem.',\n",
       " 'Now that you are ready to efficiently build advanced deep learning models with the right software and hardware tools, the techniques involved in implementing such efforts must be explored to improve model quality and obtain the performance that your organization desires.',\n",
       " 'In this article, you will learn what the basis of a vector space is, see that any vectors of the space are linear combinations of the basis vectors, and see how to change the basis using change of basis matrices.',\n",
       " \"What is Data Science really about? Is it the data, or the algorithms, or something else? Similar foundational philosophical struggles exist with other scientific fields, including computer science, and maybe we can look to these resolutions to better understand the true 'meaning' of data science.\",\n",
       " 'Edge cases occur for three basic reasons: Bias – the ML system is too ‘simple’; Variance – the ML system is too ‘inexperienced’; Unpredictability – the ML system operates in an environment full of surprises. How do we recognize these edge cases situations, and what can we do about them?',\n",
       " 'Data is beautiful, and lots of data is simply sublime, but be wary of the pitfalls. Sometimes you have so much data you can waste hours exploring without answering the important questions. These 5 tips will show you how to analyse large complex datasets productively by constraining yourself.',\n",
       " \"Also: Top 6 Data Science Online Courses in 2021; Advice for Learning Data Science from Google's Director of Research; 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist\",\n",
       " 'A common moment when training a neural network is when you realize the model isn’t training quickly enough on a CPU and you need to switch to using a GPU. It turns out multi-GPU model training across multiple machines is pretty easy with Dask. This blog post is about my first experiment in using multiple GPUs with Dask and the results.',\n",
       " 'So much progress in AI and machine learning happened in 2020, especially in the areas of AI-generating creativity and low-to-no-code frameworks. Check out these trending and popular machine learning projects released last year, and let them inspire your work throughout 2021.',\n",
       " 'Read this article on how to gain a competitive advantage in the data science job market.',\n",
       " 'Meet BERT’s Italian cousin, FiliBERTo.',\n",
       " 'Google’s Switch Transformer model could be the next breakthrough in this area of deep learning.',\n",
       " 'When your data includes geographical information, rich map visualizations can offer significant value for you to understand your data and for the end user when interpreting analytical results.',\n",
       " \"2020 is finally coming to a close. While likely not to register as anyone's favorite year, 2020 did have some noteworthy advancements in our field, and 2021 promises some important key trends to look forward to. As has become a year-end tradition, our collection of experts have once again contributed their thoughts. Read on to find out more.\",\n",
       " 'While training the AI model, multi-stage activities are performed to utilize the training data in the best manner, so that outcomes are satisfying. So, here are the 6 common mistakes you need to understand to make sure your AI model is successful.',\n",
       " 'Check out this curated list of useful frameworks and extensions for TensorFlow.',\n",
       " 'Having a machine learning model that generates interesting predictions is one thing. Understanding why it makes these predictions is another. For a tensorflow predictive model, it can be straightforward and convenient develop an explainable AI by leveraging the dalex Python package.',\n",
       " 'AI21 Labs’ new developer platform offers instant access to our 178B-parameter language model, to help you build sophisticated text-based AI applications at scale.',\n",
       " 'Read this article and discover how to find weak spots of a regression model.',\n",
       " 'In this blog post, we show how to accelerate fine-tuning the ALBERT language model while also reducing costs by using Determined’s built-in support for distributed training with AWS spot instances.',\n",
       " 'Scaling large data analyses for data science and machine learning is growing in importance. Dask and Coiled are making it easy and fast for folks to do just that. Read on to find out how.',\n",
       " 'If there is a Python library that is emblematic of the simplicity, flexibility, and utility of differentiable programming it has to be Autograd.',\n",
       " 'Can your algorithm make fair and accurate #recidivism forecasts? Take part in US National Institute of Justice “Recidivism Forecasting Challenge” with prize money totaling over $700K.',\n",
       " 'Every one of us needs a resume to showcase our skills and experience but how much effort are we putting into it to make it impactful. It is undeniable that resumes play a key role in our job application process. This article will explore some simple strategies to significantly improve the presentation as well as the content of data science resumes.',\n",
       " 'So many Python libraries exist that offer powerful and efficient foundations for supporting your data science work and machine learning model development. While the list may seem overwhelming, there are certain libraries you should focus your time on, as they are some of the most commonly used today.',\n",
       " 'To launch your data career, you’ll need both theoretical knowledge and applied skills. Bootcamp programs like Springboard’s Data Science Career Track and Data Engineering Career Track can help make you job-ready through hands-on, project-based learning and one-on-one mentorship. Wondering which data career path is right for you? Read on to find out.',\n",
       " 'AI models are necessarily trained on historical data from the real-world--data that is generated from the daily goings on of society. If social-based biases are inherent in the training data, then will the AI predictions highlight these same biases? If so, what should we do (or not do) about making AI fair?',\n",
       " 'If you are working with big data, especially on your local machine, then learning the basics of Vaex, a Python library that enables the fast processing of large datasets, will provide you with a productive alternative to Pandas.',\n",
       " 'Read the author\\'s perspective after attending 3 \"Women in Data Science\" conferences.',\n",
       " 'The data science workflow is getting automated day by day.',\n",
       " 'Easily learn what is only learned by hours of search and exploration.',\n",
       " 'Hugging Face has recently released a course on using its libraries and ecosystem for practical NLP, and it appears to be very comprehensive. Have a look for yourself.',\n",
       " 'We bring you industry predictions from 12 innovative companies - what key trends they expect in 2021 in AI, Analytics, Data Science, and Machine Learning?',\n",
       " 'Winning seed funding from venture capitalists is a daunting task, and the pitch is key. Learn how one effective slide deck resulted in a successful early funding round for an open-source start-up, Airbyte.',\n",
       " 'This article presents seven ways data professionals can add variation to their resumes.',\n",
       " 'Creating an model that works well is only a small aspect of delivering real machine learning solutions. Learn about the motivation behind MLOps, the framework and its components that will help you get your ML model into production, and its relation to DevOps from the world of traditional software development.',\n",
       " 'Read this overview of using Skater to build machine learning visualizations.',\n",
       " 'We’re excited to announce that a new open-source project has joined the Alteryx open-source ecosystem. EvalML is a library for automated machine learning (AutoML) and model understanding, written in Python.',\n",
       " \"You've seen their Big Bad NLP Database and The Super Duper NLP Repo. Now Quantum Stat is back with its most ambitious NLP product yet: The NLP Model Forge.\",\n",
       " 'The fast Walsh Hadamard transform is a simple and useful algorithm for machine learning that was popular in the 1960s and early 1970s. This useful approach should be more widely appreciated and applied for its efficiency.',\n",
       " 'A step-by-step guide on how to create a knowledge graph using NER and Relation Extraction.',\n",
       " \"We tend to think it's all about the data. However, for real data science projects at real organizations in real life, there are more fundamental aspects to consider to do data science right.\",\n",
       " 'A Data Scientist must be a jack of many, many trades. Especially when working in broader teams, understanding the roles of others, such as data engineering, can help you validate progress and be aware of potential pitfalls. So, how can you convince your analysts to realize the importance of expanding their toolkit? Examples from real life often provide great insight.',\n",
       " 'The article is about too much abstraction and how this programming concept when extended to Data Science makes Data Science non-intuitive.',\n",
       " 'Marketing scientist Kevin Gray asks Dr. Anna Farzindar of the University of Southern California about a very important subject - the ethics of AI.',\n",
       " \"CLIP is a bridge between computer vision and natural language processing. I'm here to break CLIP down for you in an accessible and fun read! In this post, I'll cover what CLIP is, how CLIP works, and why CLIP is cool.\",\n",
       " 'When its time to handle a lot of data -- so much that you are in the realm of Big Data -- what tools can you use to wrangle the data, especially in a notebook environment? Pandas doesn’t handle really Big Data very well, but two other libraries do. So, which one is better and faster?',\n",
       " \"In today's data science jobs landscape, a variety of roles are being filled from specialized engineering positions to the more generalized data scientist. However, is it possible that some of these job types are duplicative or misdirected, such as that of the Data Engineer, which might exist as we know it because of a lack of adequate tooling for Data Scientists?\",\n",
       " 'Also: How to Get Your First Job in Data Science without Any Work Experience; Telling a Great Data Story: A Visualization Decision Tree',\n",
       " 'Some exceptional female researchers and engineers are working on projects to make the world a better place with the help of AI, data science, and machine learning.',\n",
       " 'Maintaining proper organization of all your data science projects will increase your productivity, minimize errors, and increase your development efficiency. This tutorial will guide you through a framework on how to keep everything in order on your local machine and in the cloud.',\n",
       " 'This article is an overview of using SQL to manipulate time series data.',\n",
       " 'Deep Learning continues to evolve as one of the most powerful techniques in the AI toolbox. Many software packages exist today to support the development of models, and we highlight important options available with key qualities and differentiators to help you select the most appropriate for your needs.',\n",
       " 'In informed search, each iteration learns from the last, whereas in Grid and Random, modelling is all done at once and then the best is picked. In case for small datasets, GridSearch or RandomSearch would be fast and sufficient. AutoML approaches provide a neat solution to properly select the required hyperparameters that improve the model’s performance.',\n",
       " 'This article is a great overview of how to get started with PyCaret for all your machine learning projects.',\n",
       " 'All the PyTorch functionality you will ever need while doing Deep Learning. From an Experimentation/Research Perspective.',\n",
       " 'Observation bias and sub-group differences generate statistical paradoxes.',\n",
       " \"GitHub's Copilot code generation tool is currently only available via approved request. Here are 4 Copilot alternatives that you can use in your programming today.\",\n",
       " 'Google and Microsoft have recently released new frameworks for distributed deep learning training.',\n",
       " 'MuZero takes a unique approach to solve the problem of planning in deep learning models.',\n",
       " 'There are many distribution functions considered in statistics and machine learning, which can seem daunting to understand at first. Many are actually closely related, and with these intuitive explanations of the most important probability distributions, you can begin to appreciate the observations of data these distributions communicate.',\n",
       " 'Have you studied Deep Learning techniques, but never worked on a useful project? Here, we highlight eight deep learning project ideas for beginners that will help you sharpen your skills and boost your resume.',\n",
       " 'This is a small collection of Python code snippets that a beginner might find useful for data processing.',\n",
       " \"Perhaps it's time to take a look at this relatively new offering from Stanford, Ethical and Social Issues in Natural Language Processing (CS384), an advanced seminar course covering ethical and social issues in NLP.\",\n",
       " 'Data scientists wanting to stay competitive or break into the field will need the right approach. These techniques will help them search for and secure a new position.',\n",
       " \"There's a lot of data out there and so many data science techniques to master or review. Check out these great project ideas from easy to advanced difficulty levels to develop new skills and strengthen your portfolio.\",\n",
       " 'You can train your supervised machine learning models all day long, but unless you evaluate its performance, you can never know if your model is useful. This detailed discussion reviews the various performance metrics you must consider, and offers intuitive explanations for what they mean and how they work.',\n",
       " 'A hands-on tutorial on how to implement your micro-service architecture using the powerful container orchestration tool Kubernetes.',\n",
       " 'Considering a master’s in business analytics? Here are four things to know before you apply.',\n",
       " 'New book, \"Deep Learning Design Patterns\" presents deep learning models in a unique-but-familiar new way: as extendable design patterns you can easily plug-and-play into your software projects. Use code kdmath50 to save 50% off.',\n",
       " 'Normalization, standardization, and regularization all sound similar. However, each plays a unique role in your data preparation and model building process, so you must know when and how to use these important procedures.',\n",
       " 'Discover how the building blocks of TensorFlow works at the lower level and learn how to make the most of Tensor objects.',\n",
       " 'This blog post provides an overview of the package “msda” useful for time-series sensor data analysis. A quick introduction about time-series data is also provided.',\n",
       " 'Check out the most popular certifications from SAS to see what certification you want to pursue next. Now through the end of 2021, you can save 55% on your exam!',\n",
       " 'Do you have an interest in data science but lack an understanding of what, exactly, it can be used to accomplish in the real world? Read this article for a few examples of just how helpful data science can be for predicting and preventing real world problems.',\n",
       " 'Can resampling tackle the problem of too few fraudulent transactions in credit card fraud detection?',\n",
       " 'In this article, you’ll learn about integrals and the area under the curve using the practical data science example of the area under the ROC curve used to compare the performances of two machine learning models.',\n",
       " 'In this article we’ll go through 7 up-and-coming open source libraries for graph deep learning, ranked in order of increasing popularity.',\n",
       " \"This article presents tools & techniques for handling data when it's imbalanced.\",\n",
       " 'While Kaggle might be the most well-known, go-to data science competition platform to test your skills at model building and performance, additional regional platforms are available around the world that offer even more opportunities to learn... and win.',\n",
       " 'The latest KDnuggets survey is looking to determine the job satisfaction levels of the data community. Take a few moments to contribute your answer and help paint a picture of the current situation.',\n",
       " 'In this post we discuss the concepts of bias and fairness in the Machine Learning world, and show how ML biases often reflect existing biases in society. Additionally, We discuss various methods for testing and enforcing fairness in ML models.',\n",
       " 'Here are seven data security best practices to adopt this year.',\n",
       " 'Understanding the most important features to use is crucial for developing a model that performs well. Knowing which features to consider requires experimentation, and proper visualization of your data can help clarify your initial selections. The scatter pairplot is a great place to start.',\n",
       " 'Exploring data sets and understanding its structure, content, and relationships is a routine and core process for any Data Scientist. Multiple tools exist for performing such analysis, and we take a deep dive into the benefits and different approaches of two important tools, SQL and Pandas.',\n",
       " 'Learn all the statistics you need for data science for free.',\n",
       " 'If you are preparing to make a career as a Data Scientist or are looking for opportunities to skill-up in your current role, this analysis of in-demand skills for 2021, based on over 15,000 Data Scientist job postings, should offer you a good idea as to which programming languages and software tools are increasing and decreasing in importance.',\n",
       " 'In 2021, we are celebrating the 10-year anniversary of DanNet, which, in 2011, was the first pure deep convolutional neural network (CNN) to win computer vision contests. Read about its history here.',\n",
       " 'This article shows you how to create a real-time data pipeline using only pure open source technologies. These include Kafka Connect, Apache Kafka, Kibana and more.',\n",
       " 'Research shows that people skills are becoming more important with the rise of AI. A great way to boost these skills is by reading the new book: People Skills for Analytical Thinkers.',\n",
       " 'A beginner’s guide to understanding the fundamental building blocks of deep learning platforms.',\n",
       " \"Also: Goodhart's Law for Data Science and what happens when a measure becomes a target? How to become a Data Scientist: a step-by-step guide; 10 Best Machine Learning Courses in 2020.\",\n",
       " 'With all the workforce changes last year, it is not surprising that employees lack the skills to meet new demands. To be ready for today’s challenges, companies need sound methods to assess what skills their employees have, the ability to identify the gaps, and a plan to upskill them for success. You can read the survey results here, along with predicted learning and development trends, and insights for upskilling, cross-skilling and reskilling your workforce.',\n",
       " 'As of late, every year seems to be a \"break-out\" year for AI. So, it\\'s time for you to get ready for the future in the age of automation. This collection of books will help you prepare for the many opportunities to come, many of which may not have yet been imagined.',\n",
       " 'The CDMP is the best data strategy certification you’ve never heard of. (And honestly, when you consider the fact that you’re probably working a job that didn’t exist ten years ago, it’s not surprising that this certification isn’t widespread just yet.)',\n",
       " \"Predictive analytics isn't just for white-collar work. Check out these five examples that show its potential in blue-collar jobs and industries as well.\",\n",
       " 'Gradient boosted decision trees involves implementing several models and aggregating their results. These boosted models have become popular thanks to their performance in machine learning competitions on Kaggle. In this article, we’ll see what gradient boosted decision trees are all about.',\n",
       " 'Learning data science means learning the hard skills of statistics, programming, and machine learning. To complete your training, a broader set of soft skills will round out your capabilities as an effective and successful professional Data Scientist.',\n",
       " 'Inspired by Judea Pearl’s do-calculus for causal inference, the open source framework provides a programmatic interface for popular causal inference methods.',\n",
       " 'Hiring or looking to get hired in Data Science / Analytics? The INFORMS Virtual Career Fair, April 13, is for you. Register today!',\n",
       " 'You’ll start seeing matrices, not only as operations on numbers, but also as a way to transform vector spaces. This conception will give you the foundations needed to understand more complex linear algebra concepts like matrix decomposition.',\n",
       " '“The opposite of ‘Rejecting the Null’ is ‘Accepting’ isn’t it?”. Well, it is not so simple as it is construed. We need to rise above antonyms and understand one crucial concept.',\n",
       " \"Amazon's Machine Learning University is making its online courses available to the public, and this time we look at its Accelerated Computer Vision offering.\",\n",
       " 'This article takes a look at the concepts of data privacy and personal data. It presents several privacy protection techniques and explains how they contribute to preserving the privacy of individuals.',\n",
       " \"Admit it all you wanna-be, newbie, and old-old-school Data Scientists on the planet, whether you like it or not, you've probably behaved like one of these types. Or two. Or all eight.\",\n",
       " 'Today, we need much more than just numbers about our organization to understand, gain insights, and take relevant actions. While visualizations of the data are important, making an emotional connection with the stories behind the data is key. If you want to sell a story, send a missile to the heart.',\n",
       " 'Go from graph theory to path optimization.',\n",
       " 'Some ML engineering and modeling challenges encountering during the construction of Codex.',\n",
       " 'DataOps (Data Operations) has assumed a critical role in the age of big data to drive definitive impact on business outcomes. This process-oriented and agile methodology synergizes the components of DevOps and the capabilities of data engineers and data scientists to support data-focused workloads in enterprises. Here is a detailed look at DataOps.',\n",
       " 'Today, cloud data storage accounts for 45% of all enterprise data and by Q2 2021, that number could grow to 53%. Now is the time to embrace cloud than now.',\n",
       " 'These top blogs were winners of KDnuggets Top Blog Rewards Program for June: 5 Tasks To Automate With Python; Data Scientists Will be Extinct in 10 Years; How to Generate Automated PDF Documents with Python; How I Doubled My Income with Data Science and Machine Learning; Pandas vs SQL: When Data Scientists Should Use Each Tool; Top 10 Data Science Projects for Beginners.',\n",
       " 'With the rich and dynamic ecosystem of Python continuing to be a leading programming language for data science and machine learning, establishing and maintaining a cost-effective development environment is crucial to your business impact. So, do you rent or buy? This overview considers the hidden and obvious factors involved in selecting and implementing your Python platform.',\n",
       " 'Modern AI/ML systems’ success has been critically dependent on their ability to process massive amounts of raw data in a parallel fashion using task-optimized hardware. Can we leverage the power of GPU and distributed computing for regular data processing jobs too?',\n",
       " 'This article outlines a machine learning approach to detect and diagnose anomalies in the context of machine maintenance, along with a number of introductory concepts, including: Introduction to machine maintenance; What is predictive maintenance?; \\u200b\\u200b\\u200b\\u200bApproaches for machine diagnosis; Machine diagnosis using machine learning',\n",
       " 'The DATAcated Expo, hosted by DATAcated founder Kate Strachnyi, is coming up on October 5, 2021 from 11am - 6pm ET. Live-streamed on LinkedIn, the free event provides the community with an opportunity to explore and discover innovative technologies in data science & analytics.',\n",
       " 'AI-powered products that are limited to the data available within its application are like jellyfish: its autonomic system makes it functional, but it lacks a brain. However, you can evolve your models with data enriched \"brains\" through the help of a feature store.',\n",
       " 'Developing machine learning algorithms requires implementing countless libraries and integrating many supporting tools and software packages. All this magic must be written by you in yet another tool -- the IDE -- that is fundamental to all your code work and can drive your productivity. These top Python IDEs and code editors are among the best tools available for you to consider, and are reviewed with their noteworthy features.',\n",
       " 'This article covers using scikit-learn to obtain the optimal number of features for your machine learning project.',\n",
       " 'Keeping ahead of the latest developments in a field is key to advancing your skills and your career. Five foundational ideas from recent data science papers are highlighted here with tips on how to leverage these advancements in your work, and keep you on top of the machine learning game.',\n",
       " \"Are you a student of some type asking how to get into Data Science? You've come to the right place. Read on for both common and less basic advice on entering the field and excelling in the profession.\",\n",
       " 'A step-by-step guide on how to train a relation extraction classifier using Transformer and spaCy3.',\n",
       " \"Let's check out what are the new data science and analytics career trends for 2021 that may also shape the career options in the future.\",\n",
       " 'In this second article in this series, we’ll continue to take an interview-driven approach by linking some of the most commonly asked interview questions to different components of A/B testing, including selecting ideas for testing, designing A/B tests, evaluating test results, and making ship or no ship decisions.',\n",
       " 'Drop in for some tips on how this fundamental statistics concept can improve your data science.',\n",
       " \"Advance your data science career with Northwestern. Build the essential technical, analytical, and leadership skills needed for careers in today's data-driven world in Northwestern's Master of Science in Data Science program. Apply now.\",\n",
       " 'Here are a few approaches that I have settled on for managing my own reusable Python code as a data scientist, presented from most to least general code use, and aimed at beginners.',\n",
       " 'Discover how to leverage automation to create dazzling PDF documents effortlessly.',\n",
       " 'Join the Florida Hacks with IBM virtual hackathon and create a project to tackle sustainability challenges. IBM will provide mentorship and data sets to help bring your ideas to life.',\n",
       " 'Listen to this on-demand webinar and hear how WorldQuant Predictive derives insights from building models on sensitive data while maximizing value and minimizing risk.',\n",
       " 'A new king of the jungle has emerged.',\n",
       " 'Free virtual summit on June 3rd offers sessions from data industry leaders and practitioners on challenges and solutions in an ever-changing, data-driven landscape.',\n",
       " 'Everyone makes mistakes, which can be a good thing when they lead to learning and improvements over time. But, we can also try to first learn from others to expedite our personal growth. To get started, consider these lessons learned the hard way, so you don’t have to.',\n",
       " \"PyCaret, a low code Python ML library, offers several ways to tune the hyper-parameters of a created model. In this post, I'd like to show how Ray Tune is integrated with PyCaret, and how easy it is to leverage its algorithms and distributed computing to achieve results superior to default random search method.\",\n",
       " \"Google's recently launched Data Analytics Professional Certificate on Coursera is great for anyone, regardless of background or experience. The program is completely online, self-paced, and costs $39 per month. Interested in preparing for a new career in a high-growth field?\",\n",
       " 'The rapid development of Transformers have brought a new wave of powerful tools to natural language processing. These models are large and very expensive to train, so pre-trained versions are shared and leveraged by researchers and practitioners. Hugging Face offers a wide variety of pre-trained transformers as open-source libraries, and you can incorporate these with only one line of code.',\n",
       " 'Building a machine learning model is great, but to provide real business value, it must be made useful and maintained to remain useful over time. Machine Learning Operations (MLOps), overviewed here, is a rapidly growing space that encompasses everything required to deploy a machine learning model into production, and is a crucial aspect to delivering this sought after value.',\n",
       " 'Our recent survey of over 130 top data engineers, data architects, and executives uncovered details and trends of the current state of data engineering and DataOps.Read our survey report to learn more about these trends as well as our predictions for future obstacles and our recommendations for avoiding them.',\n",
       " 'A tutorial on conducting image classification inference using the Resnet50 deep learning model at scale with using GPU clusters on Saturn Cloud. The results were: 40x faster computer vision that made a 3+ hour PyTorch model run in just 5 minutes.',\n",
       " 'Learn how the use of more granular symptoms-level data combined with innovative statistical techniques has the potential to identify disease outbreaks faster while limiting false positives.',\n",
       " 'When designing reporting & visualizations, think of them as part of a navigation framework rather than stand-alone information.',\n",
       " 'Join technology experts, partners and analysts in the industry for this webinar series to see how SAS Viya can help you make the most of AI, analytics and the cloud for faster decisions and trusted results.',\n",
       " 'Coiled helps data scientists use Python for ambitious problems, scaling to the cloud for computing power, ease, and speed—all tuned for the needs of teams and enterprises.  In this demo example, see how to spin up a Coiled cluster to execute Prefect jobs during runtime.',\n",
       " 'Surfing the professional career wave in data science is a hot prospect for many looking to get their start in the world. The digital revolution continues to create many exciting new opportunities. But, jumping in too fast without fully establishing your foundational skills can be detrimental to your success, as is suggested by this advice for data science newbies from Peter Norvig, the Director of Research at Google.',\n",
       " 'Discover what ETL is, and see in what ways it’s critical for data science.',\n",
       " 'This freely-available book will show you how to perform text analytics in R, using packages from the tidyverse.',\n",
       " 'Today, I am going to talk about a few tips that I learned within more than a year of using Streamlit, that you can also use to unleash your powerful DS/AI/ML (whatever they may be) applications.',\n",
       " 'Introducing Flash: The high-level deep learning framework for beginners.',\n",
       " 'Model monitoring using a model metric stack is essential to put a feedback loop from a deployed ML model back to the model building stage so that ML models can constantly improve themselves under different scenarios.',\n",
       " 'A couple of years ago, Google published one of the most seminal papers in machine learning interpretability.']"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "content_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Sponsored Post.\\nBy Visplore. \\nUnderstanding the data is fundamental for all steps of a data science project. However, gaining that understanding can be challenging and time-consuming. It involves interpreting data patterns with deep knowledge from the project domain to understand, for example, which parts of the data are useful and which correlations actually matter.\\nOftentimes, data scientists don\\'t have that deep domain expertise. A key to project success is thus the communication to domain experts.\\nTraditionally, this communication works as follows: data scientists prepare a presentation by coding charts in languages such as Python. Then, they discuss the charts with domain experts. This typically triggers further questions such as \"What are these clusters?\" or \"Which other variables show this trend?\". Even great coders usually can’t spontaneously answer all these questions so that they get postponed to a follow-up meeting. Altogether, it may require several tedious feedback loops, significantly increasing the duration and costs of the project. Worse even, cutting this phase short may risk project success!\\n\\nTraditional data communication vs. joint exploration\\n\\xa0\\nHow can you improve this? Reconsider the use of coding for understanding the data! Coding is great for building data pipelines, models, monitoring apps, and much more. However, it is generally too static for answering ad-hoc questions from domain experts during a meeting.\\nVisplore is a graphical tool which is optimized for agile exploration of massive multi-variate data, in particular (but not limited to) time series such as sensor data.\\nExplore millions of raw values as fast as never before - even while sitting together with domain experts. This turns data exploration into a vivid dialogue and makes understanding the data an exciting step of a project.\\nSimply load data directly from Python, Matlab, R or other sources. Pre-configured analysis cockpits provide deep insights within seconds, with hardly any configuration. Built-in analytics answers complex questions on-the-fly, and powerful interaction tools support for selecting, cleaning and labeling data.\\nFor shorter project duration, less risk, and more satisfaction of all stakeholders.\\n\\xa0\\nLearn more about interactive data exploration',\n",
       " \"By Matthew Mayo, KDnuggets.\\ncomments\\nWhat better option for this week's free eBook than the brand new Manning published Deep Learning with PyTorch, made freely available via PyTorch's website for a limited time (we don't know how limited, so grab it now).\\nWritten by Eli Stevens, Luca Antiga, and Thomas Viehmann, 3 people with serious PyTorch bona fides, Soumith Chintala, co-creator of PyTorch, writes the following in the foreword:\\n\\nWith the publication of Deep Learning with PyTorch, we finally have a definitive treatise on PyTorch. It covers the basics and abstractions in great detail, tearing apart the underpinnings of data structures like tensors and neural networks and making sure you understand their implementation. Additionally, it covers advanced subjects such as JIT and deployment to production (an aspect of PyTorch that no other book currently covers).\\n\\n\\n\\xa0\\nLots of organizations have made the move to PyTorch, and it doesn't seem to be a trend that will stop anytime soon. The project has a large community, and numerous recent APIs such as PyTorch Lightning, fastai, and torchlayers make the library even more flexible and easy to use than ever. A robust ecosystem centered on PyTorch has evolved and rivals that of any other neural network framework out there.\\nWhy PyTorch? From the first chapter of the book:\\n\\nAs Python does for programming, PyTorch provides an excellent introduction to deep learning. At the same time, PyTorch has been proven to be fully qualified for use in professional contexts for real-world, high-profile work. We believe that PyTorch’s clear syntax, streamlined API, and easy debugging make it an excellent choice for introducing deep learning. We highly recommend studying PyTorch for your first deep learning library. Whether it ought to be the last deep learning library you learn is a decision we leave up to you.\\n\\nIf you head over to the PyTorch website you can grab your own PDF copy by filling out the simple form — which only asks what your role is and what it is you are going to build with PyTorch (no email == no spam) — a seemingly reasonable trade-off to get your hands on the book. Once you do, you can see what is covered in the table of contents:\\n\\nIntroduction to Deep Learning and the PyTorch Library\\nPre-trained Networks\\nIt Starts with a Tensor\\nReal-World Data Representation Using Tensors\\nThe Mechanics of Learning\\nUsing a Neural Network to Fit the Data\\nTelling Birds from Airplanes: Learning from Images\\nUsing Convolutions to Generalize\\nUsing PyTorch to Fight Cancer\\nReady, Dataset, Go!\\nTraining a Classification Model to Detect Suspected Tumors\\nMonitoring Metrics: Precision, Recall, and Pretty Pictures\\nUsing Segmentation to Find Suspected Nodules\\nEnd-to-End Nodule Analysis, and Where to Got Next\\nDeploying to Production\\n\\nManning highlights these main points on their website as to what you will find in the book:\\n\\nTraining deep neural networks\\nImplementing modules and loss functions\\nUtilizing pretrained models from PyTorch Hub\\nExploring code samples in Jupyter Notebooks\\n\\nI, for one, am excited to get into this book, and am appreciative of PyTorch's move to make it freely available for a limited time before it is officially released. A great public relations move, but also one which benefits the community of PyTorch researchers and students just the same.\\n\\xa0\\nRelated:\\n\\nDeep Learning: The Free eBook\\nDive Into Deep Learning: The Free eBook\\nMathematics for Machine Learning: The Free eBook\",\n",
       " 'comments\\nBy Parul Pandey, Data Science at H2O.ai | Editor @wicds.\\n\\nInfographic vector created by macrovector — www.freepik.com.\\nData exploration is by far one of the most important aspects of any data analysis task. The initial\\xa0probing and preliminary checks that we perform, using the vast catalog of visualization tools, give us actionable insights into the nature of data. However, the choice of visualization tool at times is more complicated than the task itself. On the one hand, we have libraries that are easier to use but are not so helpful in showing complex relationships in data. Then there are others that render interactivity but have a considerable learning curve. Fortunately, some open-source libraries have been created that try to address this pain point effectively.\\nIn this article, we’ll look at two such libraries, namely pandas_bokeh and cufflinks. We’ll learn how to create plotly and bokeh charts with the basic pandas plotting syntax, which we all are comfortable with. Since the article\\'s emphasis is on the syntax rather than the types of plots, we’ll limit ourselves to the five basic charts, i.e., line charts, bar charts, histograms, scatter plots, and pie charts. We’ll create each of these charts first with pandas plotting library and then recreate them in plotly and bokeh, albeit with a twist.\\n\\xa0\\nImport the Dataset\\n\\xa0\\nWe’ll work with the\\xa0NIFTY-50 dataset. The NIFTY 50 index is the\\xa0National Stock Exchange of India’s\\xa0benchmark for the Indian equity market. The dataset is openly available on\\xa0Kaggle, but we’ll be using a subset of the data containing the stock value of only four sectors, i.e., bank, pharma, IT, and FMCG.\\nYou can download the sample dataset from\\xa0here.\\nLet’s import the necessary libraries and dataset required for the visualization purpose:\\n\\n# Importing required modules\\r\\nimport pandas as pd\\r\\nimport numpy as np\\r\\nimport matplotlib.pyplot as plt\\r\\n%matplotlib inline\\r\\n\\r\\n# Reading in the data\\r\\nnifty_data = pd.read_csv(\\'NIFTY_data_2020.csv\\',parse_dates=[\"Date\"],index_col=\\'Date\\')\\r\\nnifty_data.head()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nCombined dataframe consisting of NIFTY indices of the bank, pharma, IT, and FMCG sectors.\\nWe can also resample/aggregate the data by month-end. The pandas’ library has a\\xa0resample()\\xa0function, which resamples the time series data.\\n\\nnifty_data_resample = nifty_data.resample(rule = \\'M\\').mean()\\r\\nnifty_data_resample\\r\\n\\r\\n\\n\\n\\xa0\\n\\nNow that we have our dataframes ready, it is time to visualize them via different plots.\\n\\xa0\\nPlotting with Pandas directly\\n\\xa0\\nLet’s begin with the most straightforward plotting technique — pandas’\\xa0plotting functions. To plot a graph using pandas, we’ll call the\\xa0.plot() method on the dataframe.\\nSyntax:\\xa0dataframe.plot()\\nThe\\xa0plot\\xa0method is just a simple wrapper around matplotlib’s\\xa0plt.plot().\\xa0We can also specify some additional parameters like the ones mentioned below:\\n\\nSome of the important Parameters\\r\\n--------------------------------\\r\\n\\r\\nx : label or position, default None\\r\\n    Only used if data is a DataFrame.\\r\\ny : label, position or list of label, positions, default None\\r\\n\\r\\ntitle: title to be used for the plot\\r\\n\\r\\nX and y label: Name to use for the label on the x-axis and y-axis.\\r\\n\\r\\nfigsize : specifies the size of the figure object.    \\r\\nkind : str\\r\\n    The kind of plot to produce:\\r\\n\\r\\n    - \\'line\\' : line plot (default)\\r\\n    - \\'bar\\' : vertical bar plot\\r\\n    - \\'barh\\' : horizontal bar plot\\r\\n    - \\'hist\\' : histogram\\r\\n    - \\'box\\' : boxplot\\r\\n    - \\'kde\\' : Kernel Density Estimation plot\\r\\n    - \\'density\\' : same as \\'kde\\'\\r\\n    - \\'area\\' : area plot\\r\\n    - \\'pie\\' : pie plot\\r\\n    - \\'scatter\\' : scatter plot\\r\\n    - \\'hexbin\\' : hexbin plot.\\r\\n\\r\\n\\n\\n\\xa0\\nFor a complete list of the parameters and their usage, please refer to the\\xa0documentation. Let’s now look at ways to create different plots. In this article, we’ll not go into detail explaining each plot. We’ll only focus on the syntax, which is self-explanatory if you have some experience in pandas. For a detailed understanding of the pandas’ plots, this article will be helpful.\\n\\nLine Plot\\n\\n\\nnifty_data.plot(title=\\'Nifty Index values in 2020\\', \\r\\n                xlabel = \\'Values\\',\\r\\n                figsize=(10,6);\\r\\n\\r\\n\\n\\n\\xa0\\n\\nLine plot with pandas plotting.\\n\\nScatter Plot\\n\\n\\nnifty_data.plot(kind=\\'scatter\\',\\r\\n        x=\\'NIFTY FMCG index\\', \\r\\n        y=\\'NIFTY Bank index\\',\\r\\n        title = \\'Scatter Plot for NIFTY Index values in 2020\\',\\r\\n        figsize=(10,6));\\r\\n\\r\\n\\n\\n\\xa0\\n\\nScatter plot with pandas plotting.\\n\\nHistograms\\n\\n\\nnifty_data[[\\'NIFTY FMCG index\\',\\'NIFTY Bank index\\']].plot(kind=\\'hist\\',figsize=(9,6), bins=30);\\r\\n\\r\\n\\n\\n\\xa0\\n\\nHistogram with pandas plotting.\\n\\nBar plots\\n\\n\\nnifty_data_resample.plot(kind=\\'bar\\',figsize=(10,6));\\r\\n\\r\\n\\n\\n\\xa0\\n\\nBar plot with pandas plotting.\\n\\nStacked bar plots\\n\\n\\nnifty_data_resample.plot(kind=\\'barh\\',figsize=(10,6));\\r\\n\\r\\n\\n\\n\\xa0\\n\\nStacked Bar plot with pandas plotting.\\n\\nPie Charts\\n\\n\\nnifty_data_resample.index=[\\'Jan\\',\\'Feb\\',\\'March\\',\\'Apr\\',\\'May\\',\\'June\\',\\'July\\']\\r\\nnifty_data_resample[\\'NIFTY Bank index\\'].plot.pie(legend=False, figsize=(10,6),autopct=\\'%.1f\\');\\r\\n\\r\\n\\n\\n\\xa0\\n\\nPie Chart with pandas plotting.\\nThese were some of the charts that can be directly created with pandas’ dataframes. However, these charts lack interactivity and capabilities like zoom and pan. Let’s now change these existing charts in syntax into their fully interactive counterparts with just a slight change in the syntax.\\n\\xa0\\nBokeh Backend for Pandas — plotting with Pandas-Bokeh.\\n\\xa0\\n\\nImage by Author.\\nThe\\xa0bokeh\\xa0library clearly stands out when it comes to data visualizations.\\xa0The\\xa0Pandas-Bokeh\\xa0provides a bokeh plotting backend for\\xa0Pandas,\\xa0GeoPandas, and\\xa0Pyspark\\xa0DataFrames. This backend adds a\\xa0plot_bokeh()\\xa0method to the DataFrames and Series.\\nInstallation\\nPandas-Bokeh can be installed from\\xa0PyPI\\xa0via\\xa0pip or conda:\\n\\npip install pandas-bokeh\\r\\n\\r\\nor\\r\\n\\r\\nconda install -c patrikhlobil pandas-bokeh\\r\\n\\r\\n\\n\\n\\xa0\\nUsage\\nThe Pandas-Bokeh library should be imported after Pandas, GeoPandas, or Pyspark.\\n\\nimport pandas as pd\\r\\nimport pandas_bokeh\\r\\n\\r\\n\\n\\n\\xa0\\nThen one needs to define the plotting output, which can be either of the two:\\n\\npandas_bokeh.output_notebook() # for embedding plots in Jupyter Notebooks.\\r\\npandas_bokeh.output_file(filename) # for exporting plots as HTML.\\r\\n\\r\\n\\n\\n\\xa0\\nSyntax\\nNow, the plotting API is accessible for a Pandas DataFrame via the\\xa0dataframe.plot_bokeh().\\nFor more details about the plotting outputs, see the reference here or the Bokeh documentation. Let’s now plot all the five kinds of plots as plotted in the above section. We’ll be using the same datasets as used above.\\n\\nimport pandas as pd\\r\\nimport pandas_bokeh\\r\\npandas_bokeh.output_notebook()\\r\\n\\r\\n\\n\\n\\xa0\\n\\n\\nLine Plot\\n\\n\\nnifty_data.plot_bokeh(kind=\\'line\\') #equivalent to nifty_data.plot_bokeh.line()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nLine plot with pandas_bokeh.\\n\\nScatter Plot\\n\\n\\nnifty_data.plot_bokeh.scatter(x=\\'NIFTY FMCG index\\', y=\\'NIFTY Bank index\\');\\r\\n\\r\\n\\n\\n\\xa0\\n\\nScatter plot with pandas_bokeh.\\n\\nHistograms\\n\\n\\nnifty_data[[\\'NIFTY FMCG index\\',\\'NIFTY Bank index\\']].plot_bokeh(kind=\\'hist\\', bins=30);\\r\\n\\r\\n\\n\\n\\xa0\\n\\nHistogram with pandas_bokeh.\\n\\nBar plots\\n\\n\\nnifty_data_resample.plot_bokeh(kind=\\'bar\\',figsize=(10,6));\\r\\n\\r\\n\\n\\n\\xa0\\n\\nBar plot with pandas_bokeh.\\n\\nStacked bar plots\\n\\n\\nnifty_data_resample.plot_bokeh(kind=\\'barh\\',stacked=True);\\r\\n\\r\\n\\n\\n\\xa0\\n\\nStacked bar plot with pandas_bokeh.\\n\\nPie Charts\\n\\n\\nnifty_data_resample.index=[\\'Jan\\',\\'Feb\\',\\'March\\',\\'Apr\\',\\'May\\',\\'June\\',\\'July\\']\\r\\nnifty_data_resample.plot_bokeh.pie(y =\\'NIFTY Bank index\\')\\r\\n\\r\\n\\n\\n\\xa0\\n\\nPie chart with pandas_bokeh.\\nAdditionally, you can also create multiple nested pie plots within the same plot:\\n\\nnifty_data_resample.plot_bokeh.pie()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nNested pie chart with pandas_bokeh.\\nThis section saw how we could seamlessly create bokeh plots without any significant change in the pandas plotting syntax. Now we can have the best of both worlds without having to learn any new format.\\n\\xa0\\nPlotly Backend for Pandas — plotting with Cufflinks.\\n\\xa0\\n\\nImage by Author.\\nAnother commonly used library for data visualization is\\xa0Plotly. With plotly, you can make interactive charts in Python, R, and JavaScript. As of version 4.8, Plotly came out with a\\xa0Plotly Express-powered\\xa0backend for Pandas plotting, which meant that one even does not need to import plotly to create plotly like visualizations.\\nHowever, the library I want to mention here is not plotly express, but an independent third-party wrapper library around Plotly called\\xa0Cufflinks.\\xa0The beauty of cufflinks is that it is more versatile, has more functionalities, and has an API similar to pandas plotting. This means you only need to add a\\xa0.iplot()\\xa0method to Pandas dataframes for plotting graphs.\\nInstallation\\nMake sure you have plotly installed before installing cufflinks. Read\\xa0this\\xa0guide for instructions.\\n\\npip install cufflinks\\r\\n\\r\\n\\n\\n\\xa0\\nUsage\\nThe\\xa0repository\\xa0has a lot of useful examples and notebooks to get started.\\n\\nimport pandas as pd\\r\\nimport cufflinks as cf\\r\\nfrom IPython.display import display,HTML\\r\\n#making all charts public and setting a global theme\\r\\ncf.set_config_file(sharing=\\'public\\',theme=\\'white\\',offline=True)\\r\\n\\r\\n\\n\\n\\xa0\\nThat is all. We can now create visualizations with the power of plotly but with the ease of pandas. The only change in the syntax is\\xa0dataframe.iplot().\\n\\nLine Plot\\n\\n\\nnifty_data.iplot(kind=\\'line\\')\\r\\n\\r\\n\\n\\n\\xa0\\n\\nLine plot with cufflinks.\\n\\nScatter Plot\\n\\nYou need to mention the plotting\\xa0mode\\xa0for scatter trace while creating a scatterplot. The mode could be lines, markers, text, or a combination of either of them.\\n\\nnifty_data.iplot(kind=\\'scatter\\',x=\\'NIFTY FMCG index\\', y=\\'NIFTY Bank index\\',mode=\\'markers\\');\\r\\n\\r\\n\\n\\n\\xa0\\n\\nScatter plot with cufflinks.\\n\\nHistograms\\n\\n\\nnifty_data[[\\'NIFTY FMCG index\\',\\'NIFTY Bank index\\']].iplot(kind=\\'hist\\', bins=30);\\r\\n\\r\\n\\n\\n\\xa0\\n\\nHistogram with cufflinks.\\n\\nBar plots\\n\\n\\nnifty_data_resample.iplot(kind=\\'bar\\');\\r\\n\\r\\n\\n\\n\\xa0\\n\\nBarplots with cufflinks.\\n\\nStacked bar plots\\n\\n\\nnifty_data_resample.iplot(kind=\\'barh\\',barmode = \\'stack\\');\\r\\n\\r\\n\\n\\n\\xa0\\n\\nStacked bar plots with cufflinks.\\n\\nPie Charts\\n\\n\\nnifty_data_resample.index=[\\'Jan\\',\\'Feb\\',\\'March\\',\\'Apr\\',\\'May\\',\\'June\\',\\'July\\']\\r\\nnifty_data_resample.reset_index().iplot(kind=\\'pie\\',labels=\\'index\\',values=\\'NIFTY Bank index\\')\\r\\n\\r\\n\\n\\n\\xa0\\n\\nPie charts with cufflinks.\\nThe Cufflinks library provides an easy way to get the power of plotly within plotly. The similarity in syntax is another point of advantage.\\n\\xa0\\nConclusion\\n\\xa0\\nThe Bokeh or a Plotly plot is self-sufficient in conveying the entire information. Based on your choice and preference, you can choose both or either of them while keeping a focus on the primary purpose of making visualizations more intuitive and interactive at the same time.\\n\\xa0\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nAnimated Bar Chart Races in Python\\nHow to Make Sure Your Analysis Actually Gets Used\\nKnow your data much faster with the new Sweetviz Python library',\n",
       " \"Sponsored Post.\\n\\nFor most enterprises, the cloud presents an opportunity to rethink not only how to use technology, but how to manage your business. The scalability and flexibility of a cloud environment offers the possibility of lower costs, greater agility and more accurate decisions.\\nBut not all cloud environments are created equal. And there’s no single way to implement. While enterprises could simply “lift and shift” their current systems to the cloud, most will embark on an approach to steadily transform their operations. No matter the approach you take, SAS® Viya® can help.\\nSAS Viya takes full advantage of the cloud's scalability, providing a solution that delivers the latest analytics and AI capabilities.\\nWith Viya, you can balance costs and agility with a built-in automation that enables you to strengthen your operations.\\u200b Massively parallel processing delivers results in seconds, not hours. Plus, built-in governance makes your decisions repeatable, explainable, transparent and trustworthy – no matter the scale of your data, workloads or users.\\nYet perhaps the biggest benefit of Viya is its ability to democratize data and analytics. Everyone – from data scientists to business analysts to the CEO – can access the power of analytics to make faster, better decisions.\\nAll of these capabilities are offered on a cloud-native platform that unifies disparate technologies, skill sets and processes with end-to-end capabilities powered by automated AI.\\u200b\\nJoin technology experts, partners and analysts in the industry for this live virtual event to see how SAS Viya can help you make the most of AI, analytics and the cloud for faster decisions and trusted results.\",\n",
       " \"comments\\nBy Daniel Foley, Data Scientist\\n\\n\\xa0\\nIntroduction\\n\\xa0\\n\\nRecently I have been trying to find better ways to improve my workflow as a data scientist. I tend to spend a decent chunk of my time modelling and building ETLs in my job. This has meant that more and more I need to rely on tools to reliably and efficiently handle large datasets. I quickly realised that using pandas for manipulating these datasets is not always a good approach and this prompted me to look into other alternatives.\\nIn this post, I want\\xa0to share some of the tools that I have been exploring recently and show you how I use them and how they helped improve the efficiency of my workflow. The two I will talk about in particular are Snowflake and Dask. Two very different tools but ones that complement each other well especially as part of the ML Lifecycle. My hope is that after reading this post you will have a good understanding of what Snowflake and Dask are, how they can be used effectively and be able to get up and running with your own use cases.\\nMore specifically, I want to show you how you can build an ETL pipeline using Snowflake and Python to generate training data for a machine learning task. I then want to introduce Dask and\\xa0Saturn Cloud\\xa0and show you how you can take advantage of parallel processing in the cloud to really speed up the ML training process so you can increase your productivity as a data scientist.\\n\\xa0\\nBuilding ETLs in Snowflake and Python\\n\\xa0\\n\\nBefore we jump into coding I better briefly explain what Snowflake is. This is a question I recently asked when my team decided to start using it. At a high level, it is a data warehouse in the cloud. After playing around with it for a while I realised how powerful it was. I think for me, one of the most useful features is the virtual warehouses that you can use. A virtual warehouse gives you access to the same data but is completely independent of other virtual warehouses so compute resources are not shared across teams. This has proven very useful as it removes any potential for performance issues caused by other users executing queries throughout the day. This has resulted in less frustration and time wasted waiting for queries to run.\\nSince we are going to be using Snowflake I will briefly outline how you can set it up and start experimenting with it yourself. We need to do the following:\\n\\nGet a Snowflake account set up\\nGet our data into Snowflake\\nWrite and test our queries using SQL and the Snowflake UI\\nWrite a Python class that can execute our queries to generate our final dataset for modelling\\n\\nSetting up an account is as easy as signing up for a free trial on their\\xa0website. Once you have done that you can download the snowsql CLI\\xa0here. This will make it straightforward to add data to Snowflake. After following these steps we can try and connect to Snowflake using our credentials and the command line.\\n\\nsnowsql -a <account_name> -u <user_name>\\n\\n\\xa0\\nYou can find your account name in the URL when you log in to the Snowflake UI. It should look something like this: xxxxx.europe-west2.gcp. Ok, let’s move onto the next step and get our data into Snowflake. There are a few steps we need to follow here namely:\\n\\nCreate our virtual warehouse\\nCreate a database\\nDefine and Create our tables\\nCreate a staging table for our CSV files\\nCopying the data into our tables\\n\\nLuckily this isn’t too difficult and we can do this entirely using the snowsql CLI. For this project, I will be using a smaller dataset than I would like but unfortunately, I cannot use any of my company’s data and it can be pretty difficult to find large suitable datasets online. I did however find some transaction data from Dunnhumby which is freely available on\\xa0Kaggle. Just for kicks though I create a much larger synthetic dataset using this data to test how well Dask handles the challenge compared to sklearn.\\nFirst of all, we need to set up a virtual warehouse and a database using the following commands in the Snowflake UI.\\ncreate\\xa0or\\xa0replace\\xa0warehouse analytics_wh\\xa0with\\nwarehouse_size=”X-SMALL”\\nauto_suspend=180\\nauto_resume=true\\ninitially_suspended=true;\\ncreate\\xa0or\\xa0replace\\xa0database\\xa0dunnhumby;\\nOur data consists of 6 CSVs which we will convert into 6 tables. I won’t spend too much time going over the dataset as this post is more about using Snowflake and Dask rather than interpreting data.\\nBelow are the commands we can use to create our tables. All you will need to know in advance is what columns and data types you will be working with.\\n\\ncreate or replace table campaign_desc ( \\r\\ndescription string, \\r\\ncampaign number,\\r\\nstart_day number,\\r\\nend_day number );\\r\\n\\r\\ncreate or replace table campaign_table ( \\r\\ndescription string, \\r\\nHousehold_key number, \\r\\ncampaign number );\\r\\n\\r\\ncreate or replace table coupon ( \\r\\nCOUPON_UPC number, \\r\\nproduct_id number, \\r\\ncampaign number );\\r\\n\\r\\ncreate or replace table coupon_redempt ( \\r\\nhousehold_key number, \\r\\nday number, \\r\\ncoupon_upc number, \\r\\ncampaign number );\\r\\n\\r\\ncreate or replace table transactions ( \\r\\nhousehold_key number, \\r\\nBASKET_ID number, \\r\\nday number, \\r\\nproduct_id number, \\r\\nquantity number, \\r\\nsales_value number, \\r\\nstore_id number, \\r\\nretail_disc decimal, \\r\\ntrans_time number, \\r\\nweek_no number, \\r\\ncoupon_disc decimal, \\r\\ncoupon_match_disc decimal );\\r\\n\\r\\ncreate or replace table demographic_data ( \\r\\nage_dec string, \\r\\nmarital_status_code string, \\r\\nincome_desc string, \\r\\nhomeowner_desc string, \\r\\nhh_comp_desc string, \\r\\nhousehold_size_desc string, \\r\\nkid_category_desc string, \\r\\nHousehold_key number);\\n\\n\\xa0\\nNow that we have our tables created we can start thinking about how to get data into them. For this, we will need to stage our CSV files. This is basically just an intermediary step so Snowflake can directly load the files from our stage into our tables. We can use the\\xa0PUT\\xa0command to put local files in our stage and then the\\xa0COPY INTO\\xa0command to instruct Snowflake where to put this data.\\n\\nuse database dunnhumby;\\r\\n\\r\\ncreate or replace stage dunnhumby_stage;\\r\\n\\r\\nPUT file://campaigns_table.csv @dunnhumby.public.dunnhumby_stage;\\r\\n\\r\\nPUT file://campaigns_desc.csv @dunnhumby.public.dunnhumby_stage;\\r\\n\\r\\nPUT file://coupon.csv @dunnhumby.public.dunnhumby_stage;\\r\\n\\r\\nPUT file://coupon_d=redempt.csv @dunnhumby.public.dunnhumby_stage; \\r\\nPUT file://transaction_data.csv @dunnhumby.public.dunnhumby_stage; \\r\\nPUT file://demographics.csv @dunnhumby.public.dunnhumby_stage;\\n\\n\\xa0\\nAs a quick check, you can run this command to check what is in the staging area.\\n\\nls @dunnhumby.public.dunnhumby_stage;\\n\\n\\xa0\\nNow we just need to copy the data into our tables using the queries below. You can execute these either in the Snowflake UI or in the command line after logging into Snowflake.\\n\\ncopy into campaign_table \\r\\nfrom @dunnhumby.public.dunnhumby_stage/campaigns_table.csv.gz \\r\\nfile_format = ( type = csv\\r\\nskip_header=1 \\r\\nerror_on_column_count_mismatch = false \\r\\nfield_optionally_enclosed_by=’”’);\\r\\n\\r\\ncopy into campaign_desc \\r\\nfrom @dunnhumby.public.dunnhumby_stage/campaign_desc.csv.gz \\r\\nfile_format = ( type = csv\\r\\nskip_header=1 \\r\\nerror_on_column_count_mismatch = false \\r\\nfield_optionally_enclosed_by=’”’);\\r\\n\\r\\ncopy into coupon \\r\\nfrom @dunnhumby.public.dunnhumby_stage/coupon.csv.gz \\r\\nfile_format = ( type = csv\\r\\nskip_header=1 \\r\\nerror_on_column_count_mismatch = false \\r\\nfield_optionally_enclosed_by=’”’);\\r\\n\\r\\ncopy into coupon_redempt \\r\\nfrom @dunnhumby.public.dunnhumby_stage/coupon_redempt.csv.gz \\r\\nfile_format = ( type = csv\\r\\nskip_header=1 \\r\\nerror_on_column_count_mismatch = false \\r\\nfield_optionally_enclosed_by=’”’);\\r\\n\\r\\ncopy into transactions \\r\\nfrom @dunnhumby.public.dunnhumby_stage/transaction_data.csv.gz \\r\\nfile_format = ( type = csv\\r\\nskip_header=1 \\r\\nerror_on_column_count_mismatch = false \\r\\nfield_optionally_enclosed_by=’”’);\\r\\n\\r\\ncopy into demographic_data \\r\\nfrom @dunnhumby.public.dunnhumby_stage/demographics.csv.gz \\r\\nfile_format = ( type = csv skip_header=1 \\r\\nerror_on_column_count_mismatch = false \\r\\nfield_optionally_enclosed_by=’”’);\\n\\n\\xa0\\nOk great, with any luck we have our data in our tables first try. Oh, if only it was that simple, this whole process took me a few tries to get right (beware of spelling things wrong). Hopefully, you can follow along with this and be good to go. We are getting closer to the interesting stuff but the steps above are a vital part of the process so make sure you understand each of these steps.\\n\\xa0\\nWriting our Pipeline in SQL\\n\\xa0\\nIn this next step, we will be writing the queries to generate our target, our features and then finally produce a training data set. One approach to creating a dataset for modelling is to read this data into memory and use pandas to create new features and join all the data frames together. This is typically the approach you see on Kaggle and in other online tutorials. The issue with this is that it is not very efficient, particularly when you are working with any reasonably sized datasets. For this reason, it is a much better idea to outsource the heavy lifting to something like Snowflake which handles massive datasets extremely well and will likely save you a huge amount of time. I won’t be spending much time diving into the specifics of our dataset here as it isn’t really vital for what I am trying to show. In general, though, you would want to spend a considerable amount of time exploring and understanding your data before you start modelling. The goal of these queries will be to preprocess the data and create some simple features which we can later use in our models.\\n\\xa0\\nTarget Definition\\n\\xa0\\nObviously, a vital component of supervised machine learning is defining an appropriate target to predict. For our use case, we will be predicting churn by calculating whether or not a user makes another visit within two weeks after a cutoff week. The choice of 2 weeks is pretty arbitrary and will depend on the specific problem we are trying to solve but let’s just assume that it is fine for this project. In general, you would want to carefully analyse your customers to understand the distribution in gaps between visits to arrive at a suitable definition of churn.\\nThe main idea here is that for each table we want to have one row per household_key containing values for each of our features.\\n\\n\\xa0\\nCampaign Features\\n\\xa0\\n\\n\\xa0\\nTransaction Features\\n\\xa0\\nBelow we create some simple metrics based on aggregate statistics such as the average, the max and standard deviation.\\n\\n\\xa0\\nDemographic Features\\n\\xa0\\nThis dataset has lots of missing data so I decided to use imputation here. There are plenty of techniques out there for missing data from dropping the missing data, to advanced imputation methods. I have just made life easy for myself here and replaced missing values with the mode. I wouldn’t necessarily recommend taking this approach in general as understanding why this data is missing is really important in deciding how to deal with it but for the purposes of this example, I will go ahead and take the easy approach. We first compute the mode for each of our features and then use coalesce to replace each row with the mode if data is missing.\\n\\n\\xa0\\nTraining Data\\n\\xa0\\nFinally, we build a query for our training data by joining our main tables together and end up with a table containing our target, our campaign, transactions and demographic features which we can use to build a model.\\nAs a brief aside, for those interested in learning more about the features and nuances of Snowflake I would recommend the following book:\\xa0Snowflake Cookbook. I started reading this book and it is full of really helpful information on how to use Snowflake and goes into far more detail than I do here.\\n\\n\\xa0\\nPython Code for ETL\\n\\xa0\\nThe final piece we require for this ETL is to write a script to execute it. Now, this is only really required if you plan on running an ETL like this regularly but this is good practice and makes it much easier to run the ETL as and when needed.\\nLet’s briefly discuss the main components of our EtlTraining class. Our class takes one input which is the cutoff week. This is due to the way data is defined in our dataset but ordinarily, this would be in a date format that corresponds to the cutoff date we want to choose for generating training data.\\nWe initialise a list of our queries so we can easily loop through these and execute them. We also create a dictionary containing our parameters which we pass to our Snowflake connection. Here we use environment variables that we set up in Saturn Cloud.\\xa0Here\\xa0is a guide on how to do this. It is not too difficult to connect to Snowflake, all we need to do is use the Snowflake connector and pass in our dictionary of credentials. We implement this in the Snowflake connect method and return this connection as an attribute.\\nTo make these queries a little bit easier to run I save each query as a python string variable in the ml_query_pipeline.py file. The execute_etl method does exactly what it says on the tin. We loop through each query, format it, execute it and finish off by closing the Snowflake connection.\\n\\nTo run this ETL we can simply type the commands below into the terminal. (where ml_pipeline is the name of the script above.)\\n\\npython -m ml_pipeline -w 102 -j ‘train’\\n\\n\\xa0\\nAs a brief aside, you will probably want to run an ETL like this at regular intervals. For example, if you want to make daily predictions then you will need to generate a dataset like this each day to pass to your model so you can identify which of your customers are likely to churn. I won’t go into this in detail here but in my job, we use Airflow to orchestrate our ETLs so I would recommend checking it out if you are interested. In fact, I recently bought a book ‘Data Pipelines with Apache Airflow’ which I think is great and really gives some solid examples and advice on how to use airflow.\\n\\xa0\\nDask and Modeling\\n\\xa0\\n\\nNow that we have our data pipeline built, we can begin to think about modelling. The other main goal I have for this post is to highlight the advantages of using Dask as part of the ML development process and show you guys how easy it is to use.\\nFor this part of the project, I also used\\xa0Saturn Cloud\\xa0which is a really nice tool I came across recently that allows us to harness the power of Dask across a cluster of computers in the cloud. The main advantages of using Saturn for me are that it is really easy to share your work, super simple to scale up your compute as and when you need it and it has a free tier option. Model development in general is a really good use case for Dask as we usually want to train a bunch of different models and see what works best. The faster we can do this the better as we have more time to focus on other important aspects of model development. Similar to Snowflake you just need to sign up\\xa0here\\xa0and you can very quickly spin up an instance of Jupyter lab and start experimenting with it yourself.\\nNow, I realise at this point I have mentioned Dask a few times but have never really explained what it is. So let me take a moment to give you a very high-level overview of Dask and why I think it is awesome. Very simply, Dask is a python library that takes advantage of parallel computing to allow you to process and perform operations on very large datasets. And, the best part is, if you are already familiar with Python, then Dask should be very straightforward as the syntax is very similar.\\nThe graph below highlights the main components of Dask.\\n\\n\\nSource:\\xa0Dask Documentation\\n\\nCollections allow us to create a graph of tasks which can then be executed across multiple computers. Some of these data structures probably sound pretty familiar such as arrays and data frames and they are similar to what you would find in python but with some important differences. For example, you can think of a Dask data frame as a bunch of pandas data frames built in such a way that allows us to perform operations in parallel.\\nMoving on from collections we have the scheduler. Once we create the task graph the scheduler handles the rest for us. It manages the workflow and sends these tasks to either a single machine or distributes them across a cluster. Hopefully, that gives you a very brief overview of how Dask works. For more info, I suggest checking out the\\xa0documentation\\xa0or this\\xa0book. Both are very good resources to dig deeper into this topic.\\n\\xa0\\nPython Code for Modelling\\n\\xa0\\nWhen modelling, I tend to have a small number of go-to algorithms that I will always try out first. This will generally give me a good idea of what might be suited to the specific problem I have. These models are Logistic Regression, Random Forest and GradientBoosting. In my experience, when working with tabular data these algorithms will usually give you pretty good results. Below we build a sklearn modelling pipeline using these 3 models. The exact models we use here are not really important as the pipeline should work for any sklearn classification model, this is just my preference.\\nWithout further ado, let’s dive into the code. Luckily we outsourced most of our preprocessing to Snowflake so we don’t have to mess around with our training data too much here but we will add a few additional steps using sklearn pipelines.\\nThe first code snippet below shows the pipeline when using sklearn. Notice our dataset is a plain old pandas data frame and our preprocessing steps are all carried out using sklearn methods. There is nothing particularly out of the ordinary going on here. We are reading in our data from the table produced by our Snowflake ETL and passing this into a sklearn pipeline. The usual modelling steps apply here. We split the dataset into train and test and do some preprocessing, namely impute missing values using the median, scale the data and one-hot encode our categorical data. I am a big fan of sklearn pipelines and basically use them whenever I develop models nowadays, they really facilitate clean and concise code.\\nHow does this pipeline perform on a dataset with about 2 million rows? Well, running this model without any hyperparameter tuning takes about 34 minutes. Ouch, kinda slow. You can imagine how prohibitively long this would take if we wanted to do any type of hyperparameter tuning. Ok, so not ideal but let’s see how Dask handles the challenge.\\n\\n\\xa0\\nDask ML Python Code\\n\\xa0\\nOur goal here is to see if we can beat the sklearn pipeline above, spoiler alert, we definitely can. The cool thing about Dask is that the barrier to entry when you are already familiar with python is pretty low. We can get this pipeline up and running in Dask with only a few changes.\\nThe first change you probably will notice is that we have some different imports. One of the key differences between this pipeline and the previous one is that we will be using a Dask data frame instead of a pandas data frame to train our model. You can think of a Dask data frame as a bunch of pandas data frames where we can perform computations on each one at the same time. This is the core of Dask’s parallelism and is what is going to reduce the training time for this pipeline.\\nNotice we use\\xa0@dask.delayed\\xa0as a decorator to our\\xa0load_training_data\\xa0function. This instructs Dask to parallelise this function for us.\\nWe are also going to import some preprocessing and pipeline methods from Dask and most importantly, we will need to import SaturnCluster which will allow us to create a cluster for training our models. Another key difference with this code is that we use\\xa0dask.persist\\xa0after our train test split. Before this point, none of our functions has actually been computed due to Dask’s lazy evaluation. Once we use the persist method though we are telling Dask to send our data to the workers and execute the tasks we have created up until this point and leave these objects on the cluster.\\nFinally, we train our models using the delayed method. Again, this enables us to create our pipeline in a lazy way. The pipeline is not executed until we reach this code:\\n\\nfit_pipelines = dask.compute(*pipelines_)\\n\\n\\xa0\\n\\nThis time it only took us around 10 minutes to run this pipeline on the exact same dataset. That is a speedup by a factor of 3.4, not too shabby. Now, if we wanted to, we could speed this up even more by scaling up our compute resources at the touch of a button in Saturn.\\n\\xa0\\nDeploying our Pipeline\\n\\xa0\\n\\nI mentioned earlier that you will probably want to run a pipeline like this quite regularly using something like airflow. It just so happens that if you don’t want the initial hassle of setting everything up for airflow Saturn Cloud offers a simple alternative with Jobs. Jobs allow us to package up our code and run it at regular intervals or as needed. All you need to do is go to an existing project and click on create a job. Once we do that, it should look like the following:\\n\\n\\nSource:\\xa0Saturn\\n\\n\\xa0\\nFrom here, all we need to do is make sure our python files above are in the directory in the image and we can enter our python command above\\n\\npython -m ml_pipeline -w 102 -j 'train'\\n\\n\\xa0\\nWe can also set up a schedule using cron syntax to run the ETL on a daily basis if we like. For those interested, here is a\\xa0Tutorial\\xa0that goes into all the nitty-gritty.\\n\\xa0\\nConclusions and Takeaways\\n\\xa0\\n\\nWell, we have reached the end of our project at this point. Now obviously I have left out some key parts of the ML development cycle such as hyperparameter tuning and deploying our model but perhaps I will leave that for another day. Do I think you should try Dask? I am no expert by any means but from what I have seen so far it certainly seems really useful and I am super excited to experiment more with it and find more opportunities to incorporate it into my daily work as a data scientist. Hopefully, you found this useful and you too can see some of the advantages of Snowflake and Dask and you will start experimenting with them on your own.\\n\\xa0\\nResources\\n\\xa0\\n\\nData Pipelines with Apache Airflow\\nSnowflake Cookbook\\nData Science at Scale with Python and Dask\\nCoursera: SQL for Data Science\\n\\n\\xa0\\nSome of my other posts you may find interesting\\n\\xa0\\nLet’s Build a Streaming Data Pipeline\\n\\xa0\\nGaussian Mixture Modelling (GMM)\\n\\xa0\\nA Bayesian Approach to Time Series Forecasting\\n\\xa0\\nNote: Some of the links in this post are affiliate links.\\n\\xa0\\nBio: Daniel Foley is a former Economist turned Data Scientist working in the mobile gaming industry.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nBigQuery vs Snowflake: A Comparison of Data Warehouse Giants\\nPandas not enough? Here are a few good alternatives to processing larger and faster data in Python\\nAre You Still Using Pandas to Process Big Data in 2021? Here are two better options\",\n",
       " 'By Jesus Rodriguez, Intotheblock.\\ncomments\\n\\n\\nSource:\\xa0https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017/\\n\\n\\xa0\\n\\nI recently started a new newsletter focus on AI education and already has over 50,000 subscribers. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:\\n\\n\\n\\xa0\\nMachine reading comprehension(MRC) is an emergent discipline in the field of deep learning. From a conceptual standpoint, MRC focuses on deep learning models that can answer intelligent questions about specific text documents. For humans, reading comprehension is a native cognitive skill developed since the early days of school or even before. At we are reading a text, we are instinctively extracting the key ideas that will allow us to answer future questions about that subject. In the case of artificial intelligence(AI) models, that skill is still largely underdeveloped.\\nThe first widely adopted generation of natural language understanding(NLU) techniques has focused mostly on detecting the intentions and concepts associated with a specific sentence. We can think about these models as a first tier of knowledge to enable reading comprehension. However, full machine reading comprehension needs additional building blocks that can extrapolate and correlate questions to specific sections of a text and build knowledge from specific sections of a document.\\nOne of the biggest challenges in the MRC domain is that most models are based on supervised training with datasets that contain not only the documents but potential questions and answers. As you can imagine, this approach is not only very difficult to scale but practically impossible to implement in some domains in which the data is simply not available. Recently, researchers from Microsoft proposed an interesting approach to deal with this challenge in MRC algorithms.\\nIn a paper titled\\xa0“Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension”, Microsoft’s Research introduced a technique called two stage synthesis networks or\\xa0SynNet\\xa0that applies transfer learning to reduce the effort to train a MRC model.\\xa0SynNet\\xa0can be seen as a two phase approach to build knowledge related to a specific text. In the first phase,\\xa0SynNet\\xa0learns a general pattern of identifying potential “interestingness” in a text document. These are key knowledge points, named entities, or semantic concepts that are usually answers that people may ask for. Then, in the second stage, the model learns to form natural language questions around these potential answers, within the context of the article.\\nThe fascinating thing about\\xa0SynNet\\xa0is that, once trained, a model can be applied to a new domain, read the documents in the new domain and then generate pseudo questions and answers against these documents. Then, it forms the necessary training data to train an MRC system for that new domain, which could be a new disease, an employee handbook of a new company, or a new product manual.\\nMany people erroneously associate MRC technique with the more developed field of machine translation. In the case of MRC models such as\\xa0SynNet, the challenge is that they need to synthesize both questions\\xa0and\\xa0answers for a document. While the question is a syntactically ﬂuent natural language sentence, the answer is mostly a salient semantic concept in the paragraph, such as a named entity, an action, or a number. Since the answer has a different linguistic structure than the question, it may be more appropriate to view answers and questions as two different types of data.\\xa0SynNet\\xa0materializes in that theory by decomposing the process of generating question-answer pairs into two fundamental steps: The answer generation conditioned on the paragraph and the question generation conditioned on the paragraph and the answer.\\n\\n\\nImage Credit: Microsoft Research\\n\\n\\xa0\\nYou can think about\\xa0SynNet\\xa0as a teacher that is very good at generating questions from documents based on its experience. As it learn about the relevant questions in one domain, it can apply the same patterns to documents in a new domain. Microsoft researchers have applied the principles of\\xa0SynNet\\xa0to different MRC models including the recently published\\xa0ReasoNet\\xa0which have shown a lot of promise towards making machine reading comprehension a reality in the near future.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nExplainable Visual Reasoning: How MIT Builds Neural Networks that can Explain Themselves\\nTeaching AI to See Like a Human\\nHow Reading Papers Helps You Be a More Effective Data Scientist',\n",
       " 'By Matthew Mayo, KDnuggets.\\ncomments\\n\\nImage source: Reputation X\\xa0\\nLet\\'s say you\\'ve built an NLP model for some specific task, whether it be text classification, question answering, translation, or what have you. You\\'ve tested it out locally and it performs well. You\\'ve had others test it out as well, and it continues to perform well. Now you want to roll it out to a larger audience, be that audience a team of developers you work with, a specific group of end users, or even the general public. You have decided that you want to do so using a REST API, as you find this to be your best option. What now?\\nFastAPI might be able to help. FastAPI is FastAPI is a web framework for building APIs with Python. We will use FastAPI in this article to build a REST API to service an NLP model which can be queried via GET request and can dole out responses to those queries.\\nFor this example, we will skip the building of our own model, and instead leverage the Pipeline class of the HuggingFace Transformers library. Transformers is full of SOTA NLP models which can be used out of the box as-is, as well as fine-tuned for specific uses and high performance. The library\\'s pipelines can be summed up as:\\nThe pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\\n\\xa0\\nUsing the Transformers library, FastAPI, and astonishingly little code, we are going to create and deploy a very simple sentiment analysis app. We will also see how extending this same approach to a more complex app would be quite straightforward.\\n\\xa0\\nGetting Started\\nAs outlined above, we will be using Transformers and FastAPI to build this app, which means you will need these installed on your system. You will also require installation of Uvicorn, an ASGI server that FastAPI relies on as part of its backend. I easily installed them all on my *buntu system using pip:\\n\\npip install transformers\\r\\npip install fastapi\\r\\npip install uvicorn\\n\\n\\xa0\\nThat\\'s it. Moving on...\\n\\xa0\\nAnalyzing Sentiment\\nSince we will be using Transformers for our NLP pipeline, let\\'s first see how we would get this working standalone. Doing so is remarkably uncomplicated, and requires very few basic parameters be passed to the pipeline object in order to get started. Specifically, what we will need to define are a task — what it is we want to do — and a model — what it is we will use to perform our task. And that\\'s really all there is to it. We can optionally provide additional parameters or fine-tune the pipeline to our specific task and data, but for our purposes using a model out of the box should work just fine.\\nFor us, the task is sentiment-analysis and the model is nlptown/bert-base-multilingual-uncased-sentiment. This is a BERT model trained for multilingual sentiment analysis, and which has been contributed to the HuggingFace model repository by NLP Town. Note that the first time you run this script the sizable model will be downloaded to your system, so ensure that you have the available free space to do so.\\nHere is the code to setup the standalone sentiment analysis app:\\n\\nfrom transformers import pipeline\\r\\n\\r\\ntext = \\'i love this movie!!! :)\\'\\r\\n\\r\\n# Instantiate a pipeline object with our task and model passed as parameters\\r\\nnlp = pipeline(task=\\'sentiment-analysis\\', \\r\\n               model=\\'nlptown/bert-base-multilingual-uncased-sentiment\\')\\r\\n\\r\\n# Pass the text to our pipeline and print the results\\r\\nprint(f\\'{nlp(text)}\\')\\n\\n\\xa0\\nWhat is returned by the call to the pipeline object is a predicted label and its corresponding probability. In our case, the combination of task and model that we are using results in labels between 1 (negative) and 5 (positive), along with the prediction probability. Let\\'s give our script a run to see how it does.\\n\\npython test_model.py\\r\\n\\r\\n>>> [{\\'label\\': \\'5 stars\\', \\'score\\': 0.923753023147583}]\\n\\n\\xa0\\nAnd that\\'s all there is to getting some functionally basic NLP task-specific out of the box model up and running using the Transformers library and its Pipeline class.\\nIf you want to test this out a bit more before we move on to deploying it via a REST API, give this modified script a shot, which allows us to pass text from the command line and responds with a more nicely formatted reply:\\n\\nimport sys\\r\\nfrom transformers import pipeline\\r\\n\\r\\nif len(sys.argv) != 2:\\r\\n    print(\\'Usage: python model_test.py <input_string>\\')\\r\\n    sys.exit(1)\\r\\n\\r\\ntext = sys.argv[1]\\r\\n\\r\\n# Instantiate a pipeline object with our task and model passed as parameters\\r\\nnlp = pipeline(task=\\'sentiment-analysis\\',\\r\\n               model=\\'nlptown/bert-base-multilingual-uncased-sentiment\\')\\r\\n\\r\\n# Get and process result\\r\\nresult = nlp(text)\\r\\n\\r\\nsent = \\'\\'\\r\\nif (result[0][\\'label\\'] == \\'1 star\\'):\\r\\n    sent = \\'very negative\\'\\r\\nelif (result[0][\\'label\\'] == \\'2 star\\'):\\r\\n    sent = \\'negative\\'\\r\\nelif (result[0][\\'label\\'] == \\'3 stars\\'):\\r\\n    sent = \\'neutral\\'\\r\\nelif (result[0][\\'label\\'] == \\'4 stars\\'):\\r\\n    sent = \\'positive\\'\\r\\nelse:\\r\\n    sent = \\'very positive\\'\\r\\n\\r\\nprob = result[0][\\'score\\']\\r\\n\\r\\n# Format and print results\\r\\nprint(f\"{{\\'sentiment\\': \\'{sent}\\', \\'probability\\': \\'{prob}\\'}}\")\\n\\n\\xa0\\nLet\\'s run this a few times to see how it performs:\\n\\npython model_test.py \\'the sky is blue\\'\\r\\n\\r\\n>>> {\\'sentiment\\': \\'neutral\\', \\'probability\\': \\'0.2726307213306427\\'}\\r\\n\\r\\npython model_test.py \\'i really hate this restaurant!\\'\\r\\n\\r\\n>>> {\\'sentiment\\': \\'very negative\\', \\'probability\\': \\'0.9228281378746033\\'}\\r\\n\\r\\npython model_test.py \\'i love this movie!!! :)\\'\\r\\n\\r\\n>>> {\\'sentiment\\': \\'very positive\\', \\'probability\\': \\'0.923753023147583\\'}\\n\\n\\xa0\\nThis is better functionality, since we don\\'t have to hard code the text we want analyzed into our program, and we have also made the results a bit more user-friendly. Let\\'s extend this more useful version and deploy it as a REST API.\\n\\xa0\\nCreating The API\\nTime to deploy this ridiculously simple sentiment analysis app via REST API built using FastAPI. If you want to learn more about FastAPI and how to format your code using the framework, check out its documentation. What you absolutely need to know here is that we will create an instance of FastAPI (app), and then must define get requests, attach them to URLs, and assign responses for these requests via functions.\\nBelow, we will define 2 such get requests; one for the root URL (\\'/\\'), which displays a welcome message, and another for accepting strings for performing sentiment analysis on (\\'/sentiment_analysis/\\'). The code for both is quite simple; you should recognize much of what is contained in the analyze_sentiment() function that the \\'/sentiment_analysis/\\' request calls from our standalone app.\\n\\nfrom transformers import pipeline\\r\\nfrom fastapi import FastAPI\\r\\n\\r\\nnlp = pipeline(task=\\'sentiment-analysis\\',\\r\\n               model=\\'nlptown/bert-base-multilingual-uncased-sentiment\\')\\r\\n\\r\\napp = FastAPI()\\r\\n\\r\\n\\r\\n@app.get(\\'/\\')\\r\\ndef get_root():\\r\\n    return {\\'message\\': \\'This is the sentiment analysis app\\'}\\r\\n\\r\\n\\r\\n@app.get(\\'/sentiment_analysis/\\')\\r\\nasync def query_sentiment_analysis(text: str):\\r\\n    return analyze_sentiment(text)\\r\\n\\r\\n\\r\\ndef analyze_sentiment(text):\\r\\n    \"\"\"Get and process result\"\"\"\\r\\n\\r\\n    result = nlp(text)\\r\\n\\r\\n    sent = \\'\\'\\r\\n    if (result[0][\\'label\\'] == \\'1 star\\'):\\r\\n        sent = \\'very negative\\'\\r\\n    elif (result[0][\\'label\\'] == \\'2 star\\'):\\r\\n        sent = \\'negative\\'\\r\\n    elif (result[0][\\'label\\'] == \\'3 stars\\'):\\r\\n        sent = \\'neutral\\'\\r\\n    elif (result[0][\\'label\\'] == \\'4 stars\\'):\\r\\n        sent = \\'positive\\'\\r\\n    else:\\r\\n        sent = \\'very positive\\'\\r\\n\\r\\n    prob = result[0][\\'score\\']\\r\\n\\r\\n    # Format and return results\\r\\n    return {\\'sentiment\\': sent, \\'probability\\': prob}\\n\\n\\xa0\\nAnd now we have a REST API capable of accepting get requests and performing sentiment analysis. Before we try it out, we have to run the Uvicorn web server which will provide the necessary back end functionality. In order to do so, and assuming you saved the above code in a file called main.py and left the name of the FastAPI instance as app, run this:\\n\\n uvicorn main:app --reload\\n\\n\\xa0\\nYou should then see something like this:\\n\\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\\r\\nINFO:     Started reloader process [18271] using statreload\\r\\nINFO:     Started server process [18273]\\r\\nINFO:     Waiting for application startup.\\r\\nINFO:     Application startup complete.\\n\\n\\xa0\\nOpen your browser as indicated to http://127.0.0.1:8000 and you should see:\\n\\nIf you see the welcome message, congrats, it works! We can try using the browser address bar to make some requests, pasting what is in the quotation marks below after the request URL and query string (http://127.0.0.1:8000/sentiment_analysis/?text=):\\n\"welcome to my home!\"\\n\\n\"i don\\'t like your cat\"\\n\\n\"that movie was just okay\"\\n\\nGreat, we get results! Now what if we want to treat this like an API and access it accordingly? In Python, we could use the requests library. Make sure it\\'s installed using:\\n\\npip install requests\\n\\n\\xa0\\nThen give a script like this a try:\\n\\nimport requests\\r\\n\\r\\nquery = {\\'text\\':\\'i love the fettucine alfredo and would definitely recommend this restaurant to my friends!\\'}\\r\\nresponse = requests.get(\\'http://127.0.0.1:8000/sentiment_analysis/\\', params=query)\\r\\nprint(response.json())\\n\\n\\xa0\\nAfter saving, execute the script and you should get a result like this:\\n\\npython rest_request.py\\r\\n\\r\\n>>> {\\'sentiment\\': \\'very positive\\', \\'probability\\': 0.8293750882148743}\\r\\n\\n\\n\\xa0\\nThis worked as well. Excellent!\\nYou can read more about the requests library here.\\n\\xa0\\nConclusions\\nThere is obviously much more we could have done here. Preprocessing the data would have been useful. Performing error checking would have been advisable. As confirmation of this, try using some crazy characters or not wrapping a lengthy request with punctuation in quotes and see what happens.\\nNext time I promise we will expand upon what we have done here today and will make something more robust and far more interesting. I\\'m already working on it.\\nRelated:\\n\\nGetting Started with 5 Essential Natural Language Processing Libraries\\nLearn Neural Networks for Natural Language Processing Now\\nMachine Learning Pipeline Optimization with TPOT',\n",
       " 'Sponsored Post.\\n\\nAs you probably know, many breakthroughs in machine learning and related disciplines can be attributed to the rapid advances in computing power and data capacity made in the last several decades. However, there are many fields outside of computer engineering and programming that have played and continue to play a role in the development of machine learning. And even within computing, there are bleeding edge topics with significance for AI and machine learning just starting to gain steam. Quantum computing, for instance, can potentially help address a major issue with current machine learning models – namely, the data going into them. Quantum computing moves beyond classical computing’s binary data, which provides opportunities to use multidimensional datasets, which should yield higher-fidelity models.\\nIf data quality is paramount, data science is then, naturally, another component field of machine learning. Simon Lee, Chief Analytics Officer at Waitr Inc., argues that, contrary to the public’s conception of their role, data science professionals spend most of their time gathering and cleaning data and finding data artifacts. “Farmers don’t just spend their time harvesting the crops and selling them. Most of their time is spent in the dirt,” he says.\\nLee’s colorful metaphor recalls yet another related field: linguistics. Writes Joseph Byrum, chief data scientist at Principal Financial Group, “Linguist Noam Chomsky once held out the possibility of a universal grammar, which, if properly analyzed, could bring human and computer language closer. The idea was that there were innate properties of communication shared between the thousands of different languages spoken around the world [14].” Natural language processing remains one of the buzziest sub-disciplines of machine learning, and linguistics experts continue to be consulted in experimental efforts.\\nThere are many more fields and subfields with ties to machine learning and INFORMS is proud that the expertise of its members spans much of this incredibly diverse space. If you’re excited about saving lives, saving money, and solving problems, join our community of likeminded data and analytics professionals, programmers, statisticians, and operations researchers. Let’s tackle the future together.\\n\\xa0\\nWith nearly 13,000 members around the world, INFORMS is the largest international association for data science professionals. INFORMS provides unique and valuable opportunities for individual professionals and organizations to better use a wide variety of big data, analytics, and operations research methods to drive strategic visions and achieve better outcomes.',\n",
       " 'comments\\nBy Ahmad Bin Shafiq, Machine Learning Student.\\nLinear Regression\\xa0is a supervised machine learning algorithm. It predicts a\\xa0linear relationship\\xa0between an\\xa0independent variable (y), based on the given\\xa0dependant variables (x), such that the\\xa0independent variable (y)\\xa0has the\\xa0lowest cost.\\n\\xa0\\nDifferent approaches to solve linear regression models\\n\\xa0\\nThere are many different methods that we can apply to our linear regression model in order to make it more efficient. But we will discuss the most common of them here.\\n\\nGradient Descent\\nLeast Square Method / Normal Equation Method\\nAdams Method\\nSingular Value Decomposition (SVD)\\n\\nOkay, so let’s begin…\\n\\xa0\\nGradient Descent\\n\\xa0\\nOne of the most common and easiest methods for\\xa0beginners\\xa0to solve linear regression problems is gradient descent.\\nHow Gradient Descent works\\nNow, let\\'s suppose we have our data plotted out in the form of a scatter graph, and when we apply a cost function to it, our model will make a prediction. Now this prediction can be very good, or it can be far away from our ideal prediction (meaning its cost will be high). So, in order to minimize that cost (error), we apply gradient descent to it.\\nNow, gradient descent will slowly converge our hypothesis towards a global minimum, where the\\xa0cost\\xa0would be lowest. In doing so, we have to manually set the value of\\xa0alpha,\\xa0and the slope of the hypothesis changes with respect to our alpha’s value. If the value of alpha is large, then it will take big steps. Otherwise, in the case of small alpha, our hypothesis would converge slowly and through small baby steps.\\n\\nHypothesis converging towards a global minimum. Image from\\xa0Medium.\\nThe Equation for Gradient Descent is\\n\\nSource:\\xa0Ruder.io.\\nImplementing Gradient Descent in Python\\n\\nimport numpy as np\\r\\nfrom matplotlib import pyplot\\r\\n\\r\\n#creating our data\\r\\nX = np.random.rand(10,1)\\r\\ny = np.random.rand(10,1)\\r\\nm = len(y)\\r\\ntheta = np.ones(1)\\r\\n\\r\\n#applying gradient descent\\r\\na = 0.0005\\r\\ncost_list = []\\r\\nfor i in range(len(y)):\\r\\n    \\r\\n    theta = theta - a*(1/m)*np.transpose(X)@(X@theta - y)\\r\\n           \\r\\n    cost_val = (1/m)*np.transpose(X)@(X@theta - y)\\r\\n    cost_list.append(cost_val)\\r\\n\\r\\n#Predicting our Hypothesis\\r\\nb = theta\\r\\nyhat = X.dot(b)\\r\\n\\r\\n#Plotting our results\\r\\npyplot.scatter(X, y, color=\\'red\\')\\r\\npyplot.plot(X, yhat, color=\\'blue\\')\\r\\npyplot.show()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nModel after Gradient Descent.\\nHere first, we have created our dataset, and then we looped over all our training examples in order to minimize our cost of hypothesis.\\nPros:\\nImportant advantages of Gradient Descent are\\n\\nLess Computational Cost as compared to SVD or ADAM\\nRunning time is O(kn²)\\nWorks well with more number of features\\n\\nCons:\\nImportant cons of Gradient Descent are\\n\\nNeed to choose some learning rate\\xa0α\\nNeeds many iterations to converge\\nCan be stuck in Local Minima\\nIf not proper Learning Rate\\xa0α, then it might not converge.\\n\\n\\xa0\\nLeast Square Method\\n\\xa0\\nThe least-square method, also known as the\\xa0normal equation,\\xa0is also one of the most common approaches to solving linear regression models easily. But, this one needs to have some basic knowledge of linear algebra.\\nHow the least square method works\\nIn normal LSM, we solve directly for the value of our coefficient. In short, in one step, we reach our optical minimum point, or we can say only in one step we fit our hypothesis to our data with the lowest cost possible.\\n\\nBefore and after applying LSM to our dataset. Image from\\xa0Medium.\\nThe equation for LSM is\\n\\nImplementing LSM in Python\\n\\nimport numpy as np\\r\\nfrom matplotlib import pyplot\\r\\n\\r\\n#creating our data\\r\\nX = np.random.rand(10,1)\\r\\ny = np.random.rand(10,1)\\r\\n\\r\\n#Computing coefficient\\r\\nb = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\\r\\n\\r\\n#Predicting our Hypothesis\\r\\nyhat = X.dot(b)\\r\\n#Plotting our results\\r\\npyplot.scatter(X, y, color=\\'red\\')\\r\\npyplot.plot(X, yhat, color=\\'blue\\')\\r\\npyplot.show()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nHere first we have created our dataset and then minimized the cost of our hypothesis using the\\nb = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\\ncode, which is equivalent to our equation.\\nPros:\\nImportant advantages of LSM are:\\n\\nNo Learning Rate\\nNo Iterations\\nFeature Scaling Not Necessary\\nWorks really well when the Number of Features is less.\\n\\nCons:\\nImportant cons are:\\n\\nIs computationally expensive when the dataset is big.\\nSlow when Number of Features is more\\nRunning Time is O(n³)\\nSometimes, your X transpose X is non-invertible, i.e., a singular matrix with no inverse. You can use np.linalg.pinv instead of\\xa0np.linalg.inv\\xa0to overcome this problem.\\n\\n\\xa0\\nAdam’s Method\\n\\xa0\\nADAM, which stands for Adaptive Moment Estimation, is an optimization algorithm that is widely used in Deep Learning.\\nIt is an iterative algorithm that works well on noisy data.\\nIt is the combination of RMSProp and Mini-batch Gradient Descent algorithms.\\nIn addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.\\nWe compute the decaying averages of past and past squared gradients respectively as follows:\\n\\nCredit:\\xa0Ruder.io.\\nAs\\xa0mt and\\xa0vt are initialized as vectors of 0’s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e., β1β1 and β2β2 are close to 1).\\nThey counteract these biases by computing bias-corrected first and second-moment estimates:\\n\\nCredit:\\xa0Ruder.io.\\nThey then update the parameters with:\\n\\nCredit:\\xa0Ruder.io.\\nYou can learn the theory behind Adam\\xa0here\\xa0or\\xa0here.\\nPseudocode for Adam\\xa0is\\n\\nSource:\\xa0Arxiv Adam.\\nLet’s see it’s code in Pure Python.\\n\\n#Creating the Dummy Data set and importing libraries\\r\\nimport math\\r\\nimport seaborn as sns\\r\\nimport numpy as np \\r\\nfrom scipy import stats\\r\\nfrom matplotlib import pyplot\\r\\nx = np.random.normal(0,1,size=(100,1))\\r\\ny = np.random.random(size=(100,1))\\r\\n\\r\\n\\n\\nNow Let’s find the actual graph of Linear Regression and values for slope and intercept for our dataset.\\n\\nprint(\"Intercept is \" ,stats.mstats.linregress(x,y).intercept)\\r\\nprint(\"Slope is \", stats.mstats.linregress(x,y).slope)\\r\\n\\r\\n\\n\\n\\xa0\\n\\nNow let us see the Linear Regression line using the Seaborn\\xa0regplot\\xa0function.\\n\\npyplot.figure(figsize=(15,8))\\r\\nsns.regplot(x,y)\\r\\npyplot.show()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nLet us code Adam Optimizer now in pure Python.\\n\\nh = lambda theta_0, theta_1, x: theta_0 + np.dot(x,theta_1) #equation of straight lines\\r\\n\\r\\n# the cost function (for the whole batch. for comparison later)\\r\\ndef J(x, y, theta_0, theta_1):\\r\\n    m = len(x)\\r\\n    returnValue = 0\\r\\n    for i in range(m):\\r\\n        returnValue += (h(theta_0, theta_1, x[i]) - y[i])**2\\r\\n    returnValue = returnValue/(2*m)\\r\\n    return returnValue\\r\\n\\r\\n# finding the gradient per each training example\\r\\ndef grad_J(x, y, theta_0, theta_1):\\r\\n    returnValue = np.array([0., 0.])\\r\\n    returnValue[0] += (h(theta_0, theta_1, x) - y)\\r\\n    returnValue[1] += (h(theta_0, theta_1, x) - y)*x\\r\\n    return returnValue\\r\\n\\r\\nclass AdamOptimizer:\\r\\n    def __init__(self, weights, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\\r\\n        self.alpha = alpha\\r\\n        self.beta1 = beta1\\r\\n        self.beta2 = beta2\\r\\n        self.epsilon = epsilon\\r\\n        self.m = 0\\r\\n        self.v = 0\\r\\n        self.t = 0\\r\\n        self.theta = weights\\r\\n        \\r\\n    def backward_pass(self, gradient):\\r\\n        self.t = self.t + 1\\r\\n        self.m = self.beta1*self.m + (1 - self.beta1)*gradient\\r\\n        self.v = self.beta2*self.v + (1 - self.beta2)*(gradient**2)\\r\\n        m_hat = self.m/(1 - self.beta1**self.t)\\r\\n        v_hat = self.v/(1 - self.beta2**self.t)\\r\\n        self.theta = self.theta - self.alpha*(m_hat/(np.sqrt(v_hat) - self.epsilon))\\r\\n        return self.theta\\r\\n\\r\\n\\n\\nHere, we have implemented all the equations mentioned in the pseudocode above using an object-oriented approach and some helper functions.\\nLet us now set the hyperparameters for our model.\\n\\nepochs = 1500\\r\\nprint_interval = 100\\r\\nm = len(x)\\r\\ninitial_theta = np.array([0., 0.]) # initial value of theta, before gradient descent\\r\\ninitial_cost = J(x, y, initial_theta[0], initial_theta[1])\\r\\n\\r\\ntheta = initial_theta\\r\\nadam_optimizer = AdamOptimizer(theta, alpha=0.001)\\r\\nadam_history = [] # to plot out path of descent\\r\\nadam_history.append(dict({\\'theta\\': theta, \\'cost\\': initial_cost})#to check theta and cost function\\r\\n\\r\\n\\n\\nAnd finally, the training process.\\n\\nfor j in range(epochs):\\r\\n    for i in range(m):\\r\\n        gradients = grad_J(x[i], y[i], theta[0], theta[1])\\r\\n        theta = adam_optimizer.backward_pass(gradients)\\r\\n    \\r\\n    if ((j+1)%print_interval == 0 or j==0):\\r\\n        cost = J(x, y, theta[0], theta[1])\\r\\n        print (\\'After {} epochs, Cost = {}, theta = {}\\'.format(j+1, cost, theta))\\r\\n        adam_history.append(dict({\\'theta\\': theta, \\'cost\\': cost}))\\r\\n        \\r\\nprint (\\'\\\\nFinal theta = {}\\'.format(theta))\\r\\n\\r\\n\\n\\n\\xa0\\n\\nNow, if we compare the\\xa0Final theta\\xa0values to the slope and intercept values, calculated earlier using\\xa0scipy.stats.mstat.linregress, they are almost 99% equal and can be 100% equal by adjusting the hyperparameters.\\nFinally, let us plot it.\\n\\nb = theta\\r\\nyhat = b[0] + x.dot(b[1])\\r\\npyplot.figure(figsize=(15,8))\\r\\npyplot.scatter(x, y, color=\\'red\\')\\r\\npyplot.plot(x, yhat, color=\\'blue\\')\\r\\npyplot.show()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nAnd we can see that our plot is similar to plot obtained using\\xa0sns.regplot.\\nPros:\\n\\nStraightforward to implement.\\nComputationally efficient.\\nLittle memory requirements.\\nInvariant to diagonal rescale of the gradients.\\nWell suited for problems that are large in terms of data and/or parameters.\\nAppropriate for non-stationary objectives.\\nAppropriate for problems with very noisy/or sparse gradients.\\nHyper-parameters have intuitive interpretation and typically require little tuning.\\n\\nCons:\\n\\nAdam and RMSProp are highly sensitive to certain values of the learning rate (and, sometimes, other hyper-parameters like the batch size), and they can catastrophically fail to converge if e.g., the learning rate is too high. (Source:\\xa0stackexchange)\\n\\n\\xa0\\nSingular Value Decomposition\\n\\xa0\\nSingular value decomposition shortened as SVD is one of the famous and most widely used dimensionality reduction methods in linear regression.\\nSVD is used (amongst other uses) as a preprocessing step to reduce the number of dimensions for our learning algorithm. SVD decomposes a matrix into a product of three other matrices (U, S, V).\\n\\nOnce our matrix has been decomposed, the coefficients for our hypothesis can be found by calculating the pseudoinverse of the input matrix\\xa0X\\xa0and multiplying that by the output vector\\xa0y. After that, we fit our hypothesis to our data, and that gives us the lowest cost.\\nImplementing SVD in Python\\n\\nimport numpy as np\\r\\nfrom matplotlib import pyplot\\r\\n\\r\\n#Creating our data\\r\\nX = np.random.rand(10,1)\\r\\ny = np.random.rand(10,1)\\r\\n\\r\\n#Computing coefficient\\r\\nb = np.linalg.pinv(X).dot(y)\\r\\n\\r\\n#Predicting our Hypothesis\\r\\nyhat = X.dot(b)\\r\\n\\r\\n#Plotting our results\\r\\npyplot.scatter(X, y, color=\\'red\\')\\r\\npyplot.plot(X, yhat, color=\\'blue\\')\\r\\npyplot.show()\\r\\n\\r\\n\\n\\n\\nThough it is not converged very well, it is still pretty good.\\nHere first, we have created our dataset and then minimized the cost of our hypothesis using\\xa0b = np.linalg.pinv(X).dot(y), which is the equation for SVD.\\nPros:\\n\\nWorks better with higher dimensional data\\nGood for gaussian type distributed data\\nReally stable and efficient for a small dataset\\nWhile solving linear equations for linear regression, it is more stable and the preferred approach.\\n\\nCons:\\n\\nRunning time is O(n³)\\nMultiple risk factors\\nReally sensitive to outliers\\nMay get unstable with a very large dataset\\n\\n\\xa0\\nLearning Outcome\\n\\xa0\\nAs of now, we have learned and implemented gradient descent, LSM, ADAM, and SVD. And now, we have a very good understanding of all of these algorithms, and we also know what are the pros and cons.\\nOne thing we noticed was that the ADAM optimization algorithm was the most accurate, and according to the actual ADAM research paper, ADAM outperforms almost all other optimization algorithms.\\n\\xa0\\nRelated:\\n\\nLinear to Logistic Regression, Explained Step by Step\\nA Beginner’s Guide to Linear Regression in Python with Scikit-Learn\\nLinear Regression In Real Life',\n",
       " 'comments\\nBy Xinran Waibel, Data Engineer at Netflix.\\n\\nPhoto by\\xa0Green Chameleon\\xa0on\\xa0Unsplash.\\nSQL\\xa0is one of the most essential programming languages for data analysis and data processing, and so SQL questions are always part of the interview process for data science-related jobs, such as data analysts, data scientists, and\\xa0data engineers. SQL interviews are meant to evaluate candidates’ technical and problem-solving skills. Therefore, it is critical to write not only correct queries based on sample data but also consider various scenarios and edge cases as if working with real-world datasets.\\nI’ve helped design and\\xa0conduct SQL interview questions for data science candidates and have undergone many SQL interviews for jobs in giant technology companies and startups myself. In this blog post, I will explain the common patterns seen in SQL interview questions and provide tips on how to neatly handle them in SQL queries.\\n\\xa0\\nAsk Questions\\n\\xa0\\nTo nail an SQL interview, the most important thing is to make sure that you have all the details of the given task and data sample by asking as many questions as you need. Understanding the requirements will save you time from iterating on problems later and enable you to handle edge cases well.\\nI noticed many candidates tend to jump right into the solution without having a good understanding of the SQL questions or the dataset. Later on, they had to repeatedly modify their queries after I pointed out problems in their solution. In the end, they wasted a lot of interview time in iteration and may not have even arrived at the right solution.\\nI recommend treating SQL interviews as if you are working with a business partner at work. You would want to gather all the requirements on the data request before you provide a solution.\\nExample\\nFind the top 3 employees who have the highest salary.\\n\\xa0\\nThe sample employee_salary table.\\nYou should ask the interviewer(s) to clarify the “top 3”. Should I include exactly 3 employees in my results? How do you want me to handle ties? In addition, carefully review the sample employee data. What is the data type of the salary field? Do I need to clean the data before calculate?\\n\\xa0\\nWhich JOIN\\n\\xa0\\n\\nSource:\\xa0dofactory.\\nIn SQL, JOIN is frequently used to combine information from multiple tables. There are\\xa0four different types of JOIN, but in most cases, we only use INNER, LEFT, and FULL JOIN because the RIGHT JOIN is not very intuitive and can be easily rewritten using LEFT JOIN. In an SQL interview, you need to choose the right JOIN to use based on the specific requirement of the given question.\\nExample\\nFind the total number of classes taken by each student. (Provide student id, name, and number of classes taken.)\\n\\xa0\\nThe sample student and class_history tables.\\nAs you might have noticed, not all students appearing in the class_history\\xa0table are present in the student\\xa0table, which might be because those students are no longer enrolled. (This is actually very typical in transactional databases, as records are often deleted once inactive.) Depending on whether the interviewer wants inactive students in the results, we need to use either LEFT JOIN or INNER JOIN to combine two tables:\\n\\nWITH class_count AS (\\r\\n    SELECT student_id, COUNT(*) AS num_of_class\\r\\n    FROM class_history\\r\\n    GROUP BY student_id\\r\\n)\\r\\nSELECT\\r\\n    c.student_id,\\r\\n    s.student_name,\\r\\n    c.num_of_class\\r\\nFROM class_count c\\r\\n-- CASE 1: include only active students\\r\\nJOIN student s ON c.student_id = s.student_id\\r\\n-- CASE 2: include all students\\r\\n-- LEFT JOIN student s ON c.student_id = s.student_id\\r\\n\\r\\n\\n\\n\\xa0\\n\\nPhoto by\\xa0petr sidorov\\xa0on\\xa0Unsplash.\\n\\xa0\\nGROUP BY\\n\\xa0\\nGROUP BY is the most essential function in SQL since it is widely used for data aggregation. If you see keywords such as sum, average, minimum, or maximum in a SQL question, it is a big hint that you should probably use GROUP BY in your query. A common pitfall is mixing WHERE and HAVING when filtering data along with GROUP BY — I have seen many people make this mistake.\\nExample\\nCalculate the average required course GPA in each school year for each student and find students who are qualified for the Dean’s List (GPA ≥ 3.5) in each semester.\\n\\xa0\\nThe sample GPA history table.\\nSince we consider only the required courses in our GPA calculation, we need to exclude optional courses using\\xa0WHERE is_required = TRUE. We need the average GPA per student per year, so we will GROUP BY both the student_id\\xa0and the school_year\\xa0columns and take the average of the gpa\\xa0column. Lastly, we only keep rows where the student has an average GPA higher than 3.5, which can be implemented using HAVING. Let’s put everything together:\\n\\nSELECT\\r\\n    student_id,\\r\\n    school_year,\\r\\n    AVG(gpa) AS avg_gpa\\r\\nFROM gpa_history\\r\\nWHERE is_required = TRUE\\r\\nGROUP BY student_id, school_year\\r\\nHAVING AVG(gpa) >= 3.5\\r\\n\\r\\n\\n\\n\\xa0\\nKeep in mind that whenever GROUP BY is used in a query, you can only select group-by columns and aggregated columns because the row-level information in other columns has already been discarded.\\nSome people might wonder what’s the difference between WHERE and HAVING, or why we don’t just write\\xa0HAVING avg_gpa >= 3.5\\xa0instead of specifying the function. I will explain more in the next section.\\n\\xa0\\nSQL query execution order\\n\\xa0\\nMost people write SQL queries from top to bottom starting from SELECT, but do you know that SELECT is one of the very last functions executed by the SQL engine? Below is the execution order of a SQL query:\\n\\nFROM, JOIN\\nWHERE\\nGROUP BY\\nHAVING\\nSELECT\\nDISTINCT\\nORDER BY\\nLIMIT, OFFSET\\n\\nConsider the previous example again. Because we want to filter out optional courses before computing average GPAs, I used\\xa0WHERE is_required = TRUE\\xa0instead of HAVING because WHERE is executed before GROUP BY and HAVING. The reason I can’t write\\xa0HAVING avg_gpa >= 3.5\\xa0is that avg_gpa\\xa0is defined as part of SELECT, so it cannot be referred to in steps executed before SELECT.\\nI recommend following the execution order when writing queries, which is helpful if you struggle with writing complicated queries.\\n\\nPhoto by\\xa0Stefano Ghezzi\\xa0on\\xa0Unsplash.\\n\\xa0\\nWindow functions\\n\\xa0\\nWindow functions\\xa0frequently appear in SQL interviews as well. There are five common window functions:\\n\\nRANK/DENSE_RANK\\xa0/ROW_NUMBER: these assign a rank to each row by ordering specific columns. If any partition columns are given, rows are ranked within a partition group that it belongs to.\\nLAG/LEAD: it retrieves column values from a preceding or following row based on a specified order and partition group.\\n\\nIn SQL interviews, it is important to understand the differences between ranking functions and know when to use LAG/LEAD.\\nExample\\nFind the top 3 employees who have the highest salary in each department.\\n\\nAnother sample employee_salary table.\\nWhen an SQL question asks for “TOP N”, we can use either ORDER BY or ranking functions to answer the question. However, in this example, it asks to calculate “TOP N X in each Y”, which is a strong hint that we should use ranking functions because we need to rank rows within each partition group.\\nThe query below finds exactly 3 highest-payed employees regardless of ties:\\n\\nWITH T AS (\\r\\nSELECT\\r\\n    *,\\r\\n    ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY employee_salary DESC) AS rank_in_dep\\r\\nFROM employee_salary)\\r\\nSELECT * FROM T\\r\\nWHERE rank_in_dep <= 3\\r\\n-- Note: When using ROW_NUMBER, each row will have a unique rank number and ranks for tied records are assigned randomly. For exmaple, Rimsha and Tiah may be rank 2 or 3 in different query runs.\\r\\n\\r\\n\\n\\n\\xa0\\n\\xa0\\nMoreover, based on how ties should be handled, we could pick a different ranking function. Again, details matter!\\n\\nA comparison of the results of ROW_NUMBER, RANK, and DENSE_RANK functions.\\n\\nPhoto by\\xa0Héctor J. Rivas\\xa0on\\xa0Unsplash.\\n\\xa0\\nDuplicates\\n\\xa0\\nAnother common pitfall in SQL interviews is ignoring data duplicates. Although some columns seem to have distinct values in the sample data, candidates are expected to consider all possibilities as if they are working with a real-world dataset. For example, in the employee_salary\\xa0table from the previous example, it is possible to have employees sharing the same name.\\nOne easy way to avoid potential problems caused by duplicates is to always use ID columns to uniquely identify distinct records.\\nExample\\nFind the total salary from all departments for each employee using the\\xa0employee_salary\\xa0table.\\nThe right solution is to GROUP BY\\xa0employee_id\\xa0and calculate the total salary using\\xa0SUM(employee_salary). If employee names are needed, join with an employee table at the end to retrieve employee name information.\\nThe wrong approach is to GROUP BY\\xa0employee_name.\\n\\xa0\\nNULL\\n\\xa0\\nIn SQL, any predicates can result in one of the three values: true, false, and\\xa0NULL, a reserved keyword for\\xa0unknown or missing data values.\\xa0Handling NULL datasets can be unexpectedly tricky. In an SQL interview, the interviewer might pay extra attention to whether your solution has handled NULL values. Sometimes it is obvious if a column is not nullable (ID columns, for instance), but for most other columns, it is very likely there will be NULL values.\\nI suggest confirming whether key columns in the sample data are nullable and, if so, utilize functions such as\\xa0IS (NOT) NULL,\\xa0IFNULL, and\\xa0COALESCE\\xa0to cover those edge cases.\\n(Want to learn more about how to deal with NULL values? Check out\\xa0my guide on working with NULL in SQL.)\\n\\xa0\\nCommunication\\n\\xa0\\nLast but not least — keep the communication going during SQL interviews.\\nI interviewed many candidates who barely talked except when they had questions, which would be okay if they came up with the perfect solution at the end. However, it is generally a good idea to keep up communication during technical interviews. For example, you can talk about your understanding of the question and data, how you plan to approach the problem, why you use some functions versus other alternatives, and what edge cases you are considering.\\nTL;DR:\\n\\nAlways ask questions to gather the required details first.\\nCarefully choose between INNER, LEFT, and FULL JOIN.\\nUse GROUP BY to aggregate data and properly use WHERE and HAVING.\\nUnderstand the differences between the three ranking functions.\\nKnow when to use LAG/LEAD window functions.\\nIf you struggle with creating complicated queries, try following the SQL execution order.\\nConsider potential data problems, such as duplicates and NULL values.\\nCommunicate your thought process with the interviewers.\\n\\nTo help you understand how to use these strategies in an actual SQL interview, I will walk you through a sample SQL interview question from end to end in the video below:\\n\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Xinran Waibel is an experienced Data Engineer in the San Francisco Bay\\xa0Area, currently working at Netflix. She is also a technical writer for Towards Data Science, Google Cloud, and The Startup on Medium.\\nRelated:\\n\\nThe Ultimate Guide to Data Engineer Interviews\\nHow to Rock a Virtual Data Interview\\nThe Data Science Interview Study Guide',\n",
       " 'comments\\nBy Angela Lin, EvalML Software Engineer\\n\\nText can be a rich and informative type of data. It can be used in a variety of tasks, including sentiment analysis, topic extraction, and spam detection. However, raw text cannot be fed directly to machine learning algorithms, because most models can only understand numeric values. Thus, to utilize text as data in machine learning, it must first be processed and transformed to numeric values.\\nIn this post, we will learn how we can use\\xa0EvalML\\xa0to detect spam text messages by framing it as a binary classification problem using text data. EvalML is an AutoML library written in Python that uses\\xa0Woodwork\\xa0to detect and specify how data should be treated, and the\\xa0nlp-primitives library\\xa0to create meaningful numeric features from raw text data.\\n\\xa0\\nSpam Dataset\\n\\xa0\\nThe dataset we will be using in this demo consists of SMS text messages in English, some of which are tagged as legitimate (“ham”), and others which are tagged as spam. For this demo, we have modified the\\xa0original dataset from Kaggle\\xa0by joining all of the input text columns and downsampling the majority class (“ham”) so that the “ham” to “spam” ratio is 3:1. The following references to the data we will be inspecting will always refer to our modified and smaller dataset.\\nLet’s load in our data and display a few rows to understand what our text messages look like:\\n\\nfrom urllib.request import urlopen\\r\\nimport pandas as pd\\r\\n\\r\\ninput_data = urlopen(\\'https://featurelabs-static.s3.amazonaws.com/spam_text_messages_modified.csv\\')\\r\\ndata = pd.read_csv(input_data)\\r\\nX = data.drop([\\'Category\\'], axis=1)\\r\\ny = data[\\'Category\\']display(X.head())\\n\\n\\nA sample of our input data\\n\\n\\xa0\\nWe can plot the frequency of our target values to verify that the ratio of “ham” to “spam” in our modified dataset is approximately 3:1.\\n\\ny.value_counts().plot.pie(figsize=(10,10))\\n\\n\\n\\nThe ratio of \"ham\" to \"spam\" is approximately 3:1\\n\\n\\xa0\\nBecause the ratio of “ham” to “spam” is 3:1, we can create a trivial model that always classifies a message as the majority “ham” class to obtain a model that has a 75%\\xa0accuracy. This model would also have a\\xa0recall\\xa0score of 0%, since it is unable to classify any of the minority “spam” class samples correctly, and a balanced accuracy score of 50%. This means that a machine learning model should have an accuracy score greater than 75%, a recall score greater than 0%, and a balanced accuracy score greater than 50% to be useful.\\n\\n\\n\\nBaseline model (always guesses majority class)\\n\\n\\nAccuracy\\n75%\\n\\n\\nBalanced Accuracy\\n50%\\n\\n\\nRecall\\n0%\\n\\n\\n\\nLet’s generate a model using EvalML and see if we can do better than this trivial model!\\n\\xa0\\nIntroducing Woodwork\\n\\xa0\\nBefore feeding our data into EvalML, we have a more fundamental issue to address: How can we specify that our data should be treated as\\xa0text\\xa0data? Using\\xa0pandas\\xa0alone, we can\\'t distinguish between text data and non-text data (such as categorical data) because pandas uses the same\\xa0object\\xa0data type to store both. How we can make sure that our models correctly treat our text messages as text data, and not as hundreds of different unique categories?\\npandas treats “Message” as an “object” data type by default\\n\\n\\xa0\\nEvalML utilizes the open-source\\xa0Woodwork\\xa0library to detect and specify how each feature should be treated, independent of its underlying physical data type. This means we can treat columns with the same physical data type differently. For example, we can specify that we want some columns that contain text to be treated as categorical columns, while we treat other columns with text as natural language columns, even if these columns have the same underlying\\xa0object\\xa0datatype. This differentiation allows us to clear up the ambiguity between features that may have the same underlying datatype in\\xa0pandas, but ultimately represent different types of data.\\nHere, we initialize a Woodwork\\xa0DataTable\\xa0with our feature. Our single\\xa0Message\\xa0feature is automatically detected as a natural language or text column.\\n\\nimport woodwork as ww\\r\\nX = ww.DataTable(X)\\r\\n\\r\\n# Note: We could have also manually set the Message column to \\r\\n# natural language if Woodwork had not automatically detected\\r\\nfrom evalml.utils import infer_feature_types\\r\\nX = infer_feature_types(X, {\\'Message\\': \\'NaturalLanguage\\'})\\n\\n\\nOur \"Message\" feature is automatically detected as a natural language (text) column\\n\\n\\xa0\\nWe can also initialize a Woodwork\\xa0DataColumn\\xa0for our target.\\n\\ny = ww.DataColumn(y)\\n\\n\\nOur target is automatically detected as a categorical column. This makes sense, since we have a binary classification problem with two categories of text messages: spam and ham.\\nOur target (\"y\") is automatically detected as a categorical column\\n\\n\\xa0\\nRunning AutoMLSearch\\n\\xa0\\nNow, let’s feed our data to\\xa0AutoMLSearch\\xa0to see if we can produce a nontrivial machine learning model to detect spam. AutoML is the process of automating the construction, training, and evaluation of machine learning models.\\xa0AutoMLSearch\\xa0is EvalML’s interface for AutoML.\\nFirst, we will split our data into training and test data sets. We will use the training data set to train and find the best model, and then validate our model’s performance on the test data.\\nEvalML offers a utility method that makes this easy. All we need to do is specify that we have a binary classification problem, and that we want to reserve 20% of our data as test data.\\n\\nfrom evalml.preprocessing import split_data\\r\\n\\r\\nX_train, X_holdout, y_train, y_holdout = split_data(X, y, problem_type=\\'binary\\', test_size=0.2)\\n\\n\\nNext, we can set up\\xa0AutoMLSearch\\xa0by specifying the problem type and passing in our training data. Again, we have a binary classification problem because we are trying to classify our messages as one of two categories: ham or spam.\\n\\nautoml = AutoMLSearch(X_train=X_train, y_train=y_train, problem_type=\\'binary\\')\\n\\n\\nCalling the constructor initializes an\\xa0AutoMLSearch\\xa0object that is configured for our data. Now, we can call\\xa0automl.search()\\xa0to start the AutoML process. This will automatically generate pipelines for our data, and then train a collection of various models.\\n\\nautoml.search()\\n\\n\\nEvalML\\'s AutoML search has trained and evaluated nine different models.\\n\\n\\xa0\\nTo understand the type of pipelines\\xa0AutoMLSearch\\xa0has built, we can grab the best performing pipeline and examine it in greater detail. We can call\\xa0automl.describe_pipeline(id)\\xa0to see detailed information about the pipeline’s components and performance, or\\xa0automl.graph(pipeline)\\xa0to see a visual representation of our pipeline as a flow of components.\\n\\n# rankings are ordered from best to worst, \\r\\n# so 0th index is the best pipeline\\r\\n\\r\\nbest_pipeline_id = automl.rankings.iloc[0][\"id\"])\\r\\nautoml.describe_pipeline(best_pipeline_id)\\n\\n\\nDescription of our best pipeline\\n\\n\\xa0\\n\\n# We can also grab the best performing pipeline like this\\r\\nautoml.best_pipeline\\r\\nautoml.graph(automl.best_pipeline)\\n\\n\\nGraphical representation of our best pipeline\\n\\n\\xa0\\nBy examining the best performing pipeline, we can better understand what\\xa0AutoMLSearch\\xa0is doing, and what pipelines it built with our text data. The best pipeline consists of an\\xa0Imputer, a\\xa0Text Featurization Component\\xa0and a\\xa0Random Forest Classifier\\xa0component. Let’s break this down and understand how this pipeline was constructed:\\n\\nAutoMLSearch\\xa0always adds an\\xa0Imputer\\xa0to each generated pipeline to handle missing values. By default, the\\xa0Imputer\\xa0will fill the missing values in numeric columns with the mean of each column, and fill the missing values in categorical columns with the most frequent category of each column. Because we don’t have any categorical or numeric columns in our input, the\\xa0Imputer\\xa0does not transform our data.\\nSince\\xa0AutoMLSearch\\xa0identified a text column (our\\xa0Message\\xa0feature), it appended a\\xa0Text Featurization Component\\xa0to each pipeline. This component first cleans the text input by removing all non-alphanumerical characters (except spaces) and converting the text input to lowercase. The component then processes the cleaned text features by replacing each text feature with representative numeric features using\\xa0LSA\\xa0and the\\xa0nlp-primitives package. This component is necessary if we want to handle text features in machine learning, because most machine learning models are not able to handle text data natively. Thus, we need this component to help extract useful information from the raw text input and convert it to numeric values that the models can understand.\\nFinally, each pipeline has an estimator (a model) which is fitted on our transformed training data and is used to make predictions. Our best pipeline has a\\xa0Random Forest classifier. If we took a look at some other pipelines, we would also see other pipelines constructed with a LightGBM classifier, Decision Tree classifier, XGBoost classifier, etc.\\n\\n\\xa0\\nBest Pipeline Performance\\n\\xa0\\nNow, let’s see how well our best pipeline performed on various metrics and if we could beat the baseline trivial model by scoring the pipeline on test data.\\n\\n>>> scores = best_pipeline.score(X_holdout, y_holdout,  objectives=evalml.objectives.get_core_objectives(\\'binary\\') + [\\'recall\\'])\\r\\n>>> scores\\r\\nOrderedDict([(\\'MCC Binary\\', 0.9278003804626707),\\r\\n\\t\\t\\t (\\'Log Loss Binary\\', 0.1137465525638786),\\r\\n\\t\\t\\t (\\'AUC\\', 0.9823022077397945),\\r\\n             (\\'Precision\\', 0.9716312056737588),\\r\\n             (\\'F1\\', 0.9448275862068964),\\r\\n             (\\'Balanced Accuracy Binary\\', 0.9552772006397513),\\r\\n             (\\'Accuracy Binary\\', 0.9732441471571907),\\r\\n             (\\'Recall\\', 0.9194630872483222)])\\n\\nOur best pipeline performs much better than the baseline\\n\\xa0\\n\\n\\n\\nBaseline model (always guesses majority class)\\nPipeline with Text Featurization Component\\n\\n\\nAccuracy\\n75%\\n97.32%\\n\\n\\nBalanced Accuracy\\n50%\\n95.53%\\n\\n\\nRecall\\n0%\\n91.95%\\n\\n\\n\\nWe have significantly outperformed the baseline model in the three metrics (accuracy, balanced accuracy, and recall) we were focused on! With EvalML, we were able to build a model that is able to detect spam fairly well with just a few lines of code, and even before doing any tuning of the binary classification decision threshold.\\n\\xa0\\nThe Importance of Text\\n\\xa0\\nWe previously discussed that Woodwork had automatically detected that our\\xa0Messages\\xa0column was a natural language feature. We now understand that\\xa0AutoMLSearch\\xa0was able to create a\\xa0Text Featurization Component\\xa0because it identified this natural language column. To explain why this was useful, we can manually set our\\xa0Messages\\xa0feature as a categorical feature, run the same steps, and compare our scores.\\n\\nfrom evalml.utils import infer_feature_types\\r\\n\\r\\n# manually set \"Message\" feature as categorical \\r\\nX = infer_feature_types(X, {\\'Message\\': \\'Categorical\\'}) \\r\\n\\r\\nX_train, X_holdout, y_train, y_holdout = split_data(X, y, problem_type=\\'binary\\', test_size=0.2, random_seed=0)\\r\\n\\r\\nautoml_no_text = AutoMLSearch(X_train=X_train, y_train=y_train,                              problem_type=\\'binary\\')\\r\\nautoml_no_text.search()\\n\\n\\nIf we score the best pipeline found this time, we get an accuracy score of 75.2%, a balanced accuracy score of 50.3%, and a recall score of 0.6%. These scores are only marginally better than the scores for our baseline model!\\n\\n>>> best_pipeline_no_text = automl_no_text.best_pipeline\\r\\n>>> scores = best_pipeline_no_text.score(X_holdout, y_holdout,\\r\\nobjectives=evalml.objectives.get_core_objectives(\\'binary\\') + [\\'recall\\'])\\r\\n>>> scores\\r\\n\\r\\nOrderedDict([(\\'MCC Binary\\', 0.0710465299061946),\\r\\n\\t\\t\\t (\\'Log Loss Binary\\', 0.5576891229036224),\\r\\n\\t\\t\\t (\\'AUC\\', 0.5066740407467751),\\r\\n             (\\'Precision\\', 1.0),\\r\\n             (\\'F1\\', 0.013333333333333332),\\r\\n             (\\'Balanced Accuracy Binary\\', 0.5033557046979866),\\r\\n             (\\'Accuracy Binary\\', 0.7525083612040134),\\r\\n             (\\'Recall\\', 0.006711409395973154)])\\n\\n\\nThe scores for our best pipeline here are not much better than our baseline scores\\n\\xa0\\n\\n\\n\\nBaseline model (always guesses majority class)\\nPipeline with Text Featurization Component\\nPipeline without Text Featurization Component\\n\\n\\nAccuracy\\n75%\\n97.32%\\n75.25%\\n\\n\\nBalanced Accuracy\\n50%\\n95.53%\\n50.34%\\n\\n\\nRecall\\n0%\\n91.95%\\n0.67%\\n\\n\\n\\nThis means that unlike the previous best model found, this model is not much better than the trivial baseline model, and is no better than always guessing the majority “ham” class. By observing the components that make up this pipeline, we can better understand why.\\n\\nautoml_no_text.graph(best_pipeline_no_text)\\n\\n\\nGraph of our best pipeline if we treat \"Message\" as a categorical feature\\n\\n\\xa0\\nBecause\\xa0AutoMLSearch\\xa0was told to treat “Message” as a categorical feature this time, each pipeline included a one-hot encoder (rather than a text featurization component). The one-hot encoder encoded the top 10 most frequent “categories” of these texts; however, because each text is unique, this means that 10 unique text messages were encoded while the rest of the messages were dropped. Doing this removed almost all of the information from our data, so our best pipeline could not do much better than our trivial baseline model.\\n\\xa0\\nWhat’s Next?\\n\\xa0\\nIn this post, we covered how EvalML can be used to classify text messages as spam or ham (non-spam), and we learned how EvalML can detect and automatically handle text features with the help of Woodwork and the nlp-primitives library. You can learn more about Woodwork and nlp-primitives through their documentation, linked in the resources below. Finally, be sure to check out a\\xa0blog post\\xa0our former intern Clara Duffy wrote to learn more about nlp-primitives.\\nSpecial thanks to Becca McBrayer for writing\\xa0the demo\\xa0which this blog post is based on!\\n\\xa0\\nMore Resources\\n\\xa0\\n\\nUsing Text Data in EvalML demo\\nBlog post about nlp-primitives\\nnlp-primitives GitHub repo\\nWoodwork documentation\\n\\n\\xa0\\nBio: Angela Lin is a software engineer on the team building the open-source EvalML automated machine learning package in Python.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nGetting Started with 5 Essential Natural Language Processing Libraries\\nNatural Language Processing Pipelines, Explained\\nHow to Clean Text Data at the Command Line',\n",
       " 'By Gaurav Menghani, Software Engineer at Google AI.\\ncomments\\n\\nIn Part 1, we discussed why efficiency is important for deep learning models to achieve high-performance models that are pareto-optimal. Let us further dive deeper into the tools and techniques for achieving efficiency.\\n\\xa0\\nFocus Areas of Efficiency in Deep Learning\\n\\xa0\\nWe can think of the work on efficiency to be categorized in roughly four pillars of modelling techniques and a foundation of infrastructure and hardware.\\n\\nFocus Areas of Efficient Deep Learning.\\n\\nCompression Techniques: These are general techniques and algorithms that look at optimizing the architecture itself, typically by compressing its layers. Often, these approaches are generic enough to be used across architectures. A classic example is quantization [1,2], which tries to compress the weight matrix of a layer by reducing its precision (e.g., from 32-bit floating-point values to 8-bit unsigned integers). Quantization can generally be applied to any network which has a weight matrix.\\nLearning Techniques: These are algorithms that focus on training the model differently so as to make fewer prediction errors. Improved accuracy can then be exchanged for a smaller footprint or a more efficient model by trimming the number of parameters if needed. An example of a learning technique is Distillation [3], which, as mentioned earlier, helps a smaller model learn from a larger, more accurate model.\\nAutomation: These are tools for automatically improving the core metrics of the given model using automation. An example is a hyper-parameter search [4], where the model architecture remains the same, but optimizing the hyper-parameters helps increase the accuracy, which could then be exchanged for a model with fewer parameters. Similarly, architecture search [5,6] falls in this category, where the architecture itself is tuned, and the search helps find a model that optimizes both the loss/accuracy and some other objective function. An example of a secondary objective function could be the model latency/size, etc.\\nEfficient Model Architectures & Layers: These form the crux of efficiency in deep learning and are the fundamental blocks that were designed from scratch (Convolutional Layers, Attention, etc.), which are a significant leap over the baseline methods used before them. As an example, convolutional layers introduce parameter sharing and filters for use in image models, which avoids having to learn separate weights for each input pixel. This clearly saves the number of parameters when you compare it to a standard multi-layer perceptron (MLP) network. Avoiding over-parameterization further helps in making the networks more robust. In this pillar, we would look at layers and architectures that have been designed specifically with efficiency in mind.\\n\\n\\xa0\\nInfrastructure & Hardware\\n\\xa0\\nFinally, we also need a foundation of infrastructure and tools that help us build and leverage efficient models. This includes the model training framework, such as Tensorflow, PyTorch, etc., as introduced earlier. Often these frameworks will be paired with the tools required specifically for deploying efficient models. For example, Tensorflow has tight integration with Tensorflow Lite (TFLite) [7] and related libraries, which allow exporting and running models on mobile devices. Similarly, TFLite Micro [8] helps in running these models on DSPs. Just like Tensorflow, PyTorch also offers PyTorch Mobile for quantizing and exporting models for inference on mobile and embedded devices.\\nWe often depend on this infrastructure and tooling to leverage the gains from efficient models. For example, for obtaining both size and latency improvements with quantized models, we need the inference platform to support common neural net layers in quantized mode. TFLite supports quantized models by allowing the export of models with 8-bit unsigned int weights and having integration with libraries like GEMMLOWP [8] and XNNPACK [9] for fast inference. Similarly, PyTorch uses QNNPACK [10] to support quantized operations.\\nOn the hardware front, we rely on devices like CPUs, GPUs, and Tensor Processing Units (TPUs) [11] to allow us to train and deploy these models. On the mobile and embedded front, we have ARM-based processors, mobile GPUs, and other accelerators [12] that let us leverage efficiency gains for deployment (inference).\\nIn our next part, we will go over examples of tools and techniques that fit in each of these pillars. Also, feel free to go over our survey paper that explores this topic in detail.\\n\\xa0\\nReferences\\n[1] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.\\n[2] Raghuraman Krishnamoorthi. 2018. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv (Jun 2018). arXiv:1806.08342\\n[3] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015)\\n[4] Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley. 2017. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. 1487–1495.\\n[5] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016).\\n[6] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2820–2828.\\n[7] ML for Mobile and Edge Devices. https://www.tensorflow.org/lite\\n[8] GEMMLOWP. - https://github.com/google/gemmlowp\\n[9] XNNPACK. - https://github.com/google/XNNPACK\\n[10] Marat Dukhan, Yiming Wu Wu, and Hao Lu. 2020. QNNPACK: Open source library for optimized mobile deep learning - Facebook Engineering. https://engineering.fb.com/2018/10/29/ml-applications/qnnpack\\n[11] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture. 1–12.\\n[12] EdgeTPU. https://cloud.google.com/edge-tpu\\n\\xa0\\nBio: Gaurav Menghani\\xa0(@GauravML)\\xa0is a Staff Software Engineer at Google Research, where he leads research projects geared towards optimizing large machine learning models for efficient training and inference on devices ranging from tiny microcontrollers to Tensor Processing Unit (TPU)-based servers. His work has positively impacted > 1 Billion active users across YouTube, Cloud, Ads, Chrome, etc. He is also an author of an upcoming book with Manning Publication on Efficient Machine Learning. Before Google, Gaurav worked at Facebook for 4.5 years and has contributed significantly to Facebook’s Search system and large-scale distributed databases. He has an M.S. in Computer Science from Stony Brook University.\\nRelated:\\n\\nHigh Performance Deep Learning, Part 1\\n5 Challenges to Scaling Machine Learning Models\\nScaling a Massive State-of-the-art Deep Learning Model in Production',\n",
       " 'By Matthew Mayo, KDnuggets.\\ncomments\\nKDnuggets recently brought you the Top YouTube Channels for Data Science, employing a qualitative approach to identifying those channels of value on the platform. As the endeavour seemed to be be useful to some of our readers, we have repeated the exercise, this time bringing you the top machine learning channels that YouTube has to offer.\\nFor this iteration we changed up our metric for determining the \"top\" channels. We have maintained our quantitative approach, but tweaked the specifics. (Also, we fully recognize that the act of creating a criteria is a form of subjectivity, but realistically some decisions need to be made.) This time around, determining which channels ended up on our list began with this YouTube search criteria:\\n\\nsearch term: \"machine learning\"\\nsearch type: channel\\nsort by: relevance\\n\\nThe results to this search were gathered on March 21, 2021, and appeared at this URL at the time.\\nThe top 100 results were scraped. After this, the following bit of data processing was applied:\\n\\nremove channels with <100K views\\nremove channels without updates for past 12 months\\nsort channels by videos/views\\n\\nWe had planned to take the top X form of the resulting list to include in the post. However, one channel was removed in an act of subjectivity given some of the channel creator\\'s recent controversies. While we wish the individual well, we could not in good faith include their content in our recommendations list.\\nFinally, we cut the results off at top 15, which is what are listed below, and what are included in the following picture-painting visualization.\\n\\nFigure 1. Top YouTube Channels for Machine Learning\\nPlotted by number of views and number of subscribers; relative size by number of videos;\\ncolor intensity by number of views / number of subscribers\\n\\xa0\\nAnd so here are the top 15 YouTube channels for machine learning by number of views / number of videos (or views per video), in order, with short descriptions directly from the channels themselves (where available).\\n\\xa0\\n1. sentdex\\nViews/Video: 76 K Subscribers: 1020 K, Videos: 1212, Views: 92 M, Views/Subscriber: 90\\n\\n“Python Programming tutorials, going further than just the basics. Learn about machine learning, finance, data analysis, robotics, web development, game development and more.”\\n\\n\\xa0\\n2. codebasics\\nViews/Video: 44 K, Subscribers: 271 K, Videos: 365, Views: 16 M, Views/Subscriber: 59\\n\\n“The goal of this channel is to fulfill this vision by teaching the programming in most simplest and intuitive manner. I teach simple programming, data science, data analytics, artificial intelligence, machine learning, data structures, software architecture etc on my channel.”\\n\\n\\xa0\\n3. DeepLearningAI\\nViews/Video: 42 K, Subscribers: 130 K, Videos: 205, Views: 8.7 M, Views/Subscriber: 67\\n\\n“Welcome to the official DeepLearning.AI YouTube channel! Here you can find the videos from our Coursera programs on machine learning as well as recorded events. DeepLearning.AI has created high-quality AI programs on Coursera that have gained an extensive global following. By providing a platform for education and fostering a tight-knit community, DeepLearning.AI has become the pathway for anyone looking to build an AI career.”\\n\\n\\xa0\\n4. deeplizard\\nViews/Video: 25 K, Subscribers: 93.8 K, Videos: 289, Views: 7.1 M, Views/Subscriber: 76\\n\\n“Building collective intelligence.”\\n\\n\\xa0\\n5. Krish Naik\\nViews/Video: 24 K, Subscribers: 334 K, Videos: 921, Views: 22 M, Views/Subscriber: 65\\n\\n“This is my YouTube channel where I explain various topics on machine learning, deep learning, and AI with many real-world problem scenarios. I have delivered over 30 tech talks on data science, machine learning, and AI at various meet-ups, technical institutions, and community-arranged forums. My main aim is to make everyone familiar of ML and AI”\\n\\n\\xa0\\n6. Kilian Weinberger\\nViews/Video: 17 K, Subscribers: 11.9 K, Videos: 39, Views: .65 M, Views/Subscriber: 54\\n\\n“Kilian has absolutely no channel description, but the content consists of machine leanring lectures from Cornell University, where Kilian is an Associate Professor of Computer Science.”\\n\\n\\xa0\\n7. Machine Learning\\nViews/Video: 14 K, Subscribers: 1.39 K, Videos: 20, Views: .28 M, Views/Subscriber: 199\\n\\n“Watch Industry Experts thoughts, Earn Free Cloud Credits, Session on Operating Systems, Modern day Technology Tutorials, Valuable sessions on IAAS, PAAS, SAAS, Hybrid cloud strategy, Get replays and Live feeds from Meetups, Conferences and much more.“\\n\\n\\xa0\\n8. Daniel Bourke\\nViews/Video: 14 K, Subscribers: 79.4 K, Videos: 270, Views: 3.7 M, Views/Subscriber: 46\\n\\n“I\\'m a machine learning engineer who plays at the intersection of technology and health. My videos will help you learn better and live healthier.”\\n\\n\\xa0\\n9. Hsuan-Tien Lin\\nViews/Video: 12 K, Subscribers: 21.8 K, Videos: 195, Views: 2.3 M, Views/Subscriber: 105\\n\\n“Hsuan-Tien Lin has no channel description, but has videos on Machine Learning for Modern Artificial Intelligence, Data Structures and Algorithms, Machine Learning Foundations/Techniques, and more. His channel has only been posting videos for ~2 months at the time of this article’s publication, and they are recorded in a mix of English and Mandarin (I believe).”\\n\\n\\xa0\\n10. Python Engineer\\nViews/Video: 11 K, Subscribers: 28 K, Videos: 121, Views: 1.3 M, Views/Subscriber: 46\\n\\n“Hi, I\\'m Patrick. I’m a passionate Software Engineer who loves Machine Learning, Computer Vision, and Data Science. I create free content in order to help more people get into those fields. If you have any questions, feedback, or comments, just shoot me a message! I am happy to talk to you :)”\\n\\n\\xa0\\n11. Data Science Courses\\nViews/Video: 10 K, Subscribers: 14.5 K, Videos: 81, Views: .85 M, Views/Subscriber: 59\\n\\n“No description, but Ali Ghodsi is a professor at Waterloo, and also a member of the university’s Artificial Intelligence Research Group. The channel includes lecture videos.”\\n\\n\\xa0\\n12. Abhishek Thakur\\nViews/Video: 9.4 K, Subscribers: 42.8 K, Videos: 85, Views: .8 M, Views/Subscriber: 19\\n\\n“I make videos about applied machine learning, deep learning and data science. I am the world\\'s first 4x grandmaster on Kaggle.”\\n\\n\\xa0\\n13. Jeff Heaton\\nViews/Video: 9.1 K, Subscribers: 48.3 K, Videos: 411, Views: 3.7 M, Views/Subscriber: 78\\n\\n“Would you like to learn about deep neural networks and other areas of my machine learning research that has allowed me to score in the top 7-10% of some Kaggle competitions?  If so, please subscribe to my channel!  My name is Jeff Heaton, Ph.D.  I am a VP of data science for a Fortune 300 company and I teach a deep learning course as an adjunct instructor for a top university.”\\n\\n\\xa0\\n14. Subalalitha C N\\nViews/Video: 8.8 K, Subscribers: 2.87 K, Videos: 40, Views: .35 M, Views/Subscriber: 123\\n\\n“I am Dr.Subalalitha C.N, working as Associate Professor in SRM Institute of Science and Technology, India.   In this channel, you can find my lectures on Machine Learning, Natural Language Processing and Design and Analysis of Algorithms.”\\n\\n\\xa0\\n15. Machine Learning TV\\nViews/Video: 8.0 K, Subscribers: 23.1 K, Videos: 126, Views: 1.0 M, Views/Subscriber: 43\\n\\n“This channel is all about machine learning (ML). It contains all the useful resources which help ML lovers and computer science students gain a better understanding of the concepts of this successful branch of Artificial Intelligence.”\\n\\n\\xa0\\nAnd these are the top 15 YouTube channels for machine learning by views per video, along with some additional data to help you decide whether these channels may have video content which may be of interest to you. Good luck with your viewing!\\n\\xa0\\nRelated:\\n\\nTop YouTube Channels for Data Science\\nBest Machine Learning Youtube Videos Under 10 Minutes\\nTop Python Libraries for Data Science, Data Visualization & Machine Learning',\n",
       " \"By Matthew Mayo, KDnuggets.\\ncomments\\nIt's been a while since we shared a free eBook with our readers, but this week we have come across another worthy entrant in the series, and wanted to share it in time for the holiday learning season (which is most definitely a thing).\\nToday we share Data Science and Machine Learning: Mathematical and Statistical Methods, by D.P. Kroese, Z.I. Botev, T. Taimre & R. Vaisman. The book was published last year, and aside from being freely-available as a PDF can also be purchased in print form (and Kindle).\\n\\n\\xa0\\nData Science and Machine Learning: Mathematical and Statistical Methods is a practically-oriented text, with a focus on doing data science and implementing machine learning models using Python. It does a good job of explaining relevant theory and introducing the necessary math as needed, which results in very nice pacing for a practical book.\\nThe book's raison d'être, according to its website, is actually somewhat at odds with my take:\\n\\nThe purpose of this book is to provide an accessible, yet comprehensive textbook intended for students interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science.\\n\\n\\xa0\\nI believe this is the opposite side of the same coin: where I see this book's strength as teaching the practical and reinforcing it with the necessary theory and underlying math, the argument can clearly be made that it focuses on the theory and underlying math and reinforces this with practical implementation.\\nEven money, I'd say.\\nRegardless of the approach you endorse, the book's table of contents are as follows:\\n\\nImporting, Summarizing, and Visualizing Data\\nStatistical Learning\\nMonte Carlo Methods\\nUnsupervised Learning\\nRegression\\nRegularization and Kernel Methods\\nClassification\\nDecision Trees and Ensemble Methods\\nDeep Learning\\n\\nLots of relevant topics are covered here, and in logical succession. I particularly like the transition from Monte Carlo methods to unsupervised learning, and how that happens prior to the introduction of supervised concepts. Classification, though likely more useful in the long run (at least seemingly so at present) seemed far less impactful than did clustering when I first encountered machine learning, and so in my view its introduction prior may prove equally captivating for other new learners.\\nTo ensure the book is self-contained for even the newest of data science and machine learning students, the book includes the adequate and useful appendices of:\\n\\nLinear Algebra and Functional Analysis\\nMultivariate Differentiation and Optimization\\nProbability and Statistics\\nPython Primer\\n\\nYou won't become a complete data science expert by reading this book, but that's not its goal. By working through Data Science and Machine Learning: Mathematical and Statistical Methods you will get a solid foundation in the basics of the field, upon which more cutting edge methods and algorithms can be added.\\nOne of my favorite machine learning books that I used as my first foray into learning the subject matter was Data Mining: Practical Machine Learning Tools and Techniques, also known as the Weka book. I really liked as a newcomer how it mixed practical and theoretical, introducing and explaining the math as needed in order to learn the practical implementation being presented at the time. I find that this book is reminiscent of that format, with the advantage of using Python instead of the Weka toolkit which, at least today, is a much more relevant implementation pathway. \\nI recommend this book to anyone learning the basics of data science and machine learning, and looking to do so in the presentation format described.\\n\\xa0\\nRelated:\\n\\nUnderstanding Machine Learning: The Free eBook\\nAn Introduction to Statistical Learning: The Free eBook\\nData Mining and Machine Learning: Fundamental Concepts and Algorithms: The Free eBook\",\n",
       " 'comments\\nBy Jason Ganz, Special Adviser, Data for Progress.\\n\\n\\xa0\\nAnalytics Engineering — An Introduction\\n\\xa0\\nThere’s a quiet revolution happening in the world of data. For years we have been\\xa0blasted with nonstop articles about “The Sexiest Job of the 21st Century” — a data scientist. A data scientist, we have been taught, is a figure of almost otherworldly intelligence who uses quasi-mystical arts to perform feats of data wizardly. But these days, if you talk to the people who watch the data space most closely — there’s a different data role that has them even more excited.\\nTo be clear, there are some very real and very cool applications of data science that can allow organizations to do things with data that can completely transform how their organization operates. But for many orgs, particularly smaller organizations without millions of dollars to invest, data science initiatives tend to fall flat because of the lack of a solid data infrastructure to support them.\\nWhile everyone was focused on the rise of data science, another discipline has been quietly taking shape, driven not by glitzy articles in Harvard Business Review but by the people working in the trenches in data-intensive roles. They call it the\\xa0analytics engineer.\\nAn analytics engineer is someone who brings together the data-savvy and domain knowledge of an analyst with software engineering tooling and best practices. Day to day, that means spending in a suite of tools that is becoming known as “The Modern Data Stack” and particularly dbt. These tools allow analytics engineers to centralize data and then model it for analysis in a way that is\\xa0remarkably\\xa0cheap and easy compared to how the ETL of traditional Business Intelligence teams of the past operated.\\nWhile data scientists are seen by some as wizardry, the attitude of the analytics engineer is a little different. You’ll hear them refer to themselves as everything from “humble data plumbers’’ to “just a pissed off data analyst.” The work of an Analytics Engineer seems easy to understand, almost banal. They combine data sources, apply logic, make sure there are clean and well-modeled materializations to analyze.\\nIt turns out analytics engineering is a\\xa0goddamn superpower.\\xa0Anyone that has worked in, well, basically any organization knows that a tremendous amount of effort goes into standardizing data points that feel like they should be a no-brainer to pull, while more complex questions just sit unanswered for years. Analytics Engineering allows you to have data systems that\\xa0just work.\\nA good analytics engineer is hugely impactful for an org, with each analytics engineer being able to help build a truly data-driven culture in ways that would be challenging for a team of people using legacy tools. While in the past there was tremendous repetitive work to do any simple analysis, Analytics Engineers can build complex data models using tools like dbt and have analysis-ready data tables built on any schedule. While before it was impossible to get anyone to agree on standard definitions of metrics, Analytics Engineers can simply build them into their codebase. And in the past, people struggled with incomplete and messy data, and Analytics Engineers… still struggle with incomplete and messy data. But at least we can have a suite of tests on our analytics systems to know when something has gone wrong!\\n\\xa0\\nThe Rise of Analytics Engineering\\n\\xa0\\nYou might think that this development would be scary for people working in data — if one analytics engineer is substantially more impactful than a data analyst, won’t our jobs be at risk? Could an org replace five data analysts with one Analytics Engineer and come out ahead?\\nBut the fact of the matter is that no data analyst, anywhere, has ever come close to performing all of the analysis they think could be impactful at their organization — the opposite is far more likely to be the problem. Most data orgs are begging for additional headcount.\\nAs analytics engineers increase the amount of insight organizations can find from data, it actually becomes\\xa0more\\xa0likely that these orgs will want to hire additional data talent (both analytics engineers and analysts). In his fantastic post\\xa0The Reorganization of the Factory, Erik Bernhardsson makes the case that as the toolsets for software engineers has become ever more efficient, the demand for software engineers has counterintuitively\\xa0grown\\xa0— as there are more and more use cases where it now makes sense to build software rather than a manual process. This point not only holds for data, but I think it actually is\\xa0more true\\xa0for data.\\nWhile every organization needs software, not every organization needs\\xa0software engineers.\\xa0But every organization needs to learn from their data, and\\xa0since the ways in which the data needs to be understood will be unique at every organization,\\xa0they will all need analytics engineers.\\xa0Software is commonly said to be eating the world — analytics engineering will be embedded in the world. As the incremental value of each data hire rises, there are substantial new areas where data insights and learnings could be applied that they aren’t today. And even if you aren’t interested in\\xa0becoming\\xa0an analytics engineer, having well modeled and accurate data makes data analysts and data scientists more effective. It’s a win across the board.\\nThat does not necessarily mean that every analytics engineering role will be doing good for the world. Having more powerful data operations allows you to question, seek insights, and look for new strategies. It can also allow an organization new ways to monitor their employees, surveil, or discriminate. One needs only look at the myriad of public issues in the tech and data science industries right now to see the ways that powerful tech can be misused. It is important to recognize the potential dangers as well as the new opportunities.\\nIf it feels like we’re at a real inflection point for Analytics Engineering — it’s because we are. What was very recently the domain of a few adventurous data teams is quickly becoming industry standard for tech organizations — and there’s every reason to think that other types of organizations will be following along shortly. The impact is just too high.\\nWe’re about to see a huge expansion in the number of and types of places where you can find employment as an analytics engineer. The coming boom in opportunities for analytics engineers will take place across three rough domains, with each having different challenges and opportunities.\\n\\nMore and more large enterprises, both tech and non-tech organizations, are going to adapt to the modern data stack. As analytics engineering is brought into the most complex legacy data systems, we’ll begin to see what patterns develop to support analytics engineering at scale. If you are interested in really figuring out what the large-scale data systems of the future look like, this will be the place to go.\\nJust about every new company is going to be searching for an analytics engineer to lead their data initiatives. This will give them a step up against any competition that isn’t investing in their core data. Being an early analytics engineer at a fast-growing company is tremendously fun and exciting, as you are able to build up a data organization from scratch and see firsthand how analytics engineering can change the trajectory of an organization.\\nFinally, many organizations outside the tech business world are going to begin seeing the impact that analytics engineering can bring. You might not have quite the same tech budget, and you might have to learn to advocate for yourself a little more but it might be the area where analytics engineering has the most potential to do good for the world. City governments will use analytics engineering to monitor programs and ensure that government resources are being used effectively. Academic institutions will use analytics engineering to create datasets, many of them public, that will aid in scientific and technological development. The possibility space is wide open.\\n\\nAnalytics engineering is fundamentally a discipline that’s about making sense of the world around us. It’s about allowing everyone in an organization to see a little bit further in their impact on the org and how their work connects to it. Right now, analytics engineering is still a new discipline — pretty soon, it will be everywhere.\\n\\xa0\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nWhy You Should Consider Being a Data Engineer Instead of a Data Scientist\\n9 Skills You Need to Become a Data Engineer\\nData Engineering — the Cousin of Data Science, is Troublesome',\n",
       " 'By Matthew Mayo, KDnuggets.\\ncomments\\n\\nPhoto by Hitesh Choudhary on Unsplash\\xa0\\nChoosing a structure for storing your data is an important part of solving programming tasks and implementing solutions, yet it is often not given the attention a choice of such potential weight deserves. Unfortunately, in Python I often see the list used as a catch-all data structure. The list has its advantages, of course, but also its drawbacks. There are lots of other data structure options as well.\\nLet\\'s take a look at 5 different Python data structures and see how they could be used to store data we might be processing in our everyday tasks, as well as the relative memory they use for storage and time they take to create and access.\\n\\xa0\\nTypes of Data Structures\\nFirst, let\\'s lay out the 5 data structures we will consider herein, and provide some preliminary insight.\\nClass\\nIn this case we are talking about vanilla classes (as opposed to data classes below), which are described at a high level in Python documentation as follows:\\nClasses provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state.\\n\\xa0\\nThe advantages of using classes is that they are conventional, and are well-used and -understood. Whether or not they are overkill in terms of relative required memory or time is something to look at.\\nData Class\\nAdded in Python 3.7, the data class is a special class meant for mainly holding data, which comes with some freebie methods out of the box for typical functionality like instantiating and printing instance contents. Creating a data class is accomplished using the @dataclass decorator.\\nAlthough they use a very different mechanism, Data Classes can be thought of as \"mutable namedtuples with defaults\". Because Data Classes use normal class definition syntax, you are free to use inheritance, metaclasses, docstrings, user-defined methods, class factories, and other Python class features. Such a class is called a Data Class, but there\\'s really nothing special about the class: the decorator adds generated methods to the class and returns the same class it was given.\\n\\xa0\\nAs you can see, the automatically generated methods and the related time-savings are the main reason to consider data classes.\\nNamed Tuple\\nNamed tuples are an elegant implementation of a useful data structure, essentially tuple subclasses with named fields.\\nNamed tuples assign meaning to each position in a tuple and allow for more readable, self-documenting code. They can be used wherever regular tuples are used, and they add the ability to access fields by name instead of position index.\\n\\xa0\\nAt first look, named tuples appear to be the closest thing to simple C-like struct types natively available in Python, making them naively attractive to many.\\nDictionary\\nThe Python dictionary is a collection of key-value pairs.\\nPython Dictionaries are mutable unordered collections (they do not record element position or order of insertion) of key-value pairs. Keys within the dictionary must be unique and must be hashable. That includes types like numbers, strings and tuples. Lists and dicts can not be used as keys since they are mutable.\\n\\xa0\\nThe advantage of dictionaries is that they are simple, the data within are easily accessible, and they are well-used and -understood.\\nList\\nHere it is, the one-size-fits-all Python data superstructure, or so lots of code would have you believe. Here is what the list really is:\\nLists are mutable ordered and indexed collections of objects. The items of a list are arbitrary Python objects. Lists are formed by placing a comma-separated list of expressions in square brackets.\\n\\xa0\\nWhy the widespread use of the list? It\\'s very simple to understand and implement, and is usually the first structure one learns when picking up Python. Are there disadvantages related to speed and memory usage? Let\\'s take a look.\\n\\xa0\\nImplementations\\nFirst off, let\\'s have a look at the creation process of each of these structures and how they compare to one another.\\nThe reason we might be using any of these data structures to store our data would vary widely, but for the unimaginative, let\\'s imagine we are extracting data from a SQL database and need to store each record in one such structure in order to perform some processing prior to moving the data further along our pipeline.\\nWith that in mind, here is instantiation code for creating each of the five structures.\\n\\n\"\"\" class \"\"\"\\r\\nclass CustomerClass:\\r\\n    def __init__(self, cust_num:str, f_name:str, l_name:str, address:str,\\r\\n                       city:str, state:str, phone:str, age:int):\\r\\n        self.cust_num = cust_num\\r\\n        self.f_name = f_name\\r\\n        self.l_name = l_name\\r\\n        self.address = address\\r\\n        self.city = city\\r\\n        self.state = state\\r\\n        self.phone = phone\\r\\n        self.age = age\\r\\n\\r\\n    def to_string(self):\\r\\n        return(f\\'{self.cust_num}, {self.f_name}, {self.l_name}, {self.age}, \\r\\n                 {self.address}, {self.city}, {self.state}, {self.phone}\\'stgrcutures)\\r\\n\\r\\n\"\"\" data class \"\"\"\\r\\nfrom dataclasses import dataclass\\r\\n@dataclass\\r\\nclass CustomerDataClass:\\r\\n    cust_num: int\\r\\n    f_name: str\\r\\n    l_name: str\\r\\n    address: str\\r\\n    city: str\\r\\n    state: str\\r\\n    phone: str\\r\\n    age: int\\r\\n\\r\\n\"\"\" named tuple \"\"\"\\r\\nfrom collections import namedtuple\\r\\nCustomerNamedTuple = namedtuple(\\'CustomerNamedTuple\\', \\r\\n                                \\'cust_num f_name l_name address city state phone age\\')\\r\\n\\r\\n\"\"\" dict \"\"\"\\r\\ndef make_customer_dict(cust_num: int, f_name: str, l_name: str, address: str, \\r\\n                       city: str, state: str, phone: str, age: int):\\r\\n    return {\\'cust_num\\': cust_num,\\r\\n            \\'f_name\\': f_name,\\r\\n            \\'l_name\\': l_name,\\r\\n            \\'address\\': address,\\r\\n            \\'city\\': city,\\r\\n            \\'state\\': state,\\r\\n            \\'phone\\': phone,\\r\\n            \\'age\\': age}\\r\\n\\r\\n\"\"\" list \"\"\"\\r\\ndef make_customer_list(cust_num: int, f_name: str, l_name: str, address: str, \\r\\n                       city: str, state: str, phone: str, age: int):\\r\\n    return [cust_num, f_name, l_name, address,\\r\\n            city, state, phone, age]\\n\\n\\xa0\\nNote the following:\\n\\nThe creation of an instance of the built-in types dictionary and list have been place inside functions\\nThe difference between the class and the data class implementations, in light of the discussion above\\nThe (clearly subjective) elegance and simplicity of the named tuple\\n\\nLet\\'s have a look at instantiation of these structures, and a comparison of the resources required to do so.\\n\\xa0\\nTesting and Results\\nWe will create a single instance of each of the 5 structures, each housing a single data record. We will repeat this process using the same data fields for each structure 1,000,000 times to get a better sense of average time, performing this process on my modest Dell notebook, using an Ubuntu-derived operating system.\\n\\nCompare the code between the 5 structure instantiations below.\\n\\n\"\"\" instantiating structures \"\"\"\\r\\n\\r\\nfrom sys import getsizeof\\r\\nimport time\\r\\n\\r\\n# class\\r\\ncustomer_1 = CustomerClass(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nprint(f\\'Data: {customer_1.to_string()}\\')\\r\\nprint(f\\'Type: {type(customer_1)}\\')\\r\\nprint(f\\'Size: {getsizeof(customer_1)} bytes\\')\\r\\n\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    customer = CustomerClass(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nt1 = time.time()\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\r\\n\\r\\n# data class\\r\\ncustomer_2 = CustomerDataClass(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nprint(f\\'Data: {customer_2}\\')\\r\\nprint(f\\'Type: {type(customer_2)}\\')\\r\\nprint(f\\'Size: {getsizeof(customer_2)} bytes\\')\\r\\n\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    customer = CustomerDataClass(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nt1 = time.time()\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\r\\n\\r\\n# named tuple\\r\\ncustomer_3 = CustomerNamedTuple(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nprint(f\\'Data: {customer_3}\\')\\r\\nprint(f\\'Type: {type(customer_3)}\\')\\r\\nprint(f\\'Size: {getsizeof(customer_3)} bytes\\')\\r\\n\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    customer = CustomerNamedTuple(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nt1 = time.time()\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\r\\n\\r\\n# dict\\r\\ncustomer_4 = make_customer_dict(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nprint(f\\'Data: {customer_4}\\')\\r\\nprint(f\\'Type: {type(customer_4)}\\')\\r\\nprint(f\\'Size: {getsizeof(customer_4)} bytes\\')\\r\\n\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    customer = make_customer_dict(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nt1 = time.time()\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\r\\n\\r\\n# list\\r\\ncustomer_5 = make_customer_list(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nprint(f\\'Data: {customer_5}\\')\\r\\nprint(f\\'Type: {type(customer_5)}\\')\\r\\nprint(f\\'Size: {getsizeof(customer_5)} bytes\\')\\r\\n\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    customer = make_customer_list(\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56)\\r\\nt1 = time.time()\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\n\\n\\xa0\\nAnd here is the output of the above:\\n\\nData: EP90210, Edward, Perez, 56, 123 Fake Street, Cityville, TX, 888-456-1234\\r\\nType: <class \\'__main__.CustomerClass\\'>\\r\\nSize: 56 bytes\\r\\nTime: 0.657s\\r\\n\\r\\nData: CustomerDataClass(cust_num=\\'EP90210\\', f_name=\\'Edward\\', l_name=\\'Perez\\', address=\\'123 Fake Street\\', city=\\'Cityville\\', state=\\'TX\\', phone=\\'888-456-1234\\', age=56)\\r\\nType: <class \\'__main__.CustomerDataClass\\'>\\r\\nSize: 56 bytes\\r\\nTime: 0.630s\\r\\n\\r\\nData: CustomerNamedTuple(cust_num=\\'EP90210\\', f_name=\\'Edward\\', l_name=\\'Perez\\', address=\\'123 Fake Street\\', city=\\'Cityville\\', state=\\'TX\\', phone=\\'888-456-1234\\', age=56)\\r\\nType: <class \\'__main__.CustomerNamedTuple\\'>\\r\\nSize: 112 bytes\\r\\nTime: 0.447s\\r\\n\\r\\nData: {\\'cust_num\\': \\'EP90210\\', \\'f_name\\': \\'Edward\\', \\'l_name\\': \\'Perez\\', \\'address\\': \\'123 Fake Street\\', \\'city\\': \\'Cityville\\', \\'state\\': \\'TX\\', \\'phone\\': \\'888-456-1234\\', \\'age\\': 56}\\r\\nType: <class \\'dict\\'>\\r\\nSize: 368 bytes\\r\\nTime: 0.318s\\r\\n\\r\\nData: [\\'EP90210\\', \\'Edward\\', \\'Perez\\', \\'123 Fake Street\\', \\'Cityville\\', \\'TX\\', \\'888-456-1234\\', 56]\\r\\nType: <class \\'list\\'>\\r\\nSize: 128 bytes\\r\\nTime: 0.184s\\n\\n\\xa0\\nFinally, another useful piece of data would be to know the relative access times of values stored within our structures (in the case below, the address). The same retrieval will be repeated 1,000,000 times, and the average time reported below.\\n\\n\"\"\" accessing an element \"\"\"\\r\\n\\r\\n# class\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    address = customer_1.address\\r\\nt1 = time.time()\\r\\nprint(f\\'Type: {type(customer_1)}\\')\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\r\\n\\r\\n# data class\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    address = customer_2.address\\r\\nt1 = time.time()\\r\\nprint(f\\'Type: {type(customer_2)}\\')\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\r\\n\\r\\n# named tuple\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    address = customer_3.address\\r\\nt1 = time.time()\\r\\nprint(f\\'Type: {type(customer_3)}\\')\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\r\\n\\r\\n# dictionary\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    address = customer_4[\\'address\\']\\r\\nt1 = time.time()\\r\\nprint(f\\'Type: {type(customer_4)}\\')\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\r\\n\\r\\n# list\\r\\nt0 = time.time()\\r\\nfor i in range(1000000):\\r\\n    address = customer_5[3]\\r\\nt1 = time.time()\\r\\nprint(f\\'Type: {type(customer_5)}\\')\\r\\nprint(\\'Time: {:.3f}s\\\\n\\'.format(t1-t0))\\n\\n\\xa0\\nAnd the output:\\n\\nType: <class \\'__main__.CustomerClass\\'>\\r\\nTime: 0.098s\\r\\n\\r\\nType: <class \\'__main__.CustomerDataClass\\'>\\r\\nTime: 0.092s\\r\\n\\r\\nType: <class \\'__main__.CustomerNamedTuple\\'>\\r\\nTime: 0.134s\\r\\n\\r\\nType: <class \\'dict\\'>\\r\\nTime: 0.095s\\r\\n\\r\\nType: <class \\'list\\'>\\r\\nTime: 0.117s\\r\\n\\n\\n\\xa0\\nThe intent of this article is not to make a recommendation one way or another as to which data structure to use, nor is it to suggest that there is a universal best structure for every case. Instead, we wanted to have a look at some different options and their relative strength and weakness. As with all things, there are trade-offs to be made, and less quantitative considerations such as understandability, ease of use, etc. are to be taken into account when making these types of decisions.\\nThat said, a few things do stand out from the above analysis:\\n\\nThe dictionary uses the greatest amount of storage of all the structures, in our case almost 3 times as much as the next greatest — though we should be careful about generalizing until we look at the effects of scaling and internal field data types\\nUnsurprisingly, the list is the fastest to instantiate, yet not the fastest from which to retrieve an element (it\\'s almost the slowest)\\nIn our case, the named tuple is the slowest structure from which to retrieve an element, yet is middle of the pack for storage space\\nBoth classes take relatively longer to instantiate (expected), but element retrieval and space used, in both cases, are very competitive with the other structures\\n\\nSo not only are we not looking to recommend a single structure in every case, there is no clear winner that could be recommended in every case. Even taking the caution to generalize based on our small experiment, it is clear that priorities will need to be taken into account for making a decision as to which structure you use for a particular job. At the very least, this limited experimentation has provided some small window of insight into the performance of data structures available in Python.\\nRelated:\\n\\nManaging Your Reusable Python Code as a Data Scientist\\n5 Python Data Processing Tips & Code Snippets\\nDate Processing and Feature Engineering in Python',\n",
       " \"Sponsored Post.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\tUsing ESG and sustainability data in the cloud to drive business value\\r\\n\\t\\t      \\n\\n\\n\\n\\n\\n\\nREGISTER NOW\\n\\n\\n\\n\\n\\n\\n\\n.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t  You're invited!\\r\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tT\\ufeffhursday, J\\ufeffuly 2\\ufeff9\\r\\n\\t\\t\\t\\t\\t      \\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\t1\\ufeff1A\\ufeffM P\\ufeffT | 2\\ufeff\\xa0P\\ufeffM\\xa0E\\ufeffT\\r\\n\\t\\t\\t\\t\\t      \\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\t60 MIN SESSION\\r\\n\\t\\t\\t\\t\\t      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nREGISTER NOW\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t      Join this webinar to discover how forward-thinking organizations are transforming environmental, social, and governance (ESG) data into actionable business intelligence.\\r\\n\\t\\t\\t    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\tIn this webinar:\\r\\n\\t\\t      \\n\\n\\n\\r\\n\\t\\t\\tIn this virtual session, our panel of experts will uncover how companies across several verticals use ESG data to move beyond the reporting benchmark, deepen business insights, and create competitive \\r\\n\\t\\t\\tdifferentiation.\\r\\n\\t\\t      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\tKey takeaways include:\\r\\n\\t\\t      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t      Leveraging third-party data and visualization tools to identify \\r\\n\\t\\t\\t      assets with environmental violations and climate risk exposures\\r\\n\\t\\t\\t    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t      Uncovering how financial institutions use ESG data to align with \\r\\n\\t\\t\\t      new integration and disclosure requirements\\r\\n\\t\\t\\t    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t      Modeling investment forecasts and implementing visualization \\r\\n\\t\\t\\t      services to meet sustainability targets and distill key business \\r\\n\\t\\t\\t      insights from diverse data sets\\r\\n\\t\\t\\t    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t      Supporting data discovery and democratization by utilizing AWS \\r\\n\\t\\t\\t      Data Exchange\\r\\n\\t\\t\\t    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t  Moderator:\\r\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tKelci Zile\\r\\n\\t\\t\\t\\t\\t      \\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tGlobal Lead, Sustainability, AWS Data Exchange\\r\\n\\t\\t\\t\\t\\t      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKelci Zile leads Sustainability for AWS Data Exchange. She is responsible for connecting AWS customers across industries with the data they need to drive sustainability-related research, innovation, and decision-making. She primarily focuses on environmental, social, and governance data applications within the financial services, consulting, and insurance space. She has been at Amazon for over five years, previously running international marketing for Prime Video Direct, and driving discovery of emerging video content. Kelci is also the Co-Chair for Inside Sustainability on the Amazon Global Sustainability Ambassadors Board, \\r\\n\\t\\t\\t\\t    and in her free time she enjoys upcycling clothing and home goods.\\r\\n\\t\\t\\t\\t  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\tPresenters:\\r\\n\\t\\t      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tDaniel Klier\\r\\n\\t\\t\\t\\t\\t      \\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tPartner, CEO of Arabesque S-Ray and President of Arabesque\\r\\n\\t\\t\\t\\t\\t\\tHolding, Arabesque S-Ray\\r\\n\\t\\t\\t\\t\\t      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDr. Daniel leads the global growth strategy and expansion of S-Ray's ESG services worldwide. Prior to joining Arabesque, Daniel was Global Head of Sustainable Finance for HSBC. During his tenure, he developed the HSBC's global climate strategy and led the development of sustainable finance business across the company's corporate, retail, and asset management business. He joined HSBC in 2013 as Group Head of Strategy in London, following nine years at McKinsey & Company, as a Partner. Daniel has also chaired the Bank of England Climate Risk Working Group and the Sustainable Finance Working Group at the Institute of International Finance. He is a member of the Board of Sustainable Energy for All.\\r\\n\\t\\t\\t\\t  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tMukesh Jain\\r\\n\\t\\t\\t\\t\\t      \\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tChief Technology and Innovation Officer, Vice President and Head - Insights & Data Technology, Capgemini\\r\\n\\t\\t\\t\\t\\t      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMukesh Jain leads technology strategy, innovation, architecture advisory, and R&D solutions development in the areas of analytics, machine learning, and artificial intelligence across key sectors like finance, retail, automotive, education, manufacturing, and life sciences. He is a veteran in the data, analytics, and artificial intelligence space with 25-plus years of experience building large-scale products at Capgemini, Microsoft, Jio, and NICE Systems. He is a known figure in the industry and often speaks at internal conferences on these topics. He is active in teaching at several universities and is the author of two books.\\r\\n\\t\\t\\t\\t  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t  Colin Marden\\r\\n\\t\\t\\t\\t\\t\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t  Solutions Architect, AWS\\r\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nColin Marden is a Solutions Architect in the financial services industry supporting AWS Enterprise Greenfield customers in their journey to modernize, transform, and migrate on-premises workloads to the AWS Cloud. Colin is a champion and specialist for Amazon QuickSight and AWS Data Exchange, regularly speaking at AWS and partner events on subjects like data engineering and visualization.\\r\\n\\t\\t\\t    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nREGISTER NOW\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t  © 2021 AWS Marketplace.\",\n",
       " 'Sponsored Post.\\n\\nPhoto by Jackson Simmer on Unsplash\\nWe are living in a world where microchips and sensors are being added to devices from televisions to keychains. To say the amount of data being created is exploding doesn’t do justice to the sheer acceleration under way. This data revolution, however, is a double edged sword. Although the data that would best serve the analysis you’re trying to do probably exists, the difficulty in finding it is significant and increasing. Choosing the wrong data can doom your analysis from the start.\\nWe started Nomad Data to help data scientists and business analysts quickly find the right commercial datasets to match their specific use case. We catalog use cases of data and use machine learning and AI to match analysis goals with datasets.\\nImagine you’re working as a data scientist for an auto loan company in late 2017, a few months after Hurricane Maria struck Puerto Rico, devastating the island. Your boss in the risk department asks you to come up with an analysis on whether people are fleeing the island permanently, therefore increasing the risk that the firm’s auto loans will go into default. The first step in tackling the problem is deciding what data is a good proxy for population migration. Using Nomad Data you would quickly be connected to:\\n\\nConsumer Transaction Data - This allows you to see near real-time consumer spend at stores on the island.\\nGeolocation Data - Using signals from anonymous consumer cell phones you can see the number of active phones on the island and how that number has changed since before the hurricane.\\nConsumer Credit Data - Using data from the three credit bureaus you can see if the volume of residents paying their bills has changed since before the hurricane.\\nAirline Manifest Data - Data from online travel agencies allows visibility into the volume of airline tickets being purchased to/from the island.\\n\\nMost data scientists don’t know the different types of external data sources that exist as there are thousands and new ones are being created daily. Even if they do know, being sure that a given dataset can answer a very particular question requires investigation which means spending time and money. If you went through and tested the above sources, you would learn that there is only one unbiased by infrastructure challenges that the island was facing. By searching with Nomad Data you save valuable time by testing only the most relevant data.',\n",
       " \"By Sara A. Metwalli, Associate Editor at Towards Data Science.\\ncomments\\n\\nPhoto by\\xa0Lewis Keegan\\xa0on\\xa0Unsplash.\\nBecause of the appeal of the field of data science and the premise of high incomes, more and more people decide to join the field every day. Some may come from a technical background, while others just join in due to curiosity; regardless of the reason you decide to join the field, your no. 1 goal will probably be to have a strong, solid portfolio that can help you land the job you want.\\n\\nSo,\\xa0how can you increase the appeal of your portfolio?\\n\\nAlthough getting\\xa0into data science doesn’t necessarily require any degrees or certificates, sometimes having some could help make you stand out in the applicants' pool when applying for a job.\\nWhat makes a good data science portfolio is collecting projects that show your skills, prove your knowledge, and demonstrate your ability to build solid data science projects. That’s the core of a good portfolio, but you can also add some certificates to prove that you put in the time, effort, and money to hone your skills and become a more qualified data scientist.\\nLuckily, not all certificates you can get require you to go to a testing center. In fact, most of the desirable data science certificates can be taken from the comfort of your couch.\\nThis article presents you with 6 highly desirable certificates that you can obtain to increase your chances of landing an internship or your dream job.\\n\\xa0\\nMicrosoft Certified: Azure Data Scientist Associate\\n\\xa0\\nMicrosoft is one of the leading names of technology and software; they offer a\\xa0certificate\\xa0that aims to measure your ability to run experiments, train machine learning models, optimize your model's performance, and deploy it using the Azure Machine Learning workspace.\\nTo obtain this certificate, you will need to pass one exam, and you can prepare for this exam in one of two ways. Microsoft offers free online materials that you can self-study to prepare for the exam. If you prefer having an instructor, they also offer a paid option where an Azure machine learning instructor can tutor you.\\nThis exam will cost around $165. The price varies based on the country you will proctor the test from.\\n\\xa0\\nIBM Data Science Professional Certificate\\n\\xa0\\nThis certificate comes from IBM and is offered at the end of a course series that takes you from being a complete data science beginner to a professional data scientist online and at your own pace.\\nIBM Data science professional certificate is offered on both\\xa0Coursera\\xa0and\\xa0edX. On either platform, you have to complete a set of courses covering all the core knowledge of data science to get the certificate and an IBM badge once you’re done.\\nTo get the certificate from Coursera, you will need to pay a fee of $39 per month, so the sooner you can finish the series, the less you will need to pay. On the other hand, edX requires $793 for the full course experience regardless of how long you will talk to complete it.\\n\\xa0\\nGoogle Professional Data Engineer Certification\\n\\xa0\\nGoogle’s professional data engineer certification\\xa0is aimed to examine the skills you need to be qualified as a data engineer. A data engineer can make data-driven decisions, build reliable models, train, test and optimize them.\\nYou can get this certificate by applying directly through the official Google certificate page, or you can take a course series and the certificate on\\xa0Coursera. The courses will teach you all you need to know about machine learning and AI fundamentals and build efficient data pipelines and analytics.\\nTo access the course series on Coursera, you will need to have Coursera Plus or pay a fee of $49 per month for as long as you need to finish the series and obtain your certificate.\\n\\xa0\\nCloudera Certified Professional (CCP) Data Engineer\\n\\xa0\\nCloudera targets open-source developers and offers the\\xa0CCP Data Engineer\\xa0certificate for developers to test their ability to collect, process, and analyze data efficiently on the Cloudera CDH environment.\\nTo pass this exam, you will be given 5-10 data science problems, each with its own large dataset and CDH cluster. Your task will be to find a high-precision solution for each of these problems and implement it correctly.\\nTo take this exam, you will need to score at least 70% in the exam. The exam will be 4 hours long and will cost you $400. You can take this exam anywhere online.\\n\\xa0\\nSAS Certified AI & Machine Learning Professional\\n\\xa0\\nUnlike the certificates we discussed so far, the\\xa0SAS AI & Machine Learning Professional certificate\\xa0is acquired by passing three exams that test three different skill sets. The three exams you will need to pass to get the certificate are:\\n\\nMachine learning exam where your skills to build, train, test performance and optimize supervised machine learning models will be tested.\\nForecast and optimization test. In this test, your ability to handle, visualize data, build data pipelines and solve optimization problems will be tested.\\nNLP and computer vision test.\\n\\nSAS offers a free 30 days worth of\\xa0preparation materials\\xa0that will get you ready to take and pass each of these three exams.\\n\\xa0\\nTensorFlow Developer Certificate\\n\\xa0\\nTensorFlow is one of the widely used packages for machine learning, AI, and deep learning applications. The\\xa0TensorFlow Developer Certificate\\xa0is given to a developer to demonstrate their ability to use TensorFlow to develop solutions for machine learning and deep learning problems.\\nYou can prepare for this certificate by finishing the\\xa0DeepLearning.AI TensorFlow Developer Professional Certificate\\xa0Coursera course series. Once you have earned this certificate, your name and picture will be added to the\\xa0Google Developers\\xa0webpage.\\nThe TensorFlow Developer Certificate is valid for 3 years. Afterward, you will need to retake the test to keep your skill level synced with the TensorFlow package's recent updates.\\n\\xa0\\nTakeaways\\n\\xa0\\nIf you ask any data scientist whether they needed their degree or certificate to land their job roles, most will tell you that they got into data science from a non-technical background with a curious mind that only wanted to learn more.\\nAnd even though you can become a data scientist and get a good job by self-studying the core concepts of data science and building real-life-sized projects or projects that can be applied easily to real-life data, sometimes having a certificate can help make your portfolio stand out and attract the eyes of recruiters to you.\\nBecause data science is one of the popular fields today, you will find a redundant amount of tutorials and guides online on what you need to do to become a “good data scientist” or “how to land your dream data science role.” Not to mention the tons of certificates that you can get and free courses you can take to improve your skills. I have been where you are, overwhelmed by the amount of information out there about data science and how to get into the field. But, I always appreciated simple, straightforward articles that get to the point without dragging the topic too long.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nData science certification – why it is important and where to get it?\\n10 resources for data science self-study\\n7 Most Recommended Skills to Learn to be a Data Scientist\",\n",
       " \"comments\\nBy Heather Fyson, KNIME\\n\\nAfter the\\xa0KNIME Fall Summit, the dinosaurs went back home… well, switched off their laptops.\\xa0Dean Abbott\\xa0and\\xa0John Elder, longstanding data science experts, were invited to the Fall Summit by\\xa0Michael\\xa0to join him in a discussion of\\xa0The Future of Data Science: A Fireside Chat with Industry Dinosaurs. The result was a sparkling conversation about data science challenges and new trends. Since switching off the studio lights,\\xa0Rosaria\\xa0has distilled and expanded some of the highlights about change management, complexity, interpretability, and more in the data science world. Let’s see where it brought us.\\n\\xa0\\nWhat is your experience with change management in AI, when reality changes and models have to be updated? What did COVID do to all our models?\\n\\xa0\\n[Dean]\\xa0Machine Learning (ML) algorithms assume consistency between past and future. When things change, the models fail. COVID has changed our habits, and therefore our data. Pre-COVID models struggle to deal with the new situation.\\n[John]\\xa0A simple example would be the Traffic layer on Google Maps. After lockdowns hit country after country in 2020, Google Maps traffic estimates were very inaccurate for a while. It had been built on fairly stable training data but now that system was thrown completely out of whack.\\n\\xa0\\nHow do you figure out when the world has changed and the models don't work anymore?\\n\\xa0\\n[Dean]\\xa0Here’s a little trick I use: I partition my data by time and label records as “before” and “after”. I then build a classification model to discriminate the “after” vs. the “before” from the same inputs the model uses. If the discrimination is possible, then the “after” is different from the “before”, the world has changed, the data has changed, and the models must be retrained.\\n\\xa0\\nHow complicated is it to retrain models in projects, especially after years of customization?\\n\\xa0\\n[John]\\xa0Training models is usually the easiest step of all! The vast majority of otherwise successful projects\\xa0die\\xa0in the implementation phase. The greatest\\xa0time\\xa0is spent in the data cleansing and preparation phase. And the most\\xa0problems\\xa0are missed or made in the business understanding / project definition phase. So if you understand what the flaw is and can obtain new data and have the implementation framework in place, creating a new model is, by comparison, very straightforward.\\n\\xa0\\nBased on your decades-long experience, how complex is it to put together a really functioning Data Science application?\\n\\xa0\\n[John]\\xa0It can vary of course, by complexity. Most of our projects get functioning prototypes at least in a few months. But for all, I cannot stress enough the importance of feedback: You have to talk to people much more often than you want to. And listen! We learn new things about the business problem, the data, or constraints, each time. Not all us quantitative people are skilled at speaking with humans, so it often takes a team. But the whole team of stakeholders has to learn to speak the same language.\\n[Dean]\\xa0It is important to talk to our business counterpart. People fear change and don’t want to change the current status. One key problem really is psychological. The analysts are often seen as an annoyance. So, we have to build the trust between the business counterpart and the analytics geeks. The start of a project should always include the following step: Sync up domain experts / project managers, the analysts, and the IT and infrastructure (DevOps) team so everyone is clear on the objectives of the project and how it will be executed. Analysts are number 11 on the top 10 list of people they have to see every day! Let’s avoid embodying data scientist arrogance: “The business can’t understand us/our techniques, but we know what works best”. What we don’t understand, however, are the domains experts are actually experts in the domain we are working in! Translation of data science assumptions and approaches into language that is understood by the domain experts is key!\\n\\xa0\\nThe latest trend now is deep learning, apparently it can solve everything. I got a question from a student lately, asking “why do we need to learn other ML algorithms if deep learning is the state of the art to solve data science problems”?\\n\\xa0\\n[Dean]\\xa0Deep learning sucked a lot of the oxygen out of the room. It feels so much like the early 1990s when neural networks ascended with similar optimism! Deep Learning is a set of powerful techniques for sure, but they are hard to implement and optimize. XGBoost, Ensembles of trees, are also powerful but currently more mainstream. The vast majority of problems we need to solve using advanced analytics really don’t require complex solutions, so start simple; deep learning is overkill in these situations. It is best to use the Occam’s razor principle: if two models perform the same, adopt the simplest.\\n\\xa0\\nAbout complexity. The other trend, opposite to deep learning, is ML interpretability. Here, you greatly (excessively?) simplify the model in order to be able to explain it. Is interpretability that important?\\n\\xa0\\n[John]\\xa0I often find myself fighting interpretability. It is nice, sure, but often comes at too high a cost of the most important model property: reliable accuracy. But many stakeholders believe interpretability is essential, so it becomes a barrier for acceptance. Thus, it is essential to discover what kind of interpretability is needed. Perhaps it is just knowing what the most important variables are? That’s doable with many nonlinear models. Maybe, as with explaining to credit applicants why they were turned down, one just needs to interpret outputs for one case at a time? We can build a linear approximation for a given point. Or, we can generate data from our black box model and build an “interpretable” model of any complexity to fit that data.\\nLastly, research has shown that if users have the chance to play with a model – that is, to poke it with trial values of inputs and see its outputs, and perhaps visualize it – they get the same warm feelings of interpretability. Overall, trust – in the people and technology behind the model – is necessary for acceptance, and this is enhanced by regular communication and by including the eventual users of the model in the build phases and decisions of the modeling process.\\n[Dean]\\xa0By the way KNIME Analytics Platform has a great feature to quantify the importance of the input variables in a Random Forest! The\\xa0Random Forest Learner\\xa0node outputs the statistics of candidate and splitting variables. Remember that, when you use the Random Forest Learner node.\\n\\xa0\\nThere is an increase in requests for explanations of what a model does. For example, for some security classes, the European Union is demanding verification that the model doesn’t do what it’s not supposed to do. If we have to explain it all, then maybe Machine Learning is not the way to go. No more Machine Learning?\\n\\xa0\\n[Dean]\\xa0\\xa0Maybe full explainability is too hard to obtain, but we can achieve progress by performing a grid search on model inputs to create something like a score card describing what the model does. This is something like regression testing in hardware and software QA. If a formal proof what models are doing is not possible, then let’s test and test and test! Input Shuffling and Target Shuffling can help to achieve a rough representation of the model behavior.\\n[John]\\xa0Talking about understanding what a model does, I would like to raise the problem of reproducibility in science. A huge proportion of journal articles in all fields -- 65 to 90% -- is believed to be unreplicable. This is a true crisis in science. Medical papers try to tell you how to reproduce their results. ML papers don’t yet seem to care about reproducibility. A recent study showed that only 15% of AI papers share their code.\\n\\xa0\\nLet’s talk about Machine Learning Bias. Is it possible to build models that don’t discriminate?\\n\\xa0\\n[John]\\xa0(To be a nerd for a second, that word is unfortunately\\xa0overloaded. To “discriminate” in the ML world word is your very goal: to make a distinction between two classes.) But to your real question, it depends on the data (and on whether the analyst is clever enough to adjust for weaknesses in the data): The models will pull out of the data the information reflected therein. The computer knows nothing about the world except for what’s in the data in front of it. So the analyst has to curate the data -- take responsibility for those cases reflecting reality. If certain types of people, for example, are under-represented then the model will pay less attention to them and won’t be as accurate on them going forward. I ask, “What did the data have to go through to get here?” (to get in this dataset) to think of how other cases might have dropped out along the way through the process (that is survivor bias). A skilled data scientist can look for such problems and think of ways to adjust/correct for them.\\n[Dean]\\xa0The bias is not in the algorithms. The bias is in the data. If the data is biased, we’re working with a biased view of the world. Math is just math, it is not biased.\\n\\xa0\\nWill AI take over humanity?!\\n\\xa0\\n[John]\\xa0I believe AI is just good engineering. Will AI exceed human intelligence? In my experience anyone under 40 believes yes, this is inevitable, and most over 40 (like me, obviously): no! AI models are fast, loyal, and obedient. Like a good German Shepherd dog, an AI model will go and get that ball, but it knows nothing about the world other than the data it has been shown. It has no common sense. It is a great assistant for specific tasks, but actually quite dimwitted.\\n[Dean]\\xa0On that note, I would like to report two quotes made by Marvin Minsky in 1961 and 1970, from the dawn of AI, that I think describe well the future of AI.\\n“Within our lifetime some machines may surpass us in general intelligence”\\xa0(1961)\\n“In three to eight years we’ll have a machine with the intelligence of a human being”\\xa0(1970)\\nThese ideas have been around for a long time. Here is one reason why AI will not solve all the problems: We’re judging its behavior based on one number, one number only! (Model error.) For example, predictions of stock prices over the next five years, predicted by building models using root mean square error as the error metric, cannot possibly paint the full picture of what the data are actually doing and severely hampers the model and its ability to flexibly uncover the patterns. We all know that RMSE is too coarse of a measure. Deep Learning algorithms will continue to get better, but we also need to get better at judging how good a model really is. So, no! I do not think that AI will take over humanity.\\n\\xa0\\nWe have reached the end of this interview. We\\xa0would like to thank Dean and John for their time and their pills of knowledge. Let’s hope we meet again soon!\\n\\xa0\\nAbout Dean Abbott and John Elder\\n\\xa0\\n\\n\\n\\n\\nDean Abbott\\xa0is Co-Founder and Chief Data Scientist at SmarterHQ. He is an internationally recognized expert and innovator in data science and predictive analytics, with three decades of experience solving problems in omnichannel customer analytics, fraud detection, risk modeling, text mining & survey analysis. Included frequently in lists of pioneering data scientists and data scientists, he is a popular keynote speaker and workshop instructor at conferences worldwide, also serving on Advisory Boards for the UC/Irvine Predictive Analytics and UCSD Data Science Certificate programs. He is the author of Applied Predictive Analytics (Wiley, 2014) and co-author of The IBM SPSS Modeler Cookbook (Packt Publishing, 2013).\\n\\n\\n\\n\\n\\n\\n\\n\\nJohn Elder\\xa0founded Elder Research, America’s largest and most experienced data science consultancy in 1995. With offices in Charlottesville VA, Baltimore MD, Raleigh, NC, Washington DC, and London, they’ve solved hundreds of challenges for commercial and government clients by extracting actionable knowledge from all types of data. Dr. Elder co-authored three books — on practical data mining, ensembles, and text mining — two of which won “book of the year” awards. John has created data mining tools, was a discoverer of ensemble methods, chairs international conferences, and is a popular workshop and keynote speaker.\\n\\n\\n\\n\\xa0\\nBio: Heather Fyson is the blog editor at KNIME. Initially on the Event Team, her background is actually in translation & proofreading, so by moving to the blog in 2019 she has returned to her real passion of working with texts. P.S. She is always interested to hear your ideas for new articles.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nMastering Time Series Analysis with Help From the Experts\\nHow to break a model in 20 days — a tutorial on production model analytics\\nMain 2020 Developments and Key 2021 Trends in AI, Data Science, Machine Learning Technology\",\n",
       " 'comments\\nBy Ekta Sharma, Data Science Enthusiast\\n\\n\\nPhoto Credit — Unsplash\\n\\n\\xa0\\nPyCaret is an\\xa0open-source library\\xa0that provides a variety of machine learning functions through various modules one of which is anomaly detection.\\nPyCaret’s anomaly detection module is an\\xa0unsupervised\\xa0machine learning module that is used for identifying extreme values present in the data that can sometimes indicate suspicious activity or an abnormal instance.\\nPyCaret’s anomaly detection module provides\\xa0twelve different\\xa0anomaly detection techniques to choose from depending on the problem you are working on. It also lets us perform feature engineering tasks through a function called “setup”\\xa0by using various parameter values passed to this function.\\nIn this article, we are going to apply three of the anomaly detection techniques provided by PyCaret on one of the datasets provided by PyCaret itself. The three techniques covered in this article are —\\xa0Isolation Forest, K Nearest Neighbors, and Clustering.\\nBefore we implement any of these techniques, let’s take a look at the steps that we will need to follow in a specific order to identify anomalies in the data by using the following functions. These steps are common to all techniques provided by PyCaret for anomaly detection.\\n\\nget_data()\\xa0— This function is used to access the PyCaret dataset. This is an optional step.\\nsetup()\\xa0— This function initializes the environment and performs the preprocessing tasks needed before anomaly detection. The only required parameter that it takes is a Dataframe in the “data” parameter but below is an example of various preprocessing tasks that can be achieved through the setup function.\\n\\n\\nsetup(data, categorical_features = None, categorical_imputation = ‘constant’, ordinal_features = None, high_cardinality_features = None, numeric_features = None, numeric_imputation = ‘mean’, date_features = None, ignore_features = None, normalize = False, normalize_method = ‘zscore’, transformation = False, transformation_method = ‘yeo-johnson’, handle_unknown_categorical = True, unknown_categorical_method = ‘least_frequent’, pca = False, pca_method = ‘linear’, pca_components = None, ignore_low_variance = False, combine_rare_levels = False, rare_level_threshold = 0.10, bin_numeric_features = None, remove_multicollinearity = False, multicollinearity_threshold = 0.9, group_features = None, group_names = None, supervised = False, supervised_target = None, session_id = None, profile = False, verbose=True)\\n\\n\\n\\ncreate_model()\\xa0— This function creates the model and trains it on the dataset passed as the parameter during the setup stage. Hence, this function requires setup() function to be called before it is used.\\n\\n\\ndf = pd.read_csv(path_to_csv) # to access your own dataset\\r\\nor\\r\\ndf = get_data(“anomaly”) # to access PyCaret’s anomaly datasetsetup_data = setup(data=df)\\r\\nsample_model = create_model(“iforest”)\\n\\n\\n\\nplot_model()\\xa0— This function takes the trained model created during create_model() function and plots the data passed during the setup() function. Hence, this method requires both setup() and create_model() function to be called before it is called. The returned plot cleary shows anomaly data in a different color.\\n\\n\\nplot_model(sample_model)\\n\\n\\n\\npredict_model()\\xa0— This function takes the trained model and uses it to make predictions on the new data. The new data must be in the form of a Pandas Dataframe. The output of this function is a Dataframe with predictions called “Label” and the associated decision Score.\\n\\n\\n\\nLabel = 0 means normal data or inlier\\nLabel = 1 means an anomaly or outlier\\n\\n\\nNow, when we have a basic understanding of how PyCaret Anomaly Detection Functions work, let’s dive into the actual implementation.\\n\\n# Importing PyCaret dependencies.\\r\\nfrom pycaret.datasets import get_data\\r\\nanomaly = get_data(“anomaly”)# Importing anomaly detection module.\\r\\nfrom pycaret.anomaly import *# Initializing the setup function used for pre-processing.\\r\\nsetup_anomaly_data = setup(anomaly)\\n\\n\\n\\xa0\\nIsolation Forest Implementation\\n\\xa0\\n\\n# Instantiating Isolation Forest model.\\r\\niforest = create_model(“iforest”)# Plotting the data using Isolation Forest model.\\r\\nplot_model(iforest)# Generating the predictions using Isolation Forest trained model.\\r\\niforest_predictions = predict_model(iforest, data = anomaly)\\r\\nprint(iforest_predictions)# Checking anomaly rows. Label = 1 is the anomaly data.\\r\\niforest_anomaly_rows = iforest_predictions[iforest_predictions[“Label”] == 1]\\r\\nprint(iforest_anomaly_rows.head())# Checking the number of anomaly rows returned by Isolaton Forest.\\r\\nprint(iforest_anomaly_rows.shape) # returned 50 rows\\n\\n\\n\\n\\n\\nTop 5 Anomaly Rows (Label 1)\\n\\n\\n\\nAnomaly Plots created using Isolation Forest (Anomaly highlighted in Yellow color)\\n\\n\\nIsolation Forest Based Anomaly Plot\\n\\n\\xa0\\n\\xa0\\nK Nearest Neighbors (KNN) Implementation\\n\\xa0\\n\\n# Instantiating KNN model.\\r\\nknn = create_model(“knn”)# Plotting the data using KNN model.\\r\\nplot_model(knn)# Generating the predictions using KNN trained model.\\r\\nknn_predictions = predict_model(knn, data = anomaly)\\r\\nprint(knn_predictions)# Checking KNN anomaly rows. Predictions with Label = 1 are anomalies.\\r\\nknn_anomaly_rows = knn_predictions[knn_predictions[“Label”] == 1]\\r\\nknn_anomaly_rows.head()# Checking the number of anomaly rows returned by KNN model.\\r\\nknn_anomaly_rows.shape # returned 46 rows\\n\\n\\n\\n\\n\\nTop 5 Anomaly Rows (Label 1)\\n\\n\\n\\nAnomaly Plot created using K Nearest Neighbors (Anomaly highlighted in Yellow color)\\n\\n\\nKNN Based Anomaly Plot\\n\\n\\xa0\\n\\xa0\\nClustering\\xa0Implementation\\n\\xa0\\n\\n# Instantiating Cluster model.\\r\\ncluster = create_model(“cluster”)# Plotting the data using Cluster model.\\r\\nplot_model(cluster)# Generating the predictions using Cluster trained model.\\r\\ncluster_predictions = predict_model(cluster, data = anomaly)\\r\\nprint(cluster_predictions)# Checking cluster anomaly rows. Predictions with Label = 1 are anomalies.\\r\\ncluster_anomaly_rows = cluster_predictions[cluster_predictions[“Label”] == 1]\\r\\nprint(cluster_anomaly_rows.head())# Checking the number of anomaly rows returned by Cluster model.\\r\\ncluster_anomaly_rows.shape # returned 50 rows\\n\\n\\n\\n\\n\\nTop 5 Anomaly Rows (Label 1)\\n\\n\\n\\nAnomaly Plot created using Clustering (Anomaly highlighted in Yellow color)\\n\\n\\nClustering Based Anomaly Plot\\n\\n\\xa0\\nReferences\\n\\nhttps://pycaret.org/\\n\\n\\xa0\\nBio: Ekta Sharma is a Data Science Enthusiast.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to use Machine Learning for Anomaly Detection and Conditional Monitoring\\nBayesian Hyperparameter Optimization with tune-sklearn in PyCaret\\nIntroducing MIDAS: A New Baseline for Anomaly Detection in Graphs',\n",
       " 'comments\\nBy Juhi Sharma, Product Analyst\\n\\nSource — https://www.wvgazettemail.com/\\n\\xa0\\nIntroduction\\n\\xa0\\nKmart is a leading online retailer in the US and as part of their annual sales review meeting, they need to decide on their sales strategy for the year 2020 based on the insights from the sales data in 2019.\\nData is related to sales for each month of 2019 and the task is to generate key insights which will help the\\xa0sales team\\xa0of Kmart to take some key business decisions towards Fine-tuning their sales strategy.\\n\\xa0\\nData Understanding\\n\\xa0\\n\\nData Belongs to Kmart -a leading online retailer in the US.\\nTime Period — January 2019 — December 2019\\nUnique Product — 19\\nTotal Orders — 178437\\nCities — 9\\nKPI’s — Total Sales, Total Products Sold\\n\\n\\n\\nSource — By Author\\n\\n\\xa0\\nBusiness Problem Statements\\n\\xa0\\n\\nWhat was the best month for sales? How much was earned that month?\\nWhich city had the highest number of sales?\\nRecommend the most appropriate time to display advertising to maximize the likelihood of customers buying the products?\\nWhat products sold the most? Why do you think it sold the most?\\n\\n\\xa0\\nData Analysis Using Python\\n\\xa0\\n\\nLoaded Data of each month and made data frame using pandas\\nConcatenated Dataset to make one Dataset for 2019 sales.\\nTreating Null Values and Junk Data.\\nMade a Filtered Dataset after preprocessing data\\nAnalysis and answers to business problems. (visualizations using matplot and seaborn library)\\n\\n\\xa0\\n1. Importing Libraries\\n\\xa0\\n\\nimport pandas as pd\\n\\n\\n\\xa0\\n2. Loading Dataset and making Dataframes\\n\\xa0\\n\\ndf1=pd.read_csv(\"Sales_January_2019.csv\")\\r\\ndf1[\"month\"]=\"Jan\"\\r\\ndf2=pd.read_csv(\"Sales_February_2019.csv\")\\r\\ndf2[\"month\"]=\"feb\"\\r\\ndf3=pd.read_csv(\"Sales_March_2019.csv\")\\r\\ndf3[\"month\"]=\"mar\"\\r\\ndf4=pd.read_csv(\"Sales_April_2019.csv\")\\r\\ndf4[\"month\"]=\"apr\"\\r\\ndf5=pd.read_csv(\"Sales_May_2019.csv\")\\r\\ndf5[\"month\"]=\"may\"\\r\\ndf6=pd.read_csv(\"Sales_June_2019.csv\")\\r\\ndf6[\"month\"]=\"june\"\\r\\ndf7=pd.read_csv(\"Sales_July_2019.csv\")\\r\\ndf7[\"month\"]=\"july\"\\r\\ndf8=pd.read_csv(\"Sales_August_2019.csv\")\\r\\ndf8[\"month\"]=\"aug\"\\r\\ndf9=pd.read_csv(\"Sales_September_2019.csv\")\\r\\ndf9[\"month\"]=\"sep\"\\r\\ndf10=pd.read_csv(\"Sales_October_2019.csv\")\\r\\ndf10[\"month\"]=\"oct\"\\r\\ndf11=pd.read_csv(\"Sales_November_2019.csv\")\\r\\ndf11[\"month\"]=\"nov\"\\r\\ndf12=pd.read_csv(\"Sales_December_2019.csv\")\\r\\ndf12[\"month\"]=\"dec\"list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12]\\n\\n\\n\\xa0\\n3. The shape of each month’s dataset\\n\\xa0\\n\\nfor i in list:\\r\\n    print(i.shape)\\n\\n\\n\\n\\nSource- By Author\\n\\n\\xa0\\n\\xa0\\n4. Concatenating dataset\\n\\xa0\\n\\nframe=pd.concat(list)\\n\\n\\n\\n\\nSource- By Author\\n\\n\\xa0\\n\\xa0\\n5. Columns of Final Dataset\\n\\xa0\\n\\nframe.columns\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\xa0\\n6. Dataframe Information\\n\\xa0\\n\\nframe.info()\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\xa0\\n7. Null values in the dataset\\n\\xa0\\n\\nframe.isnull().sum() # there are 545 null values in each column except month\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\n(frame.isnull().sum().sum())/len(frame)*100  # we have 1.75 percent null values , so we can drop them\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\xa0\\n8. Dropping Null Values\\n\\xa0\\n\\nframe=frame.dropna()\\r\\nframe.isnull().sum()\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\xa0\\n9. Removing Junk Data\\n\\xa0\\nwe observed that there are 355 columns in which values in rows are the same as the header. so making a new data frame where these values will be excluded.\\n\\nframe[frame[\\'Quantity Ordered\\'] == \"Quantity Ordered\"]\\n\\n\\n\\n\\n\\n\\ndf_filtered = frame[frame[\\'Quantity Ordered\\'] != \"Quantity Ordered\"] \\r\\ndf_filtered.head(15) \\r\\ndf_filtered.shape\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\xa0\\nSolutions to Business Problems\\n\\xa0\\nQ 1. What was the best month for sales? How much was earned that month?\\n\\ndf_filtered[\"Quantity Ordered\"]=df_filtered[\"Quantity Ordered\"].astype(\"float\")\\r\\ndf_filtered[\"Price Each\"]=df_filtered[\"Price Each\"].astype(\"float\")# Creating Sales Column By multiplying Quantity Ordered and Price of Each Productdf_filtered[\"sales\"]=df_filtered[\"Quantity Ordered\"]*df_filtered[\"Price Each\"]\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\nmonth=[\"dec\",\"oct\",\"apr\",\"nov\",\"may\",\"mar\",\"july\",\"june\",\"aug\",\\'feb\\',\"sep\",\"jan\"] \\r\\ndf[\"month\"]=monthfrom matplotlib import pyplot as plt\\r\\na4_dims = (11.7, 8.27)\\r\\nfig, ax = pyplot.subplots(figsize=a4_dims)\\r\\nimport seaborn as sns\\r\\nsns.barplot(x = \"sales\",\\r\\n            y = \"month\",\\r\\n            data = df)\\r\\nplt.title(\"Month wise Sale\")\\r\\nplt.show()\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\nThe best Month for sales was DECEMBER.\\nTotal sales in December is $ 4619297.\\n\\xa0\\nQ 2. Which city had the highest number of sales?\\n\\ndftemp = df_filtered\\r\\nlist_city = []\\r\\nfor i in dftemp[\\'Purchase Address\\']:\\r\\n    list_city.append(i.split(\",\")[1])\\r\\ndftemp[\\'City\\'] = list_city\\r\\ndftemp.head()\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\ndf_city=df_filtered.groupby([\"City\"])[\\'sales\\'].sum().sort_values(ascending=False)\\r\\ndf_city=df_city.to_frame()\\r\\ndf_city\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\ncity=[\"San Francisco\",\"Los Angeles\",\"New York City\",\"Boston\",\"Atlanta\",\"Dallas\",\"Seattle\",\"Portland\",\"Austin\"]\\r\\ndf_city[\"city\"]=cityfrom matplotlib import pyplot\\r\\na4_dims = (11.7, 8.27)\\r\\nfig, ax = pyplot.subplots(figsize=a4_dims)\\r\\nsns.barplot(x = \"sales\",\\r\\n            y = \"city\",\\r\\n            data = df_city)\\r\\nplt.title(\"City wise Sales\")\\r\\nplt.show()\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\nSan Francisco has the highest sales f around $8262204.\\n\\xa0\\nQ 3 What products sold the most?\\n\\nprint(df_filtered[\"Product\"].unique())\\r\\nprint(df_filtered[\"Product\"].nunique())\\n\\n\\n\\n\\nsource- By Author\\n\\n\\xa0\\n\\ndf_p=df_filtered.groupby([\\'Product\\'])[\\'Quantity Ordered\\'].sum().sort_values(ascending=False).head()\\r\\ndf_p=df_p.to_frame()\\r\\ndf_p\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\nproduct=[\"AAA Batteries (4-pack)\",\"AA Batteries (4-pack)\",\"USB-C Charging Cable\",\"Lightning Charging Cable\",\"Wired Headphones\"]\\r\\ndf_p[\"Product\"]=productfrom matplotlib import pyplot\\r\\na4_dims = (11.7, 8.27)\\r\\nfig, ax = pyplot.subplots(figsize=a4_dims)\\r\\nsns.barplot(x = \"Quantity Ordered\",\\r\\n            y = \"Product\",\\r\\n            data = df_p)\\r\\nplt.title(\"Prouct and Quantity Ordered\")\\r\\nplt.show()\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n31017.0 quantity of AAA Batteries (4-pack) is sold in a year. It is sold maximum because it is the cheapest product.\\n\\xa0\\nQ 4 Recommend the most appropriate time to display advertising to maximize the likelihood of customers buying the products?\\n\\ndftime = df_filtered\\r\\nlist_time = []\\r\\nfor i in dftime[\\'Order Date\\']:\\r\\n    list_time.append(i.split(\" \")[1])\\r\\ndftime[\\'Time\\'] = list_time\\r\\ndftime.head()\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\ndf_t=df_filtered.groupby([\\'Time\\'])[\\'sales\\'].sum().sort_values(ascending=False).head()\\r\\ndf_t=df_t.to_frame()\\r\\ndf_t\\n\\n\\n\\n\\nSource-By Author\\n\\n\\xa0\\n\\ndf_t.columns\\n\\n\\n\\n\\nSource -By Author\\n\\n\\xa0\\n\\xa0\\nBefore You Go\\n\\xa0\\nThanks for reading! If you want to get in touch with me, feel free to reach me at jsc1534@gmail.com or my\\xa0LinkedIn Profile. Also, you can find the code for this article and some really useful data science projects on my\\xa0GitHub\\xa0account.\\n\\xa0\\nBio: Juhi Sharma (Medium, GitHub) has 2+ years of work experience as an Analyst with the role of Project Management, Business Analysis, and Client Handling. Currently, Juhi is working as a Data Analyst for a Product Company. Juhi has hands-on experience in analyzing datasets, creating machine learning and deep learning models. Juhi is passionate about solving business problems with data-driven approaches.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nPandas Profiling: One-Line Magical Code for EDA\\nThe question that makes your data project more valuable\\nHow to frame the right questions to be answered using data',\n",
       " 'By Jesus Rodriguez, Intotheblock.\\ncomments\\n\\n\\nSource:\\xa0https://www.accenture.com/nl-en/blogs/insights/responsible-ai-with-opportunity-comes-responsibility\\n\\n\\xa0\\n\\nI recently started a new newsletter focus on AI education. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:\\n\\n\\n\\xa0\\nEnsuring fairness and safety in artificial intelligence(AI) applications is considered by many the biggest challenge in the space. As AI systems match or surpass human intelligence in many areas, it is essential that we establish a guideline to align this new form of intelligence with human values. The challenge is that, as humans, we understand very little about how our values are represented in the brain or we can’t even formulate specific rules to describe a specific value. While AI operates in a data universe, human values are a byproduct of our evolution as social beings. We don’t describe human values like fairness or justice using neuroscientific terms but using arguments from social sciences like psychology, ethics or sociology. Last year, researchers from OpenAI\\xa0published a paper\\xa0describing the importance of social sciences to improve the safety and fairness or AI algorithms in processes that require human intervention.\\nWe often hear that we need to avoid bias in AI algorithms by using fair and balanced training datasets. While that’s true in many scenarios, there are many instances in which fairness can’t be described using simple data rules. A simple question such as “do you prefer A to B” can have many answers depending on the specific context, human rationality or emotion. Imagine the task of inferring a pattern of “happiness”, “responsibility” or “loyalty” given a specific dataset. Can we describe those values simply using data? Extrapolating that lesson to AI systems tells us that in order to align with human values we need help from the disciplines that better understand human behavior.\\n\\xa0\\nAI Value Alignment: Learning by Asking the Right Questions\\n\\xa0\\nIn their research paper, the OpenAI team introduced the notion of AI value alignment as\\xa0“the task of ensuring that artificial intelligence systems reliably do what humans want”.\\xa0AI value alignment requires a level of understanding of human values in a given context. However, many times, we can’t simply explain the reasoning for a specific value-judgment in a data-rule. In those scenarios, the OpenAI team believes that the best way to understand human values is by simply asking questions.\\nImagine a scenario in which we are trying to train a machine learning classifier in whether the outcome of a specific event is\\xa0“better”\\xa0or\\xa0“worse”.\\xa0Is an\\xa0“increase in taxes better or worse?”, maybe is better for government social programs and worse for your economic plans.\\xa0“Would it be better or worse if it rains today?”, maybe it would be better for the farmers and worse for the folks that were planning a biking trip. Questions about human values can have different subjective answers depending on a specific context. From that perspective, if we can get AI systems to ask specific questions maybe we can learn to imitate human judgement in specific scenarios.\\nAsking the right question is an effective method for achieving AI value alignment. Unfortunately, this type of learning method is vulnerable to three well-known limitations of human value judgment:\\n\\nReflective Equilibrium:\\xa0In many cases, humans can’t arrive to the right answer to a question related to value judgement. Cognitive or ethical biases, lack of domain knowledge or fuzzy definition of “correctness” are factors that might introduce ambiguity in the answers. However, if we remove many of the contextual limitations of the question, a person might arrive to the “right answer”. In philosophy this is known as the “reflective equilibrium” as is one of the mechanism that any AI algorithm that tries to learn about human values should try to imitate.\\nUncertainly:\\xa0Even if we can achieve a reflective equilibrium for a given question, there might be many circumstances in which uncertainly or disagreement prevent humans for arriving to the right answer. Any activities related to future planning often entail uncertainty.\\nDeception:\\xa0Humans have a unique ability to provide plausible answers to a question but that might wrong in some non-obvious way. Intentionally or unintentionally, deceptive or misleading behavior often results in a misalignment between the outcome of a given event and the values of the parties involved. Recognizing deceptive behavior is a non-trivial challenge that needs to be solved to achieve AI value alignment.\\n\\n\\xa0\\nLearning Human Values by Debating\\n\\xa0\\nSo far we have two main arguments to the thesis of AI value alignment:\\n\\nAI systems can learn human values by asking questions.\\nQuestions are often vulnerable to challenges like uncertainty, deception or the absence of a reflective equilibrium.\\n\\nBringing these two ideas together, the OpenAI team decided to induce AI agents to learn human values by relying on one of the purest question-answering dynamics: debates. Conceptually, debating is a form of discussion that breaks down a complex argument into an iterative set of simpler questions in order to formulate a reasoning path towards a specific answer. In applying debate techniques to achieve AI value alignment, the OpenAI team relied on an operating hypothesis:\\n“Optimal play in the debate game (giving the argument most convincing to a human) results in true, useful answers to questions.”\\nWith that hypothesis as the foundation, OpenAI created a game in which two AI agents engaged in debate, trying to convince a human judge. The debaters are trained only to win the game, and are not motivated by truth separate from the human’s judgments. On the human side, the objective is to understand whether people are strong enough as judges in debate to make this scheme work, or how to modify debate to fix it if it doesn’t. Using AI debaters in the OpenAI debate is an ideal setting but the technology hasn’t really caught up to that point. Most real debates leverage sophisticated natural language patterns that are beyond the capabilities of AI systems today. Certainly, efforts like IBM Project Debater are rapidly closing this gap.\\nTo avoid the limitations of AI debaters, OpenAI uses a scheme with two human debaters and a human judge. The outcome of this debate game are used to train the AI-AI-Human setting.\\n\\n\\nSource:\\xa0https://openai.com/blog/ai-safety-needs-social-scientists/\\n\\n\\xa0\\nTo test the idea of training AI systems using this debate model, the OpenAI team created a\\xa0prototype website\\xa0where two debaters argue over the contents of an image. The games chooses an image of a cat or dog, and show the image to the two debaters but not the judge. One debater is honest and argues for the true contents of the image; the other debater lies. The debaters can talk to the judge and illustrate their points by drawing rectangles on the image, but the judge sees only the rectangles. At the end of the debate, each debater is allowed to reveal a single pixel to the judge, which is the only part of the debate which cannot be a lie. The outputs of the debate are used to train sophisticated image classifiers.\\n\\n\\nSource:\\xa0https://openai.com/blog/ai-safety-needs-social-scientists/\\n\\n\\xa0\\nUsing debates as the underlying technique, can help to answer important questions about the relationship between humans and AI agents.\\nThe idea of applying social sciences to AI is not a new one but the OpenAI efforts are some of the first pragmatic steps in this area. While social sciences focus on understanding human behavior in the real world, AI sorts of takes the best version of human behavior as a starting point. From that perspective, the intersection of social sciences and AI can lead to a more fairer and safer machine intelligence.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nFree From Stanford: Ethical and Social Issues in Natural Language Processing\\nThe Ethics of AI\\nAlgorithms of Social Manipulation',\n",
       " 'comments\\nBy Kurtis Pykes, AI Writer.\\n\\nPhoto by\\xa0Martin Péchy\\xa0on\\xa0Unsplash.\\nAs of writing this post, I am currently on the job hunt for a new role as a Data Scientist due to difficulties in correspondence to Covid-19 at my previous company.\\nThis time around, I’ve noticed things seem to be much harder than they\\xa0were when I was last was on the market, but instead of using these challenges to prolong our dreams of becoming a Data Scientist or end them in a worst case scenario, I’ve endeavored to better understand these challenges so I could come up with some solutions to make them work best in my favor, and now yours!\\n\\xa0\\nOutlandish Job Requirements\\n\\xa0\\nThis seems to be a thread in most discussions I’ve had with Data Science job seekers —\\n\\nNobody feels qualified anymore.\\n\\nMany Data Science Job descriptions do not communicate the\\xa0actual\\xa0requirements of the role being advertised.\\nOne major effect of this is that aspiring Data Scientists who prioritize their personal and technical skills based on job descriptions would be mislead regarding the requirements to fulfill a role. Another issue with this is that recruiters would get plenty of applications that do not meet the requirements.\\nAccording to a fabulous post by\\xa0Jeremie Harris\\xa0titled\\xa0The Problem With Data Science Job Postings, there are many reasons why a Job description may seem incomprehensible, and it’s down to you to distinguish what category the one you’re on falls in:\\n\\nMany ways to solve problems, so it is hard to narrow it down for a job description.\\nNew Data Science teams may encourage people to be a jack of all trades, which translates into the job description.\\nThe company lacks the experience to know what problems they have and what capabilities the person who can solve them should have.\\nWritten by recruiters.\\n\\nSolution\\nThough it requires some discernment on your part, it is important to identify what the potential reason for an outlandish job description may be as some scenarios may be harmful to your growth as a Data Scientist, such as being a jack of all trades.\\nA great way to overcome this challenge is to acknowledge that\\xa0Job Descriptions are merely a wish-list from the company, and they want to hire someone they believe has the ability to solve a real problem they actually have.\\nOn that note, definitely go about displaying your ability in a way that allows others (the companies) to perceive you have what it takes to tackle their challenges. Additionally, if you meet at least 50% of requirements on any job description, then you’re probably qualified and should definitely attempt to go for the role — if you meet the job description 100%, you’re probably overqualified.\\n\\xa0\\nData Science is becoming more Productionized\\n\\xa0\\nBeing able to spin up a Jupyter Notebook and do some visualizations then build a model has worked in the past, but it’s no longer enough to get you noticed, in my opinion.\\nJupyter Notebooks are great for conducting experiments, but when you get into the real world, there’s a point we move past the experimental phase. I believe it should be expected of every Data Scientist to know how to spin up a Jupyter Notebook, but as Data Science is becoming more productionized, bonus points are given to the Data Scientist that can write production-level code because you’ll be saving cost and time.\\nHere are 3 reasons every Data Scientist should know how to write production-level code:\\n\\nThings can get lost in translation from Data Scientist to Engineer.\\nEradicates the delay in the process.\\nKilling 2 birds with 1 stone since one person can do 2 people’s job — makes you more valuable.\\n\\nSolution\\nAlthough controversial, I believe the skills of a Data Scientist and a Software engineer are converging when it comes to product facing Data Science applications, so more Data Scientist should be learning about software engineering best practices.\\nGiven you already know how to write production-level code, you may want to check out\\xa0Schaun Wheeler\\xa0post titled\\xa0What does it mean to “Productionize” Data Science?, which exceptionally summarizes the focus of employment of systems beyond the code level best practices for Data Science productionization — a really interesting read to say the least.\\n\\n“For something to be ‘in production’ means it is part of the pipeline from the business to its customers. […] In data science, if something is in production, it’s on the path to putting information in a place where it is consumed.”\\n\\n\\xa0\\nCompetition\\n\\xa0\\nData Science is among the fastest growing and emerging technologies on the planet, and tons of people are flocking to update their skills to have a shot a making a career as a Data Scientist. And, just in-case you don’t believe me, over 3.5 million people have enrolled in Andrew Ng Machine Learning course (which is an important part of Data Science) on Coursera since its inception.\\n\\nIt’s the sexiest job of the 21st Century for a reason.\\nThese days more and more people are trying to break into the field, so the jobs have huge competition. However, this doesn’t have to be a reason to decide not to look for jobs!\\nSolution\\nYes, we ought to do more to stand out. However, according to a recent poll I took on my LinkedIn profile, this doesn’t necessarily mean having the most fancy looking CV.\\n\\nSource:\\xa0Kurtis Pykes LinkedIn.\\nOf course, having a great portfolio is a great way to stand out, yet what seems to be undefeated in increasing your chances of being handed an opportunity is reaching out to hiring managers or Data Scientist in senior roles in the places that you are applying for.\\nLinkedIn has made it so easy to find people who work at a particular company, and it should be made part of the job application process when applying for jobs.\\n\\xa0\\nConclusion\\n\\xa0\\nThe fact that it’s difficult to get a job in Data Science should never be the reason you don’t have one. There are many challenges you’ll face at any job in itself, and getting the job is just the qualification phase to see if the employers believe you are capable of facing the challenges and whether you believe the employers are whom you would like to be on your team. Always seek to improve yourself, don’t wait to be ready to apply because you may never feel ready, and don’t be afraid to be rejected or to reject companies that don’t align with where you are going.\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Kurtis Pykes\\xa0is passionate about harnessing the power of machine learning and data science to help people become more productive and effective.\\nRelated:\\n\\nHow I Levelled Up My Data Science Skills In 8 Months\\nUnpopular Opinion – Data Scientists Should Be More End-to-End\\n9 Developing Data Science & Analytics Job Trends',\n",
       " 'comments\\nBy Vatsal Saglani, Machine Learning Engineer at Quinnox\\n\\n\\nPhoto by\\xa0Tech Daily\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nIn the blog,\\xa0Generating storylines using a T5 Transformer\\xa0we saw how we can fine-tune a Sequence2Sequence (Text-To-Text) Transformer (T5) to generate storylines/plots by providing inputs like genre, director, cast, and ethnicity. In this blog, we will check out how we can use that trained T5 Model for inference. Later, we will also see how can we deploy it using\\xa0gunicorn\\xa0and\\xa0flask.\\n\\xa0\\nHow to do Model Inference?\\n\\xa0\\n\\nLet’s set up the script with the imports\\n\\n\\nimport os\\r\\nimport re\\r\\nimport random\\r\\nimport pandas as pd\\r\\nimport numpy as np\\r\\nimport matplotlib.pyplot as plt\\r\\nfrom tqdm import tqdm_notebook, tnrange\\r\\nfrom sklearn.utils import shuffle\\r\\nimport pickle\\r\\nimport math\\r\\nimport torch\\r\\nimport torch.nn.functional as F\\r\\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\\n\\n\\n\\nSet the\\xa0SEED\\xa0value and load the model and tokenizer\\n\\n\\ntorch.manual_seed(3007)model = T5ForConditionalGeneration.from_pretrained(\\'./outputs/model_files\\')\\r\\ntokenizer = T5Tokenizer.from_pretrained(\\'./outputs/model_files\\')\\n\\n\\n\\nUse the\\xa0model.generate\\xa0function to generate sequences\\n\\n\\ntext = \"generate plot for genre: horror\"\\r\\ninput_ids = tokenizer.encode(text, return_tensors=\"pt\")\\r\\ngreedyOp = model.generate(input_ids, max_length=100)\\r\\ntokenizer.decode(greedyOp[0], skip_special_tokens=True)\\n\\n\\nNote: Read\\xa0this\\xa0amazing Hugging Face blog regarding how you to use different decoding strategies for Text Generation using Transformers\\n\\nLet’s put this in a function\\n\\n\\ndef generateStoryLine(text, seq_len, seq_num):\\r\\n\\t\\t\\t\\t\\'\\'\\'\\r\\n\\t\\t\\t\\targs:\\r\\n\\t\\t\\t\\t\\ttext: input text eg. generate plot for: {genre} or generate plot for: {director}\\r\\n\\t\\t\\t\\t\\tseq_len: Max sequence length for the generated text\\r\\n\\t\\t\\t\\t\\tseq_num: Number of sequences to generate\\r\\n\\t\\t\\t\\t\\'\\'\\'\\r\\n        outputDict = dict()\\r\\n        outputDict[\"plots\"] = {}\\r\\n        input_ids = tokenizer.encode(text, return_tensors = \"pt\")\\r\\n        beamOp = model.generate(\\r\\n            input_ids,\\r\\n            max_length = seq_len,\\r\\n            do_sample = True,\\r\\n            top_k = 100,\\r\\n            top_p = 0.95,\\r\\n            num_return_sequences = seq_num\\r\\n        )        for ix, sample_op in enumerate(beamOp):\\r\\n            outputDict[\"plots\"][ix] = self.tokenizer.decode(sample_op, skip_special_tokens = True)\\r\\n            \\r\\n        \\r\\n        return outputDict\\n\\n\\n\\xa0\\nHow to deploy this with Flask?\\n\\xa0\\nThere are multiple ways a user can provide the inputs and the model might need to generate the plots. The user can provide only the genre, or they can provide genre and cast or they can even provide all the four i.e. genre, director, cast and ethnicity. But for the purpose of this implementation, I have kept it mandatory to provide a genre at the least.\\nYou can check out the link below to see how the API will work.\\nMovie Plot Generator\\nI generate vague movie plots on the web (but sometimes they are good). But I can assure you that it will always be…\\n\\xa0\\nLet’s develop a backend to achieve the API call used for the link above\\n\\xa0\\nInstall the requirements\\n\\xa0\\n\\npip install flask flask_cors tqdm rich gunicorn\\n\\n\\n\\xa0\\nCreate an app.py file\\n\\xa0\\n\\nImports\\n\\n\\n# app.pyfrom flask import Flask, request, jsonify\\r\\nimport json\\r\\nfrom flask_cors import CORS\\r\\nimport uuidfrom predict import PredictionModelObjectapp = Flask(__name__)\\r\\nCORS(app)print(\"Loading Model Object\")\\r\\npredictionObject = PredictionModelObject()\\r\\nprint(\"Loaded Model Object\")\\n\\n\\n\\nAdd an API route\\n\\n\\n@app.route(\\'/api/generatePlot\\', methods=[\\'POST\\'])\\r\\ndef gen_plot():    req = request.get_json()\\r\\n    genre = req[\\'genre\\']\\r\\n    director = req[\\'director\\'] if \\'director\\' in req else None\\r\\n    cast = req[\\'cast\\'] if \\'cast\\' in req else None\\r\\n    ethnicity = req[\\'ethnicity\\'] if \\'ethnicity\\' in req else None\\r\\n    num_plots = req[\\'num_plots\\'] if \\'num_plots\\' in req else 1\\r\\n    seq_len = req[\\'seq_len\\'] if \\'seq_len\\' in req else 200    if not isinstance(num_plots, int) or not isinstance(seq_len, int):\\r\\n        return jsonify({\\r\\n            \"message\": \"Number of words in plot and Number of plots must be integers\",\\r\\n            \"status\": \"Fail\"\\r\\n        })\\r\\n    \\r\\n    try:\\r\\n        plot, status = predictionObject.returnPlot(\\r\\n            genre = genre, \\r\\n            director = director,\\r\\n            cast = cast,\\r\\n            ethnicity = ethnicity,\\r\\n            seq_len = seq_len,\\r\\n            seq_num = num_plots\\r\\n        )        if status == \\'Pass\\':\\r\\n            \\r\\n            plot[\"message\"] = \"Success!\"\\r\\n            plot[\"status\"] = \"Pass\"\\r\\n            return jsonify(plot)\\r\\n        \\r\\n        else:            return jsonify({\"message\": plot, \"status\": status})\\r\\n    \\r\\n    except Exception as e:        return jsonify({\"message\": \"Error getting plot for the given input\", \"status\": \"Fail\"})\\n\\n\\n\\nThe main block to run the\\xa0flask\\xa0app\\n\\n\\nif __name__ == \"__main__\":\\r\\n    app.run(debug=True, port = 5000)\\n\\n\\nThis script won’t work yet. You might receive an ImportError when executing the script at this point as we haven’t yet created the\\xa0predict.py\\xa0script with the\\xa0PredictionModelObject\\n\\xa0\\nCreate the\\xa0PredictionModelObject\\n\\xa0\\n\\nCreate an\\xa0predict.py\\xa0file and import the following\\n\\n\\n# predict.py\\r\\nimport os\\r\\nimport re\\r\\nimport random\\r\\nimport torch\\r\\nimport torch.nn as nn\\r\\nfrom rich.console import Console\\r\\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\\r\\nfrom collections import defaultdictconsole = Console(record = True)torch.cuda.manual_seed(3007)\\r\\ntorch.manual_seed(3007)\\n\\n\\n\\nCreate the\\xa0PredictionModelObject\\xa0class\\n\\n\\n# predict.py\\r\\nclass PredictionModelObject(object):    def __init__(self):console.log(\"Model Loading\")\\r\\n        self.model = T5ForConditionalGeneration.from_pretrained(\\'./outputs/model_files\\')\\r\\n        self.tokenizer = T5Tokenizer.from_pretrained(\\'./outputs/model_files\\')\\r\\n        console.log(\"Model Loaded\")\\r\\n    \\r\\n    def beamSearch(self, text, seq_len, seq_num):        outputDict = dict()\\r\\n        outputDict[\"plots\"] = {}\\r\\n        input_ids = self.tokenizer.encode(text, return_tensors = \"pt\")\\r\\n        beamOp = self.model.generate(\\r\\n            input_ids,\\r\\n            max_length = seq_len,\\r\\n            do_sample = True,\\r\\n            top_k = 100,\\r\\n            top_p = 0.95,\\r\\n            num_return_sequences = seq_num\\r\\n        )        for ix, sample_op in enumerate(beamOp):\\r\\n            outputDict[\"plots\"][ix] = self.tokenizer.decode(sample_op, skip_special_tokens = True)\\r\\n            \\r\\n        \\r\\n        return outputDict    def genreToPlot(self, genre, seq_len, seq_num):        text = f\"generate plot for genre: {genre}\"        return self.beamSearch(text, seq_len, seq_num)    def genreDirectorToPlot(self, genre, director, seq_len, seq_num):        text = f\"generate plot for genre: {genre} and director: {director}\"\\r\\n        \\r\\n        return self.beamSearch(text, seq_len, seq_num)    def genreDirectorCastToPlot(self, genre, director, cast, seq_len, seq_num):        text = f\"generate plot for genre: {genre} director: {director} cast: {cast}\"        return self.beamSearch(text, seq_len, seq_num)    def genreDirectorCastEthnicityToPlot(self, genre, director, cast, ethnicity, seq_len, seq_num):        text = f\"generate plot for genre: {genre} director: {director} cast: {cast} and ethnicity: {ethnicity}\"        return self.beamSearch(text, seq_len, seq_num)\\r\\n    \\r\\n    def genreCastToPlot(self, genre, cast, seq_len, seq_num):        text = f\"genreate plot for genre: {genre} and cast: {cast}\"        return self.beamSearch(text, seq_len, seq_num)    def genreEthnicityToPlot(self, genre, ethnicity, seq_len, seq_num):        text = f\"generate plot for genre: {genre} and ethnicity: {ethnicity}\"        return self.beamSearch(text, seq_len, seq_num)    def returnPlot(self, genre, director, cast, ethnicity, seq_len, seq_num):\\r\\n        console.log(\\'Got genre: \\', genre, \\'director: \\', director, \\'cast: \\', cast, \\'seq_len: \\', seq_len, \\'seq_num: \\', seq_num, \\'ethnicity: \\',ethnicity)\\r\\n        \\r\\n        seq_len = 200 if not seq_len else int(seq_len)\\r\\n        \\r\\n        seq_num = 1 if not seq_num else int(seq_num)\\r\\n        \\r\\n        if not director and not cast and not ethnicity:            return self.genreToPlot(genre, seq_len, seq_num), \"Pass\"\\r\\n        \\r\\n        elif genre and director and not cast and not ethnicity:            return self.genreDirectorToPlot(genre, director, seq_len, seq_num), \"Pass\"        elif genre and director and cast and not ethnicity:            return self.genreDirectorCastToPlot(genre, director, cast, seq_len, seq_num), \"Pass\"        elif genre and director and cast and ethnicity:            return self.genreDirectorCastEthnicityToPlot(genre, director, cast, ethnicity, seq_len, seq_num), \"Pass\"        elif genre and cast and not director and not ethnicity:            return self.genreCastToPlot(genre, cast, seq_len, seq_num), \"Pass\"\\r\\n        \\r\\n        elif genre and ethnicity and not director and not cast:            return self.genreEthnicityToPlot(genre, ethnicity, seq_len, seq_num), \"Pass\"        else:            return \"Genre cannot be empty\", \"Fail\"\\n\\n\\nSave the\\xa0predict.py\\xa0file and then run the\\xa0app.py\\xa0file in debug mode using,\\n\\npython app.py\\n\\n\\n\\xa0\\nTest your API\\n\\xa0\\n\\nCreate a\\xa0test_api.py\\xa0file and execute\\n\\n\\n# test_api.py\\r\\nimport requests\\r\\nimport osurl = \"<http://localhost:5000/api/generatePlot>\"\\r\\njson = {\\r\\n    \"genre\": str(input(\"Genre: \")),\\r\\n    \"director\": str(input(\"Director: \")),\\r\\n    \"cast\": str(input(\"Cast: \")),\\r\\n    \"ethnicity\": str(input(\"Ethnicity: \")),\\r\\n    \"num_plots\": int(input(\"Num Plots: \")),\\r\\n    \"seq_len\": int(input(\"Sequence Length: \")),\\r\\n}r = requests.post(url, json = json)\\r\\nprint(r.json())\\n\\n\\n\\xa0\\nHow to run with\\xa0gunicorn\\xa0?\\n\\xa0\\nUsing\\xa0gunicorn\\xa0with\\xa0flask\\xa0is very easy. While installing the requirements at the start we have installed the\\xa0gunicorn\\xa0command and now we need to go to the folder where the\\xa0app.py\\xa0file is located via. the terminal and run the following command\\n\\ngunicorn -k gthread -w 2 -t 40000 --threads 3 -b:5000 app:app\\n\\n\\nThe format and flags we use above represent the following\\n\\nk: kind (type of workers)-\\xa0gthread,\\xa0gevent, etc...\\nw: number of workers\\nt: timeout time\\nthreads: number of threads per worker\\nb: bind port number\\n\\nIf your filename is\\xa0server.py\\xa0or\\xa0flask_app.py\\xa0the\\xa0app:app\\xa0part will change to\\xa0server:app\\xa0or\\xa0flask_app:app\\n\\xa0\\nIn Summary\\n\\xa0\\nIn this blog, we saw how can we use our previously trained T5 transformer to generate storylines and deploy it using\\xa0flask\\xa0and\\xa0gunicorn. This blog is made to be followed pretty easily so you don\\'t waste time going around different platforms to check out the issues. Hope you have fun reading and implementing this.\\n\\xa0\\nBio: Vatsal Saglani (@saglanivatsal) is a Machine Learning Engineer at Quinnox.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHugging Face Transformers Package – What Is It and How To Use It\\nMultilingual CLIP with Huggingface + PyTorch Lightning\\nGPT-2 vs GPT-3: The OpenAI Showdown',\n",
       " \"By Matthew Mayo, KDnuggets.\\ncomments\\n\\n\\xa0\\nDr. Tuomo Hiippala, Assistant Professor in English Language and Digital Humanities in the Department of Languages at the University of Helsinki, has shared his videos and other learning materials for a pair of courses that he teaches, all in a single website for those looking to learn Applied Language Technology.\\nWhile it appears that some of the material is not available to users beyond the University, specifically at least one hosted instance of the course code notebooks, besides the course website, the videos are all available in a single playlist as well. \\nHere is a high-level overview of what you can expect from these courses:\\n\\nTogether, these two courses provide an introduction to applied language technology for audiences who are unfamiliar with language technology and programming. The learning materials assume no previous knowledge of the Python programming language.\\n\\n\\xa0\\n\\n\\xa0\\nThe material is broken down into 3 major top-level learning components:\\n\\nPart I: A Minimal Introduction to Python\\nPart II: Working with Text in Python\\nPart III: Natural Language Processing for Linguists\\n\\nThe material has a language focus and emerges from the field of linguistics — in contrast to a technology focus, emerging from the field of computer science — as evidenced by the following, which is a different perspective from most natural language processing courses and learning materials I have done my best to highlight in the past:\\n\\nInstead of treating text simply as data and a source of some information to be extracted, these learning materials emphasise text as the product of linguistic processes, which are inextricably related to language use in society. If you are already familiar with language technology, these materials may hopefully broaden your perspectives on language.\\n\\n\\xa0\\nBut don't let that fool you; you will be delving into the technologies available via existing tools in the Python ecosystem to work on applied language tasks. What this means, notably, is that you will use existing libraries to accomplish these goals, as opposed to writing your own implementations of NLP algorithms, a fact which which should not be surprising given the applied nature of this course.\\nAlso, you should not be intimidated by anything covered. Hiippala starts slowly with basic Python, moves on to how to think about text in relation to technology, and then on to more advanced applied NLP, so there is a lot of hand-holding and explanation along the way. This course assume no knowledge of either the technological or linguistic sides of this equation, and so is a great fit for any beginner level learner.\\nComing out on the other side of this course with a knowledge of Python, text processing, and general NLP usefulness and application, you would be well positioned to then take on more advanced NLP learning materials and state of the art approaches to applied NLP, numerous courses of which exist. I could recommend the recently-released Hugging Face course on this very subject.\\nDid I mention this is all freely-available? Thanks goes out to Dr. Tuomo Hiippala at the University of Helsinki for such great material.\\n\\xa0\\nRelated:\\n\\nThe Best Way to Learn Practical NLP?\\nHow to Create and Deploy a Simple Sentiment Analysis App via API\\nThe Essential Guide to Transformers, the Key to Modern SOTA AI\",\n",
       " 'By Hadrien Jean, Machine Learning Scientist.\\ncomments\\n\\n\\xa0\\nIn the chapter 02 of\\xa0Essential Math for Data Science, you can learn about basic descriptive statistics and probability theory. We’ll cover probability mass and probability density function in this sample. You’ll see how to understand and represent these distribution functions and their link with histograms.\\nDeterministic\\xa0processes give the same results when they are repeated multiple times. This is not the case for random variables, which describe\\xa0stochastic\\xa0events, in which randomness characterizes the process.\\nThis means that random variables can take various values. How can you describe and compare these values? One good way is to use the probability that each outcome will occur. The probability distribution of a random variable is a function that takes the sample space as input and returns probabilities: in other words, it maps possible outcomes to their probabilities.\\nIn this section, you’ll learn about probability distributions for discrete and continuous variables.\\n\\xa0\\nProbability Mass Functions\\n\\xa0\\nProbability functions of discrete random variables are called\\xa0probability mass functions\\xa0(or PMF). For instance, let’s say that you’re running a dice-rolling experiment. You call\\xa0X\\xa0the random variable corresponding to this experiment. Assuming that the die is fair, each outcome is\\xa0equiprobable: if you run the experiment a large number of times, you will get each outcome approximately the same number of times. Here, there are six possible outcomes, so you have one chance over six to draw each number.\\nThus, the probability mass function describing\\xa0X\\xa0returns\\xa01616\\xa0for each possible outcome and 0 otherwise (because you can’t get something different than 1, 2, 3, 4, 5 or 6).\\nYou can write\\xa0,  and so on.\\nProperties of Probability Mass Functions\\nNot every function can be considered as a probability mass function. A probability mass function must satisfy the following two conditions:\\n\\nThe function must return values between 0 and 1 for each possible outcome:\\n\\n\\n\\nThe sum of probabilities corresponding to all the possible outcomes must be equal to 1:\\n\\n\\nThe value of\\xa0x\\xa0can be any real number because values outside of the sample space are associated with a probability of 0. Mathematically, for any value\\xa0x\\xa0not in the sample space\\xa0S,\\xa0P(x)=0.\\nSimulation of the Dice Experiment\\nLet’s simulate a die experiment using the function\\xa0np.random.randint(low, high, size)\\xa0from Numpy which draw\\xa0n\\xa0(size) random integers between\\xa0low\\xa0and\\xa0high\\xa0(excluded). Let’s simulate 20 die rolls:\\n\\n\\r\\nrolls = np.random.randint(1, 7, 20)\\r\\nrolls\\r\\n\\n\\n\\n\\narray([6, 3, 5, ..., 6, 5, 1])\\r\\n\\n\\n\\nThis array contains the 20 outcomes of the experiment. Let’s call\\xa0X\\xa0the discrete random variable corresponding to the die rolling experiment. The probability mass function of\\xa0X\\xa0is defined only for the possible outcomes and gives you the probability for each of them.\\nAssuming the die is fair, you should have an\\xa0uniform distribution, that is, equiprobable outcomes.\\nLet’s visualize the quantity of each outcome you got in the random experiment. You can divide by the number of trials to get the probability. Let’s use\\xa0plt.stem()\\xa0from Matplotlib to visualize these probabilities:\\n\\nval, counts = np.unique(rolls, return_counts=True)\\r\\nplt.stem(val, counts/len(rolls), basefmt=\"C2-\", use_line_collection=True)\\r\\n\\r\\n\\n\\n\\n\\nFigure 1: Probability mass function of the random variable\\xa0X\\xa0corresponding to a die rolling a six-sided die estimated from 20 rolls.\\nWith a uniform distribution, the plot would have the same height for each outcome (since the height corresponds to the probability, which is the same for each outcome of a die throw). However, the distribution shown in Figure 1 doesn’t look uniform. That’s because you didn’t repeat the experiment enough: the probabilities will stand when you repeat the experiment a large number of times (in theory, an infinite number of times).\\nLet’s increase the number of trials:\\n\\nthrows = np.random.randint(1, 7, 100000)\\r\\nval, counts = np.unique(throws, return_counts=True)\\r\\nplt.stem(val, counts/len(throws), basefmt=\"C2-\", use_line_collection=True)\\n\\n\\n\\nFigure 2: Probability mass function of the random variable\\xa0X\\xa0corresponding to a die rolling experiment estimated from 100,000 rolls.\\nWith enough trials, the probability mass function showed in Figure 2 looks uniform. This underline the importance of the number of trials from a frequentist probability point of view.\\n\\xa0\\nProbability Density Functions\\n\\xa0\\nWith continuous variables, there is an infinite number of possible outcomes (limited by the number of decimals you use). For instance, if you were drawing a number between 0 and 1 you might get an outcome of, for example, 0.413949834. The probability of drawing each number tends towards zero: if you divide something by a very large number (the number of possible outcomes), the result will be very small, close to zero. This is not very helpful in describing random variables.\\nIt is better to consider the probability of getting a specific number within a range of values. The\\xa0y-axis of probability density functions is not a probability. It is called a\\xa0probability density\\xa0or just\\xa0density. Thus, probability distributions for continuous variables are called\\xa0probability density functions\\xa0(or PDF).\\nThe integral of the probability density function over a particular interval gives you the probability that a random variable takes a value in this interval. This probability is thus given by the area under the curve in this interval (as you can see in\\xa0Essential Math for Data Science).\\nNotation\\nHere, I’ll denote probability density functions using a lowercase\\xa0p. For instance, the function\\xa0p(x)\\xa0gives you the density corresponding to the value\\xa0x.\\nExample\\nLet’s inspect an example of probability density function. You can randomly draw data from a normal distribution using the Numpy function\\xa0np.random.normal\\xa0(you’ll find more details about the normal distribution in\\xa0Essential Math for Data Science).\\nYou can choose the parameters of the normal distribution (the mean and the standard deviation) and the number of samples. Let’s create a variable\\xa0data\\xa0with 1,000 values drawn randomly from a normal distribution with a mean of 0.3 and a standard deviation of 0.1.\\n\\nnp.random.seed(123)\\r\\ndata = np.random.normal(0.3, 0.1, 1000)\\r\\n\\n\\n\\nLet’s look at the shape of the distribution using an histogram. The function\\xa0plt.hist()\\xa0returns the exact values for the\\xa0x- and\\xa0y-coordinates of the histogram. Let’s store this in a variable called hist for latter use:\\n\\nhist = plt.hist(data, bins=13, range=(-0.3, 1))\\r\\n\\n\\n\\n\\nFigure 3: Histogram of the data generated from a normal distribution. The\\xa0x-axis is the value of the element in the vector and the\\xa0y-axis the number of elements (count) that are in the corresponding range.\\nHistograms\\nHistograms\\xa0show how values are distributed. It is a way to model a probability distribution using a finite number of values from the distribution. Since we\\'re dealing with continuous distributions, this histogram corresponds to the number of values for specific intervals (the intervals depends on the parameter\\xa0bins\\xa0in the function\\xa0hist()).\\nFor instance, Figure 3 shows that there are around 347 elements in the interval (0.2, 0.3). Each bin corresponds to a width of 0.1, since we used 13 bins to represent data in the range -0.3 to 1.\\nLet’s have a closer look at the distribution with more bins. You can use the parameter\\xa0density\\xa0to make the\\xa0y-axis correspond to the probability density instead of the count of values in each bin:\\n\\nhist = plt.hist(data, bins=24, range=(-0.2, 1), density=True)\\n\\n\\n\\nFigure 4: Histogram using 30 bins and density instead of counts.\\nYou can see in Figure 4 that there are more bins in this histogram (24 instead of 13). This means that each bin has now a smaller width. The\\xa0y-axis is also on a different scale: it corresponds to the density, not the counter of values as before.\\nTo calculate the probability to draw a value in a certain range from the density, you need to use the area under the curve. In the case of histograms, this is the area of the bars.\\nLet’s take an example with the bar ranging from 0.2 to 0.25, associated with the following density:\\n\\nprint(f\"Density: {hist[0][8].round(4)}\")\\r\\nprint(f\"Range x: from {hist[1][8].round(4)} to {hist[1][9].round(4)}\")\\n\\n\\n\\nDensity: 2.8\\r\\nRange x: from 0.2 to 0.25\\n\\n\\nSince there are 24 bins and the range of possible outcomes is from -0.2 to 1, each bar corresponds to a range of\\xa0. In our example, the height of the bar (the one from 0.2 to 0.25) is around 2.8, so the area of this bar is\\xa0. This means that the probability of getting a value between 0.2 and 0.25 is around 0.14, or 14%.\\nYou saw that the sum of the probabilities must be equal to one, so the sum of the bar’s areas should be equal to one. Let’s check that: you can take the vector containing the densities (hist[0]) and multiply it by the bar width (0.05):\\n\\n(hist[0] * 0.05).sum().round(4)\\n\\n\\n\\n1.0\\n\\n\\nAll good: the sum of the probabilities is equal to one.\\nFrom Histograms to Continuous Probability Density Functions\\nHistograms represent a binned version of the probability density function. Figure 5 shows a representation of the true probability density function. The blue shaded area in the figure corresponds to the probability of getting a number between 0 and 0.2 (the area under the curve between 0 and 0.2).\\n\\nFigure 5: The probability to draw a number between 0 and 0.2 is the highlighted area under the curve.\\nProperties of Probability Density Functions\\nLike probability mass functions, probability density functions must satisfy some requirements. The first is that it must return only non negative values. Mathematically written:\\n\\nThe second requirement is that the total area under the curve of the probability density function must be equal to 1:\\n\\nIn this part on probability distributions, you saw that probability mass functions are for discrete variables and probability density functions for continuous variables. Keep in mind that the value on the\\xa0y\\xa0axis of probability mass functions are probabilities, which is not the case for probability density functions. Look at the density values (for instance in Figure 4): they can be larger than one, which shows that they are not probabilities.\\n\\xa0\\nBio: Hadrien Jean is a machine learning scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog (hadrienj.github.io).\\nOriginal. Reposted with permission.\\nRelated:\\n\\nEssential Math for Data Science: Integrals And Area Under The Curve\\nBoost your data science skills. Learn linear algebra.\\nEssential Math for Data Science:\\u200a ‘Why’ and ‘How’',\n",
       " \"comments\\nBy Jean-Yves Stephan, Data Mechanics.\\n\\nThe Spark UI is the open source monitoring tool shipped with Apache Spark, the #1 big data engine. It generates a lot of frustration among Apache Spark users, beginners and experts alike.\\n\\n“It’s hard to understand what’s going on.”\\n“Even if there’s a critical information, it’s buried behind a lot of noisy information that only experts know how to navigate.”\\n“There’s a lot of tribal knowledge involved.”\\n“The Spark history server is a pain to setup.”\\n\\nData Mechanics is a YCombinator startup building a serverless platform for Apache Spark — a Databricks, AWS EMR, Google Dataproc, or Azure HDinsight alternative — that makes Apache Spark more easy-to-use and performant.\\nIn this article, we present our ambition to replace the Spark UI and Spark History Server with a free and cross-platform monitoring tool for Spark called the Data Mechanics UI. The project is at the prototype phase, but we'd love your feedback before we push it to production.\\n\\xa0\\nWhat’s wrong with the Spark UI?\\n\\xa0\\n\\nThe familiar Spark UI (jobs page).\\nIt’s hard to get the bird’s eye view of what is going on.\\n\\nWhich jobs/stages took most of the time?\\nHow do they match with my code?\\nIs there a critical stability or performance issue?\\nWhat is the bottleneck of my app (I/O bound, CPU bound, memory bound)?\\n\\nThe Spark UI lacks essential node metrics (CPU, Memory, and I/O usage).\\n\\nWithout them, any significant infrastructure change is a dangerous leap of faith.\\nTo enable them, you need to set up a separate metrics monitoring system (such as Ganglia, or Prometheus + Grafana), and then constantly jump back and further between this system and the Spark UI, trying to match the timestamps.\\n\\nThe Spark History Server (rendering the Spark UI for terminated Spark apps) is hard to setup.\\n\\nYou need to persist Spark event logs to long-term storage and often deploy it yourself.\\nIt can take forever to load, and it often crashes.\\n\\n\\xa0\\nWhat does the Data Mechanics UI look like?\\n\\xa0\\n\\nThis GIF shows our prototype Data Mechanics UI in action!\\nWhat is new about it? Let's go over the main sections.\\nSummary statistics\\n\\nTheData Mechanics UI - Summary Statistics.\\nThe section shows the duration of the app, the total amount of resources (CPU uptime), the total duration of all the Spark tasks (should be close to your CPU uptime if your app is well parallelised). This information - surprisingly hard to get! - is critical if you care about your infrastructure costs.\\nRecommendations\\n\\nThe Data Mechanics UI – Recommendations.\\nThis section builds upon the Data Mechanics platform auto-tuning feature where infrastructure parameters and Spark configurations are continuously optimized to boost performance and stability based on the history of the past runs of a given application.\\nThis section gives high-level actionable feedback to developers, such as:\\n\\n“Job 4 suffers from an input data skew. Consider repartitioning your data or salting the partition key”.\\n“The default number of tasks (200) is too small compared to the number of CPU cores (400) available. Increase\\xa0spark.sql.shuffle.partitions\\xa0to 1200.”\\n\\nExecutors CPU Usage\\n\\nThe Data Mechanics UI - Executors CPU Usage.\\nThis screen lets you visually align system metrics on CPU utilization with the different Spark phases of your app. In a couple of seconds, you should see if your app is spent on an expensive shuffle operation, if a lot of resources are wasted due to inefficient parallelism, or if it is bottlenecked by I/O operations or CPU-intensive operations.\\nSo this information is critical to understand your application performance and make smarter choices. You can then click on a specific job or stage to dive deeper into the problematic phase.\\nExecutors Peak Memory Usage\\n\\nThe Data Mechanics UI - Executors Peak Memory Usage.\\nThis screen shows you the memory usage breakdown for each executor when the total memory consumption was at its peak. Again, you'll immediately see if you're flirting with your container memory limits (maybe hitting OutOfMemory issues) or, on the contrary, if your memory is largely overprovisioned.\\nMemory issues are the most common sources of crashes for Apache Spark. OutOfMemory comes in two flavors:\\n\\nThe JVM can run out of memory. The JVM heap grew to its maximum size, and despite a full GC, it couldn't allocate more space. This can happen due to skewed shuffles, high concurrency, improper use of caching, or simply too small heap size settings.\\nThe cluster manager (like YARN or, our favorite, Kubernetes) can kill a container because it exceeded its memory limit. This happens a lot when using PySpark, as a Spark executor will spawn one python process per running task, and these processes memory usage can quickly add up. This is very hard to troubleshoot for PySpark users, as almost no monitoring tool reports the memory usage of python processes, even though PySpark makes up the larger portion of the Spark community.\\n\\nSo this screen should give you critical information to make and keep your Spark applications stable.\\n\\xa0\\nHow does the Data Mechanics UI work? How can I use it?\\n\\xa0\\nDue to technical reasons, the Data Mechanics UI will not be implemented in Spark open-source. But it will work on top of any Spark platform, entirely free of charge.\\nTo use it, you’ll need to install an agent - a single jar attached to Spark. The code for the agent will be open-sourced, and we'll provide init scripts to install it automatically for each major Spark platform. Once this is done, you're done! The agent will send the Spark event logs to the Data Mechanics backend infrastructure, which will serve the Data Mechanics UI in your web browser!\\nInitially, it will only be available for terminated apps (a few minutes after they've run), so it will be more of Spark History Server than a live Spark UI replacement. We hope it'll be useful to you nonetheless!\\n\\xa0\\nConclusion: We need YOU to make this happen\\n\\xa0\\nData Mechanics\\xa0is a managed platform for\\xa0Apache Spark - like Amazon EMR, Google Dataproc, Databricks, and others. Our serverless features make Spark more easy-to-use and performant. It is deployed inside our customer's cloud account on a Kubernetes cluster that we manage for them, and it is available on AWS, GCP, and Azure.\\nThe Data Mechanics UI will be a great complement to this platform — it would give Spark developers the high-level feedback about their code that they need to develop, scale, and maintain stable and performant Spark applications.\\nUPDATE November 2020. A first milestone of Data Mechanics Delight has been released. For now it only consists of a free hosted Spark History Server, but the team is planning a next release with new screens and metrics by January. Check out our website to sign up.\\n\\xa0\\nBio:\\xa0Jean-Yves Stephan, a former software engineer and Spark infrastructure lead at Databricks,\\xa0is now the\\xa0Co-Founder and CEO at Data Mechanics, a serverless platform making Apache Spark easy to use and performant.\\nRelated:\\n\\nThe Benefits & Examples of Using Apache Spark with PySpark\\nNitpicking Machine Learning Technical Debt\\nPractical Apache Spark in 10 Minutes\",\n",
       " \"By Tom Baeyens, CTO and Co-Founder, Soda.io\\n\\xa0\\nWhy data management?\\n\\xa0\\nIn the last three years I’ve transitioned from being a software engineer to a data engineer. I fell into the area of data management when Maarten Masschelein, my fellow co-founder at Soda, and I started working together to solve the problem of data issues that are silent and undetected. Coming from a software engineering background, writing unit tests and monitoring applications in production is a given but in data, it’s quite different. Whilst most organizations are aware they should test, there is no strategy in place and they just don’t know how to start addressing the problem which leaves their systems exposed and can result in serious downstream issues for the data products they are building.\\nWith more and more products being built using data as the core input, it’s never been more important to test and monitor the quality of data being used. And so we set about building a data observability platform that enables organizations to discover, prioritize and resolve data issues.\\n\\xa0\\nDefine good data quality\\n\\xa0\\nWe started with Soda SQL, made available in February 2021. It’s our first open source data testing, monitoring and profiling tool for data-intensive environments. It works with your existing data engineering workflows to create a quick and easy way to define what good quality data means to your business. This enables data engineers to define tests and protect against the silent data issues that go undetected in datasets, data lakes, and data warehouses.\\n\\xa0\\nOpen source to the rescue\\n\\xa0\\nSoda SQL is an open source tool with simple Command Line Interface (CLI) and Python library to test your data through metric collection. It utilizes YAML config files as input to prepare SQL queries that run tests on tables in a database to compute a wide range of metrics and tests. It's super easy to find invalid, missing, or unexpected data. Because Soda SQL leverages --you guessed it-- SQL, the data can stay where it is and existing compute engines can be leveraged.\\nIf tests fail, Soda SQL allows you to stop the pipeline and prevent bad data from causing damage. As metrics are computed, diagnostic information is captured as well to help with the analysis if a data issue is detected. Steps can then be taken to prioritize and collaboratively resolve issues as one data team. Soda SQL can be used manually on its own or integrated with a data orchestration tool to schedule scans and automate actions based on scan results.\\nYou can check out the 5-minute tutorial on how to get started but here’s a quick example:\\n\\nSimple metrics and tests can be configured in scan YAML configuration files. An example of the contents of such a file is as follows:\\n\\n\\n\\nBased on these configuration files, Soda SQL will scan your data each time new data arrived like this:\\n\\n\\n\\xa0\\n\\xa0\\nBring everyone closer to the data\\n\\xa0\\nWe have just released Soda Cloud, which is a web application where the Soda SQL metrics and test results can be monitored over time. Soda Cloud creates transparency from engineers to other people in the data team. With this collaboration data teams get ahead of the silent data issues. Soda Cloud extends Soda SQL and the two work together seamlessly.\\nFirst of all Soda Cloud extends Soda SQL with a metrics database so that measurements and test results can be visualized over time. This enables monitoring change over time and anomaly detection on all of the metrics.\\nThese visualizations and data profiles already create transparency between different people in the larger data team. All people in the data team get to see what data is actually present, what tests are performed.\\nBut the Soda Cloud goes one step further. It enables non-technical people to build and maintain their own monitors in a simple UI with a 3-step wizard. This is important because it removes the bottleneck to monitoring the domain knowledge that Subject Matter Experts have. If they don't need to involve data engineers to get their domain logic tested, that means a lot more of that domain knowledge will be used to define what good data looks like. And as a result, a lot more bad data will be captured preventing various kinds of damages.\\nSoda Cloud prescriptively solves the problem of discovering the silent data issues, by giving data teams a central platform to track and score the health of data across core quality dimensions.\\nData and analytics engineers are equipped with a way to test data each and every time it transforms to ensure data pipelines are reliable. Via Soda SQL, data production can be stopped and quarantined. Soda Cloud visualizes the health of data sets and acts as a communication hub for data issues.\\n\\n\\xa0\\n\\n\\xa0\\nData consumers and producers can now easily align on what’s important, what’s expected, and what to measure so that data remains fit for purpose. We’ve also built integrations with email and Slack to ensure the right people are alerted, at the right time to diagnose, prioritize and resolve the data issues.\\n\\n\\xa0\\n\\n\\xa0\\nWe’re on a mission to bring everyone closer to the data, as we believe that data quality is a team sport. Everyone who has a stake in the data (and we think that’s everyone in the business nowadays), needs to understand it, trust it, and stay on top of it.\\nMy main responsibility at Soda is to ensure data engineers love using our products and help them solve real problems quickly. We help solve the problem with a combination of a cloud platform and a set of open source developer tools, that give data teams the configurability they need to create end-to-end observability.\\nGood quality data is for everyone. Access Soda SQL on GitHub and Soda Starter, our free trial, on Soda.io (extended to June 30, 2021). Our Slack Community and Docs contain best practices and helpful resources.\\nGet ahead of the silent data issues. Good luck!\",\n",
       " 'Sponsored Post.\\n\\n\\xa0\\n\\xa0\\nSpeaking at DataOps Summit showcases your leadership in the DataOps movement. We are looking for Data Rock Stars versed in building, deploying, and operationalizing DataOps practices and procedures. The Call for Papers is open until May 31st so submit your abstract now!\\n\\xa0\\nSubmit\\xa0at https://www.dataopssummit-sf.com/about\\xa0\\n\\xa0\\nYou may be asking, What is DataOps Summit 2021?\\n\\xa0\\nDataOps Summit 2021 is the premier professional conference dedicated to best practices, thought leadership, and technical education for the emerging domain of DataOps. To be held September 28-30, virtually, the DataOps Summit brings together enterprise data professionals, industry experts, and key technology companies in the DataOps ecosystem.\\xa0\\n\\xa0\\nThe three-day conference will feature keynotes, three speaking tracks with over 30 sessions, hands-on training, and a Learning Zone, where customers, partners, and industry peers will gather to learn, educate each other, and network with other professionals amid the emerging trends in DataOps.\\xa0\\n\\xa0\\nSubmit for CFP\\xa0and we hope to see you there!',\n",
       " \"comments\\nBy Ahmed Besbes, AI Engineer // Blogger // Runner.\\n\\nsource\\nThis comes out as a personal observation, but I’m sure that many of you will share the same feeling upon reading this post.\\nI’m a data scientist, and I like my job because I think it covers various interdependent domains that make it rich and stimulating. However, I sometimes have to deal with people who don’t exactly understand this role in the organization nor the field in general. This, quite frankly, is what makes things a little bit\\xa0frustrating\\xa0for me and also for a lot of people I know.\\nBefore you keep reading, I should mention that I don’t aim to discourage anyone from aspiring to this role. I’m only stating some negative aspects that occur in the industry in general and the possible solutions for avoiding them.\\n\\xa0\\nSome people don’t exactly understand what you do … and don’t even bother to explain!\\n\\xa0\\n\\nimgflip\\nIn principle, this is fine. I don’t understand what most other people do either. What I do not get, however, is the total lack of interest and curiosity of some parties in learning about what you do while helping them. I don’t mean they should get every small algorithmic detail of your neural network, for example, but at least, they should get to know your approach, your way of solving the problem. Sometimes, it’s as if you were commissioned with the painful, dirty task that no one cares about.\\nSome project managers take zero interest in what you’re doing unless you’re done doing it. I think these fellas bring management to a whole new level.\\n\\nOh! You’re a data scientist? You must be really good with the numbers. Why don’t you have a look at my files and crunch the data? I hear your “python” thing can pop out the magic real quick. Here, go play with my files and come see me when you’re done.\\n\\n— What to do?\\nTo make everybody on the same page, one solution is to provide training and awareness to the teams who have no technical background. This goes through internal workshops, certifications, or MOOC subscriptions in broad technical topics such as introductory lectures to machine learning, deep learning, or NLP. When building knowledge in these areas, teammates become proactive and more engaged in the building process. Project managers also become aware of the challenges.\\n\\xa0\\nData scientists are still considered marketing tools to pimp proposals\\n\\xa0\\n\\nimgflip\\nWell, this worked quite well ten years ago when the field started to emerge, and the words Hadoop and Spark were all over the place. You could stack all the buzzwords you know and hope for a big check (and it worked!).\\nThis isn’t 2010 anymore. Companies now pay close attention to what you’re willing to sell. They know the market, the competitors, and the challenges. They’ve scanned nearly everything thoroughly. They also know what’s feasible and what’s not. If you don’t stand out of the crowd and are not clear enough about your value proposition and the technical expertise that your data science team can bring, you’re most likely to lose the deal.\\nOf course, despite all of this, there is always some ballsy guy in a suit to make this kind of inspirational statement:\\n\\nLet’s throw a little bit of data science here and there to beef up our pitch and make the client pay a buttload of money!\\n\\nIsn’t that beautiful?\\n— What to do?\\nDon’t act as if a data scientist would completely change and disrupt your organization. The market starts to know what the limitations are. Be in line with the market.\\n\\xa0\\nYou shouldn’t be the tiny hand that doesn’t take enough credit for its work.\\n\\xa0\\n\\nimgflip\\nWe all know this feeling, and it sucks. You bust your a** working hard, and some other guy presents your results and takes all the credit. This is common everywhere and happens even more when you work in a data science team in collaboration with business partners.\\nIf you’re valuable to the team, your colleagues should naturally let you shine in front of the stakeholders. Your voice is then heard and engaged in the decision process.\\nIf you’re feeling, however, that you’re treated like an interchangeable resource or put aside working in the shadow and producing numbers for those who speak, maybe it’s time to rethink your position.\\n— What to do?\\nEveryone is important when building a data product. This should not only be a statement that we tell ourselves. It must materialize in our meetings, presentations, and daily relationships.\\n\\xa0\\nData scientists cannot produce insights upon request\\n\\xa0\\n\\nSource\\xa0dilbert.com.\\nWell, as tempting as it sounds, this is not as easy as we think. Just because we’re equipped with the tools doesn’t necessarily mean that you can expect immediate actionable results. This requires building knowledge about the business, forging the right intuitions, and the assumptions. This takes time, and it’s a learning process.\\n\\nLet’s crunch the data and make it speak.\\n\\n— What to do?\\nAccept the fact that a data scientist has to spend a substantial amount of time learning about the business and building his own intuitions about it. This goes through interviewing different actors in the organization, running all sorts of analyses on the data, experimenting, failing, and getting continuous constructive feedback.\\nIf you also want to provide the best conditions for your data science teams, make sure you have, at least, clean data pipelines with clear descriptions.\\n\\xa0\\nA data scientist cannot be the go-to person for every data related issue\\n\\xa0\\n\\nSource:\\xa0medium.\\nThere’s still a strong misconception about the role of the data scientist. Not only non-technical executives but other colleagues in tech believe that data scientists know their way around Spark, Hadoop, SQL, TensorFlow, NLP, AWS, production-level applications, docker, and more. It’s great to master these tools, but this process takes several years and a lot of experience.\\nIf you’re a data scientist and you’re applying for a company that mentions all of these techy words in one application, double-check the company. It’s possible that it hasn’t a clear vision of its data strategy nor a clear definition of the role it’s hiring for.\\n\\nWe need to fix our data problems. Let’s hire a data scientist.\\n\\n— What to do?\\nA data scientist is not always the ultimate solution to your\\xa0data problems—double-check before hiring. Maybe what you need is a data analyst or a back-end developer. A data scientist is not a ninja that masters everything.\\n\\xa0\\nPro-tip for those who want to build a strong data team\\n\\xa0\\nIf you want your team to succeed in building whatever you intend to build, make sure you surround yourself with complementary skills.\\nAt the delivery level:\\n\\nData scientiststo build sophisticated machine learning models, draw complex analyses, and formulate business needs in terms of metrics.\\nData Engineers\\xa0to build, among other things, robust data pipelines so that data is clean and accessible for the data science team at any time\\nML / AI Engineers:\\xa0this is a new role emerging in the field. I see it as a hybrid profile between a data scientist and a data engineer. In practice, it’s a data scientist who goes beyond modeling and thinks about deployment aspects. Questions he solves, for example, are:\\xa0how do I make the model scalable? How do I dockerize my application properly? How do I ensure low latency at inference time? etc.\\nFront and back-end devs\\xa0to build web applications that integrate and package the machine learning logic. They deal with code quality, robustness, security, design, stability, building APIs, etc.\\nA data scientist can find his way around building small web applications but remember that\\xa0this not his expertise.\\xa0If you want a professional mobile or web application, hire a team of developers.\\n\\nAt the management level:\\n\\nData science managers:\\xa0these are the most technical profiles within the management team. They supervise the data science teams and make sure they take the right (modeling) decisions.\\nProject managers:\\xa0They make sure things are on track in terms of deadlines. They spot the blocking issues and interact directly with the business or the client.\\nChier Data Officer (CDO):\\xa0This is the top management role. His goal is to infuse the culture within the organization, to look for the projects, and build the business.\\n\\nThis is based on a compilation of discussions and several feedbacks coming from friends and colleagues.\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Ahmed Besbes\\xa0is a data scientist working across many industries, such as financial services, media, and the public sector. Part of Ahmed's work includes crafting, building, and deploying AI applications to answer business issues.\\nRelated:\\n\\nIf I had to start learning Data Science again, how would I do it?\\nWhat every Data Scientist needs to learn from Business Leaders\\nSoftware engineering fundamentals for Data Scientists\",\n",
       " 'comments\\nBy Prad Upadrashta, SVP & Chief Data Science Officer (AI solutions) at Mastech InfoTrellis\\n\\nA client recently asked if our entity matching algorithms are “antifragile.” This got me thinking. It is a\\xa0really\\xa0interesting question. Bear with me as I take you on a mind trip to explore this question and its implications. First, let’s start with a common understanding of “antifragile.” We’ve all read the google definition: When a system gains from stressors, shocks, volatility, noise, disorder, mistakes, faults, attacks, or failures, it is termed “antifragile.”\\nSo, what does that look like? Well, at first pass, we might think it would look something like this:\\n\\nFigure.\\xa0A linear relationship between\\xa0Entropy and Gain.\\nThe line shows clearly that the system is gaining from disorder, i.e., +1 unit of gain for every +1 unit of disorder (for instance), but this is where things become weird. The fact is, when you are dealing with antifragile systems, they also exhibit a peculiar response function to each incremental increase in the level of disorder, where the previous gains feedback into the system in such a way as to compound the effect – this gives it a curved or convex appearance. This convexity is a critical feature of antifragile systems.\\nSimply, antifragility is defined as a\\xa0convex\\xa0response to a stressor or source of harm (for some range of variation), leading to a positive sensitivity to increase in volatility (or variability, stress, dispersion of outcomes, or uncertainty, what is grouped under the designation \"disorder cluster\"). Likewise, fragility is defined as a concave sensitivity to stressors, leading to a negative sensitivity to an increase in volatility. The relation between fragility, convexity, and\\xa0sensitivity\\xa0to disorder is mathematical, obtained by theorem, not derived from empirical\\xa0data mining\\xa0or some historical narrative. It is\\xa0a priori.\\n—\\u2009Taleb, N. N., Philosophy: \\'Antifragility\\' as a mathematical idea. Nature, 2013 Feb 28; 494 (7438), 430.\\nSo, the line should really be curved upward as follows:\\n\\nFigure.\\xa0Convexity of antifragility where the accelerating curve is due to feedback.\\nCompound interest is a well-known concept and serves here as a useful illustration of how the simple act of re-investing returns can lead to the convexity of the return stream. When you take your prior gains (%s) and re-invest them into a consistently producing process, your prior gains also see gains, and so on ad\\xa0infinitum. As these gains cumulate, we see a snowball effect over time. This results in a curved (accelerating) line, not a straight line. This curvature is convexity. Convex systems are nonlinear. I’m not suggesting here that compound interest is antifragile – rather, it is one practical example of a process in which positive feedback (reinvestment) drives the slope of the function to increase nonlinearly\\nIn his books, Nassim Taleb gives three examples of antifragility: Airlines, Restaurants, and Silicon Valley. All three of these become stronger every time something goes wrong. If an airplane goes down, every manufacturer will take pains to make sure that the next generation of airplanes will never experience the same problem. Silicon Valley is especially interesting because they see every failure or inefficiency in the market as an opportunity, and that leads to value creation through the formation of companies that address the problem. It is important to point out that these systems were not engineered to be antifragile. This realization seems to be\\xa0ex post facto. \\nSo, one open question is whether we can purposefully engineer systems and/or processes to make them antifragile?\\nWithin the subset of natural systems, those that benefit from feedback turn out to be antifragile; for instance, the body’s immune system is strengthened in response to external stressors (viruses, bacteria, etc.). This, in fact, is the basis of all vaccines – the introduction of a weak stressor that triggers the immune system to adapt by learning to recognize the surface proteins that make up the viral shell – so that antibodies can be produced to attack the full-strength virus when it enters the system.\\nMost man-made systems and/or processes do not exhibit this sort of antifragile behavior, though at a process level, they turn out to be antifragile because humans have a natural inclination towards process improvement by studying past failures and adapting themselves accordingly. So, we impose our own antifragile tendencies on the systems we design because we simply can’t leave things alone. The vast majority of the natural physical world exhibits a tendency to decay over time. This is particularly true of the failure modes observed in complex systems (where system failures rise exponentially over time). So, most real systems exhibit fragility, which looks more like this:\\n\\nFigure.\\xa0A failure curve that points downward. As one thing fails, it creates the conditions for something else to fail, leading to negative convexity.\\nIn fragile systems, each failure leads to successively more failures. So, failures compounding on top of failures leads to a rapidly decaying system; again, we note that explicitly that it is not a linearly decaying scenario – intuitively, we know that once something breaks, it sets in motion the tendency for other things to break (especially where there are strong direct or mechanical dependencies). Every failure makes the next failure more likely because failures are not independent of one another – in mechanical systems, in particular, failures tend to be highly dependent. This gives rise to the “right edge” of the so-called “bathtub curve” that is famous in the reliability world. One of the most frequently asked questions in the reliability world is: How likely is it that component B will fail, given that component A has already failed?\\n\\nFigure.\\xa0A “Bathtub Curve” model is used for modeling things like machine component failures. On the far right, you see an accelerating curve that represents the accelerating failure rate of a machine that is in its “wear out” phase.\\nWe can develop an intuition for the behavior of “antifragile” systems by studying and contrasting these extreme cases, i.e., the edges of the “known,” if you will. Studying the extremes of any problem can help you identify the “boundaries” of a problem. By bounding a problem between two lines, we can start to understand the characteristic behaviors we are likely to expect in-between, all the while implicitly making a few mathematical assumptions about continuity and regularity, which we won’t go into here. The point is, we can interpolate between the extremes to understand the characteristics of the system under a variety of different operating parameters.\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Prad Upadrashta has over 20 years of experience, culminating in the role of Chief Data Science Officer at Mastech InfoTrellis. His focus areas are Artificial Intelligence, Machine/Deep Learning, Blockchain, IIoT/IoT, and Industry 4.0.\\nRelated:\\n\\nWhat is Noise?\\n10 Must-Know Statistical Concepts for Data Scientists\\nHow To Overcome The Fear of Math and Learn Math For Data Science',\n",
       " 'comments\\nBy Arnuld on Data, freelance Data Scientist\\nEric Weber (yes, that nice-looking guy with a lovely dog) wrote a post on\\xa0LinkedIn\\xa0recently about 10 things he wished he had done\\xa0less\\xa0when he started his data science career. This post is my journey through those 10 points. First, you should go ahead and\\xa0read his post. Here is a screenshot:\\n\\n\\nOriginal Post\\xa0on LinkedIn by\\xa0Eric Weber\\n\\n\\xa0\\nFirst things first, this is not going to be a “content” post.\\nThere are so many articles and blog-posts on that already, so check them out. Here we will talk about your focus and direction when it comes to your desire to become a data scientist and get noticed by the industry.\\n\\xa0\\n1) Thinking I needed to learn everything\\n\\xa0\\nYeah, this one takes lots of your time and energy. This obstacle is one you should deal with right away. I struggled with it in the beginning but in a few months, it died down. I attribute this breakthrough to my daily reading habit.\\nI keep on reading LinkedIn posts (especially from Eric Weber himself). Also, I read a lot on Towards Data Science, Medium, KDnuggets, and individual blogs from different data scientists and machine learning engineers for an hour or two or more every single day. This has taught me the importance of data science when it comes to industrial work:\\xa0how much value you are adding to an organization with your skill set.\\xa0You define value by building something you have an interest in or by building something to solve a problem. You choose what to learn by answering this question and it will give you an idea of what to learn and what not to.\\nIt took me several months to realize this (I guess 6 months). I will add these months together as we progress point by point to see how much time we could have saved.\\n\\n\\nImage by\\xa0Andrew Martin\\xa0from\\xa0Pixabay\\n\\n\\xa0\\n2) Prepping for interview trivia.\\n\\xa0\\nYes, this is another struggle, primarily because of several reasons:\\n\\nThere is no one agreeable definition of what a data scientist is. Only a vague idea on his job responsibilities and how those responsibilities are different from a data analyst or a machine learning engineer?\\nThen there are confusing job descriptions. Since there is no agreeable definition of data scientist, you will see descriptions who want you to be a master of everything: machine learning, software engineering, Python, R, years of Statistics, Calculus, Linear Algebra, Big-O, and whatnot. Looking at the job description, you feel like you need to be 50+ to apply for the jobs.\\n\\nDon’t fall for this. Don’t take a job description to your heart. Mostly “interview trivia” is a combination of this newness of data science along with a poor communication channel between talent acquisition, data science, and software engineering teams in an organization. Rather than feeling overwhelmed at this, you need to focus on how to crack it.\\nOne way to crack this is by looking at reality. If you know any real-life data scientists, data analysts, and machine learning engineers (offline, in the physical world), it will be a great idea to talk to them about their work. If you don’t know anyone then you can always check blogs and articles.\\nI don’t know any professional in this field offline. So I learned by reading blogs and articles. What I learned is companies get many people for interviews, all of the kind who “know” stuff but very few who have “built” stuff. So focus on building stuff than mere learning and education (e.g. deployment and production are two major things). It took me 5–6 months to realize this.\\n6 + 6 = 12 months so far\\n\\xa0\\n3) Trying to emulate someone else’s path\\n\\xa0\\nAha, this is my favorite :-) because this is where I had wasted most of my time:\\n\\nTetiana Ivanova\\xa0landed a job in 6 months\\nKelly Peng\\xa0landed\\xa0a job in one year\\xa0after she quit her data analyst job\\nNatassha Selvaraj\\xa0landed a job and she is\\xa0studying in college\\nMikko Koskinen\\xa0did not even\\xa0plan to become\\xa0a data scientist\\nThomas Hepner\\xa0felt lost at at anything\\xa0other than Titanic dataset\\xa0and a year later he landed as a data scientist in the industry\\n\\n\\n\\nPhoto by\\xa0Edward Jenner\\xa0from\\xa0Pexels\\n\\n\\xa0\\nLook at\\xa0my profile, I have 4.5 years of experience in software development (C language) and been doing data science for 8 months now and still nowhere near answering this question:\\n\\nWhat’s your favorite machine learning algorithm and why?\\n\\n\\xa0\\nYes, I agree my case looks like the worst case of\\xa0Big-O: O(n^n)\\nI have read hundreds and thousands (no, I am not exaggerating) of blog-posts and articles of the people who have landed data science jobs and changed industries. I traced and emulated their data science journeys into my life, from their thinking-patterns to the choice of their courses, even their choice of certain chapters in certain books, like a perfect\\xa0carbon-copy. And I still failed at answering the question above because I don’t even know why I will like one machine learning algorithm over the other. After all, I am just mindlessly chewing all the models in the name of “becoming like them”.\\nTwo days ago I gave it up and decided to follow what I think I should do. (Surprisingly, I came across Eric’s post today. It is as if Universe is trying to tell me I am on the right path, a path that belongs to me)\\nI think each of us have to personalize our journeys. Our environment, our talent, our experience, our attitude, our work ethic, our backgrounds, and our learning capacities, all are different and unique. That is why maybe tracing someone else’s path never works.\\nSo I decided I will experiment and carve my own path to become a data scientist. This is not to say that I will stop reading other people’s journeys, I will still read but instead of following them blindly and trying to copy it into my life, I will use them as a compass, as a guiding mechanism. This has cost me 8 months. Better late than never though.\\n6 + 6 + 8 = 20 months\\n\\n\\nImage by\\xa0Gerd Altmann\\xa0from\\xa0Pixabay\\n\\n\\xa0\\n4) Focusing on perfect solutions.\\n\\xa0\\nMy computer programming experience took care of this. I spent half of a decade doing programming in the industry, writing code to generate money for my employers, that already taught me\\xa0“done” is better than “perfect”. Finding a problem someone is facing and building a solution is actually the only important thing that matters. Mere learning and education don’t.\\n6 + 6 + 8 + 0 = 20 months\\n\\xa0\\n5) Learning advanced stats I rarely used\\n\\xa0\\nBack in 2018, I spent a lot of time learning Mathematics and Statistics for data science. I spent 4 months studying:\\n\\nAlgebra I and II at Khan Academy\\nCollege Level Algebra and Problem Solving from\\xa0Arizona State University at edX\\nMIT Big Picture Calculus from\\xa0YouTube\\nCalculus Made Easy\\xa0by Silvanus P. Thompson. Available for free from\\xa0Project Gutenberg\\nCalculus 1A: Differentiation from MIT\\xa0at edX.\\nLimits and Integral Calculus from\\xa0Calculus-1 at Khan Academy.\\nReading different books on Statistics\\xa0to get a statistical mindset\\n\\nWhat a mistake it was :-( . From what I know today, all I needed was this:\\n\\nBasics of Statistics. Not Statistics per se but only the topics specifically necessary for Machine Learning and Data Analysis\\nBasics of Bayes Theorem\\nBasics of Linear Algebra (only a few small things like matrix multiplication and transposing etc )\\nBasics of Big-O Notation (Check out\\xa0Interview Cake’s Explanation)\\n\\nYes, nothing fancy but only the basics. All the fancy stuff you can do after you land a job. Till then you use Python or R Libraries. Instead of trying to learn Mathematical formulas just like in school or college, try to learn how to use it using Library calls in Python e.g. calculating t-test using Scipy, and learn the math needed to understand it:\\n3.1. Statistics in Python - Scipy lecture notes\\nA simple linear regression Given two set of observations, x and y, we want to test the hypothesis that y is a linear...\\n\\xa0\\nWell, there went 8–10 months:\\n6 + 6 + 8 + 0 + 10 = 30 months\\n\\n\\nPhoto by\\xa0Vlad Dediu\\xa0from\\xa0Pexels\\n\\n\\xa0\\n6) Thinking the R vs. Python debate required picking just 1.\\n\\xa0\\nI struggled with this one:\\n\\nStarted with\\xa0R for Data Science\\xa0by\\xa0Hadley Wickham. Read a few chapters and then gave up because I read Python is gaining ground in the industrial world.\\nI started with Python and tried a few books and then I came back to R because ggplot looked better than matplotlib.\\nThen I went back to Python because it had a more software Engineering feel to it.\\nWent back to R because tidyverse as a package looked much more mature at data analysis and visualization than Python’s tools.\\n\\nThis problem went away when I got a take-home assignment from a company who approached me for R related work. After using both R and Python for take-home assignment work, I never wanted to touch R again. From my experience Python suits better for software engineering practices and software engineering practices are definitely needed when it comes to writing data science code for real-life industrial work. It is almost the same as when you are doing software development. I went fully Python after that. Personally, If I ever have to use another language, I will use\\xa0Julia\\xa0instead. Around 4–6 months on this.\\n6 + 6 + 8 + 0 + 10 + 4 = 34 months\\n\\xa0\\n7) Spending lots of time thinking about unstructured data\\n\\xa0\\nThis mistake I did after the “the math mistake”. I spent months contemplating SQL vs NoSQL. We look at something and we think of it from our viewpoint and think this is what it means. E.g we all know this is the age of data and millions and millions of megabytes of data is being generated each day. Most of it is unstructured. I guessed I should learn NoSQL. But then almost all of the job descriptions mention only SQL. Then I will think of doing SQL.\\n\\n\\nPhoto by\\xa0Mika Baumeister\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nI learned neither SQL nor NoSQL. This is how being two-minds about a thing kills months of your time.\\nInstead of interpreting things in my way, I started looking at the people who landed data science jobs and what they learned. All of them had listed SQL as a skill. So I switched to SQL. A good place to start is\\xa0SQLBolt.\\nI won’t consider any time wastage here because even though I did not learn anything, I used that time to learn other stuff. So, the equation so far is:\\n6 + 6 + 8 + 0 + 10 + 4 + 0 = 34 months\\n\\xa0\\n8) Thinking about the tech, not the business\\n\\xa0\\nThis is one area where you need a serious change in mindset and I needed such change too. My computer programming background makes me a 100% tech guy who really does not know how to be more than a team-worker. Contributing to the team is where my social and my communication skills ended.\\nI never knew this in beginning but thanks to my reading habit, I came across so many characteristics of data science that put it at odds with other tech jobs. One way I overcome this is by talking about Big Data with people I know or I meet. By explaining data science, machine learning concepts to my friends and other people. But because my freelance work and data science learning require me to spend a lot of time in front of my computer, I don’t get the opportunity to exercise this method much.\\n\\n\\nImage by\\xa0Lorenzo Cafaro\\xa0from\\xa0Pixabay\\n\\n\\xa0\\nData science is not just programming, data science is not just web-development, it is not just about analyzing data and building models. This is half of the story. Another half of data science is being able to communicate to not so tech-savvy people. Business stakeholders, decision-makers in management, and clients are three different types of non-tech people you are going to deal with. So collaborating with people is going to be a big pain if we think of it as “another tech job”. There is an excellent book on communicating data insights titled “Storytelling With Data”\\xa0by\\xa0Cole Nussbaumer Knaflic. It is kind of a must-read.\\nThere is another side to this.\\xa0Business Problems. The model you build, the comparisons you did, and the accuracy you achieved, how it is benefiting the business? You see, a data scientist’s job has no meaning if he can’t bring some profit or benefit or some value addition to the business. This is a hard thing to get hold of and become good at if you come from a tech-background like mine. What the tech-mentality does, in this case, is to make your mind focus only on building the model and analyzing data because it is what we do.\\xa0We do not have a business context.\\nI don’t have a great solution for this because never had any personal experience with it.\\xa0So take my advice with a grain of salt here. Search yourself too. I could only read blogs, posts and articles to understand what to do. I don’t know any product manager either (I have met one or two managers in IT service but I don’t know if that qualifies). The only method I have come across to solve this is two-fold:\\n\\nRead about case-studies, product case-studies. This is what a product manager does. So if you know any product manager (or even a project manager) you should talk to them about how their product/project brought value to the company.\\nRead books like Cracking the PM Interview by\\xa0Gayle Laakmann McDowell\\xa0and\\xa0Jackie (Bodine) Bavaro\\n\\nNot understanding this makes you work on your tech skills long and hard if you are a programmer or a software developer. Wastage of 6 months:\\n6 + 6 + 8 + 0 + 10 + 4 + 0 + 6 = 40 months\\n\\xa0\\n9) Trying to keep up with all the papers\\n\\xa0\\nAnother pitfall you need to avoid. I got stuck in this for a while. I want to implement a paper or two myself but now the first focus of mine is always on “building something”.\\xa0Learn as less as you need to start working on to build something.\\nYes, all those papers look really, really impressive, and beautiful. And papers are mostly about academics. You are trying to land a job in the industry. Academics and industry do not match, with two possible exceptions:\\n\\nYou are looking for a research position within the industry. In this case, your portfolio will be limited to only 10–20% of the employers.\\nYou want to work for the big 4 a.k.a Facebook, Amazon, Google, and Microsoft.\\n\\nExcept for the above, I don’t see any point in drifting from my focus of\\xa0landing a data scientist position at a good tier I or II company.\\xa0Don’t take me wrong, I love to do research. In fact, back in college, I wanted to do a Ph.D. in\\xa0microkernel research. Research work takes a hell of lot of time and energy. I think a better way to live is to find balance in your career:\\xa0a balance between your interests and the market/industry needs. Avoid falling on either side.\\n\\n\\nPhoto by\\xa0Furkan\\xa0from\\xa0Pexels\\n\\n\\xa0\\nInstead of keeping up with all the papers, a better way to balance your learning is:\\n\\nLearn the\\xa0basics of data cleaning using Pandas\\xa0(Kaggle datasets have done the 90% of work for you. In real-life, you gotta do all the cleaning. Learn to scrape some data and clean it)\\nLearn the basics of machine learning modeling and why we choose one model over the other. What kinds of model fit what kind of domain problems e.g. healthcare vs finances\\nLearn how to deploy a model into production (you will know a bit of how the real work feels like when you will use\\xa0Strealmlit,\\xa0Heroku, and\\xa0Voila. I have implemented the bear-detection model\\xa0using Voila here. )\\n\\n6 + 6 + 8 + 0 + 10 + 4 + 0 + 6 + 10= 50 months\\n\\xa0\\n10) Believing there was only one way to do something\\n\\xa0\\nThis one is a biggie. I think I am struggling with this for life. Some people have it and some people don’t. I am inclined to say that maybe smart people don’t have this problem (the smart ones I have met or read about, they don’t). People like me spend a lifetime trying to beat it. It is a jail, trust me. It is quite frustrating to live with a mindset of “only one way to do something”. Ideas don’t have any limits if you look at real-life stories.\\n\\n\\nPhoto by\\xa0Timo Volz\\xa0from\\xa0Pexels\\n\\n\\xa0\\nThis is more of a personal-development obstacle than a technical one because no matter which field you will work in, this one will show up there, it absolutely has nothing to do with the tech. I am still trying to work on it. A solution I have found so far is when I can’t find my way around a problem then I will get off the machine and go for a walk if it is evening or read a completely unrelated book if it is not evening (some non-fiction e.g.) or go on a motorcycle ride and completely forget about the problem. Then I will come back later and try to learn the same thing from a different article or blog post while not referring to the original point where I was stuck. Just a fresh new perspective on the same problem from someone else.\\nI can’t put any time-limit on this. I have struggled this for all of my life:\\n6 + 6 + 8 + 0 + 10 + 4 + 0 + 6 + 10 + Life = 50 + Life\\nSo, I wasted almost 50 months?\\nNot really.\\nAll of these points overlap with each other when it comes to where I wasted time. It is actually 12 months. Dec 2019 to Nov 2020. For a few months, in the beginning, I did not even know what I needed to do. Things started making sense only in March 2020 this year. I think I could have saved 4–6 months if things were clearer to me but this is just a wild guess, some really smart people have told me: it takes whatever time it takes to break down the obstacles. Let me re-iterate:\\nEach of us has a personal data science journey. Our environment, our talent, our experience, our attitude, our work ethic, our backgrounds, and our learning capacities, all are different and unique. That is why maybe tracing someone else’s path never works. That is why you need to keep on pushing yourself to learn what you can, to keep yourself informed of what is going on in the industry and keep on correcting your path (just like apps like maps on our smartphones keep on correcting us and showing the way)\\n\\xa0\\nBONUS — Your Mental Outlook\\n\\xa0\\nI was trying to learn neural networks before I could comprehend what kind of problems logistic regression fits better than linear regression. I was doing deep learning before machine learning made any sense. In my case it was because of:\\n\\nMedia-hype about AI and deep learning\\nMy focus on building something great and truly impressive\\nThe assumption that everyone is doing it and I need to do better than them if I want to land a job. After all, the market is so competitive.\\nFocusing on the big 4\\nI have an interest in healthcare data and\\xa0Practical Deep Learning for Coders\\xa0has chapters on medical imaging diagnosis. You can see\\xa0one example here.\\n\\nDeep learning and AI are in media everywhere. We tend to think we need to be better than everyone else and others are already writing highly mathematical blog posts with their flashy formulas along with lots of code. Don’t believe me? Check\\xa0this out\\xa0then. Who will approach us when such people have already mastered deep learning and data science?\\nYeah, it is so common that they got a name for it. It is called “Imposter Syndrome”. Go read about it a bit. I thought I was\\xa0the only one suffering\\xa0from it. But then I realized it is so common. Yes, the market is competitive and because of\\xa0the current pandemic, many have lost jobs. I have seen posts on LinkedIn where several data scientists and machine learning engineers have lost jobs. I have seen them even literally begging to “like and share” that they are looking for a job. It is heartbreaking to see that. Everyone deserves a good life.\\n\\n\\nPhoto by\\xa0Engin Akyurt\\xa0from\\xa0Pexels\\n\\n\\xa0\\nLet’s look at the positive side, this pandemic has disrupted the world, it has brought many businesses to a halt while some businesses have their client number shot sky-high (podcast and video conferencing services for one). In such disruptive times, we need to be more resilient to pain and suffering and find ways to strengthen our resolve. I believe it is not by chance that we were born in a certain year and that is how we got in the middle of this pandemic. I think we were supposed to learn from it, we are supposed to make\\xa0a better life out of these times. I wish you good luck in your data science learning journey and I hope we keep on learning from each other to make ourselves better.\\n\\xa0\\nBio: Arnuld is an industrial software developer with 5 years of experience working in C, C++, Linux, and UNIX. After transitioning to Data Science and working as data science content writer for over a year, Arnuld currently works as a freelance data scientist.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nMy Data Science Online Learning Journey on Coursera\\nLearn Data Science for free in 2021\\nA Journey from Software to Machine Learning Engineer',\n",
       " 'By Gregory Piatetsky, KDnuggets.  \\n\\nHere are the most popular June 2021 stories on KDnuggets. As you see, a lot of attention was on several blogs on the topic of demand for Data Scientists in 10 years, which was also a topic for KDnuggets poll in June: Relax! Data Scientists will not go extinct in 10 years.\\r\\n\\r\\nStories in green font are also the winners of KDnuggets Top Blog Rewards for June.\\r\\n\\nMost Viewed - Platinum Badge (>32,000 UPV)\\n\\n 5 Tasks To Automate With Python, by Dylan Roy (*)\\r\\n Data Scientists Will be Extinct in 10 Years, by Mikhail Mew (*)\\r\\n How to Generate Automated PDF Documents with Python, by Mohammad Khorasani (*)\\r\\n\\n\\nMost Viewed - Gold Badge (>16,000 UPV)\\n\\n How I Doubled My Income with Data Science and Machine Learning, by Terence Shin\\r\\n Top 10 Data Science Projects for Beginners, by Natassha Selvaraj (*)\\r\\n Pandas vs SQL: When Data Scientists Should Use Each Tool, by Matthew Przybyla (*)\\r\\n Five types of thinking for a high performing data scientist, by Anand Rao\\r\\n Will There Be a Shortage of Data Science Jobs in the Next 5 Years?, by Pranjal Saxena (*)\\r\\n What will the demand for Data Scientists be in 10 years? Will Data Scientists be extinct?, by Matthew Mayo (*)\\r\\n\\n\\nMost Viewed - Silver Badge (> 8,000 UPV)\\n\\n Get Interactive Plots Directly With Pandas, by Parul Pandey\\r\\n How to Make Python Code Run Incredibly Fast, by Pralabh Saxena (*)\\r\\n How to Land a Data Analytics Job in 6 Months, by Natassha Selvaraj (*)\\r\\n Add A New Dimension To Your Photos Using Python, by Dylan Roy (*)\\r\\n Managing Your Reusable Python Code as a Data Scientist, by Matthew Mayo\\r\\n\\n\\n\\n\\nMost Shared - Gold Badge (>500 shares)\\n\\n Data Scientists Will be Extinct in 10 Years, by Mikhail Mew\\r\\n Five types of thinking for a high performing data scientist, by Anand Rao\\r\\n 5 Tasks To Automate With Python, by Dylan Roy\\r\\n Analytics Engineering Everywhere, by Jason Ganz\\r\\n\\n\\nMost Shared - Silver Badge (>300 shares)\\n\\n How I Doubled My Income with Data Science and Machine Learning, by Terence Shin\\r\\n Pandas vs SQL: When Data Scientists Should Use Each Tool, by Matthew Przybyla\\r\\n Top 10 Data Science Projects for Beginners, by Natassha Selvaraj\\r\\n How to Generate Automated PDF Documents with Python, by Mohammad Khorasani\\r\\n\\n\\r\\n(*) indicates that badge added or upgraded based on these monthly results.\\r\\n\\nMost Shareable (Viral) Blogs\\r\\nAmong the top blogs, here are the blogs with the highest ratio of shares/unique views, which suggests that people who read it really liked it. \\r\\n\\n 10 Mistakes You Should Avoid as a Data Science Beginner, by Isabelle Flueckiger\\r\\n Analytics Engineering Everywhere, by Jason Ganz\\r\\n  The Essential Guide to Transformers, the Key to Modern SOTA AI, by Matthew Mayo\\r\\n This Data Visualization is the First Step for Effective Feature Selection, by Benjamin Obi Tayo\\r\\n  Major changes: Where Analytics, Data Science, Machine Learning were applied in 2020/21, by Gregory Piatetsky',\n",
       " \"comments\\nBy Matti Grotheer, startup enthusiast and Co-Founder of Kuwala.\\nThere are a plethora of use cases that require detailed population data. For example, having a detailed breakdown of the demographic structure is a significant factor in predicting real estate prices or finding the perfect retail outlet location. Also, humanitarian projects such as vaccination campaigns or rural electrification plans highly depend on good population data.\\nIt is very challenging to find high-quality and up-to-date data on a global scale for these use cases. Usually, census data is published every four years, which makes those datasets outdated quickly. Arguably the best datasets out there for population densities and demographics are published by Facebook under their Data for Good initiative. They combine official census data with their internal data and leverage machine learning algorithms for image recognition to determine buildings' location and type.\\n\\nFacebook Data for Good and Kuwala (2021).\\nUsing those different sources can give a detailed statistical breakdown of demographic groups in 1-arcsecond blocks, a resolution of approximately 30 meters. Each square contains statistical values for the following demographic groups:\\n\\nTotal\\nFemale\\nMale\\nChildren under 5\\nYouth 15 - 24\\nElderly 60 plus\\nWomen of reproductive age 15 – 49\\n\\nFacebook delivers for each country a file per demographic group, either as a GeoTIFF or CSV. The CSV contains the latitude and longitude of the cell and the respective population value.\\n\\nThe files are stored per country and key metric on 1-arcsecond blocks. This results in gigabytes and millions of rows of data for a single country. If you want to prototype or visualize the data for a single city, you need to browse through the endless files and parse the information.\\nThat is why we created an open-source wrapper that exposes the data through a package. You can directly download the data for entire countries over a CLI. We preprocess the data to make it easily queryable. For that, we are leveraging the power of Uber's H3 spatial indexing.\\nThanks to the H3 indexing, it is easy to build queries on top of the database. Using either H3 cells or coordinate pairs, you can retrieve the population based on a point, a given radius, or polygon. Furthermore, it is straightforward to aggregate the population on a zip code level, for example.\\n\\nUber H3 and Kuwala (2021).\\nThe data integration follows a sequential process. The CSV files for countries and demographic characteristics are automatically loaded and linked by Spark. The data is efficiently stored in a Parquet file. The Parquet file is then automatically loaded to a Neo4j database (graph database). Then, using Cypher, queries can be made for specific polygons, points with a given radius, and different aggregations using H3. For a medium-sized country like Germany (approximately 7-8 GB), the data is processed locally in less than 30 minutes and ready for your spatial analysis.\\nNeo4j was chosen as the database because it can intuitively connect other pipelines of the Kuwala ecosystem. In a similar process, POI information from OpenStreetMap can be loaded and directly related to the demographics data. Many more geo-related sources, such as Google Trends, location-based urban events, or social media data, will follow as connectors to enable you with fast and holistic queries on comparable worldwide datasets.\\n\\nFor quick data exploration and visualization, you can directly create datasets compatible with Kepler.gl or Unfolded.ai to make beautiful maps. We published an example map for Malta. It is directly visible where the highly populated regions are and where the heart of the city is.\\n\\nBy having Facebook's population data now directly queryable, it is much faster to create predictive models or visualizations so data teams can spend time on the value-adding tasks. That is also the main reason why we are building an open-source community for third-party data integration with Kuwala. So if you want to get your hands on more connectors like these, star us on GitHub and join our Slack community.\\nBut our open-source project does not stop here. Our big goal is to facilitate access to external data sources, ensure data quality, and help data scientists quickly develop features that they can incorporate into their modeling. For example, we are planning a Jupyter notebook that can be used to manipulate and observe the data swiftly. So stay tuned for that!\\n\\xa0\\nRelated:\\n\\nThe secret to analysing large, complex datasets quickly and productively?\\n3 Key Data Science Questions to Ask Your Big Data\\nHow Visualization is Transforming Exploratory Data Analysis\",\n",
       " 'By Rahul Agarwal, MLE @ FB | Ex-Walmart DS | MLWhiz.\\ncomments\\nTransformers have become the defacto standard for any NLP tasks nowadays. Not only that, but they are now also being used in Computer Vision and to generate music. I am sure you would all have heard about the GPT3 Transformer and its applications thereof.\\xa0But all these things aside, they are still hard to understand as ever.\\nIt has taken me multiple readings through the Google research\\xa0paper\\xa0that first introduced transformers along with just so many blog posts to really understand how a transformer works.\\nSo, I thought of putting the whole idea down in as simple words as possible along with some very basic Math and some puns as I am a proponent of having some fun while learning. I will try to keep both the jargon and the technicality to a minimum, yet it is such a topic that I could only do so much. And my goal is to make the reader understand even the goriest details of Transformer by the end of this post.\\nAlso, this is officially my longest post both in terms of time taken to write it as well as the length of the post. Hence, I will advise you to Grab A Coffee.\\xa0☕️\\nSo, here goes — This post will be a highly conversational one and it is about “Decoding The Transformer”.\\nQ: So, Why should I even understand Transformer?\\nIn the past, the LSTM and GRU architecture(as explained here in my past\\xa0post\\xa0on NLP) along with attention mechanism used to be the State of the Art Approach for Language modeling problems (put very simply, predict the next word) and Translation systems. But, the main problem with these architectures is that they are recurrent in nature, and the runtime increases as the sequence length increases. That is, these architectures take a sentence and process each word in a\\xa0sequential\\xa0way, and hence with the increase in sentence length the whole runtime increases.\\nTransformer, a model architecture first explained in the paper Attention is all you need, lets go of this recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. And that makes it FAST.\\n\\n\\nSource\\n\\n\\xa0\\nThis is the picture of the full transformer as taken from the paper. And, it surely is intimidating. So, I will aim to demystify it in this post by going through each individual piece. So read ahead.\\n\\xa0\\nThe Big Picture\\n\\xa0\\nQ: That sounds interesting. So, what does a transformer do exactly?\\nEssentially, a transformer can perform almost any NLP task. It can be used for language modeling, Translation, or Classification as required, and it does it fast by removing the sequential nature of the problem. So, the transformer in a machine translation application would convert one language to another, or for a classification problem will provide the class probability using an appropriate output layer.\\nIt all will depend on the final outputs layer for the network but, the Transformer basic structure will remain quite the same for any task. For this particular post, I will be continuing with the machine translation example.\\nSo from a very high place, this is how the transformer looks for a translation task. It takes as input an English sentence and returns a German sentence.\\n\\n\\nTransformer for Translation (Image by author)\\n\\n\\xa0\\nThe Building Blocks\\n\\xa0\\nQ: That was too basic.\\xa0😎\\xa0Can you expand on it?\\nOkay, just remember in the end, you asked for it. Let’s go a little deeper and try to understand what a transformer is composed of.\\nSo, a transformer is essentially composed of a stack of encoder and decoder layers. The role of an encoder layer is to encode the English sentence into a numerical form using the attention mechanism, while the decoder aims to use the encoded information from the encoder layers to give the German translation for the particular English sentence.\\nIn the figure below, the transformer is given as input an English sentence, which gets encoded using 6 encoder layers. The output from the final encoder layer then goes to each decoder layer to translate English to German.\\n\\n\\nData Flow in a Transformer (Image by author)\\n\\n\\xa0\\n1. Encoder Architecture\\n\\xa0\\nQ: That’s alright but, how does an encoder stack encode an English sentence exactly?\\nPatience, I am getting to it. So, as I said the encoder stack contains six encoder layers on top of each other(As given in the paper, but the future versions of transformers use even more layers). And each encoder in the stack has essentially two main layers:\\n\\na multi-head self-attention Layer, and\\na position-wise fully connected feed-forward network\\n\\n\\n\\nVery basic encoder Layer (Image by author)\\n\\n\\xa0\\nThey are a mouthful. Right? Don’t lose me yet as I will explain both of them in the coming sections. Right now, just remember that the encoder layer incorporates attention and a position-wise feed-forward network.\\nQ: But, how does this layer expect its inputs to be?\\nThis layer expects its inputs to be of the shape\\xa0SxD\\xa0(as shown in the figure below) where\\xa0S\\xa0is the source sentence(English Sentence) length, and\\xa0D\\xa0is the dimension of the embedding whose weights can be trained with the network. In this post, we will be using\\xa0D\\xa0as 512 by default throughout. While S will be the maximum length of sentence in a batch. So it normally changes with batches.\\n\\n\\nEncoder — Input and Output shapes are the same (Image by author)\\n\\n\\xa0\\nAnd what about the outputs of this layer? Remember that the encoder layers are stacked on top of each other. So, we want to be able to have an output of the same dimension as the input so that the output can flow easily into the next encoder. So the output is also of the shape,\\xa0SxD.\\nQ: Enough about the sizes talk, I understand what goes in and what goes out but what actually happens in the Encoder layer?\\nOkay, let’s go through the attention layer and the feedforward layer one by one:\\n\\xa0\\nA) Self-attention layer\\n\\xa0\\n\\n\\nHow Self-Attention Works (Image by author)\\n\\n\\xa0\\nThe above figure must look daunting but it is easy to understand. So just stay with me here.\\nDeep Learning is essentially nothing but a lot of matrix calculations and what we are essentially doing in this layer is a lot of matrix calculations intelligently. The self-attention layer initializes with 3 weight matrices — Query(W_q), Key(W_k), and Value(W_v). Each of these matrices has a size of (Dxd) where d is taken as 64 in the paper. The weights for these matrices will be trained when we train the model.\\nIn the first calculation(Calc 1 in the figure), we create matrices Q, K, and V by multiplying the input with the respective Query, Key, and Value matrix.\\nTill now it is trivial and shouldn’t make any sense, but it is at the second calculation where it gets interesting. Let’s try to understand the output of the softmax function. We start by multiplying the Q and Kᵀ matrix to get a matrix of size (SxS) and divide it by the scalar √d. We then take a softmax to make the rows sum to one.\\nIntuitively, we can think of the resultant\\xa0SxS\\xa0matrix as the contribution of each word in another word. For example, it might look like this:\\n\\n\\nSoftmax(QxKt/sqrt(d)) (Image by author)\\n\\n\\xa0\\nAs you can see the diagonal entries are big. This is because the word contribution to itself is high. That is reasonable. But we can see here that the word “quick” devolves into “quick” and “fox” and the word “brown” also devolves into “brown” and “fox”. That intuitively helps us to say that both the words — “quick” and “brown” each refers to the “fox”.\\nOnce we have this SxS matrix with contributions we multiply this matrix by the Value matrix(Sxd) of the sentence and it gives us back a matrix of shape Sxd(4x64). So, what the operation actually does is that it replaces the embedding vector of a word like “quick” with say .75 x (quick embedding) and .2x(fox embedding) and thus now the resultant output for the word “quick” has attention embedded in itself.\\nNote that the output of this layer has the dimension (Sxd) and before we get done with the whole encoder we need to change it back to D=512 as we need the output of this encoder as the input of another encoder.\\nQ: But, you called this layer Multi-head self-attention Layer. What is the multi-head?\\nOkay, my bad but in my defense, I was just getting to that.\\nIt’s called a multi-head because we use many such self-attention layers in parallel. That is, we have many self-attention layers stacked on top of each other. The number of attention layers,h, is kept as 8 in the paper. So the input X goes through many self-attention layers parallelly, each of which gives a z matrix of shape (Sxd) = 4x64. We concatenate these 8(h) matrices and again apply a final output linear layer, Wo, of size DxD.\\nWhat size do we get? For the concatenate operation we get a size of SxD(4x(64x8) = 4x512). And multiplying this output by Wo, we get the final output Z with the shape of SxD(4x512) as desired.\\nAlso, note the relation between h,d, and D i.e. h x d = D\\n\\n\\nThe Full multi-headed self-attention Layer (Image by author)\\n\\n\\xa0\\nThus, we finally get the output Z of shape 4x512 as intended. But before it goes into another encoder we pass it through a Feed-Forward Network.\\n\\xa0\\nB) Position-wise feed-forward network\\n\\xa0\\nOnce we understand the multi-headed attention layer, the Feed-forward network is actually pretty easy to understand. It is just a combination of various linear and dropout layers on the output Z. Consequentially, it is again just a lot of Matrix multiplication here.\\n\\n\\nEach word goes into the feed-forward network. (Image by author)\\n\\n\\xa0\\nThe feed-forward network applies itself to each position in the output Z parallelly(Each position can be thought of as a word) and hence the name Position-wise feed-forward network. The feed-forward network also shares weight, so that the length of the source sentence doesn’t matter(Also, if it didn’t share weights, we would have to initialize a lot of such networks based on max source sentence length and that is not feasible)\\n\\n\\nIt is actually just a linear layer that gets applied to each position(or word) (Image by author)\\n\\n\\xa0\\nWith this, we near an okayish understanding of the encoder part of the Transformer.\\nQ: Hey, I was just going through the picture in the paper, and the encoder stack has something called “positional encoding” and “Add & Norm” also. What are these?\\n\\n\\nI am back again here so you don’t have to scroll\\xa0Source\\n\\n\\xa0\\nOkay, These two concepts are pretty essential to this particular architecture. And I am glad you asked this one. So, we will discuss these steps before moving further to the decoder stack.\\n\\xa0\\nC. Positional Encodings\\n\\xa0\\nSince, our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of both the encoder and decoder stacks(as we will see later). The positional encodings need to have the same dimension, D as the embeddings have so that the two can be summed.\\n\\n\\nAdd a static positional pattern to X (Image by author)\\n\\n\\xa0\\nIn the paper, the authors used sine and cosine functions to create positional embeddings for different positions.\\n\\n\\nSource\\n\\n\\xa0\\nThis particular mathematical thing actually generates a 2d matrix which is added to the embedding vector that goes into the first encoder step.\\nPut simply, it’s just a constant matrix that we add to the sentence so that the network could get the position of the word.\\n\\xa0\\xa0\\n\\nPositional encoding matrix for the first 300 and 3000 positions (Image by author)\\n\\n\\xa0\\nAbove is the heatmap of the position encoding matrix that we will add to the input that is to be given to the first encoder. I am showing the heatmap for the first 300 positions and the first 3000 positions. We can see that there is a distinct pattern that we provide to our Transformer to understand the position of each word. And since we are using a function comprised of sin and cos, we are able to embed positional embeddings for very high positions also pretty well as we can see in the second picture.\\nInteresting Fact:\\xa0The authors also let the Transformer learn these encodings too and didn’t see any difference in performance as such. So, they went with the above idea as it doesn’t depend on sentence length and so even if the test sentence is bigger than train samples, we would be fine.\\n\\xa0\\nD. Add and Normalize\\n\\xa0\\nAnother thing, that I didn’t mention for the sake of simplicity while explaining the encoder is that the encoder(the decoder architecture too) architecture has skip level residual connections(something akin to resnet50) also. So, the exact encoder architecture in the paper looks like below. Simply put, it helps traverse information for a much greater length in a Deep Neural Network. This can be thought of as akin(intuitively) to information passing in an organization where you have access to your manager as well as to your manager’s manager.\\n\\n\\nThe Skip level connections help information flow in the network (Image by author)\\n\\n\\xa0\\n2. Decoder Architecture\\n\\xa0\\nQ: Okay, so till now we have learned that an encoder takes an input sentence and encodes its information in a matrix of size SxD(4x512). That’s all great but how does it help the decoder decode it to German?\\nGood things come to those who wait. So, before understanding how the decoder does that, let us understand the decoder stack.\\nThe decoder stack contains 6 decoder layers in a stack (As given in the paper again) and each decoder in the stack is comprised of these main three layers:\\n\\nMasked multi-head self-attention Layer\\nmulti-head self-attention Layer, and\\na position-wise fully connected feed-forward network\\n\\nIt also has the same positional encoding as well as the skip level connection as well. We already know how the multi-head attention and feed-forward network layers work, so we will get straight into what is different in the decoder as compared to the encoder.\\n\\n\\nDecoder Architecture (Image by author)\\n\\n\\xa0\\nQ: Wait, but do I see the output we need flowing into the decoder as input? What? Why?\\xa0😖\\nI am noticing that you are getting pretty good at asking questions. And that is a great question, something I even though myself a lot of times, and something that I hope will get much clearer by the time you reach the end of this post.\\nBut to give an intuition, we can think of a transformer as a conditional language model in this case. A model that predicts the next word given an input word and an English sentence on which to condition upon or base its prediction on.\\nSuch models are inherently sequential as in how would you train such a model? You start by giving the start token(<s>) and the model predicts the first word conditioned on the English sentence. You change the weights based on if the prediction is right or wrong. Then you give the start token and the first word (<s> der) and the model predicts the second word. You change weights again. And so on.\\nThe transformer decoder learns just like that but the beauty is that it doesn’t do that in a sequential manner. It uses masking to do this calculation and thus takes the whole output sentence (although shifted right by adding a\\xa0<s>\\xa0token to the front) while training. Also, please note that at prediction time we won’t give the output to the network\\nQ: But, how does this masking exactly work?\\n\\xa0\\nA) Masked Multi-Head Self Attention Layer\\n\\xa0\\nIt works, as usual, you wear it I mean\\xa0😷. Kidding aside, as you can see that this time we have a\\xa0Masked\\xa0Multi-Head attention Layer in our decoder. This means that we will mask our shifted output (that is the input to the decoder) in a way that the network is never able to see the subsequent words since otherwise, it can easily copy that word while training.\\nSo, how does the mask exactly work in the masked attention layer? If you remember, in the attention layer we multiplied the query(Q) and keys(K) and divided them by sqrt(d) before taking the softmax.\\nIn a masked attention layer, though, we add the resultant matrix before the softmax(which will be of shape (TxT)) to a masking matrix.\\nSo, In a masked layer, the function changes from:\\n\\n\\n(Image by author)\\n\\n\\xa0\\nQ: I still don’t get it, what happens if we do that?\\nThat’s understandable actually. Let me break it in steps. So, our resultant matrix(QxK/sqrt(d)) of shape (TxT) might look something like below:(The numbers can be big as softmax not applied yet)\\n\\n\\nSchnelle currently attends to both Braune and Fuchs (Image by author)\\n\\n\\xa0\\nThe word Schnelle will now be composed of both Braune and Fuchs if we take the above matrix’s softmax and multiply it with the value matrix V. But we don’t want that, so we add the mask matrix to it to give:\\n\\n\\nThe mask operation applied to the matrix. (Image by author)\\n\\n\\xa0\\nAnd, now what will happen after we do the softmax step?\\n\\n\\nSchnelle never attends to any word after Schnelle. (Image by author)\\n\\n\\xa0\\nSince e^{-inf} = 0, all positions subsequent to Schnelle have been converted to 0. Now, if we multiply this matrix with the value matrix V, the vector corresponding to Schnelle’s position in the Z vector passing through the decoder would not contain any information of the subsequent words Braune and Fuchs just like we wanted.\\nAnd that is how the transformer takes the whole shifted output sentence at once and doesn’t learn in a sequential manner. Pretty neat I must say.\\nQ: Are you kidding me? That’s actually awesome.\\nSo glad that you are still with me and you appreciate it. Now, coming back to the decoder. The next layer in the decoder is:\\n\\xa0\\nB) Multi-Headed Attention Layer\\n\\xa0\\nAs you can see in the decoder architecture, a Z vector(Output of encoder) flows from the encoder to the multi-head attention layer in the Decoder. This Z output from the last encoder has a special name and is often called as memory. The attention layer takes as input both the encoder output and data flowing from below(shifted outputs) and uses attention. The Query vector Q is created from the data flowing in the decoder, while the Key(K) and value(V) vectors come from the encoder output.\\nQ: Isn’t there any mask here?\\nNo, there is no mask here. The output coming from below is already masked and this allows every position in the decoder to attend over all the positions in the Value vector. So for every word position to be generated the decoder has access to the whole English sentence.\\nHere is a single attention layer(which will be part of a multi-head just like before):\\n\\n\\n(Image by author)\\n\\n\\xa0\\nQ: But won’t the shapes of Q, K, and V be different this time?\\nYou can look at the figure where I have done all the weights calculation. I would also ask you to see the shapes of the resultant Z vector and how our weight matrices until now never used the target or source sentence length in any of their dimensions. Normally, the shape cancels away in all our matrix calculations. For example, see how the S dimension cancels away in calculation 2 above. That is why while selecting the batches during training the authors talk about tight batches. That is in a batch all source sentences have similar lengths. And different batches could have different source lengths.\\nI will now talk about the skip level connections and the feed-forward layer. They are actually the same as in ….\\nQ: Ok, I get it. We have the skip level connections and the FF layer and get a matrix of shape TxD after this whole decode operation.\\xa0But where is the German translation?\\n\\xa0\\n3. Output Head\\n\\xa0\\nWe are actually very much there now friend. Once, we are done with the transformer, the next thing is to add a task-specific output head on the top of the decoder output. This can be done by adding some linear layers and softmax on top to get the probability\\xa0across all the words in the german vocab. We can do something like this:\\n\\n\\n(Image by author)\\n\\n\\xa0\\nAs you can see we are able to generate probabilities. So far we know how to do a forward pass through this Transformer architecture. Let us see how we do the training of such a Neural Net Architecture.\\n\\xa0\\nTraining:\\n\\xa0\\nTill now, if we take a bird-eye view of the structure we have something like:\\n\\n\\n(Image by author)\\n\\n\\xa0\\nWe can give an English sentence and shifted output sentence and do a forward pass and get the probabilities over the German vocabulary. And thus we should be able to use a loss function like cross-entropy where the target could be the German word we want, and train the neural network using the Adam Optimizer. Just like any classification example. So, there is your German.\\nIn the paper though, the authors use slight variations of optimizers and loss. You can choose to skip the below 2 sections on KL Divergence Loss and Learning rate schedule with Adam if you want as it is done only to churn out more performance out of the model and not an inherent part of the Transformer architecture as such.\\nQ: I have been here for such a long time and have I complained?\\xa0😒\\nOkay. Okay. I get you. Let’s do it then.\\n\\xa0\\nA) KL Divergence with Label Smoothing:\\n\\xa0\\nKL Divergence is the information loss that happens when the distribution P is approximated by the distribution Q. When we use the KL Divergence loss, we try to estimate the target distribution(P) using the probabilities(Q) we generate from the model. And we try to minimize this information loss in the training.\\n\\n\\n(Image by author)\\n\\n\\xa0\\nIf you notice, in this form(without label smoothing which we will discuss) this is exactly the same as cross-entropy. Given two distributions like below.\\n\\xa0\\xa0\\n\\nTarget distribution and probability distribution for a word(token) (Image by author)\\n\\n\\xa0\\nThe KL Divergence formula just plain gives\\xa0-logq(oder)\\xa0and that is the cross-entropy loss.\\nIn the paper, though the authors used label smoothing with α = 0.1 and so the KL Divergence loss is not cross-entropy. What that means is that in the target distribution the output value is substituted by (1-α) and the remaining 0.1 is distributed across all the words. The authors say that this is so that the model is not too confident.\\n\\xa0\\xa0\\n\\n(Image by author)\\n\\n\\xa0\\nQ: But, why do we make our models not confident? It seems absurd.\\nYes, it does but intuitively, you can think of it as when we give the target as 1 to our loss function, we have no doubts that the true label is True and others are not. But vocabulary is inherently a non-standardized target. For example, who is to say that you cannot use good in place of great? So we add some confusion in our labels so our model is not too rigid.\\n\\xa0\\nB) A particular Learning Rate schedule with Adam\\n\\xa0\\nThe authors use a learning rate scheduler to increase the learning rate until warmup steps and then decrease it using the below function. And they used the Adam optimizer with β¹ = 0.9, β² = 0.98. Nothing too interesting here just some learning choices.\\n\\n\\nSource:\\xa0Paper\\n\\n\\xa0\\nQ: But wait I just remembered that we won’t have the shifted output at the prediction time, would we? How do we do predictions then?\\nIf you realize what we have at this point is a generative model and we will have to do the predictions in a generative way as we won’t know the output target vector when doing prediction. So predictions are still sequential.\\n\\xa0\\nPrediction Time\\n\\xa0\\n\\n\\nPredicting with a greedy search using the Transformer (Image by author)\\n\\n\\xa0\\nThis model does piece-wise predictions. In the original paper, they use the Beam Search to do prediction. But a greedy search would work fine as well for the purpose of explaining it. In the above example, I have shown how a greedy search would work exactly. The greedy search would start with:\\n\\nPassing the whole English sentence as encoder input and just the start token\\xa0<st>\\xa0as shifted output(input to the decoder) to the model and doing the forward pass.\\nThe model will predict the next word —\\xa0der\\nThen, we pass the whole English sentence as encoder input and add the last predicted word to the shifted output(input to the decoder =\\xa0<st> der) and do the forward pass.\\nThe model will predict the next word —\\xa0schnelle\\nPassing the whole English sentence as encoder input and\\xa0<st> der schnelle\\xa0as shifted output(input to the decoder) to the model and doing the forward pass.\\nand so on, until the model predicts the end token\\xa0</s>\\xa0or we generate some maximum number of tokens(something we can define) so the translation doesn’t run for an infinite duration in any case it breaks.\\n\\n\\xa0\\nBeam Search:\\n\\xa0\\nQ: Now I am greedy, Tell me about beam search as well.\\nOkay, the beam search idea is inherently very similar to the above idea. In beam search, we don’t just look at the highest probability word generated but the top two words.\\nSo, For example, when we gave the whole English sentence as encoder input and just the start token as shifted output, we get two best words as\\xa0i(p=0.6) and\\xa0der(p=0.3). We will now generate the output model for both output sequences,<s> i\\xa0and\\xa0<s> der\\xa0and look at the probability of the next top word generated. For example, if\\xa0<s> i\\xa0gave a probability of (p=0.05) for the next word and\\xa0<s> der>\\xa0gave (p=0.5) for the next predicted word, we discard the sequence\\xa0<s> iand go with\\xa0<s> der\\xa0instead, as the sum of probability of sentence is maximized(<s> der next_word_to_der\\xa0p = 0.3+0.5 compared to\\xa0<s> i next_word_to_i\\xa0p = 0.6+0.05). We then repeat this process to get the sentence with the highest probability.\\nSince we used the top 2 words, the beam size is 2 for this Beam Search. In the paper, they used beam search of size 4.\\nPS: I showed that the English sentence is passed at every step for brevity, but in practice, the output of the encoder is saved and only the shifted output passes through the decoder at each time step.\\nQ: Anything else you forgot to tell me? I will let you have your moment.\\nYes. Since you asked. Here it is:\\n\\xa0\\nBPE, Weight Sharing and Checkpointing\\n\\xa0\\nIn the paper, the authors used Byte pair encoding to create a common English German vocabulary. They then used shared weights across both the English and german embedding and pre-softmax linear transformation as the embedding weight matrix shape would work (Vocab Length X D).\\nAlso, the authors average the last k checkpoints to create an ensembling effect to reach the performance.\\xa0This is a pretty known technique where we average the weights in the last few epochs of the model to create a new model which is sort of an ensemble.\\nQ: Can you show me some code?\\nThis post has already been so long, so I will do that in the next post. Stay tuned.\\nNow, finally, my turn to ask the question: Did you get how a transformer works? Yes, or No, you can answer in the comments. :)\\n\\xa0\\nReferences\\n\\nAttention Is All You Need: The Paper which started it all.\\nThe Annotated Transformer: This one has all the code. Although I will write a simple transformer in the next post too.\\nThe Illustrated Transformer: This is one of the best posts on transformers.\\n\\nIn this post, I covered how the Transformer architecture works from a detail-oriented, intuitive perspective.\\nIf you want to learn more about NLP, I would like to call out an excellent course on\\xa0Natural Language Processing\\xa0from the Advanced Machine Learning Specialization. Do check it out.\\nI am going to be writing more of such posts in the future too. Let me know what you think about them. Should I write on heavily technical topics or more beginner level articles? The comment section is your friend. Use it. Also, follow me up at\\xa0Medium\\xa0or Subscribe to my\\xa0blog.\\nAnd, finally a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\\nThis story was first published\\xa0here.\\n\\xa0\\nBio: Rahul Agarwal is Senior Statistical Analyst at WalmartLabs. Follow him on Twitter\\xa0@mlwhiz.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nA Deep Dive Into the Transformer Architecture – The Development of Transformer Models\\nThe Hitchhiker’s Guide to Feature Extraction\\nDeep Learning’s Most Important Ideas',\n",
       " 'comments\\nBy Loryn Cole, Dataquest\\nIn my previous role as a marketing data analyst for a blogging company, one of my most important tasks was to track how blog posts performed.\\nOn the surface, it’s a fairly straightforward goal. With Google Analytics, you can quickly get just about any metric you need for your blog posts, for any date range.\\nBut when it comes to comparing blog post performance, things get a bit trickier.\\nFor example, let’s say we want to compare the performance of the blog posts we published on the Dataquest blog in June (using the month of June as our date range).\\n\\nBut wait… two blog posts with more than 1,000 pageviews were published earlier in the month, And the two with fewer than 500 pageviews were published at the end of the month. That’s hardly a fair comparison!\\nMy first solution to this problem was to look up each post individually, so that I could make an even comparison of how each post performed in their first day, first week, first month, etc.\\nHowever, that required a lot of manual copy-and-paste work, which was extremely tedious if I wanted to compare more than a few posts, date ranges, or metrics at a time.\\nBut then, I learned R, and realized that there was a much better way.\\nIn this post, we\\'ll walk through how it\\'s done, so you can do my better blog post analysis for yourself!\\n\\xa0\\nWhat we\\'ll need\\n\\xa0\\nTo complete this tutorial, you’ll need basic knowledge of R syntax and the tidyverse, and access to a Google Analytics account.\\nNot yet familiar with the basics of R? We can help with that! Our interactive online courses teach you R from scratch, with no prior programming experience required.\\xa0Sign up and start today!\\n\\n\\xa0\\nYou’ll also need the\\xa0dyplr,\\xa0lubridate, and\\xa0stringr\\xa0packages installed — which, as a reminder, you can do with the\\xa0install.packages()\\xa0command.\\nFinally, you will need a CSV of the blog posts you want to analyze. Here’s what’s in my dataset:\\npost_url: the page path of the blog post\\npost_date: the date the post was published (formatted m/d/yy)\\ncategory: the blog category the post was published in (optional)\\ntitle: the title of the blog post (optional)\\nDepending on your content management system, there may be a way for you to automate gathering this data — but that’s out of the scope of this tutorial!\\nFor this tutorial, we’ll use a manually-gathered dataset of the past ten Dataquest blog posts.\\n\\xa0\\nSetting up the googleAnalyticsR package\\n\\xa0\\nTo access data from the Google Analytics API, we’ll use the excellent\\xa0googleAnalyticsR\\xa0package by Mark Edmonson.\\nAs described in the documentation, there are two \"modes\" to the googleAnalyticsR package. The first mode, which we’ll use here, is a “Try it out” mode, which uses a shared Google Project to authorize your Google Analytics account.\\nIf you want to make this report a recurring tool for your blog or client, be sure to create your own Google Project, which will help keep the traffic on the shared Project to a minimum. To find out how to set this up, head over to\\xa0the package setup documentation.\\nFor now, though, we’ll stick with “Try it out” mode.\\nFirst, we\\'ll install the package using this code:\\ninstall.packages(\\'googleAnalyticsR\\', dependencies = TRUE)\\nThis installs the package, as well as the required dependencies.\\nNext, we\\'ll load the library, and authorize it with a Google Analytics account using the\\xa0ga_auth()\\xa0function.\\n\\nlibrary(googleAnalyticsR)\\r\\nga_auth()\\n\\n\\nWhen you run this code the first time, it will open a browser window and prompt you to log in to your Google account. Then, it will give you a code to paste into your R console. After that, it will save an authorization token so you only have to do this once!\\nOnce you’ve completed the Google Analytics authorization, we’re ready to set up the rest of the libraries and load in our blog posts. We’ll also use\\xa0dplyr::mutate()\\xa0to change the post_date to a Date class while we’re at it!\\n\\nlibrary(dplyr)\\r\\nlibrary(lubridate)\\r\\nlibrary(stringr)\\r\\nlibrary(readr)\\r\\n\\r\\nblog_posts <- read.csv(\"articles.csv\") %>%\\r\\n  mutate(\\r\\n    post_date = as.Date(post_date, \"%m/%d/%y\") # changes the post_date column to a Date\\r\\n  )\\n\\n\\nHere’s what the blog post data frame looks like:\\n\\nFinally, to get data from your Google Analytics account, you will need the ID of the Google Analytics view you want to access.\\xa0ga_account_list()\\xa0will return a list of your available accounts.\\n\\naccounts <- ga_account_list()\\r\\n\\r\\n# select the view ID by view and property name, and store it for ease of use\\r\\nview_id <- accounts$viewId[which(accounts$viewName == \"All Web Site Data\" & accounts$webPropertyName == \"Dataquest\")]\\r\\n# be sure to change this out with your own view and/or property name!\\n\\n\\nNow, we’re ready to do our first Google Analytics API requests!\\n\\xa0\\nAccessing blog post data with googleAnalyticsR\\n\\xa0\\nIn this tutorial, our goal is to gather data for the first week each post was active, and compile it in a dataframe for analysis. To do this, we’ll create a function that runs a for loop and requests this data for each post in our\\xa0blog_posts\\xa0dataframe.\\nSo, let’s take a look at how to send a request to the Google Analytics API using\\xa0googleAnalyticsR.\\n\\ngoogle_analytics(view_id,\\r\\n                  date_range = c(as.Date(\"2020-06-01\"), as.Date(\"2020-06-30\")),\\r\\n                  metrics = c(\"pageviews\"),\\r\\n                  dimensions = c(\"pagePath\")\\r\\n)\\n\\n\\nThis request has a few components. First, enter the\\xa0view_id, which we already stored from our\\xa0ga_accounts()\\xa0dataframe.\\nNext, specify the date range, which needs to be passed in as a list of dates.\\nThen, we input the metrics (like pageviews, landing page sessions, or time on page) and dimensions (like page path, channel, or device). We can use any dimension or metric that’s available in the Google Analytics UI — here’s a\\xa0useful reference\\xa0for finding the API name of any UI metric or dimension.\\nSo, the request above will return a dataframe of all pageviews in June, by page path (by default\\xa0googleAnalyticsR will only return the first 1,000 results).\\nBut, in our case, we only want to retrieve pageviews for a specific page – so we need to filter on the pagePath dimension using a dimension filter, which looks like this:\\n\\npage_filter <- dim_filter(dimension = \"pagePath\",\\r\\n                          operator = \"REGEXP\",\\r\\n                          expressions = \"^www.dataquest.io/blog/r-markdown-guide-cheatsheet/$\")\\n\\n\\nTo use this filter in our request, googleAnalyticsR wants us to create a filter clause – which is how you would combine filters if you wanted to use multiple dimension filters. But in our case, we just need the one:\\n\\npage_filter_clause <- filter_clause_ga4(list(page_filter))\\n\\n\\nNow, let’s try sending a response with this filter:\\n\\ngoogle_analytics(view_id,\\r\\n              date_range = c(as.Date(\"2020-07-01\"), Sys.Date()),\\r\\n              metrics = c(\"pageviews\"),\\r\\n              dimensions = c(\"pagePath\"),\\r\\n              dim_filters = page_filter_clause)\\n\\n\\n\\nThe result is a dataframe with the pageviews for the R Markdown post!\\n\\xa0\\nCreating the for loop\\n\\xa0\\nNow that we can gather data and filter it by dimension, we are ready to build out our function to run our for loop! The steps to the function are:\\n\\nSet up a data frame to hold the results\\nBegin the loop based on the number of rows in the data frame\\nAccess the post URL and post date for each post\\nCreate a page filter based on the post URL\\nSend a request to Google Analytics using the post_date as the start date, and date the week later as the end date\\nAdd the post URL and pageview data to the final data frame\\n\\nI also have added a\\xa0print()\\xa0command to let us know how far along the loop is (because it can take awhile) and a\\xa0Sys.Sleep()\\xa0command to keep us from hitting the Google Analytics API rate limit.\\nHere’s what that looks like all put together!\\n\\nget_pageviews <- function(posts) {\\r\\n\\r\\n  # set up dataframe to be returned, using the same variable names as our original dataframe\\r\\n  final <- tibble(pageviews = numeric(),\\r\\n                      post_url = character())\\r\\n\\r\\n  # begin the loop for each row in the posts dataframe\\r\\n  for (i in seq(1:nrow(posts))) {\\r\\n\\r\\n    # select the post URL and post date for this loop — also using the same variable names as our original dataframe\\r\\n    post_url <- posts$post_url[i]\\r\\n    post_date <- posts$post_date[i]\\r\\n\\r\\n    # set up the page filter and page filter clause with the current post URL\\r\\n    page_filter <- dim_filter(dimension = \"pagePath\",\\r\\n                              operator = \"REGEXP\",\\r\\n                              expressions = post_url)\\r\\n\\r\\n    page_filter_clause <- filter_clause_ga4(list(page_filter))\\r\\n\\r\\n    # send the request, and set the date range to the week following the date the post was shared\\r\\n    page_data <- google_analytics(view_id,\\r\\n                                    date_range = c(post_date, post_date %m+% weeks(1)),\\r\\n                                    metrics = c(\"pageviews\"),\\r\\n                                    dim_filters = page_filter_clause)\\r\\n\\r\\n    # add the post url to the returned dataframe\\r\\n    page_data$post_url <- post_url\\r\\n\\r\\n    # add the returned data to the data frame we created outside the loop\\r\\n    final <- rbind(final, page_data)\\r\\n\\r\\n    # print loop status\\r\\n    print(paste(\"Completed row\", nrow(final), \"of\", nrow(posts)))\\r\\n\\r\\n    # wait two seconds\\r\\n    Sys.sleep(2)\\r\\n\\r\\n  }\\r\\n\\r\\n  return(final)\\r\\n\\r\\n}\\n\\n\\nWe could potentially speed this up with a “functional” in R, such as\\xa0purrr::map(). The\\xa0map()\\xa0function takes a function as an input and returns a vector as output. Check out Dataquest\\'s\\xa0interactive online lesson on the map function\\xa0if you\\'d like to deepen your knowledge!\\nFor this tutorial, though, we\\'ll use a for loop because it\\'s a bit less abstract.\\nNow, we’ll run the loop on our\\xa0blog_posts\\xa0dataframe, and merge the results to our\\xa0blog_posts\\xa0data.\\n\\nrecent_posts_first_week <- get_pageviews(blog_posts)\\r\\nrecent_posts_first_week <- merge(blog_posts, recent_posts_first_week)\\r\\n\\r\\nrecent_posts_first_week\\n\\n\\n\\nAnd that’s it! Now, we can get on to the good stuff —\\xa0analyzing and visualizing the data.\\n\\xa0\\nBlog post data, visualized!\\n\\xa0\\nFor demonstration, here\\'s a ggplot bar chart that shows how many pageviews each of our most recent 10 blog posts got in the first week after they were published:\\n\\nlibrary(ggplot2)\\r\\nlibrary(scales)\\r\\n\\r\\nrecent_posts_first_week %>%\\r\\n  arrange(\\r\\n    post_date\\r\\n  ) %>%\\r\\n  mutate(\\r\\n    pretty_title = str_c(str_extract(title, \"^(\\\\\\\\S+\\\\\\\\s+\\\\\\\\n?){1,5}\"), \"...\"),\\r\\n    pretty_title = factor(pretty_title, levels = pretty_title[order(post_date)])\\r\\n  ) %>%\\r\\n  ggplot(aes(pretty_title, pageviews)) +\\r\\n  geom_bar(stat = \"identity\", fill = \"#39cf90\") +\\r\\n  coord_flip() +\\r\\n  theme_minimal() +\\r\\n  theme(axis.title = element_blank()) +\\r\\n  labs(title = \"Recent Dataquest blog posts by first week pageviews\") +\\r\\n  scale_y_continuous(labels = comma)\\n\\n\\n\\nNow we can see how useful it is to be able to compare blog posts on \"even footing\"!\\nFor more information on the googleAnalyticsR package and what you can do with it, check out\\xa0its very helpful resource page.\\n\\xa0\\nBio: Loryn Cole is a product marketer at Dataquest who enjoys learning to code with data. She rides motorcycles and sometimes writes about them.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nUnderstanding Time Series with R\\nWrapping Machine Learning Techniques Within AI-JACK Library in R\\nmodelStudio and The Grammar of Interactive Explanatory Model Analysis',\n",
       " 'comments\\nBy Mohammad Khorasani, Data Scientist/Engineer Hybrid\\n\\n\\nPhoto by\\xa0Isaac Smith\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nMicrosoft Excel and Word are without a shred of doubt the two most abundantly used software in the corporate and non-corporate world. They are practically synonymous with the term ‘work’ itself. Oftentimes, not a week goes by without us firing up the combination of the two and one way or another putting their goodness to use. While for the average daily purpose automation would not be solicited, there are times when automation can be a necessity. Namely, when you have a multitude of charts, figures, tables, and reports to generate, it can become an exceedingly tedious undertaking if you choose the manual route. Well, it doesn’t have to be that way. There is in fact a way to create a pipeline in Python where you can seamlessly integrate the two to produce spreadsheets in Excel and then transfer the results to Word to generate a report virtually instantaneously.\\n\\xa0\\nOpenpyxl\\n\\xa0\\n\\xa0\\nMeet Openpyxl, arguably one of the most versatile bindings in Python that makes interfacing with Excel quite literally a stroll in the park. Armed with it you can read and write all current and legacy excel formats i.e. xlsx and xls. Openpyxl allows you to populate rows and columns, execute formulae, create 2D and 3D charts, label axes and titles, and a plethora of other\\xa0abilities\\xa0that can come in handy. Most importantly however, this package enables you to iterate over an endless numbers of rows and columns in Excel, thereby saving you from all that pesky number crunching and plotting that you had to do previously.\\n\\xa0\\nPython-docx\\n\\xa0\\n\\xa0\\nAnd then comes along Python-docx, this package is to Word what Openpyxl is to Excel. If you haven’t already studied their\\xa0documentation, then you should probably take a look. Python-docx is without exaggeration one of the simplest and most self-explanatory toolkits I have worked with ever since I started working with Python itself. It allows you to automate document generation by inserting text, filling in tables and rendering images into your report automatically without any overhead whatsoever.\\nWithout further ado let’s create our very own automated pipeline. Go ahead and fire up Anaconda (or any other IDE of your choice) and install the following packages:\\n\\npip install openpyxlpip install python-docx\\n\\n\\xa0\\n\\xa0\\nMicrosoft Excel Automation\\n\\xa0\\n\\xa0\\nInitially, we’ll load an Excel workbook that has already been created (shown below):\\n\\nworkbook = xl.load_workbook(\\'Book1.xlsx\\')\\r\\nsheet_1 = workbook[\\'Sheet1\\']\\n\\n\\xa0\\n\\n\\nImage by the author.\\n\\n\\xa0\\nSubsequently, we’ll iterate over all of the rows in our spreadsheet to compute and insert the values for power by multiplying current by voltage:\\n\\nfor row in range(2, sheet_1.max_row + 1):\\r\\n    current = sheet_1.cell(row, 2)\\r\\n    voltage = sheet_1.cell(row, 3)\\r\\n    power = float(current.value) * float(voltage.value)\\r\\n    power_cell = sheet_1.cell(row, 1)\\r\\n    power_cell.value = power\\n\\n\\xa0\\nOnce that is done, we will use the calculated values for power to generate a line chart that will be inserted into the specified cell as shown below:\\n\\nvalues = Reference(sheet_1, min_row = 2, max_row = sheet_1.max_row, min_col = 1, max_col = 1)\\r\\nchart = LineChart()\\r\\nchart.y_axis.title = \\'Power\\'\\r\\nchart.x_axis.title = \\'Index\\'\\r\\nchart.add_data(values)\\r\\nsheet_1.add_chart(chart, \\'e2\\') \\r\\nworkbook.save(\\'Book1.xlsx\\')\\n\\n\\xa0\\n\\n\\nAutomatically generated Excel spreadsheet. Image by the author.\\n\\n\\xa0\\n\\xa0\\nExtracting Chart\\n\\xa0\\n\\xa0\\nNow that we have generated our chart, we need to extract it as an image so that we can use it in our Word report. First, we’ll declare the exact location of our Excel file and also where the output chart image should be saved:\\n\\ninput_file = \"C:/Users/.../Book1.xlsx\"\\r\\noutput_image = \"C:/Users/.../chart.png\"\\n\\n\\xa0\\nThen access the spreadsheet using the following method:\\n\\noperation = win32com.client.Dispatch(\"Excel.Application\")\\r\\noperation.Visible = 0\\r\\noperation.DisplayAlerts = 0\\r\\nworkbook_2 = operation.Workbooks.Open(input_file)\\r\\nsheet_2 = operation.Sheets(1)\\n\\n\\xa0\\nSubsequently, you can iterate over all of the chart objects in the spreadsheet (if there are more than one) and save them in the specified location as such:\\n\\nfor x, chart in enumerate(sheet_2.Shapes):\\r\\n    chart.Copy()\\r\\n    image = ImageGrab.grabclipboard()\\r\\n    image.save(output_image, \\'png\\')\\r\\n    passworkbook_2.Close(True)\\r\\noperation.Quit()\\n\\n\\xa0\\n\\xa0\\nMicrosoft Word Automation\\n\\xa0\\n\\xa0\\nNow that we have our chart image generated, we must create a template document that is basically a normal Microsoft Word Document (.docx) formulated exactly in the way we want our report to look, including typefaces, font sizes, formatting, and page structure. Then all we need to do is to create placeholders for our automated content i.e. table values and images and declare them with variable names as shown below.\\n\\n\\nMicrosoft Word document template. Image by the author.\\n\\n\\xa0\\nAny automated content can be declared inside a pair of double curly brackets {{variable_name}}, including text and images. For tables, you need to create a table with a template row with all the columns included, and then you need to append one row above and one row below with the following notation:\\nFirst row:\\n\\n{%tr for item in variable_name %}\\n\\n\\xa0\\nLast row:\\n\\n{%tr endfor %}\\n\\n\\xa0\\nIn the figure above the variable names are\\n\\ntable_contents\\xa0for the Python dictionary that will store our tabular data\\nIndex\\xa0for the dictionary keys (first column)\\nPower, Current, and Voltage\\xa0for the dictionary values (second, third and fourth columns)\\n\\nThen we import our template document into Python and create a dictionary that will store our table’s values:\\n\\ntemplate = DocxTemplate(\\'template.docx\\')\\r\\ntable_contents = []for i in range(2, sheet_1.max_row + 1):\\r\\n    table_contents.append({\\r\\n        \\'Index\\': i-1,\\r\\n        \\'Power\\': sheet_1.cell(i, 1).value,\\r\\n        \\'Current\\': sheet_1.cell(i, 2).value,\\r\\n        \\'Voltage\\': sheet_1.cell(i, 3).value\\r\\n        })\\n\\n\\xa0\\nNext we‘ll’ import the chart image that was previously produced by Excel and will create another dictionary to instantiate all of the placeholder variables declared in the template document:\\n\\nimage = InlineImage(template,\\'chart.png\\',Cm(10))context = {\\r\\n    \\'title\\': \\'Automated Report\\',\\r\\n    \\'day\\': datetime.datetime.now().strftime(\\'%d\\'),\\r\\n    \\'month\\': datetime.datetime.now().strftime(\\'%b\\'),\\r\\n    \\'year\\': datetime.datetime.now().strftime(\\'%Y\\'),\\r\\n    \\'table_contents\\': table_contents,\\r\\n    \\'image\\': image\\r\\n    }\\n\\n\\xa0\\nAnd finally, we’ll render the report with our table of values and chart image:\\n\\ntemplate.render(context)\\r\\ntemplate.save(\\'Automated_report.docx\\')\\n\\n\\xa0\\n\\xa0\\nResults\\n\\xa0\\n\\xa0\\nAnd there you go, an automatically generated Microsoft Word report with numbers and a chart created in Microsoft Excel. And with that, you have a fully automated pipeline that can be used to create as many tables, charts, and documents as you could possibly ever need.\\n\\n\\nAutomatically generated report. Image by the author.\\n\\n\\xa0\\n\\xa0\\nSource Code\\n\\xa0\\n\\xa0\\n\\n\\xa0\\nIf you want to learn more about data visualization and Python, then feel free to check out the following (affiliate linked) courses:\\n\\xa0\\nData Visualization with Python\\n\\xa0\\nPython for Everybody Specialization\\n\\xa0\\nThe source code and template for this tutorial can be found in the following GitHub repository.\\n\\xa0\\nmkhorasani/excel_word_automation\\n\\xa0\\nIn addition, feel free to subscribe to Medium and explore more of my tutorials\\xa0here.\\n\\xa0\\nBio: Mohammad Khorasani is a hybrid of a data scientist and an engineer. Logistician. Candid. Realpolitik. Unlearning dogma one belief at a time. Read more of Mohammad\\'s writings.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Generate Automated PDF Documents with Python\\n5 Tasks To Automate With Python\\nData Scientists, You Need to Know How to Code',\n",
       " \"comments\\nBy Diego Lopez Yse, Data Scientist\\n\\n\\nPhoto by\\xa0PNG Design\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nNatural Language Processing (NLP)\\xa0is one of the most exciting fields in Artificial Intelligence. It allows machines to process and understand human language in a variety of ways, and it’s triggering a revolution in the way we interact with systems and technology.\\nIn a\\xa0previous post\\xa0I talked about NLP, its real-world applications, and some of its core concepts. Now I want to show you that NLP is as real as it gets, and anyone can start learning it. How? Let’s start with a simple text, and perform some Exploratory Data Analysis (EDA) around it using some NLP techniques. This way we can make sense out of data with simple and powerful tools before getting ourselves busy with any model or more complex tasks.\\n\\xa0\\nDefine your text\\n\\xa0\\nStephen Hawking once said:\\n\\n\\n“Artificial Intelligence (AI) is likely to be either the best or the worst thing to happen to humanity”\\n\\n\\nI couldn’t agree more with him, and time will tell what will actually happen. Nevertheless, this is a proper sentence to test some NLP techniques. To do that, let’s start by saving the phrase as a variable called “text”:\\n\\ntext = “Artificial Intelligence (AI) is likely to be either the best or the worst thing to happen to humanity.”\\n\\n\\nUsing the\\xa0langdetect\\xa0library, we can check its language, and find out the probability of being written in that language:\\n\\nimport langdetect\\r\\nfrom langdetect import detect_langs\\r\\nprint(detect_langs(text))\\n\\n\\n\\n\\n\\nWith a certainty of more than 99,9% we can state that this phrase is written in English language. You should also consider using\\xa0spelling check functionalities\\xa0to correct any grammatical mistakes.\\nWhat about the number of characters?\\n\\nlen(text)\\n\\n\\n\\n\\n\\nWe have 102 characters, including blank spaces. And the number of distinct characters?\\n\\nlen(set(text))\\n\\n\\n\\n\\n\\nLet’s take a look at them:\\n\\nprint(sorted(set(text)))\\n\\n\\n\\n\\n\\nThere’s something interesting here. We’re not only counting the non-alphanumerical characters like ‘(‘ and ‘.’, but also the blank spaces, and what’s even more, capitalized letters are considered different characters in relation to the lowercased ones.\\n\\xa0\\nTokenization\\n\\xa0\\nTokenization is the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called\\xa0tokens.\\xa0We use the\\xa0NLTK\\xa0library to perform this task:\\n\\nimport nltk\\r\\nfrom nltk.tokenize import word_tokenize\\r\\ntokenized_word = word_tokenize(text)\\r\\nprint(tokenized_word)\\n\\n\\n\\n\\n\\nWe can see that tokenization produces a list of words:\\n\\ntype(tokenized_word)\\n\\n\\n\\n\\n\\nWhich means we can call elements within it.\\n\\ntokenized_word[2:9]\\n\\n\\n\\n\\n\\nHow many tokens we have?\\n\\nlen(tokenized_word)\\n\\n\\n\\n\\n\\nAnd unique tokens?\\n\\nlen(set(tokenized_word))\\n\\n\\n\\n\\n\\nNow we can calculate a measure related to the\\xa0lexical richness\\xa0of the text:\\n\\nlen(set(tokenized_word)) / len(tokenized_word)\\n\\n\\n\\n\\n\\nThis shows that the number of distinct words is 85,7% of the total number of words.\\n\\xa0\\nLowercase & punctuation\\n\\xa0\\nNow let’s lowercase the text to standardize characters and for future stopwords removal:\\n\\ntk_low = [w.lower() for w in tokenized_word]\\r\\nprint(tk_low)\\n\\n\\n\\n\\n\\nNext, we remove non-alphanumerical characters:\\n\\nnltk.download(“punkt”)\\r\\ntk_low_np = remove_punct(tk_low)\\r\\nprint(tk_low_np)\\n\\n\\n\\n\\n\\nLet’s visualize the cumulative frequency distribution of words:\\n\\nfrom nltk.probability import FreqDist\\r\\nfdist = FreqDist(tk_low_np)\\r\\nfdist.plot(title = ‘Word frequency distribution’, cumulative = True)\\n\\n\\n\\n\\n\\nWe can see that the words “to” and “the” appear most often, but they don’t really add information to the text. They are what’s known as\\xa0stopwords.\\n\\xa0\\nStopwords removal\\n\\xa0\\nThis process includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.\\nFirst, we need to create a list of stopwords and filter them our from our list of tokens:\\n\\nfrom nltk.corpus import stopwords\\r\\nstop_words = set(stopwords.words(“english”))\\r\\nprint(stop_words)\\n\\n\\n\\n\\n\\nWe’ll use this list from NLTK library, but bear in mind that you can create your own set of stop words. Let’s look for the word “the” in the list:\\n\\nprint(‘the’ in stop_words)\\n\\n\\n\\n\\n\\nNow, let’s clean our text from these stopwords:\\n\\nfiltered_text = []\\r\\nfor w in tk_low_np:\\r\\n   if w not in stop_words:\\r\\n      filtered_text.append(w)\\r\\nprint(filtered_text)\\n\\n\\n\\n\\n\\nWe can see that the words “is”, “to”, “be”, “the” and “or” were removed from our text. Let’s update the cumulative frequency distribution of words:\\n\\n\\n\\nRemoving stop words should be done in a very conscious way, since it can bring huge problems while performing other tasks like sentiment analysis. If a word’s context is affected (e.g. by removing the word ‘not’, which is a negation of a component), that action can alter the meaning of the passage.\\nBeyond this example, it could be necessary to deal with other types of characteristics like\\xa0contractions\\xa0(like the word “doesn’t”, which should be expanded), or\\xa0accents and diacritics\\xa0(like the words “cliché” or “naïve”, which should be normalized by removing their diacritics).\\n\\xa0\\nRegular Expressions\\n\\xa0\\nRegular expressions\\xa0(called REs, or RegExes) are a tiny, highly specialized programming language embedded inside Python and made available through the\\xa0re\\xa0module. By using them, you specify the rules for the set of possible strings that you want to match. You can ask questions such as “Does this string match the pattern?”, or “Is there a match for the pattern anywhere in this string?”.\\nFor example, let’s search for words ending with “st”:\\n\\nimport re\\r\\n[w for w in filtered_text if re.search(‘st$’, w)]\\n\\n\\n\\n\\n\\nOr count the number of vowels in the first word (“artificial”):\\n\\nlen(re.findall(r’[aeiou]’, filtered_text[0]))\\n\\n\\n\\n\\n\\nYou can even modify texts based on conditions. For example, replace letters “ce” with letter “t” in the second word (“intelligence”):\\n\\nx = re.sub('ce', 't', filtered_text[1])\\r\\nprint(x)\\n\\n\\n\\n\\n\\nYou can find more examples of regular expressions following\\xa0this link.\\n\\xa0\\nConclusion\\n\\xa0\\nWe’ve only scratched the surface of all the possible and more complex NLP techniques out there. And it’s not just structured texts that you may want to analyze, but all that data generated from conversations, declarations or even tweets, which are examples of unstructured data.\\xa0Unstructured data\\xa0doesn’t fit neatly into the traditional row and column structure of relational databases, and represent the vast majority of data available in the actual world. It is messy and hard to manipulate.\\nNLP is seriously booming thanks to the huge improvements in the access to data and the increase in computational power, which allow us to achieve meaningful results in areas like healthcare, media, finance and human resources, among others.\\n\\n\\nMy suggestion is:\\xa0learn about NLP.\\xa0Try different data sources and techniques. Experiment, fail and improve yourself. This discipline will impact every possible industry, and we are likely to reach a level of advancement in the coming years that will blow our minds.\\n\\n\\nInterested in these topics? Follow me on\\xa0Linkedin\\xa0or\\xa0Twitter\\n\\xa0\\nBio: Diego Lopez Yse is an experienced professional with a solid international background acquired in different industries (capital markets, biotechnology, software, consultancy, government, agriculture). Always a team member. Skilled in Business Management, Analytics, Finance, Risk, Project Management and Commercial Operations. MS in Data Science and Corporate Finance.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nExploratory Data Analysis on Steroids\\nGetting Started with 5 Essential Natural Language Processing Libraries\\nNatural Language Processing Pipelines, Explained\",\n",
       " 'comments\\nBy Nate Rosidi, Data Scientist and Product Manager.\\n\\nBecoming a data scientist is considered a prestigious trait. Back in 2012, Harvard Business Review called \\'data scientist\\' the sexiest job of the 21st century, and the growing trend of roles in the industry seems to be confirming that statement. To confirm this sexiness is still ongoing, the info from Glassdoor shows being a data scientist is the second-best job in America in 2021.\\n\\nSource: Glassdoor.\\nTo get such a prestigious job, you have to go through rigorous job interviews. Data science questions asked can be very broad and complex. This is expected, considering the role of a data scientist usually incorporates so many areas.\\xa0 To help you prepare for the data science job interviews, I have reviewed all the applicable questions and separated them into different question categories. Here’s how I did that.\\nDescription and Methodology of the Analysis\\nI gathered data from various job search boards and websites and company review platforms such as Glassdoor, Indeed, Reddit, and Blind App. To be more precise, there are 903 questions collected over the past four years.\\nThe questions are sectioned into pre-determined categories. These categories are the result of an expert analysis of the interview experience description taken from our sources.\\nThe categories are:\\n\\nCoding\\nModelling\\nAlgorithms\\nStatistics\\nProbability\\nProduct\\nBusiness case\\nSystem design\\nTechnical\\n\\nWhat types of interview questions should you expect?\\nThis chart shows you the question type per category according to the collected data.\\n\\nTranslated to percentages, the chart looks like this:\\n\\nAs you can see, the coding and modelling questions are most dominant. More than half of all questions come from that area. It’s not surprising when you think about it. Coding and modelling are probably the two most important skills for a data scientist. Coding-type questions are widespread, comprising more than one-third of all questions. Other question types, such as algorithms and statistics, are also fairly significant; 24% of all questions come from these two categories. Other categories are not as represented. I find that reasonable, considering the nature of a data scientist role.\\nNow I want to guide you through every question category and show you some examples of the questions being asked.\\nThe most tested concepts on data science interview questions\\n\\xa0\\n\\xa0\\nCoding\\u200b\\n\\xa0\\nAs you already saw, coding questions are the single most important topic in data science. Such questions will require some sort of data manipulation using the code to identify insights. The questions are designed to test coding ability, problem-solving skills, and creativity. You’ll usually do that on a computer or a whiteboard.\\nCoding interview question example\\nOne example from Microsoft is this one:\\nQUESTION: “Calculate the share of new and existing users. Output the month, share of new users, and share of existing users as a ratio. New users are defined as users who started using services in the current month. Existing users are users who started using services in the current month and used services in any previous month.\\xa0Assume that the dates are all from the year 2020.”\\nYou’ll be using the table fact_events, with the sample data looking like this:\\n\\nTo get the desired output, you should write this code:\\n\\nwith all_users as (\\r\\n    SELECT date_part(\\'month\\', time_id) AS month,\\r\\n           count(DISTINCT user_id) as all_users\\r\\n    FROM fact_events\\r\\n    GROUP BY month),\\r\\nnew_users as (\\r\\n    SELECT date_part(\\'month\\', new_user_start_date) AS month,\\r\\n           count(DISTINCT user_id) as new_users\\r\\n    FROM\\r\\n         (SELECT user_id,\\r\\n           min(time_id) as new_user_start_date\\r\\n          FROM fact_events\\r\\n          GROUP BY user_id) sq\\r\\n    GROUP BY month\\r\\n)\\r\\nSELECT\\r\\n  au.month,\\r\\n  new_users / all_users::decimal as share_new_users,\\r\\n  1- (new_users / all_users::decimal) as share_existing_users\\r\\nFROM all_users au\\r\\nJOIN new_users nu ON nu.month = au.month\\r\\n\\r\\n\\n\\n\\xa0\\nWriting a code in SQL is the most often tested concept when it comes to coding. It’s no surprise since SQL has been the most used tool in data science. One of the concepts you almost can’t avoid in the interviews is the joins. So make sure you know the difference between different joins and how to use them to get the required result.\\nAlso, you can expect to group data using the GROUP BY clause very often. Some other concepts that are usually asked are filtering data using the WHERE and/or HAVING clause. You’ll also be asked to select distinct data. And also, make sure that you know the aggregate functions, such as SUM(), AVG(), COUNT(), MIN(), MAX().\\nSome concepts don’t occur that much often, but it’s worth mentioning them and being prepared for such questions. For example, Common Table Expressions or CTEs is one such topic. The other one is the CASE() clause. Also, don’t forget to refresh your memory on handling the string data types and dates.\\n\\xa0\\nModeling\\n\\xa0\\nModelling was the second-largest category in our research data, with 20% of all questions coming from here. These questions are designed to test your knowledge of building statistical models and implementing machine learning models.\\nModelling interview question example\\nRegression, the most common technical data science concept asked in interviews. It’s not surprising, considering the nature of the statistical modelling.\\nOne example from Galvanise would be the following:\\nQUESTION: “What is regularisation in regression?”\\nHere is how you could answer this question:\\nANSWER: “A regularisation is a special type of regression where the coefficient estimates are constrained (or regularised) to zero. By doing this, it is possible to reduce the variance of the model while at the same time decreasing the sampling error. Regularisation is used to avoid or reduce overfitting. Overfitting happens when the model learns training data so well it undermines the model’s performance on new data. To avoid overfitting, Ridge or Lasso regularisations are usually used.”\\nSome of the concepts tested regularly are, again, other regression analysis concepts, such as logistic regression, Bayesian logistic regression, and naive Bayes classifiers. You can also be asked about the random forests, as well as testing and evaluating models.\\n\\xa0\\nAlgorithms\\n\\xa0\\nQuestions on algorithms are all questions that require solving a mathematical problem, mainly through code by using one of the programming languages. These questions involve a step-by-step process, usually requiring adjustment or computation to produce an answer. These questions test the basic knowledge of problem-solving and data manipulation, which can be implemented for complex problems at work.\\nAlgorithm interview question example\\nThe technical concept tested most under algorithms is solving a mathematical or syntax problem with a programming language.\\nHere is one example you can find on Leetcode:\\nQUESTION: “You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list.\"\\nThe example of the data could be something like this:\\n\\nSource: Leetcode.\\nANSWER: The code written in Java should be:\\n\\npublic ListNode addTwoNumbers(ListNode l1, ListNode l2) {\\r\\n    ListNode dummyHead = new ListNode(0);\\r\\n    ListNode p = l1, q = l2, curr = dummyHead;\\r\\n    int carry = 0;\\r\\n    while (p != null || q != null) {\\r\\n        int x = (p != null) ? p.val : 0;\\r\\n        int y = (q != null) ? q.val : 0;\\r\\n        int sum = carry + x + y;\\r\\n        carry = sum / 10;\\r\\n        curr.next = new ListNode(sum % 10);\\r\\n        curr = curr.next;\\r\\n        if (p != null) p = p.next;\\r\\n        if (q != null) q = q.next;\\r\\n    }\\r\\n    if (carry > 0) {\\r\\n        curr.next = new ListNode(carry);\\r\\n    }\\r\\n    return dummyHead.next;\\r\\n}\\r\\n\\r\\n\\n\\n\\xa0\\nThe other general concepts often tested by this type of question are arrays, dynamic programming, strings, greedy algorithm, depth-first search, tree, hash table, and binary search.\\n\\xa0\\nStatistics\\n\\xa0\\nThe statistics interview questions are questions testing the knowledge of statistical theory and associated principles. These questions intend to try how familiar you are with the founding theoretical principles in data science. Being able to understand the theoretical and mathematical background of analyses being done is important. Answer those questions well, and every interviewer will appreciate you.\\nStatistics interview question example\\nThe most mentioned technical concept is sampling and distribution. For a data scientist, this is one of the most commonly used statistics principles the data scientist implements daily.\\nFor example, an interview question from IBM asks:\\nQUESTION: “What is an example of a data type with a non-Gaussian distribution?”\\nTo answer the question, you could first define a Gaussian distribution. Then you could follow this by giving examples of the non-Gaussian distribution. Something like this:\\nANSWER: “A Gaussian distribution is a distribution where a certain known percentage of the data can be found when examining standard deviations from the mean, otherwise known as a normal distribution. Some of the examples of the non-Gaussian distribution can be exponential distribution or binomial distribution.”\\nWhen preparing for the job interview, make sure you also cover the following topics: variance and standard deviation, covariance and correlation, the p-value, mean and median, hypothesis testing, and Bayesian statistics. These are all concepts you’ll need as a data scientist, so expect them in the job interviews too.\\n\\xa0\\nProbability\\n\\xa0\\nThese questions require theoretical knowledge only on probability concepts. Interviewers ask these questions to get a deep understanding of your knowledge on the methods and uses of probability to complete the complex data studies usually performed in the workplace.\\nProbability interview question example\\nIt’s highly probable, pun intended, that the question you’ll get is to calculate the probability of getting a certain card/number from a set of dice/cards. This seems to be the most common element of questioning for most companies in our research, as many of them have asked these types of questions.\\nAn example of such a probability question from Facebook:\\nQUESTION: “What is the probability of getting a pair by drawing two cards separately in a 52-card deck?”\\nHere is how you can answer this:\\nANSWER: “This first card you draw can be whatever, so it does not impact the result other than that there is one card less left in the deck. Once the first card is drawn, there are three remaining cards in the deck that can be drawn to get a pair. So, the chance of matching your first card with a pair is 3 out of 51 (remaining cards). This means that the probability of this event occurring is 3/51 or 5.89%.”\\nSince this is a kind of “specialised” question that deals only with probability, no other concepts are asked. The only difference is how imaginative the question is. But basically, you’ll always have to calculate the probability of some event and show your thinking.\\n\\xa0\\nProduct\\n\\xa0\\nProduct interview questions will ask you to evaluate the performance of a product/service through data. These questions test your knowledge of adapting and using data science principles in any environment, as is the case with daily work.\\nProduct interview question example\\nThe most prominent technical concept in this category is identifying a company’s product and proposing improvements from a data scientist’s perspective. The high variance in technical concepts tested on the product side can be explained by the nature of product questions and the higher level of creativity required to answer these.\\nAn example of a product question from Facebook would be:\\nQUESTION: “What is your favourite Facebook product, and how would you improve it?”\\nANSWER: Due to the nature of the question, we will let you answer this one yourself.\\nThe general concepts tested heavily depend on the company that’s interviewing you. Just make sure you are familiar with the company’s business and their products (ideally, you’re their user, as well), and you’ll be fine.\\n\\xa0\\nBusiness Case\\n\\xa0\\nThis category includes case studies and generic questions related to the business that would test a data science skill. The significance of knowing how to answer these questions can be enormous as some interviewers would like the candidates to know how to apply data science principles to solve a company’s specific problems before hiring them.\\nBusiness case question example\\nDue to the nature of the question type, I could not identify a single technical concept that stands out. Since most of the questions categorised here are case studies, they are unique in a certain way.\\nHowever, here is an example of a business case question from Uber:\\nQUESTION: “There is a pool of people who took Uber rides from two cities that were close in proximity, for example, Menlo Park and Palo Alto, and any data you could think of could be collected. What data would you collect so that the city the passenger took a ride from could be determined?”\\nANSWER: “To determine the city, we need to have access to the location/geographical data. The data collected could be GPS coordinates, longitude/latitude, and ZIP code.”\\n\\xa0\\nSystem Design\\n\\xa0\\nSystem design questions are all questions related to designing technology systems. They are asked to analyse the candidate’s process in solving problems, creating, and designing systems to help customers/clients. Knowing system design can be quite important for a data scientist; even if your role is not to design a system, you will most likely play a role in an established system and need to know how it works in order to do your work.\\nSystem design interview question example\\nThese questions cover different topics and tasks. But the one that stands out is building a database. Data scientists deal heavily with databases daily, so it makes sense to ask this question to see whether you can build a database from scratch.\\nHere is one question example from Audible uncovered in our research:\\nQUESTION: “Can you walk us through how you would build a recommendation system?”\\nANSWER: Since there is such a variety of approaches to answer this question, we will leave you to come up with your own way of building one.\\nAgain, to answer these questions, it’s essential to know the company’s business. Think a little about databases that the company most probably needs, and try to elaborate your approach a little before the interview.\\n\\xa0\\nTechnical\\n\\xa0\\nTechnical questions are all questions that are asking about the explanation of various data science technical concepts. The technical questions are theoretical and require knowledge of the technology you will be using at the company. Due to nature, they can seem similar to coding questions. Knowing the theory behind what you are doing is quite important, so technical questions can often be asked in interviews.\\nTechnical interview question example\\nThe most tested area is theoretical knowledge of Python and SQL. Not surprising, since these two languages are dominant in data science, along with R to complement Python.\\nAn example of a real-world technical question from Walmart would be:\\nQUESTION: “What are the data structures in Python?”\\nANSWER: “The data structures are used for storing data. There are four data structures in Python: List, Dictionary, Tuple, and Set. Those are the built-in data structures. Lists are used for creating lists that can contain different types of data. Dictionary is basically a set of keys; they are used to store a value with a key and getting the data using the same key. Tuples are the same as lists. The difference is that in a tuple, the data can’t be changed. Set contains the unordered elements with no duplicates. Along with the built-in data structures, there are also the user-defined data structures.”\\nThese are catch-all types of questions. It’s a category for all the questions that can’t cleanly fit into other categories. Due to that, there are no specific concepts that occur more or less often.\\nConclusion\\nThis data science interview guide has been written to support the research undertaken to understand the types of questions being asked at a data science interview. The interview questions’ data are taken from dozens of companies over a four-year period and analysed. The questions have been categorised under nine different question types (algorithms, business case, coding, modelling, probability, product, statistics, system design, and technical questions).\\nAs part of the analysis, I talked about some of the most common technical concepts from each question type category. For example, the most asked statistics questions have to do with sampling and distribution. Every question category is supported by one practical example of the real question.\\nThe article is intended to serve you as an important guide for interview preparation or simply learning more about data science. I hope I have helped you to feel more comfortable about the data science interview process. Good luck with your interviews!\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\n30 Most Asked Machine Learning Questions Answered\\nTop Python Data Science Interview Questions\\nHow Can You Distinguish Yourself from Hundreds of Other Data Science Candidates?',\n",
       " 'comments\\nBy Tyler Richards, Data Scientist @ Facebook\\n\\n\\nImage by author\\n\\n\\xa0\\nOver the past couple of weeks, I’ve been playing around with a new Streamlit feature called Streamlit sharing, which makes it super easy to deploy your custom apps. I’m going to go through a bit of background first, so if you want to see the docs for Streamlit sharing to get started you can find them\\xa0here.\\n\\xa0\\nStreamlit background\\n\\xa0\\nFor a bit of background, Streamlit is a framework that lets you quickly and confidently turn a python script into a web app and is an incredible tool for data scientists working on teams where they need to quickly share a model or an interactive analysis, or for data scientists working on personal projects they want to show the world. Here’s a\\xa0Streamlit beginner tutorial\\xa0if you want to try it out!\\nI’ve been using Streamlit for the past ~6 months, and it’s been\\xa0so\\xa0useful. Previously, if I knew I wanted to make a web app at the end of a project, I would always opt to switch to R for the wonderful R shiny framework, even though I am a much better python programmer than an R one. Going through Django or flask is just so much development friction to take on that it’s rarely worth it for a personal project and always takes too long for anything at work. But after using Streamlit, I now not only had options but found myself preferring python+Streamlit to R+shiny.\\n\\xa0\\nStreamlit sharing\\n\\xa0\\nThis brings me to a couple of months ago. I started a\\xa0DS project\\xa0focused on analyzing reading habits using data from the\\xa0Goodreads\\xa0app. I decided to try Streamlit out, and it turned a multi-day long process of getting a Django/flask app running well locally into one that took around a half-hour for local Streamlit use. It really is as easy as throwing your analysis into a script, and calling Streamlit functions whenever you want to put a graph, widget, or text explainer on the app.\\nHowever, the most annoying process on Streamlit was the deployment and management process. The\\xa0tutorial I followed\\xa0was straightforward, and didn’t take that much time, but was fairly extensive. It required launching an ec2 instance, configuring SSH, using tmux, and going back to this terminal every time you wanted to change anything about your web app.\\xa0It was doable but annoying.\\n\\n\\nImage by author\\n\\n\\xa0\\nA few weeks ago, Streamlit saw my Goodreads app and asked if I wanted to test out their Streamlit sharing beta, which was supposed to remove the friction explained above. I, obviously, gave it a shot.\\nAll I had to do was:\\n\\nPush my app to a Github repo\\nAdd a requirements.txt file that listed all the python libraries I used\\nPoint Streamlit to my app via the link to the repository\\nClick Deploy\\n\\nIt genuinely was\\xa0that easy\\xa0to figure out. I had sectioned off a couple of hours to figure it out, as I expected various bugs to pop up (it is in beta!), but it took me fewer than 10 minutes to get it up and running.\\nI currently have three apps running, one is a test app, the second is the\\xa0Goodreads book recommendation app\\xa0I mentioned earlier, and the third is an\\xa0interactive analysis\\xa0of a tech survey that I spun up (from idea to functioning and deployed web app) in around an hour and a half.\\nSwitching to Streamlit sharing has also saved me the ~$5 a month AWS bill, which I would gladly pay for this feature just for the savings in time spent on deployment alone.\\n\\n\\nImage by author\\n\\n\\xa0\\nIf I wanted to try out a new app, I could just click the new app button, point it to my repo, and they would handle literally everything else.\\n\\n\\nImage by author\\n\\n\\xa0\\nIf your Streamlit app uses any other packages, make sure to include a requirements.txt file in your repo — otherwise you’ll immediately get an error when deploying. You can use something like pip freeze to get requirements but that will give you all of the packages in the environment including those that you don’t use in your current project. And that will slow down your app deployment! So I’d suggest using something like pipreqs to keep it to just the core requirements for your app.\\n\\npip install pipreqs\\r\\npipreqs /home/project/location\\n\\n\\nIf you have requirements for apt-get, add them to\\xa0packages.txt -, one package per line.\\n\\xa0\\nConclusion\\n\\xa0\\nSo as a wrap-up, Streamlit sharing has saved me $ on both a development time saved and hosting cost basis (shoutout to the VC funds that make this all possible), has made my personal projects more interactive and prettier, and has taken away the headaches of deploying quick models or analyses. No wonder I’m a Streamlit fan.\\nWant to see more of this content? You can find me on\\xa0Twitter,\\xa0Substack, or on\\xa0my portfolio site.\\nHappy Stream(lit)ing!\\n\\xa0\\nBio: Tyler Richards is a Data Scientist at Facebook.\\nOriginal. Reposted with permission.\\nRelated:\\n\\n12-Hour Machine Learning Challenge: Build & deploy an app with Streamlit and DevOps tools\\nBuild an app to generate photorealistic faces using TensorFlow and Streamlit\\nMachine Learning Model Deployment',\n",
       " \"comments\\nBy Tessa Xie, Senior Data Scientist at Cruise\\n\\nPhoto by\\xa0bruce mars\\xa0on\\xa0Unsplash.\\n\\xa0\\nWhen I first made the transition from finance to data science, I felt like I was on the top of the world — I got a job in my dream field, my career track is set, I will just keep my head down and work hard, what could go wrong? Well, there were a couple of things… For the following year as a data scientist, there were several mistakes that I’m glad I caught myself making early in my career. This way, I had time to reflect and course-correct before it was too late. After a while, I realized that these mistakes are quite common. In fact, I have observed a lot of DS around me still making these mistakes, unaware that they can hurt their data career in the long run.\\nIf my 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist were what I learned from the best, the lessons in this article are those that I learned the hard way, and I hope I can help you avoid making the same mistakes.\\n\\xa0\\nMistake 1: Seeing yourself as a foot soldier instead of a thought partner\\n\\xa0\\n\\xa0\\nGrowing up, we have always been evaluated based on how well we can follow the rules and orders, especially in school. You will be the top student if you follow the textbook and practice exams and just put in the hard work. A lot of people seem to carry this “foot soldier” mindset into their working environment. In my opinion, this is the exact mindset that’s hindering a lot of data scientists from maximizing their impact and standing out from their peers. I have observed a lot of DS, especially junior ones, think they have nothing to contribute to the decision-making process and would rather retreat to the background and passively implement decisions made for them. This kicks off a vicious cycle — the less you contribute to those discussions, the less likely stakeholders will involve you in future meetings, and the less opportunity you will get to contribute in the future.\\nLet me give you a concrete example of the difference between a foot soldier and a thought partner in the case of model development. In the data collection and feature brainstorming meetings, the old me used to passively take notes on stakeholders’ suggestions so I can implement them “perfectly” later on. When someone proposed a feature that I knew we didn’t have data for, I would not say anything based on the assumption that they are more senior and they must know something that I overlooked. But guess what, they didn’t. I would later face the situation that 50% of the features we brainstormed would require additional data collection that would put our project deadline at risk. As a result, I often found myself in the undesirable position of the bad-news-bearing messenger in the end. Striving to be a thought partner nowadays, I involve myself early in the conversation and leverage my unique position as the person that’s closest to the data. This way, I can manage the expectations of stakeholders early on and make suggestions to help the team move forward.\\nHow to avoid this:\\n\\nMake sure you don’t hold back in meetings in which you can contribute something from the data perspective: are stakeholders’ definitions of metrics sufficient for what they want to measure? Is data available for measuring the set of metrics? If not, can we find proxies for the data we DO have?\\nImposter syndrome\\xa0is real, especially among junior DS. Make sure you are aware of this, and whenever you are questioning whether you should say something that “others might have already thought of” or ask a “stupid clarifying question,” YOU SHOULD.\\nMaintain a level of curiosity about what other people are working on. There are a lot of occasions where I found I could add value by noticing gaps other people may have overlooked due to their lack of understanding of the company’s data.\\n\\n\\xa0\\nMistake 2: Pigeonhole yourself into a specific area of data science\\n\\xa0\\n\\xa0\\nDo I want to be a data engineer or a data scientist? Do I want to work with marketing & sales data or do the geospatial analysis? You may have noticed that I have been using the term DS so far in this article as a general term for a lot of\\xa0data-related career paths\\xa0(e.g., data engineer, data scientist, data analyst, etc.). That’s because the lines are so blurred between these titles in the data world these days, especially in smaller companies. I have observed a lot of data scientists see themselves as ONLY data scientists building models and don’t pay attention to any business aspects or data engineers who only focus on data pipelining and don’t want to know anything about the modeling that’s going on in the company.\\nThe best data talents are the ones who can wear multiple hats or are at least able to understand the processes of other data roles. This comes in especially handy if you want to work in an early stage or growth stage startup, where functions might not be as specialized yet, and you are expected to be flexible and cover a variety of data-related responsibilities. Even if you are in a clearly defined job profile, as you get more experience over time, you might discover that you are interested in transitioning into a different type of data role. This pivot will be much easier if you did not pigeonhole yourself and your skillset into the narrow focus of one specific role.\\nHow to avoid this:\\n\\nAgain, be curious about the projects other data roles are working on. Schedule periodic meetings with colleagues to talk to each other about interesting projects or have different data teams share their work/projects with each other periodically.\\nIf you can’t get exposure to other data roles at work, try to keep up/practice the data skills you don’t use during your free time. For example, if you are a data analyst and haven’t touched modeling in a while, consider practicing the skills through outside projects like a Kaggle competition.\\n\\n\\xa0\\nMistake 3: Not keeping up with the development in the field\\n\\xa0\\n\\xa0\\n\\nComplacency Kills\\n\\n\\xa0\\nEvery soldier knows this, and every DS should, too. Being complacent about your data skills and not putting in the time to learn new ones is a common mistake. Doing this in the data field is more dangerous than in some other areas because data science is a field that’s relatively new and is still experiencing drastic changes and developments. There are constantly new algorithms, new tools, and even new programming languages being introduced.\\nIf you don’t want to be that one data scientist who still only knows how to use STATA in 2021 (he exists, I worked with him), then you need to keep up with the developments in the field.\\n\\ufeff\\nDon’t let this be you (GIF\\xa0by GIPHY)\\nHow to avoid this:\\n\\nSign up for online classes to learn about new concepts and algorithms or to brush up on the ones you already know but haven’t used in a while on the job. The ability to learn is a muscle everyone should keep practicing, and being a life-long learner is probably the best gift you can give to yourself.\\nSign up for a DS newsletter or follow a DS blogger/publication on Medium and develop a habit of following the DS “news.”\\n\\n\\xa0\\nMistake 4: Overflexing your analytical muscle\\n\\xa0\\n\\xa0\\nIf all you have is a hammer, everything looks like a nail. Don’t be that DS who tries to use ML on everything. When I first entered the world of data science, I was so excited about all the fancy models I learned in school and couldn’t wait to try all of them on real-world problems. But the real world is different from academic research, and the\\xa080/20 rule\\xa0is always at play.\\nIn my previous article about “5 Lessons McKinsey Taught Me,” I wrote about how\\xa0business impact and interpretability sometimes are more important than the extra several percentage points of your model’s accuracy. Sometimes maybe an assumptions-driven Excel model makes more sense than a multi-layered neural net. In those cases, don’t over-flex your analytical muscle and make your approach overkill. Instead, flex your business muscle and be the DS who also has business acumen.\\nHow to avoid this:\\n\\nHave a full range of analytical skills/tools in your armory, from simple Excel to advanced ML modeling skills, so you can always assess which tool is the best to use in the situation and not bring a gun to a knife fight.\\nUnderstand the business needs before delving into the analysis. Sometimes stakeholders would request an ML model because it’s a popular concept, and they have unrealistic expectations about what ML models can do. It’s your job as a DS to manage the expectations and help them find better and simpler ways to achieve their goals. Remember?\\xa0Be a thought partner, not a foot soldier.\\n\\n\\xa0\\nMistake 5: Think building a data culture is someone else’s job\\n\\xa0\\n\\xa0\\nIn my article “6 Essential Steps to Building a Great Data Culture,” I wrote about how the lives of data scientists can be horrible and unproductive if the company doesn’t have a great data culture. In fact, I have heard a lot of DS complaining about unproductive ad hoc data requests that should be easily handled by stakeholders in a self-sufficient fashion (for example, changing an aggregation from monthly to daily in Looker, which literally consists of two clicks). Don’t think changing that culture is someone else’s job. If you want to see changes, make them. After all, who is better positioned to build the data culture and educate stakeholders about data than data scientists themselves? Helping to build up the data culture in the company will make your life a lot easier down the road as well as your stakeholders.\\nHow to avoid this:\\n\\nMake it your responsibility to conduct training for the non-analytical stakeholders and develop self-serve resources.\\nMake sure you start practicing what you are preaching, start linking queries to slides, link data sources of truth to documents, and start documenting your code and databases. You can’t build up a data culture overnight, so it definitely takes patience.\\n\\nI do want to point out that it’s OKAY to make mistakes in your career. The most important thing is to learn from those mistakes and to avoid them in the future. Or even better, write them down to help others avoid making the same mistakes.\\n\\xa0\\nOriginal. Reposted with permission.\\nBio:\\xa0Tessa Xie\\xa0is an experienced Advanced Analytics Consultant skilled in data science, SQL, R, Python, Consumer Research and Economic Research with a strong engineering background following a Master's degree focused in Financial Engineering from MIT.\\nRelated:\\n\\nHow a Single Mistake Wasted 3 Years of My Data Science Journey\\nData Scientists think data is their #1 problem. Here’s why they’re wrong.\\nLearning from 3 big Data Science career mistakes\",\n",
       " 'comments\\nBy Angelica Lo Duca, Institute of Informatics and Telematics of the National Research Council\\nAccording to the Cambridge Dictionary [1], Data is information, especially facts or numbers, collected to be examined and considered and used to help decision-making, or information in an electronic form that can be stored and used by a computer. In other words, Data is a set of variables which can be quantitative or qualitative [2,3].\\n\\xa0\\nData Types\\n\\xa0\\n\\nData can be either quantitative or qualitative. Understanding the difference between quantitative and qualitative data is very important, because they are treated and analyzed in different ways: for example, you cannot calculate statistics for qualitative data, or you cannot exploit Natural Language Processing techniques for quantitative data.\\n\\n\\xa0\\nQuantitative Data\\n\\xa0\\n\\nQuantitative data include data which can be expressed as numbers, thus they can be measured, counted and analysed through statistics computations. Quantitative data can be used to describe and analyze a phenomenon, in order to discover trends, compare differences and perform predictions. Often, quantitative data are already structured, thus it is quite easy to perform further analysis.\\nQuantitative data include:\\n\\xa0\\n1. Continuous data, which can take any numeric value.\\nExamples of continuous data are:\\n\\nthe average temperature over the years (e.g. 35°C or 84.2 °F)\\nprice of a product over a month (e.g. $ 23.50 or 45.00 €)\\n\\nUsually continuous data are distributed into an interval, which can assume both negative and positive values (interval data).\\n\\xa0\\n2. Discrete data, which can take only certain numeric values.\\nExamples of discrete data are:\\n\\nscores of an exam, e.g 18 or 30\\nthe shoes number, e.g. 42 EU\\n\\nUsually discrete data are equidistant and non-negative (ratio data).\\n\\xa0\\nQualitative Data\\n\\xa0\\n\\nQualitative data cannot be measured through standard computation techniques, because they express feelings, sensations and experiences. Qualitative data can be used to understand the context around a given phenomenon and discover new aspects. Often, qualitative data are unstructured, thus they require additional techniques to extract meaningful information.\\nQuantitative data include:\\n\\xa0\\n1. Nominal data, which are used to label quantities which cannot be measured, without following a specific order. Usually, nominal data group similar objects.\\nExamples of nominal data include:\\n\\nthe languages spoken by a person (e.g. English, Italian, French)\\nColour palette (e.g. Red, Green)\\n\\n\\xa0\\n2. Ordinal data, which differ from nominal data only for the fact that they can be ordered.\\nExamples of nominal data include:\\n\\nthe opinion regarding a given product (e.g. poor quality, medium quality, high quality)\\nthe time of day (e.g. morning, afternoon, night)\\n\\n\\xa0\\nTypes of Data Analysis\\n\\xa0\\n\\nThe goal of data analysis is to discover hidden trends, patterns and relationships in the data.\\nAccording to the data type, different analyses can be performed: quantitative and qualitative analysis for quantitative and qualitative data, respectively.\\n\\n\\xa0\\nQuantitative Analysis\\n\\xa0\\n\\nQuantitative analysis [4] refers to quantitative data and includes the classical techniques for statistics:\\n\\xa0\\n1. Descriptive Statistics\\nDescriptive Statistics [5] analyses the past, by describing the basic features of data.\\nDescriptive Statistics is based on the calculation of some measures:\\n\\nFrequency (count, percentage)\\nCentral tendency (mean, median, mode)\\nVariability (maximum, minimum, range, quartile, variance)\\n\\n\\xa0\\n2. Inferential Statistics\\nInferential Statistics aims at building predictive models to understand the trend of a given phenomenon.\\nInferential Statistics includes the following types of analysis:\\n\\nHypothesis testing (ANOVA, t-test, Box-Cox, …)\\nConfidence interval estimation\\n\\n\\xa0\\nQualitative Analysis\\n\\xa0\\n\\nQualitative analysis [6] exploits qualitative data and tries to understand data context. Since it is not possible to measure data, the following strategies can be adopted to analyse qualitative data:\\n\\xa0\\n1. Deductive Analysis\\nIn Deductive Analysis, the researcher formulates some a-priori structures or questions to investigate data. This approach can be used when the research has at least a minimum overview of data.\\n\\xa0\\n2. Inductive Analysis\\nInductive Analysis starts looking at data in the hope of extracting some useful information. This kind of analysis is quite time consuming, since it requires a deep investigation of data. Inductive Analysis is used when the researcher has no idea of data.\\n\\xa0\\nConclusion\\n\\xa0\\nThis article has introduced the basic concept of data, which include quantitative and qualitative data. Quantitative analysis focuses on numbers, while qualitative analysis focuses on categories. A great effort has been done in both types of analysis, but the research is still open.\\n\\xa0\\nReferences\\n\\xa0\\n[1] Cambridge Dictionary: Definition of Data:\\nhttps://dictionary.cambridge.org/dictionary/english/data\\n[2] Qualitative vs Quantitative Data: Definitions, Analysis, Examples:\\nhttps://www.intellspot.com/qualitative-vs-quantitative-data/\\n[3] \\u200b\\u200bHow to Understand the Quantitative and Qualitative Data in Your Business:\\nhttps://laconteconsulting.com/2020/02/14/quantitative-qualitative-data/\\n[4] Quantitative Data: Definition, Types, Analysis and Examples:\\nhttps://www.questionpro.com/blog/quantitative-data/\\n[5] A Gentle Introduction to Descriptive Analytics:\\nhttps://medium.com/analytics-vidhya/a-gentle-introduction-to-descriptive-analytics-8b4e8e1ad238\\n[6] Qualitative Data – Definition, Types, Analysis and Examples:\\nhttps://www.questionpro.com/blog/qualitative-data/\\n\\xa0\\nBio: Angelica Lo Duca (Medium) works as post-doc at the Institute of Informatics and Telematics of the National Research Council (IIT-CNR) in Pisa, Italy. She is Professor of \"Data Journalism\" for the Master degree course in Digital Humanities at the University of Pisa. Her research interests include Data Science, Data Analysis, Text Analysis, Open Data, Web Applications and Data Journalism, applied to the fields of society, tourism and cultural heritage. She used to work on Data Security, Semantic Web and Linked Data. Angelica is also an enthusiastic tech writer.\\nRelated:\\n\\nThe Brutal Truth About Data Science\\nWhy and how should you learn “Productive Data Science”?\\nTop 10 Data Science Projects for Beginners',\n",
       " 'comments\\nBy Yulia Gavrilova, AI and Ethics of Tech at serokell.io.\\nYou have probably heard the terms ‘low-code’ and ‘no-code’ before.\\nLow-code simply stands for a reduced amount of coding. A lot of elements can be simply dragged and dropped from the library. However, it is also possible to customize them by writing your own code, which gives increased flexibility.\\n\\nNo-code platforms require no knowledge of programming at all. They can be used by different people like artists, teachers, top managers. They need AI in their work but don’t want to dive deep into programming and computer science. No-code solutions are quite limited in functionality but allow you to build something simple quickly.\\nIn practice, the border between no-code and low-code platforms is pretty thin. Platforms that promote themselves as ‘no-code’ still usually leave some space for customization.\\nLow-code platforms for beginners\\nLow-code libraries can be used even with minimal experience in coding.\\n\\xa0\\nPyCaret\\n\\xa0\\nThis is an\\xa0open-source machine learning library\\xa0in Python that allows you to create and deploy machine learning models with minimal coding.\\nBasically, PyCaret is a low-code alternative that can replace hundreds of lines of code with just a few words. It greatly increases the speed of software development and makes it more accessible for beginners. PyCaret is a Python wrapper over several machine learning libraries such as scikit-learn, XGBoost, Microsoft LightGBM, spaCy, and many more.\\n\\n\\xa0\\nAuto-ViML\\n\\xa0\\nAutoViML\\xa0is a tool that enables anyone to build a machine learning model fast. It automatically renders your data through different machine learning models in order to discover which one gives the best results in each particular case. Another great plus is that you don’t have to preprocess your data because AutoViML automatically cleans, transforms, and normalizes it. The program works with different types of variables, including textual, numeral, and visual data.\\n\\xa0\\nH2O AutoML\\n\\xa0\\nH2O\\xa0is an open-source machine learning platform. It has tools for deploying the most widely used machine learning algorithms such as gradient descent, linear regression, deep artificial neural networks, and others. What this platform is famous for is its cutting-edge AutoML. This feature provides for automating the process of building multiple models at once so you can create and test functional ML models even without prior experience.\\n\\nNo-code ML platforms you should use in 2021\\nHere is an assortment of no-code platforms that you can explore if you want to quickly deploy a machine learning element and integrate it with your existing software.\\n\\xa0\\nGoogle Cloud Auto ML\\n\\xa0\\nThis\\xa0no-code tool\\xa0enables anyone to train and deploy custom machine learning models without any ML expertise. The platform works with different types of data and covers a broad range of use cases, from computer vision and video intelligence to natural language processing and translation. You will be able to prepare and store your datasets and use automatic tools for facilitated labeling. If you need more power and more flexible tools, you can upgrade to use Google Cloud.\\n\\xa0\\nGoogle ML Kit\\n\\xa0\\nThis\\xa0toolkit\\xa0was made for Android and iOS developers who want to make their apps more engaging. Its API can be used to implement bar scanning, face detection, image labeling features, and more without having to create an ML model from scratch. All the necessary processing happens on the mobile device of the user in real-time, so there is no need for you to worry about setting up and hosting expensive servers.\\n\\n\\xa0\\nTeachable Machine\\n\\xa0\\nTeachable Machine\\xa0is another project by Google that facilitates the use of ML for apps and websites. This platform is easy to use even for non-tech-savvy people due to its user-friendly interface. The program works with images and allows you to train the machine to recognize and classify photos. It also processes sounds. The platform is interesting to play with if you’re a newbie, and it’s also free. But it is up to you to collect and prepare the data that you will use for training the model.\\n\\xa0\\nRunway AI\\n\\xa0\\nRunway AI\\xa0was built for creators with no programming experience in the domains of video and photo editing with the green screen option, filtering, and other interesting features. This toolkit can help you expand your creativity with technological tools in a few simple clicks, turning your videos into top-notch cinema art.\\n\\n\\xa0\\nLobe\\n\\xa0\\nThis\\xa0ML platform\\xa0has project templates that are easy to use, even for your first ML project. The project is relatively new, so only image classification is available right now. In the future, its creators also want to launch object detection and data classification templates. However, an image classifier is one of the most useful tools for retailers, advertisers, and business professionals, so be sure to check it out.\\n\\xa0\\nObviously AI\\n\\xa0\\nIf you are looking for a convenient tool for making predictions based on data without writing code,\\xa0Obviously AI\\xa0is for you. It can be used by marketers and business owners who want to forecast revenue flow, optimize business processes, build a more effective supply chain, and conduct personalized automated marketing campaigns. All you need is to provide data, pick a column based on which your custom ML algorithm would be created, and get your report.\\n\\n\\xa0\\nCreateML\\n\\xa0\\nCreateML\\xa0is a user-friendly drag-and-drop platform by Apple that allows you to train models on your Mac device. It can help you build classifiers and recommender systems. The tool can process images, videos, photos, tabular data, and texts. The model you get can be tested and deployed in IOS applications. You can preview the model’s performance and pause, save, resume, and extend your training process whenever you like. CreateML allows you to train multiple models on different datasets at once for a single project. It has standard Apple SDK and documentation that includes code samples and explanatory articles.\\n\\n\\xa0\\nMakeML\\n\\xa0\\nMakeML\\xa0enables iOS developers to implement object segmentation and object detection solutions. Using this tool, you can outline and edit elements not only in photos but also in videos. Create your own datasets, build custom ML models in a few clicks, and integrate your model into your app. This platform also allows you to work with AR.\\n\\xa0\\nFritz AI\\n\\xa0\\nIf you are looking for more exciting solutions for iOS and Android apps, you can also check out\\xa0Fritz AI. It gives you flexibility in how much you want to be invested in ML model development ― you can train custom models in the Studio or use pre-trained models. In the program, you can create or import your own datasets, monitor the model’s performance, and re-train it. If you do Snapchat lens development, this tool will help you add no-code machine learning to your augmented reality filters.\\n\\xa0\\nSuperAnnotate\\n\\xa0\\nMaking annotations to videos and texts is a tedious job, but it can be automated with\\xa0SuperAnnotate. The solution covers a multitude of cases across different industries, such as aerial photography, autonomous driving, robotics, and medicine. If you quickly need to process images and you don’t want to hire a whole team of data scientists, we recommend checking it out.\\n\\n\\xa0\\nRapid Miner\\n\\xa0\\nRapidMiner\\xa0is a tool created for data mining. It is based on the idea that business analysts or data analytics don’t necessarily have to program to do their job. At the same time, mining requires data, so the tool was equipped with a good set of operators solving a wide range of tasks for obtaining and processing information from various sources (databases, files). Overall, this tool makes data analytics simple enough for anyone to use it.\\n\\xa0\\nWhat-If Tool\\n\\xa0\\nThis is a super useful tool to assess the performance of the models without coding.\\xa0WIT\\xa0visually displays how model behavior changes over time and over different subsets of data. You can also compare the performance of two models to see which one works best.\\n\\xa0\\nDataRobot\\n\\xa0\\nDataRobot\\xa0is a platform that enables business analysts to build predictive analytics without knowledge of machine learning or programming. The platform uses automated machine learning (AutoML) to generate accurate predictive models in a short amount of time. DataRobot provides a user-friendly user interface for creating machine learning models. In just a few steps, a company can deploy a real-time predictive analytics service.\\n\\n\\xa0\\nNanonets AI\\n\\xa0\\nIntelligent document processing is possible with\\xa0Nanonets. It captures data from documents automatically, saving you from hours of manual document management. Nanonets AI processes unseen, semi-structured documents even if they don’t follow a standard template, automatically validates data, and improves over time through multiple usages.\\n\\xa0\\nMonkey Learn Studio\\n\\xa0\\nMonkeyLearn Studio\\xa0provides tools for working with textual data and is aimed at being used by companies. This platform can automatically tag business data, for example, support tickets or emails. It also helps with the visualization of data. MonkeyLearn makes it easy to work with machine learning because it has ready-made machine learning models that can be trained and built code-free.\\nFinal words\\nThese tools are cool for what they are: no-code platforms for quick deployment of simple projects by non-tech experts or newbies in ML. By no means can they substitute custom ML model development for high-load, data-intensive projects. So if you have a unique idea in mind that involves the processing of big data, automation of intensive industrial processes, or sensitive prediction models,\\xa0contact us. Together, we can think of solutions that will fit your particular needs.\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nPushing No-Code Machine Learning to the Edge\\nThe next-generation of AutoML frameworks\\nEasy AutoML in Python',\n",
       " \"comments\\nBy Edward Krueger, Senior Data Scientist and Tech Lead & Douglas Franklin, Aspiring Data Scientist and Teaching Assistant\\n\\nPhoto by Brad Neathery on Unsplash\\n\\xa0\\nIntroduction\\n\\xa0\\nOur goal is to create an easy way to time functions in Python. We do this by coding a decorator with Python’s libraries\\xa0functools\\xa0and\\xa0time. This decorator will then be applied to functions whose runtime we are interested in.\\n\\xa0\\nTiming Decorator: @timefunc\\n\\xa0\\nThe code below represents a common decorator pattern that has a reusable and flexible structure. Notice the placement of\\xa0functool.wraps. It is a decorator for our closure. This decorator preserves\\xa0func’s metadata as it is passed to the closure.\\n\\nFunctools becomes significant on line 16, where we access\\xa0func.__name__\\xa0in our print statement. If we did not use\\xa0functools.wraps\\xa0to decorate our closure, the wrong name would be returned.\\nThis decorator returns the runtime of the function passed to\\xa0timefunc(). On line 13,\\xa0start\\xa0initiates timing. Then, line 14's\\xa0result\\xa0stores the value of\\xa0func(*args, **kwargs).After that, time_elapsed is calculated. The print statement reports\\xa0func’s name and execution time.\\n\\xa0\\nApplying timefunc with the @ symbol\\n\\xa0\\nIn Python, decorators can be easily applied with the\\xa0@\\xa0symbol. Not all applications of decorators use this syntax, but all\\xa0@\\xa0symbols are an application of a decorator.\\nWe decorate\\xa0single_thread\\xa0with\\xa0timefunc\\xa0using the\\xa0@\\xa0symbol.\\n\\nNow that\\xa0single_thread\\xa0is decorated, when it’s called on line 13 we’ll see its\\xa0func.__name__\\xa0and runtime.\\n\\n\\nOutput of single_thread decorated by timefunc\\n\\n\\xa0\\nIf you want to know how this works, below we will go a little deeper into the why and how of coding a decorator to time functions.\\n\\xa0\\nWhy One Might Time a Function\\n\\xa0\\nThe reason is relatively straightforward. Faster functions are better functions.\\n\\n\\nTime is money, friend. — Gazlowe\\n\\n\\nThe timing decorator shows us a function’s runtime. We can apply the decorator to several versions of a function, benchmark them and choose the fastest one. Additionally, it is useful to know how long executions will take when testing code. Got a five-minute runtime ahead? That's a nice window for getting up, moving your legs and refilling your coffee!\\nTo write decorator functions in Python we rely on\\xa0functools\\xa0and an awareness of scope. Let's review scope and decoration.\\n\\xa0\\nDecoration, Closures and Scope\\n\\xa0\\nDecoration is a design pattern in Python that allows you to modify the behavior of a function. A decorator is a function that takes in a function and returns a modified function.\\nWhen writing closures and decorators, you must keep the scope of each function in mind. In Python, functions define scope. Closures have access to the scope of the function that returns them; the decorator’s scope.\\nIt is important to preserve a decorated function's metadata as it is passed to a closure. Knowing our scope lets us properly decorate our closures with\\xa0functools.wraps.\\nFor more on these concepts read this three-minute piece.\\nDecorators and Closures by Example in Python\\nHow to augment the behavior of a function using a decorator\\n\\xa0\\nOn the reusability of this decorator\\n\\xa0\\nNotice that\\xa0func\\xa0is taken as an argument on line 7. Then on line 11, we pass\\xa0*args, **kwargs, into our closure. These\\xa0*args, **kwargs\\xa0are used to calculate the\\xa0result\\xa0of\\xa0func(*args, **kwargs)\\xa0on line 10.\\n\\nThe flexibility of\\xa0*args\\xa0and\\xa0**kwargsallow\\xa0timefunc\\xa0to work on almost any function. Our closure’s print statement is designed to access the functions\\xa0__name__,\\xa0args,\\xa0kwargs\\xa0and\\xa0resultto create a useful timing output for\\xa0func.\\n\\xa0\\nConclusion\\n\\xa0\\nDecoration is a powerful tool to augment the behavior of functions. By coding a decorator to time your functions, you gain an elegant, reusable pattern to track a function’s runtime.\\nFeel free to copy\\xa0timefunc into your codebase, or you can try coding your own timing decorator!\\n\\xa0\\nEdward Krueger is a Senior Data Scientist and Technical Lead at Business Laboratory and an Instructor at McCombs School of Business at The University of Texas at Austin.\\nDouglas Franklin is a Teaching Assistant at McCombs School of Business at The University of Texas at Austin.\\nOriginal. Reposted with permission.\\nRelated:\\n\\n15 common mistakes data scientists make in Python (and how to fix them)\\nHow to Speed Up Pandas with Modin\\n11 Essential Code Blocks for Complete EDA (Exploratory Data Analysis)\",\n",
       " \"comments\\nBy Marco Santoni, Data Product Manager\\nHow many data engineers should we hire? Are they too many compared to our data scientists?\\nOne of the key decisions to take when building a data science team is the\\xa0mix of roles. This means choosing the right mix of background and of activities that each member of the team should have. I'll compare two models of teams I've experienced so far and define them as\\xa0chess-team\\xa0model and\\xa0checkers-team\\xa0model.\\n\\xa0\\nChess-Team Model\\n\\xa0\\n\\nThe chess-team model is the common model we read about in literature. In a chess-team, each member of the team has a\\xa0specific role. Roles are usually:\\xa0data engineers,\\xa0data scientists, and\\xa0machine learning engineers. These roles typically correspond to different sets of skills (eg ML and statistics vs coding and devops) and to different set of activities (model selection vs data preparation vs model deployment).\\nSimilarly to a chess piece which has a clear role that is different from the other pieces, a member of a data science chess-team is assigned a subset of the tasks that are part of the development pipeline. Let's consider a simplistic development pipeline:\\n\\ndata preparation -> data engineer\\nmodel development -> data scientists\\nmodel deployment -> machine learning engineer\\n\\nThe three activities of this development pipeline correspond to the three roles of the team, and there is little space for confusion. A data engineer probably won't work a lot on the model development and selection, while a data scientist probably won't be the one deploying the model in production.\\n\\xa0\\nCheckers-Team Model\\n\\xa0\\n\\nThe checkers-team model is a definition of a team model that I introduce in this post. In a checkers-team, each member of the team does not have a specific role because he may in charge of working on\\xa0any step of the development\\xa0pipeline. There are no roles like\\xa0data engineer\\xa0or\\xa0data scientist\\xa0because taking such a role implies limiting the scope of activities a team member should work on. Let' make an example. In a checkers-team, there is no\\xa0data scientist\\xa0because no one is in charge of model development\\xa0only.\\nSo, what is the role of someone working in a checkers-team? A member of the team can be defined as a\\xa0full-stack data developer. A full-stack data developer is someone that for example works on data extraction\\xa0AND\\xa0model development\\xa0AND\\xa0model deployment. In a checkers-team, everyone works possibly on every piece of the development lifecycle. In this sense, the team is more similar to checkers pieces. There is no move that a piece can take and another piece cannot. Similarly, there is no activity that any team member cannot do. For example, everyone can contribute to building devops pipelines and automation.\\nOf course, every team member has a different\\xa0background\\xa0and a different set of skills from his/her teammates. One can come from a software engineering experience, another one can come from data science studies. However, the strategy of building a checkers-team is to invest in\\xa0training\\xa0team members to grow\\xa0horizontally\\xa0their set of skills.\\n\\xa0\\nPros and Cons\\n\\xa0\\nLet's consider some key differences between a chess and a checkers team model.\\nFlexibility.\\xa0The balance of types of activities is not stable over time in a team. There can be times when there is a peak of work items in data engineering and little or no work items in ML model development. These peaks can be due to different phases of the data product development cycle or due to varying business requirements. A checkers-team is flexible and can adapt quickly to these peaks. A checkers-team could for example dedicate the entire team to develop data engineering pipelines in a Scrum sprint if needed. The same flexibility is not as easy in a chess-team model where you have constraints due to different skills and different responsibilities.\\nComplexity.\\xa0Not every data science team is facing the same level of complexity in their projects. Imagine a team that is building an AI model for self-driving cars. It is a complex problem to solve that requires advanced skills in computer vision and AI. These skills cannot be learned quickly but usually need a specific education or career path. When facing such problems, you need team members which are specialists in area like vision or AI. A chess-team is designed to host specialists in certain fields and is designed to grow vertically such skills. In a checkers-team, there are not such specialists.\\nAwareness.\\xa0A member of a checkers-team knows in details every phase of the development cycle. While he is designing a ML model, he is aware at the same time of how the release pipeline and the operations of the model work. He may take decisions during model selection that take into consideration where the model will be hosted and possible constraints of the production platform. On the other hand, a data scientist of a chess-team knows less details (because he has not being working on it by himself) of how the model will be deployed and run. This minor awareness may lead to assumptions taken during model development, and these assumptions can bring to more complexity to those in charge of deploying such model.\\nSense of Ownership.\\xa0In a checkers-team, you are in charge of both engineering data pipelines, developing models, and deploying them. Any issue that may occur in these phases is also\\xa0your\\xa0issue. You can't delegate too much, and, therefore, you naturally feel responsible to contribute to the resolution. Distributing the ownership makes every team member more active in improving the development life cycle.\\n\\xa0\\nWhen is a Team Model Right?\\n\\xa0\\nThe answer depends on the context and the organization you work at. Is the data science team is working on the\\xa0core product\\xa0of the company? If this is the case, the models that are developed may need a level of specialization that can't just be achieved by a checkers-team.\\nOr is the team rather working on adding tiny features or on improving the operations of the company? In this case, probably you won't be developing state-of-the-art AI models, and you can rely existing\\xa0libraries or SaaS\\xa0that make life easier for you. As complexity is not an obstacle, going for checkers-team may be a good option.\\nWhat is the size of your data science team? Or even how many teams do you have? Large organizations go for multiple data teams. These teams may be divided\\xa0functionally\\xa0(eg 1 team of data engineers + 1 separate team of data sciensts) or they may be divided by\\xa0business units\\xa0(eg 1 data team for marketing and 1 data team for recommender system). You can't of course adopt the checkers-team model in an large organization that design the data teams by functions, but you may still adopt this model in a large organization that creates multiple self-organized teams each dedicated to a specific business unit.\\nA last point to consider is the\\xa0IT architecture. A checkers-team requires the same person to work on very different tasks. This is viable only if the complexity of such tasks is small. Adopting\\xa0SaaS and PaaS\\xa0resources simplifies every task by hiding the complexity of managing and running the resources. They let you focus on your goal. For example, building an API endpoint hosted by a function-as-a-service is something feasible by a data scientist with a mathematical background. Doing the same from scratch on an on-premise server is not as feasible.\\nImages courtesy of\\xa0@pecanlie\\xa0and\\xa0@rafaelrex\\n\\xa0\\nBio: Marco Santoni is currently a Data Product Manager with experience both as data scientist and software developer. He is passionate about data science and software engineering, and he is lucky to have turned them into my daily work. Always driven by curiosity and by an attitude for learning, he enjoys working both on the façade and on the foundations of a data product, ranging from machine learning to data engineering, devops, and data intensive architectures.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nSix Tips on Building a Data Science Team at a Small Company\\nThe Maslow’s hierarchy your data science team needs\\nThe Missing Teams For Data Scientists\",\n",
       " 'Sponsored Post.\\nBy Laura Gorrieri, expert.ai\\nPlease find the notebook version of this thread here.\\nLet\\'s build a small application to investigate one of my favourite artists. They are called \"The Penguin Café Orchestra\" and if you don\\'t know them you are going to find out what they are about.\\nOur dataset: a list of their album\\'s reviews that I took from Piero Scaruffi\\'s website and saved in a dedicated folder.\\nOur goal: to understand more about an artist using album reviews.\\nOur practical goal: to see how expert.ai’s NL API works and what it can do.\\nWhat is The Penguin Café Orchestra about?\\nFirst let\\'s see what comes out from the reviews just analysing the words used in them. We\\'ll firstly concatenate all the reviews in one variable, in order to have a whole artist\\'s review. Then we are going to take a look at the most frequent words in them, hoping that it will reveal more on the Penguin Café Orchestra.\\n\\r\\n## Code for iterating on the artist\\'s folder and concatenate albums\\' reviews in one single artist\\'s review\\r\\nimport os\\r\\n\\r\\nartist_review = \\'\\'\\r\\nartist_path = \\'penguin_cafe_orchestra\\'\\r\\nalbums = os.listdir(artist_path)\\r\\n\\r\\nfor album in albums:\\r\\nalbum_path = os.path.join(artist_path, album)\\r\\n\\xa0 \\xa0 \\xa0 with open(album_path, \\'r\\', encoding = \\'utf8\\') as file:\\r\\n           review = file.read()\\r\\n           artist_review += review\\r\\n\\nUsing a shallow-linguistics approach we can investigate the artist review, which contains all the available reviews. To do so we use matplotlib and word cloud to produce a word cloud that will tell us more about the most frequent words in the text.\\n\\xa0\\n# Import packages\\n\\r\\nimport matplotlib.pyplot as plt\\r\\n%matplotlib inline\\r\\n\\r\\n# Define a function to plot word cloud\\r\\ndef plot_cloud(wordcloud):\\r\\n    # Set figure size\\r\\n    plt.figure(figsize=(30, 10))\\r\\n    # Display image\\r\\n    plt.imshow(wordcloud)\\r\\n    # No axis details\\r\\n    plt.axis(\"off\");\\r\\n\\r\\n# Import package\\r\\nfrom wordcloud import WordCloud, STOPWORDS\\r\\n\\r\\n# Generate word cloud\\r\\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color=\\'white\\', collocations=False, stopwords = STOPWORDS).generate(artist_review)\\r\\n\\r\\n# Plot\\r\\nplot_cloud(wordcloud)\\r\\n\\n\\nFig.1: A word cloud in which the most used words appear in a bigger font and the less used one in a smaller font.\\nHow does their music make you feel?\\nThanks to the word cloud, we know more about The Penguin Café Orchestra. We know that they use instruments such as the ukulele, piano and violin, and that they mix genres such as folk, ethnic, and classic.\\nStill, we have no idea of the style of the artist. We can know more by looking at what emotions come out of their work.\\n\\xa0\\nTo do so, we are going to use expert.ai’s NL API. Please register here, find the documentation on the SDK here and on the features here.\\n### Install the python SDK\\n\\r\\n!pip install expertai-nlapi\\r\\n\\r\\n## Code for initializing the client and then use the emotional-traits taxonomy\\r\\n\\r\\nimport os\\r\\n\\r\\nfrom expertai.nlapi.cloud.client import ExpertAiClient\\r\\nclient = ExpertAiClient()\\r\\n\\r\\nos.environ[\"EAI_USERNAME\"] = \\'your_username\\'\\r\\nos.environ[\"EAI_PASSWORD\"] = \\'your_password\\'\\r\\n\\r\\nemotions =[]\\r\\nweights = []\\r\\n\\r\\noutput = client.classification(body={\"document\": {\"text\": artist_review}}, params={\\'taxonomy\\': \\'emotional-traits\\', \\'language\\': \\'en\\'})\\r\\n\\r\\nfor category in output.categories:\\r\\n    emotion = category.label\\r\\n    weight = category.frequency\\r\\n    emotions.append(emotion)\\r\\n    weights.append(weight)\\r\\n\\r\\nprint(emotions)\\r\\nprint(weights)\\r\\n\\n\\n[\\'Happiness\\', \\'Excitement\\', \\'Joy\\', \\'Amusement\\', \\'Love\\']\\n[15.86, 31.73, 15.86, 31.73, 4.76]\\n\\nFor retrieving weights we used “frequency” which is actually a percentage.\\xa0 The sum of all the frequencies is 100. This makes the frequencies of the emotions a good candidate for a pie chart, that is plotted using matplotlib.\\n# Import libraries\\n\\r\\nfrom matplotlib import pyplot as plt\\r\\nimport numpy as np\\r\\n\\r\\n# Creating plot\\r\\ncolors = [\\'#0081a7\\',\\'#2a9d8f\\',\\'#e9c46a\\',\\'#f4a261\\', \\'#e76f51\\']\\r\\nfig = plt.figure(figsize =(10, 7))\\r\\nplt.pie(weights, labels = emotions, colors=colors, autopct=\\'%1.1f%%\\')\\r\\n\\r\\n# show plot\\r\\nplt.show()\\r\\n\\n\\nFig.2: A pie chart representing each emotion and its percentage.\\nWhat\\'s their best album?\\nIf you wanted to start to listen to them, to see if you feel the same emotions that Scaruffis found in their work, where could you start? We can take a look at the sentiment analysis for each album and get an idea of their best ones. To do so, we iterate on each album\\'s review and use expert.ai NL API to retrieve their sentiment and its strength.\\n## Code for iterating on each album and retrieving the sentiment\\n\\r\\nsentiment_ratings = []\\r\\nalbums_names = [album[:-4] for album in albums]\\r\\n\\r\\nfor album in albums:\\r\\n    album_path = os.path.join(artist_path, album)\\r\\n    with open(album_path, \\'r\\', encoding = \\'utf8\\') as file:\\r\\n        review = file.read()\\r\\n        output = client.specific_resource_analysis(\\r\\n            body={\"document\": {\"text\": review}}, params={\\'language\\': \\'en\\', \\'resource\\': \\'sentiment\\' })\\r\\n            sentiment = output.sentiment.overall sentiment_ratings.append(sentiment)\\r\\n\\r\\nprint(albums_names)\\r\\nprint(sentiment_ratings)\\r\\n\\n[\\'Broadcasting From Home\\', \\'Concert Program\\', \\'Music From the Penguin Cafe\\', \\'Signs of Life\\']\\n[11.6, 2.7, 10.89, 3.9]\\n\\xa0\\nNow we can visualize the sentiment for each review using a bar chart.\\xa0 This will give us quick visual feedback on the best album of The Penguin Cafe Orchestra, and on their career. To do so we use once again matplotlib.\\n\\r\\nimport matplotlib.pyplot as plt\\r\\nplt.style.use(\\'ggplot\\')\\r\\n\\r\\nalbums_names = [name[:-4] for name in albums]\\r\\n\\r\\nplt.bar(albums_names, sentiment_ratings, color=\\'#70A0AF\\') plt.ylabel(\"Album rating\")\\r\\nplt.title(\"Ratings of Penguin Cafe Orchestra\\'s album\")\\r\\nplt.xticks(albums_names, rotation=70)\\r\\nplt.show()\\r\\n\\n\\nOriginally posted here.',\n",
       " \"comments\\nBy Mehmet Suzen, Theoretical Physicist | Research Scientist.\\nTime series analysis\\xa0is needed almost in any quantitative field and real-life systems that collect data over time, i.e., temporal datasets. Building predictive models on temporal datasets for the future evolution of systems in consideration are usually called\\xa0forecasting. The validation of such models deviates from the\\xa0standard holdout method\\xa0of having random disjoint splits of train, test, and validation sets used in supervised learning. This stems from the fact that time series are ordered, and order induces all sorts of statistical properties that should be retained. For this reason, applying\\xa0direct cross-validation\\xa0to time-series model building is not possible and only restricted to out-of-sample (OOS) validation, using the end-tail of a temporal set as a single test set. Recent work proposed an approach that overcomes the known limitation of achieving full cross-validation for time series. The approach opens up a possibility to produce learning curves for the time-series models as well, which is usually also not possible due to similar reasons.\\n\\xa0\\nReconstructive Cross-validation (rCV):\\xa0A meta-algorithm design principles\\n\\xa0\\nrCV is proposed recently in the paper titled\\xa0Generalised learning of time-series: Ornstein-Uhlenbeck\\xa0processes. The\\xa0design principles of rCV for time-series aims at the following principles:\\n\\nFigure 1: rCV meta-algorithm for time series cross-validation and learning curves.\\n\\nLogically close to standard cross-validation: Arbitrary test-set size and number of folds.\\nPreserve correlations and data order.\\nDoes not create the absurdity of predicting the past from the future data.\\nApplicable in a generic fashion regardless of the learning algorithm.\\nApplicable to multi-dimensional time series.\\nEvaluation metric agnostic.\\n\\n\\xa0\\nIdea of introducing missing data: Temporal cross-validation and learning curves\\n\\xa0\\nThe key idea of rCV is to create cross-validation sets via creating missing-data sets K-times, as in K-fold, with a given degree of missing ratio, i.e., random data point removal. Each fold will have a disjoint set of missing data points. By an imputation method, we would fill out the K-disjoint missing data sets and generate K-different training datasets. \\xa0This would allow us to have K-different models, and we could measure the generalised performance of the modelling approach by testing the primary model's prediction on the Out-of-sample (OOS) test set. To avoid confusion about\\xa0what is a model?, what we are trying to achieve is to find out the hypothesis, i.e., the modelling approach. \\xa0By changing the ratio of missing data and repeating the cross-validation, the exercise will yield to set of the ratio of missing-missing data introduced and their corresponding rCV errors, where the plot is nothing but a learning curve from a supervised learning perspective. \\xa0Note that the imputation and prediction models are different models. The primary model we are trying to build is the prediction model we used for producing OOS predictions. The procedure is summarised in Figure 1.\\n\\nFigure 2: Synthetic\\xa0data and\\xa0reconstructions.\\n\\xa0\\nShowcase with Gaussian process models on Ornstein-Uhlenbeck processes\\n\\xa0\\nTo demonstrate the utility of rCV, the mentioned paper uses synthetic data generated by the Ornstein-Uhlenbeck process, i.e., Gaussian process with a certain parameter setting. \\xa0Figure 2 shows the synthetic data and example locations of generated missing dataset’s reconstruction errors. Figure 3 shows learning curves depending on the different ratios of the missing data setting.\\n\\nFigure 3: Learning curves for the Gaussian Process model generated by rCV.\\n\\xa0\\nConclusion\\n\\xa0\\nrCV provides a logically consistent way of practicing cross-validation in time series. It is usually not possible to produce learning curves on the same time window for the time series model, but using rCV with different ratios of missing data achieves this as well. rCV paves the way to do generalised learning for time series.\\nFurther Reading\\nApart from the paper\\xa0Generalised learning of time-series: Ornstein-Uhlenbeck\\xa0processes, the results can be reproduced with the Python prototype implementation\\xa0here.\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nTraining Sets, Test Sets, and 10-fold Cross-validation\\nTime Series Forecasting with PyCaret Regression Module\\nVisualizing Cross-validation Code\",\n",
       " 'comments\\nBy Hamel Husain, Staff Machine Learning Engineer at GitHub\\n\\n\\xa0\\nBackground\\n\\xa0\\nI recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn\\'t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That\\'s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions.\\nAnd that\\'s exactly what I did over the past month! I\\'m pleased to report that it has been the most efficient learning experience I\\'ve ever experienced. I\\'ve discovered that writing documentation forced me to deeply understand not just what the code does but also\\xa0why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose,\\xa0fastcore\\xa0is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique.\\nFor the uninitiated,\\xa0fastcore\\xa0is a library on top of which many\\xa0fast.ai\\xa0projects are built on. Most importantly,\\xa0fastcore\\xa0extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I\\'m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more!\\n\\xa0\\nWhy fastcore is interesting\\n\\xa0\\n\\nGet exposed to ideas from other languages without leaving python:\\xa0I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages.\\nYou get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems.\\nLearn more about the Python programming language:\\xa0Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work.\\n\\n\\xa0\\nA whirlwind tour through fastcore\\n\\xa0\\nHere are some things you can do with fastcore that immediately caught my attention.\\n\\xa0\\nMaking **kwargs transparent\\n\\xa0\\nWhenever I see a function that has the argument\\xa0**kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example:\\n\\ndef baz(a, b=2, c =3, d=4): return a + b + c\\r\\n\\r\\ndef foo(c, a, **kwargs):\\r\\n    return c + baz(a, **kwargs)\\r\\n\\r\\ninspect.signature(foo)\\r\\n\\n\\n\\n\\n<Signature (c, a, **kwargs)>\\n\\n\\nWithout reading the source code, it might be hard for me to know that\\xa0foo\\xa0also accepts and additional parameters\\xa0b\\xa0and\\xa0d. We can fix this with\\xa0delegates:\\n\\ndef baz(a, b=2, c =3, d=4): return a + b + c\\r\\n\\r\\n@delegates(baz) # this decorator will pass down keyword arguments from baz\\r\\ndef foo(c, a, **kwargs):\\r\\n    return c + baz(a, **kwargs)\\r\\n\\r\\ninspect.signature(foo)\\r\\n\\n\\n\\n\\n<Signature (c, a, b=2, d=4)>\\n\\n\\nYou can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping\\xa0**kwargs:\\n\\n@delegates(baz, keep=True)\\r\\ndef foo(c, a, **kwargs):\\r\\n    return c + baz(a, **kwargs)\\r\\n\\r\\ninspect.signature(foo)\\r\\n\\n\\n\\n\\n<Signature (c, a, b=2, d=4, **kwargs)>\\n\\n\\nYou can also exclude arguments. For example, we exclude argument\\xa0d\\xa0from delegation:\\n\\ndef basefoo(a, b=2, c =3, d=4): pass\\r\\n\\r\\n@delegates(basefoo, but= [\\'d\\']) # exclude `d`\\r\\ndef foo(c, a, **kwargs): pass\\r\\n\\r\\ninspect.signature(foo)\\r\\n\\n\\n\\n\\n<Signature (c, a, b=2)>\\n\\n\\nYou can also delegate between classes:\\n\\nclass BaseFoo:\\r\\n    def __init__(self, e, c=2): pass\\r\\n\\r\\n@delegates()# since no argument was passsed here we delegate to the superclass\\r\\nclass Foo(BaseFoo):\\r\\n    def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs)\\r\\n        \\r\\ninspect.signature(Foo)\\r\\n\\n\\n\\n\\n<Signature (a, b=1, c=2)>\\n\\n\\nFor more information, read the\\xa0docs on delegates.\\n\\xa0\\nAvoid boilerplate when setting instance attributes\\n\\xa0\\nHave you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in\\xa0__init__?\\n\\nclass Test:\\r\\n    def __init__(self, a, b ,c): \\r\\n        self.a, self.b, self.c = a, b, c\\r\\n\\n\\n\\nOuch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout\\xa0store_attr:\\n\\nclass Test:\\r\\n    def __init__(self, a, b, c): \\r\\n        store_attr()\\r\\n        \\r\\nt = Test(5,4,3)\\r\\nassert t.b == 4\\r\\n\\n\\n\\nYou can also exclude certain attributes:\\n\\nclass Test:\\r\\n    def __init__(self, a, b, c): \\r\\n        store_attr(but=[\\'c\\'])\\r\\n    \\r\\nt = Test(5,4,3)\\r\\nassert t.b == 4\\r\\nassert not hasattr(t, \\'c\\')\\r\\n\\n\\n\\nThere are many more ways of customizing and using\\xa0store_attr\\xa0than I highlighted here. Check out\\xa0the docs\\xa0for more detail.\\nP.S. you might be thinking that Python\\xa0dataclasses\\xa0also allow you to avoid this boilerplate. While true in some cases,\\xa0store_attr\\xa0is more flexible.1\\n1. For example, store_attr does not rely on inheritance, which means you won\\'t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩\\n\\xa0\\nAvoiding subclassing boilerplate\\n\\xa0\\nOne thing I hate about python is the\\xa0__super__().__init__()\\xa0boilerplate associated with subclassing. For example:\\n\\nclass ParentClass:\\r\\n    def __init__(self): self.some_attr = \\'hello\\'\\r\\n        \\r\\nclass ChildClass(ParentClass):\\r\\n    def __init__(self):\\r\\n        super().__init__()\\r\\n\\r\\ncc = ChildClass()\\r\\nassert cc.some_attr == \\'hello\\' # only accessible b/c you used super\\r\\n\\n\\n\\nWe can avoid this boilerplate by using the metaclass\\xa0PrePostInitMeta. We define a new class called\\xa0NewParent\\xa0that is a wrapper around the\\xa0ParentClass:\\n\\nclass NewParent(ParentClass, metaclass=PrePostInitMeta):\\r\\n    def __pre_init__(self, *args, **kwargs): super().__init__()\\r\\n\\r\\nclass ChildClass(NewParent):\\r\\n    def __init__(self):pass\\r\\n    \\r\\nsc = ChildClass()\\r\\nassert sc.some_attr == \\'hello\\' \\r\\n\\n\\n\\n\\xa0\\nType Dispatch\\n\\xa0\\nType dispatch, or\\xa0Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a\\xa0conceptual example\\xa0of how multiple dispatch works in Julia, returning different values depending on the input types of x and y:\\n\\ncollide_with(x::Asteroid, y::Asteroid) = ... \\r\\n# deal with asteroid hitting asteroid\\r\\n\\r\\ncollide_with(x::Asteroid, y::Spaceship) = ... \\r\\n# deal with asteroid hitting spaceship\\r\\n\\r\\ncollide_with(x::Spaceship, y::Asteroid) = ... \\r\\n# deal with spaceship hitting asteroid\\r\\n\\r\\ncollide_with(x::Spaceship, y::Spaceship) = ... \\r\\n# deal with spaceship hitting spaceship\\r\\n\\n\\n\\nType dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks.\\nUnfortunately, Python does not support this out-of-the box. Fortunately, there is the\\xa0@typedispatch\\xa0decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function:\\n\\n@typedispatch\\r\\ndef f(x:str, y:str): return f\\'{x}{y}\\'\\r\\n\\r\\n@typedispatch\\r\\ndef f(x:np.ndarray): return x.sum()\\r\\n\\r\\n@typedispatch\\r\\ndef f(x:int, y:int): return x+y\\r\\n\\n\\n\\nBelow is a demonstration of type dispatch at work for the function\\xa0f:\\n\\nf(\\'Hello \\', \\'World!\\')\\r\\n\\n\\n\\n\\n\\'Hello World!\\'\\n\\n\\n\\nf(2,3)\\r\\n\\n\\n\\n\\n5\\n\\n\\n\\nf(np.array([5,5,5,5]))\\r\\n\\n\\n\\n\\n20\\n\\n\\nThere are limitations of this feature, as well as other ways of using this functionality that\\xa0you can read about here. In the process of learning about typed dispatch, I also found a python library called\\xa0multipledispatch\\xa0made by\\xa0Mathhew Rocklin\\xa0(the creator of Dask).\\nAfter using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing.\\n\\xa0\\nA better version of functools.partial\\n\\xa0\\nfunctools.partial\\xa0is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values >=\\xa0val:\\n\\ntest_input = [1,2,3,4,5,6]\\r\\ndef f(arr, val): \\r\\n    \"Filter a list to remove any values that are less than val.\"\\r\\n    return [x for x in arr if x >= val]\\r\\n\\r\\nf(test_input, 3)\\r\\n\\n\\n\\n\\n[3, 4, 5, 6]\\n\\n\\nYou can create a new function out of this function using\\xa0partial\\xa0that sets the default value to 5:\\n\\nfilter5 = partial(f, val=5)\\r\\nfilter5(test_input)\\r\\n\\n\\n\\n\\n[5, 6]\\n\\n\\nOne problem with\\xa0partial\\xa0is that it removes the original docstring and replaces it with a generic docstring:\\n\\nfilter5.__doc__\\r\\n\\n\\n\\n\\n\\'partial(func, *args, **keywords) - new function with partial application\\\\n    of the given arguments and keywords.\\\\n\\'\\n\\n\\nfastcore.utils.partialler\\xa0fixes this, and makes sure the docstring is retained such that the new API is transparent:\\n\\nfilter5 = partialler(f, val=5)\\r\\nfilter5.__doc__\\r\\n\\n\\n\\n\\n\\'Filter a list to remove any values that are less than val.\\'\\n\\n\\n\\xa0\\nComposition of functions\\n\\xa0\\nA technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers:\\n\\ndef add(arr, val): return [x + val for x in arr]\\r\\ndef arrsum(arr): return sum(arr)\\r\\n\\r\\n# See the previous section on partialler\\r\\nadd2 = partialler(add, val=2)\\r\\n\\r\\ntransform = compose(filter5, add2, arrsum)\\r\\ntransform([1,2,3,4,5,6])\\r\\n\\n\\n\\n\\n15\\n\\n\\nBut why is this useful? You might me thinking, I can accomplish the same thing with:\\n\\narrsum(add2(filter5([1,2,3,4,5,6])))\\r\\n\\n\\n\\nYou are not wrong! However, composition gives you a convenient interface in case you want to do something like the following:\\n\\ndef fit(x, transforms:list):\\r\\n    \"fit a model after performing transformations\"\\r\\n    x = compose(*transforms)(x)\\r\\n    y = [np.mean(x)] * len(x) # its a dumb model.  Don\\'t judge me\\r\\n    return y\\r\\n\\r\\n# filters out elements < 5, adds 2, then predicts the mean\\r\\nfit(x=[1,2,3,4,5,6], transforms=[filter5, add2])\\r\\n\\n\\n\\n\\n[7.5, 7.5]\\n\\n\\nFor more information about\\xa0compose, read\\xa0the docs.\\n\\xa0\\nA more useful\\xa0__repr__\\n\\xa0\\nIn python,\\xa0__repr__\\xa0helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using\\xa0store_attr, which was discussed earlier).\\n\\nclass Test:\\r\\n    def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously\\r\\n    \\r\\nTest(1)\\r\\n\\n\\n\\n\\n<__main__.Test at 0x7ffcd766cee0>\\n\\n\\nWe can use\\xa0basic_repr\\xa0to quickly give us a more sensible default:\\n\\nclass Test:\\r\\n    def __init__(self, a, b=2, c=3): store_attr() \\r\\n    __repr__ = basic_repr(\\'a,b,c\\')\\r\\n    \\r\\nTest(2)\\r\\n\\n\\n\\n\\nTest(a=2, b=2, c=3)\\n\\n\\n\\xa0\\nMonkey Patching With A Decorator\\n\\xa0\\nIt can be convenient to\\xa0monkey patch\\xa0with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the\\xa0decorator @patch\\xa0from\\xa0fastcore.foundation\\xa0along with type hints like so:\\n\\nclass MyClass(int): pass  \\r\\n\\r\\n@patch\\r\\ndef func(self:MyClass, a): return self+a\\r\\n\\r\\nmc = MyClass(3)\\r\\n\\n\\n\\nNow,\\xa0MyClass\\xa0has an additional method named\\xa0func:\\n\\nmc.func(10)\\r\\n\\n\\n\\n\\n13\\n\\n\\nStill not convinced? I\\'ll show you another example of this kind of patching in the next section.\\n\\xa0\\nA better pathlib.Path\\n\\xa0\\nWhen you see\\xa0these extensions\\xa0to pathlib.path you won\\'t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as:\\n\\nPath.readlines: same as\\xa0with open(\\'somefile\\', \\'r\\') as f: f.readlines()\\nPath.read: same as\\xa0with open(\\'somefile\\', \\'r\\') as f: f.read()\\nPath.save: saves file as pickle\\nPath.load: loads pickle file\\nPath.ls: shows the contents of the path as a list.\\netc.\\n\\nRead more about this here. Here is a demonstration of\\xa0ls:\\n\\nfrom fastcore.utils import *\\r\\nfrom pathlib import Path\\r\\np = Path(\\'.\\')\\r\\np.ls() # you don\\'t get this with vanilla Pathlib.Path!!\\r\\n\\n\\n\\n\\n(#7) [Path(\\'2020-09-01-fastcore.ipynb\\'),Path(\\'README.md\\'),Path(\\'fastcore_imgs\\'),Path(\\'2020-02-20-test.ipynb\\'),Path(\\'.ipynb_checkpoints\\'),Path(\\'2020-02-21-introducing-fastpages.ipynb\\'),Path(\\'my_icons\\')]\\n\\n\\nWait! What\\'s going on here? We just imported\\xa0pathlib.Path\\xa0- why are we getting this new functionality? Thats because we imported the\\xa0fastcore.utils\\xa0module, which patches this module via the\\xa0@patch\\xa0decorator discussed earlier. Just to drive the point home on why the\\xa0@patch\\xa0decorator is useful, I\\'ll go ahead and add another method to\\xa0Path\\xa0right now:\\n\\n@patch\\r\\ndef fun(self:Path): return \"This is fun!\"\\r\\n\\r\\np.fun()\\r\\n\\n\\n\\n\\n\\'This is fun!\\'\\n\\n\\nThat is magical, right? I know! That\\'s why I\\'m writing about it!\\n\\xa0\\nAn Even More Concise Way To Create Lambdas\\n\\xa0\\nSelf, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let\\'s create a lambda for taking the sum of a Numpy array:\\n\\narr=np.array([5,4,3,2,1])\\r\\nf = lambda a: a.sum()\\r\\nassert f(arr) == 15\\r\\n\\n\\n\\nYou can use\\xa0Self\\xa0in the same way:\\n\\nf = Self.sum()\\r\\nassert f(arr) == 15\\r\\n\\n\\n\\nLet\\'s create a lambda that does a groupby and max of a Pandas dataframe:\\n\\nimport pandas as pd\\r\\ndf=pd.DataFrame({\\'Some Column\\': [\\'a\\', \\'a\\', \\'b\\', \\'b\\', ], \\r\\n                 \\'Another Column\\': [5, 7, 50, 70]})\\r\\n\\r\\nf = Self.groupby(\\'Some Column\\').mean()\\r\\nf(df)\\r\\n\\n\\n\\n\\n\\n\\n\\nAnother Column\\n\\n\\nSome Column\\n\\n\\n\\n\\n\\na\\n6\\n\\n\\nb\\n60\\n\\n\\n\\n\\xa0\\nRead more about\\xa0Self\\xa0in\\xa0the docs).\\n\\xa0\\nNotebook Functions\\n\\xa0\\nThese are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell:\\n\\nfrom fastcore.imports import in_notebook, in_colab, in_ipython\\r\\nin_notebook(), in_colab(), in_ipython()\\r\\n\\n\\n\\n\\n(True, False, True)\\n\\n\\nThis is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment.\\n\\xa0\\nA Drop-In Replacement For List\\n\\xa0\\nYou might be pretty happy with Python\\'s\\xa0list. This is one of those situations that you don\\'t know you needed a better list until someone showed one to you. Enter\\xa0L, a list like object with many extra goodies.\\nThe best way I can describe\\xa0L\\xa0is to pretend that\\xa0list\\xa0and\\xa0numpy\\xa0had a pretty baby:\\ndefine a list (check out the nice\\xa0__repr__\\xa0that shows the length of the list!)\\n\\nL(1,2,3)\\r\\n\\n\\n\\n\\n(#3) [1,2,3]\\n\\n\\nShuffle a list:\\n\\np = L.range(20).shuffle()\\r\\np\\r\\n\\n\\n\\n\\n(#20) [8,7,5,12,14,16,2,15,19,6...]\\n\\n\\nIndex into a list:\\n\\np[2,4,6]\\r\\n\\n\\n\\n\\n(#3) [5,14,2]\\n\\n\\nL has sensible defaults, for example appending an element to a list:\\n\\n1 + L(2,3,4)\\r\\n\\n\\n\\n\\n(#4) [1,2,3,4]\\n\\n\\nThere is much more\\xa0L\\xa0has to offer. Read\\xa0the docs\\xa0to learn more.\\n\\xa0\\nBut Wait ... There\\'s More!\\n\\xa0\\n\\nThere are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn\\'t demo in this blog post:\\n\\xa0\\nUtilities\\n\\xa0\\nThe\\xa0Utilites\\xa0section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides.\\n\\nmk_class: quickly add a bunch of attributes to a class\\nwrap_class: add new methods to a class with a simple decorator\\ngroupby: similar to Scala\\'s groupby\\nmerge: merge dicts\\nfasttuple: a tuple on steroids\\nInfinite Lists: useful for padding and testing\\nchunked: for batching and organizing stuff\\n\\n\\xa0\\nMultiprocessing\\n\\xa0\\nThe\\xa0Multiprocessing section\\xa0extends python\\'s multiprocessing library by offering features like:\\n\\nprogress bars\\nability to pause to mitigate race conditions with external services\\nprocessing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks\\n\\n\\xa0\\nFunctional Programming\\n\\xa0\\nThe\\xa0functional programming section\\xa0is my favorite part of this library.\\n\\nmaps: a map that also composes functions\\nmapped: A more robust\\xa0map\\nusing_attr: compose a function that operates on an attribute\\n\\n\\xa0\\nTransforms\\n\\xa0\\nTransforms\\xa0is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post.\\n\\xa0\\nFurther Reading\\n\\xa0\\nIt should be noted that you should read the\\xa0main page of the docs\\xa0first, followed by the section on\\xa0tests\\xa0to fully understand the documentation.\\n\\nThe\\xa0fastcore documentation site.\\nThe\\xa0fastcore GitHub repo.\\nBlog post on\\xa0delegation.\\n\\n\\xa0\\nShameless plug: fastpages\\n\\xa0\\nThis blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting?\\xa0Check out fastpages.\\n\\xa0\\nBio: Hamel Husain is a Staff Machine Learning Engineer @ GitHub.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nData Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes\\nHow to Automate Tasks on GitHub With Machine Learning for Fun and Profit\\nAutomated Machine Learning\\u200a—\\u200aA Paradigm Shift That Accelerates Data Scientist Productivity',\n",
       " 'Sponsored Post.\\n\\nKDnuggets Partners with WorldData.AI to Empower Data Science Community with External Data, News Sentiment and Geo Intelligence\\n\\nBrookline, MA & Houston, Texas:\\xa0KDnuggets\\xa0and\\xa0WorldData.AI\\xa0signed a partnership agreement that will join forces to empower data science community with the billions of curated External Data, News Sentiment, and Geo Intelligence for Data Science and AI initiatives.\\nUnder the partnership agreement, KDnuggets subscribers will now have access to the WorldData.AI Partners Plan at no cost, including access to some of the premium datasets only available to enterprise members. Access to such extensive external data will allow them to connect their internal data with over 3.5 billion World Data and enhance their predictive intelligence.\\n\\n\\nIf you are not a KDnuggets subscriber yet,\\nsubscribe for free to KDnuggets here,\\nand get free access to WorldData.AI Partners Plan.\\n\\nFor any questions, please email contactus@worlddata.ai. \\n\\nWorldData.AI, the world’s largest external curated data platform, integrates data from all leading global sources. WorldData.AI database includes data on News Sentiments & Entity Analysis from a global news source, Global Geo-intelligence, billions of time series data covering data on Agriculture, Climate Change, Community, Demographics, Education, Energy (Oil & Gas), Environment, Natural Resources, Financial Market, Healthcare, Industry & Services, Labor Statistics, Macroeconomics, Finance, Politics, Population, Social Conditions, Science & Technology, Trade Macro Statistics, and Transportation.\\n\"If utilized correctly, data can allow you to make intelligent business decisions, drive growth and improve profitability. Companies know they can gain valuable insights by analyzing the data they generate from their operations. But internally generated information can leave gaps, and companies are increasingly moving to incorporate new, nontraditional, and external sources of data into their analyses. This data can include almost anything, from historical demographic and weather data to satellite imagery and private company information. We look forward to working with KDnuggets and get Global Data Science Community access to World\\'s largest curated external data platform,\" said Vivek Singh, Global Head of Marketing at WorldData.AI.\\n\"Leading enterprises are now looking to connect their internal data with 1000s of external factors in real-time to enhance predictive and prescriptive intelligence. However, one of the most significant challenges Enterprises and Data Scientists faces is the lack of a comprehensive External Data source. WorldData.AI has solved this problem by streaming all major external data sources to accumulate billions of curated datasets under one roof. We are delighted to now offer WorldData.AI to our readers at no cost through this partnership,\" said Dr. Gregory Piatetsky-Shapiro, Co-Founder & President at KDnuggets.\\n\\xa0\\nAbout KDnuggets\\nKDnuggets is a leading site on AI, Analytics, Big Data, Data Science, and Machine Learning and is edited by Gregory Piatetsky-Shapiro and Matthew Mayo. KDnuggets received numerous awards/mentions as a leading publication. KDnuggets is currently reaching over 800,000 unique monthly visitors, and over 350,000 subscribers/followers via email and social media.\\nAbout WorldData.AI\\nWorldData.AI\\xa0is a preeminent AI platform that empowers enterprises to leverage external data and build predictive intelligence. WorldData.AI offers over 3.5 billion trends and News Sentiments. WorldData.AI Machine Learning Lab enables data scientists to integrate outside data with WorldData.AI and build applications to solve a real-world problem. WorldData.AI founder Rajdeep Golecha is a recognized technology leader and entrepreneur.',\n",
       " \"Sponsored Post.\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                    How to use external consumer insights and marketing data to build\\r\\n                                                    a customer-centric business\\r\\n                                                \\n\\n\\n\\n\\n\\n\\nREGISTER NOW\\n\\n\\n\\n\\n\\n\\n\\n.\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                                        You're invited!\\r\\n                                                                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                                                    Mon, September 27\\r\\n                                                                                                \\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                                                    11am PT | 2pm eT\\r\\n                                                                                                \\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                                                    60 MIN SESSION\\r\\n                                                                                                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nREGISTER NOW\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                Join this webinar to learn how to leverage external data to understand market needs and consumer behavior – helping you\\r\\n                                                                build a more customer-centric business.\\r\\n                                                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                    In this virtual session, AWS Data Exchange will host a discussion with thought leaders from companies such as Acxiom and\\r\\n                                                    BlastPoint. They will share how organizations from big box retailers to automotive brands are using consumer insights\\r\\n                                                    data to reach new customers, drive real business change, and increase longevity.\\r\\n                                                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                    Key takeaways include:\\r\\n                                                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                How marketers can leverage consumer-level data to reap insights in a dynamically changing marketplace\\r\\n                                                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                Integrating external consumer insights data into personalized customer experiences with AI\\r\\n                                                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                Visualizing data workflows using AWS Analytics tools to drive business intelligence\\r\\n                                                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                How AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud\\r\\n                                                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                        Moderator:\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                                                    Claire O’Brien\\r\\n                                                                                                \\n\\n\\n\\r\\n                                                                                                    Category Manager, Retail & Consumer Goods, AWS Data Exchange\\r\\n                                                                                                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                            Claire leads the Retail and Consumer Goods verticals for AWS Data Exchange’s Business Development team. In this role,\\r\\n                                                                            Claire helps industry-leading providers grow their business through AWS Data Exchange and data subscribers discover and\\r\\n                                                                            use third-party data on AWS.\\r\\n                                                                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                        Presenters:\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                                                    Linda Harrison\\r\\n                                                                                                \\n\\n\\n\\r\\n                                                                                                    Director of Data Strategy, Acxiom\\r\\n                                                                                                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                            Linda Harrison often goes by the title of Data Guru. Her responsibilities include the ethical and appropriate use of\\r\\n                                                                            third-party data and segmentation best practices for effective campaigns across all channels – email, direct mail, and\\r\\n                                                                            digital.  Linda also oversees an RFP and Digital Hotline for recommendations and strategy for targeting using Acxiom and\\r\\n                                                                            partner third party data. \\r\\n                                                                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                                                    Michele Fitzpatrick\\r\\n                                                                                                \\n\\n\\n\\r\\n                                                                                                    Retail and Consumer Brands Strategy Consultant,\\r\\n                                                                                                    Acxiom\\r\\n                                                                                                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMichele is an experienced consumer\\r\\n                                                                                experience (CX) consultant and customer relationship management (CRM)\\r\\n                                                                                marketing\\r\\n                                                                                strategist focused on helping her clients better understand their\\r\\n                                                                                customers. By utilizing a combination of customer\\r\\n                                                                                intelligence and data-enabled digital and traditional channels, Michele\\r\\n                                                                                helps Acxiom’s major brands connect effectively\\r\\n                                                                                with their consumers throughout the buyer journey and customer\\r\\n                                                                                lifecycle.\\nMichele is passionate about helping brands\\r\\n                                                                                connect with consumers in ways that matter to them, and which deliver\\r\\n                                                                                value\\r\\n                                                                                to the brand. This requires gaining unique insights into the hearts,\\r\\n                                                                                minds, and behaviors of real people, then\\r\\n                                                                                determining how, when, what, and most importantly, why they need\\r\\n                                                                                something.\\nPrior to joining Acxiom, Michele held senior level CX consulting and CRM\\r\\n                                                                                marketing strategy roles at MRM//McCann and\\r\\n                                                                                Harte Hanks serving clients in retail, consumer brands, automotive,\\r\\n                                                                                financial services, healthcare, agriculture, and\\r\\n                                                                                associations.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                                                    Tomer Borenstein\\r\\n                                                                                                \\n\\n\\n\\r\\n                                                                                                    Co-founder and Chief Technology Officer, BlastPoint\\r\\n                                                                                                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                            Tomer is credited with generating millions of dollars in sales and securing partnerships with a growing list of national\\r\\n                                                                            and international corporate customers. A Business Times’ “30 under 30” honoree, Tomer has authored numerous articles and\\r\\n                                                                            speaks regularly about the benefits of artificial intelligence and machine learning for customer engagement and business\\r\\n                                                                            growth. His previous work in the startup space includes an exit to Apple. Tomer holds a Bachelor’s degree in Electrical\\r\\n                                                                            and Computer Engineering and a Master’s in Machine Learning and Computer Vision from Carnegie Mellon University.\\r\\n                                                                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                                                    Nam Le\\r\\n                                                                                                \\n\\n\\n\\r\\n                                                                                                    Specialist Solutions\\r\\n                                                                                                    Architect, AWS\\r\\n                                                                                                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                                                            Nam is a Specialist Solutions Architect at AWS covering AWS Marketplace, Service Catalog, Migration Services, and\\r\\n                                                                            Control Tower. He helps customers implement security and governance best practices using native AWS Services and Partner\\r\\n                                                                            products. He is an AWS-Certified Solutions Architect, and his skills include security, compliance, cloud computing,\\r\\n                                                                            enterprise architecture, and software development. Nam has also worked as a consulting services manager, cloud\\r\\n                                                                            architect, and technical marketing manager.\\r\\n                                                                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                        *The views and opinions of Acxiom and BlastPoint are their own, and do not necessarily reflect the positions of AWS or\\r\\n                                        AWS Marketplace.\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout AWS Data Exchange:\\r\\n                                        AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud. Once subscribed to\\r\\n                                        a data\\r\\n                                        product, you can use the AWS Data Exchange API to load data directly into Amazon S3 and then analyze it with a\\r\\n                                        wide\\r\\n                                        variety of AWS analytics and\\r\\n                                        machine-learning services. Click here to browse thousands of data products now available\\r\\n                                        from more than 80 qualified data providers in AWS Marketplace.\\r\\n                                        Visit aws.amazon.com/data-exchange to learn more.\\r\\n                                    \\n\\n\\n\\n\\n\\n\\nREGISTER NOW\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                        © 2\\ufeff021 AWS Marketplace.\",\n",
       " \"comments\\nBy Ahmad Bin Shafiq, Machine Learning Student.\\n\\nPhoto by\\xa0Sharon McCutcheon\\xa0on\\xa0Unsplash.\\nData science is without a doubt the most in-demand field today. No wonder data scientists with proficient skills are handsomely rewarded in jobs across the world.\\nNow, you could be a data scientist who’s comfortable in the current job or an aspiring one looking to make inroads into data science. Regardless, the ideas I’m about to suggest would certainly help you upskill, earn a good side income as a data scientist, and most importantly, be your own boss.\\n\\xa0\\n1. Writing articles\\n\\xa0\\n\\nPhoto by\\xa0Dan Counsell\\xa0on\\xa0Unsplash.\\nIf you have a basic knowledge of data science and your writing skills are good, you can earn a handsome amount of money just by writing articles.\\xa0Medium\\xa0is the best platform for beginners to get started with writing. Along with this, there are different websites that pay writers for original blogs. You can write for those websites, or you can even start your own blog (this will not only help you financially, but it will also improve your resume as well). In the case of your own blog, you will write blogs related to AI/ML/DL/DS. Once your blog reaches a certain threshold (views/traffic-wise), you can apply for monetization. If your monetization is enabled, then you will earn money for every view of your blog.\\n\\xa0\\n2. Participating in Kaggle competitions\\n\\xa0\\n\\nThere are many competitions regarding data science on Kaggle. By participating in them, not only do you learn something new and improve your skills, but if you win those competitions, Kaggle offers cash prizes to its winners. Also, if you have an excellent Kaggle profile, this will definitely result in a lot of exposure from recruiters, which can help you in getting a job!\\n\\xa0\\n3. Doing freelance work\\n\\xa0\\n\\nPhoto by\\xa0imtips.co.\\nFreelancing is the most trending way to earn money from home. Especially in 2020–21 when everything is locked down and people are in their homes. So, you can start doing freelance work regarding data science on websites like\\xa0Fiverr,\\xa0Upwork,\\xa0freelancer, etc. And as you complete more and more projects, your profile becomes more valuable, and you start to get more orders.\\nTo learn more about how to start your career in freelancing, refer to these articles:\\n5 Tips for Novice Freelance Data Scientists\\nHow to become a freelance Data Scientist\\n\\xa0\\n4. Teaching\\n\\xa0\\n\\nPhoto by\\xa0NeONBRAND\\xa0on\\xa0Unsplash.\\nThere is a common saying:\\nThe best way to learn something is to teach it\\nIn order to become a skilled data scientist, all of your concepts should be crystal clear. So focusing on the above saying, you can teach data science at your university as an assistant professor or start an academy at home or somewhere that teaches data science. Start from the beginner level and change things along the way. By starting to teach, you not only earn a reasonable amount of money, but you also start giving back to the programmer's community.\\n\\xa0\\n5. Starting a YouTube channel\\n\\xa0\\n\\nPhoto by\\xa0Sara Kurfeß\\xa0on\\xa0Unsplash.\\nCreating educational videos around the topic you have expertise in is another great way to earn revenue. YouTube offers impressive rewards to content creators. Creating educational videos also allow you to build a personal brand and create your own influencer network. On YouTube, you can upload any content as long as it doesn’t violate the content policies.\\n\\xa0\\n6. Job or Internship\\n\\xa0\\n\\nPhoto by\\xa0Ant Rozetsky\\xa0on\\xa0Unsplash.\\nRecent studies show:\\nInternships can be an effective bridge to stable employment\\nThis is especially true in STEM fields — such as data science — where internships are a game-changer. This is because some data science skills can only be solidified when learned in a hands-on environment.\\nWhile doing an internship, you get to learn from a team of professionals and build a strong network while gaining practical and meaningful experience. Also, internships help you financially as well, and they open many doors in the future.\\nTips and Advice on How to Get Your First Data Science Internship\\n\\xa0\\nConclusion\\n\\xa0\\nIn this article, I talked about 6 ways I have used or seen others make extra income as a data scientist. These ways vary in difficulty, risk level, and the amount of money that you can make, but all of them have the potential to help you develop your skills and earn $$$.\\n\\xa0\\n\\xa0\\nRelated:\\n\\nHow to become an online data science tutor\\nHow to Succeed in Becoming a Freelance Data Scientist\\nData careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space\",\n",
       " \"comments\\nBy Susan Maina, Passionate about data, machine learning enthusiast, writer at Medium\\nExploratory Data Analysis, or EDA, is one of the first steps of the\\xa0data science process. It involves learning as much as possible about the data, without spending too much time. Here, you get an instinctive as well as a high-level practical understanding of the data. By the end of this process, you should have a general idea of the structure of the data set, some cleaning ideas, the target variable and, possible modeling techniques.\\nThere are some general strategies to quickly perform EDA in most problems. In this article, I will use the\\xa0Melbourne Housing snapshot dataset\\xa0from kaggle to demonstrate the 11 blocks of code you can use to perform a satisfactory exploratory data analysis. The dataset includes\\xa0Address,\\xa0Type\\xa0of Real estate,\\xa0Suburb,\\xa0Method\\xa0of Selling,\\xa0Rooms,\\xa0Price, Real Estate Agent\\xa0(SellerG),\\xa0Date\\xa0of Sale and,\\xa0Distance\\xa0from C.B.D. You can follow along by downloading the dataset\\xa0here.\\nThe first step is importing the libraries required. We will need\\xa0Pandas,\\xa0Numpy,\\xa0matplotlib\\xa0and\\xa0seaborn. To make sure all our columns are displayed, use\\xa0pd.set_option(’display.max_columns’, 100)\\xa0. By default, pandas displays 20 columns and hides the rest.\\n\\nimport pandas as pd\\r\\npd.set_option('display.max_columns',100)import numpy as npimport matplotlib.pyplot as plt\\r\\n%matplotlib inlineimport seaborn as sns\\r\\nsns.set_style('darkgrid')\\n\\n\\nPanda’s\\xa0pd.read_csv(path)\\xa0reads in the csv file as a DataFrame.\\n\\ndata = pd.read_csv('melb_data.csv')\\n\\n\\n\\xa0\\nBasic data set Exploration\\n\\xa0\\n1. Shape (dimensions) of the DataFrame\\nThe\\xa0.shape\\xa0attribute of a Pandas DataFrame gives an overall structure of the data. It returns a\\xa0tuple\\xa0of length 2 that translates to how many rows of observations and columns the dataset has.\\n\\ndata.shape### Results\\r\\n(13580, 21)\\n\\n\\nWe can see that the dataset has 13,580 observations and 21 features, and one of those features is the target variable.\\n\\xa0\\n2. Data types of the various columns\\nThe DataFrame’s\\xa0.dtypes\\xa0attribute displays the data types of the columns as a Panda’s\\xa0Series\\xa0(Series means a column of values and their indices).\\n\\ndata.dtypes### Results\\r\\nSuburb            object\\r\\nAddress           object\\r\\nRooms              int64\\r\\nType              object\\r\\nPrice            float64\\r\\nMethod            object\\r\\nSellerG           object\\r\\nDate              object\\r\\nDistance         float64\\r\\nPostcode         float64\\r\\nBedroom2         float64\\r\\nBathroom         float64\\r\\nCar              float64\\r\\nLandsize         float64\\r\\nBuildingArea     float64\\r\\nYearBuilt        float64\\r\\nCouncilArea       object\\r\\nLattitude        float64\\r\\nLongtitude       float64\\r\\nRegionname        object\\r\\nPropertycount    float64\\r\\ndtype: object\\n\\n\\nWe observe that our dataset has a combination of\\xa0categorical\\xa0(object) and\\xa0numeric\\xa0(float and int) features. At this point, I went back to the Kaggle page for an understanding of the columns and their meanings. Check out the table of columns and their definitions\\xa0here\\xa0created with\\xa0Datawrapper.\\nWhat to look out for;\\n\\nNumeric features that should be categorical and vice versa.\\n\\nFrom a quick analysis, I did not find any mismatch for the datatypes. This makes sense as this dataset version is a cleaned snapshot of the original\\xa0Melbourne data.\\n\\xa0\\n3. Display a few rows\\nThe Pandas DataFrame has very handy functions for displaying a few observations.\\xa0data.head()displays the first 5 observations,\\xa0data.tail()\\xa0the last 5, and\\xa0data.sample()\\xa0an observation chosen randomly from the dataset. You can display 5 random observations using\\xa0data.sample(5)\\n\\ndata.head()\\r\\ndata.tail()\\r\\ndata.sample(5)\\n\\n\\n\\n\\n\\nWhat to look out for:\\n\\nCan you understand the column names? Do they make sense? (Check with the variable definitions again if needed)\\nDo the values in these columns make sense?\\nAre there significant missing values (NaN) sighted?\\nWhat types of classes do the categorical features have?\\n\\nMy insights; the\\xa0Postcode\\xa0and\\xa0Propertycount features both changed according to the Suburb feature. Also, there were significant missing values for the\\xa0BuildingArea\\xa0and\\xa0YearBuilt.\\n\\xa0\\nDistribution\\n\\xa0\\nThis refers to how the values in a feature are distributed, or how often they occur. For numeric features, we’ll see how many times groups of numbers appear in a particular column, and for categorical features, the classes for each column and their frequency. We will use both\\xa0graphs\\xa0and actual summary\\xa0statistics. The graphs enable us to get an overall idea of the distributions while the statistics give us factual numbers. These two strategies are both recommended as they complement each other.\\n\\xa0\\nNumeric Features\\n\\xa0\\n4. Plot each numeric feature\\nWe will use Pandas\\xa0histogram. A histogram groups numbers into ranges (or bins) and the height of a bar shows how many numbers fall in that range.\\xa0df.hist()\\xa0plots a histogram of the data’s numeric features in a grid. We will also provide the\\xa0figsize\\xa0and\\xa0xrot\\xa0arguments to increase the grid size and rotate the x-axis by 45 degrees.\\n\\ndata.hist(figsize=(14,14), xrot=45)\\r\\nplt.show()\\n\\n\\n\\n\\nHistogram by author\\n\\n\\nWhat to look out for:\\n\\nPossible outliers that cannot be explained or might be measurement errors\\nNumeric features that should be categorical. For example,\\xa0Gender\\xa0represented by 1 and 0.\\nBoundaries that do not make sense such as percentage values> 100.\\n\\nFrom the histogram, I noted that\\xa0BuildingArea\\xa0and\\xa0LandSize\\xa0had potential outliers to the right. Our target feature\\xa0Price\\xa0was also highly skewed to the right. I also noted that\\xa0YearBuilt\\xa0was very skewed to the left and the boundary started at the year 1200 which was odd. Let’s move on to the summary statistics for a clearer picture.\\n\\xa0\\n5. Summary statistics of the numerical features\\nNow that we have an intuitive feel of the numeric features, we will look at actual statistics using\\xa0df.describe()which displays their summary statistics.\\n\\ndata.describe()\\n\\n\\n\\n\\n\\nWe can see for each numeric feature, the\\xa0count\\xa0of values in it, the\\xa0mean\\xa0value,\\xa0std\\xa0or standard deviation,\\xa0minimum\\xa0value, the\\xa025th\\xa0percentile, the\\xa050th\\xa0percentile or median, the\\xa075th\\xa0percentile, and the\\xa0maximum\\xa0value. From the count we can also identify the features with\\xa0missing values; their count is not equal to the total number of rows of the dataset. These are\\xa0Car,\\xa0LandSize\\xa0and\\xa0YearBuilt.\\nI noted that the minimum for both the\\xa0LandSize\\xa0and\\xa0BuildingArea\\xa0is 0. We also see that the\\xa0Price\\xa0ranges from 85,000 to 9,000,000 which is a big range. We will explore these columns in detailed analysis later in the project.\\nLooking at the\\xa0YearBuilt\\xa0feature, however, we note that the minimum year is 1196. This could be a possible data entry error that will be removed during cleaning.\\n\\xa0\\nCategorical features\\n\\xa0\\n6. Summary statistics of the categorical features\\nFor categorical features, it is important to show the summary statistics before we plot graphs because some features have a lot of unique classes (like we will see for the\\xa0Address) and the classes would be unreadable if visualized on a countplot.\\nTo check the summary statistics of only the categorical features, we will use\\xa0df.describe(include=’object’)\\n\\ndata.describe(include='object')\\n\\n\\n\\n\\nCategorical summary statistics by author\\n\\n\\nThis table is a bit different from the one for numeric features. Here, we get the\\xa0count\\xa0of the values of each feature, the number of\\xa0unique\\xa0classes, the\\xa0top\\xa0most frequent class, and how\\xa0frequently\\xa0that class occurs in the data set.\\nWe note that some classes have a lot of unique values such as\\xa0Address, followed by\\xa0Suburb\\xa0and\\xa0SellerG. From these findings, I will only plot the columns with 10 or less unique classes. We also note that\\xa0CouncilArea\\xa0has missing values.\\n\\xa0\\n7. Plot each categorical feature\\nUsing the statistics above, we note that\\xa0Type,\\xa0Method\\xa0and\\xa0Regionname\\xa0have less than 10 classes and can be effectively visualized. We will plot these features using the\\xa0Seaborn countplot, which is like a histogram for categorical variables. Each bar in a countplot represents a unique class.\\nI created a\\xa0For loop. For each categorical feature, a countplot will be displayed to show how the classes are distributed for that feature. The line\\xa0df.select_dtypes(include=’object’)\\xa0selects the categorical columns with their values and displays them. We will also include an\\xa0If-statement\\xa0so as to pick only the three columns with 10 or fewer classes using the line\\xa0Series.nunique() < 10. Read the\\xa0.nunique()\\xa0documentation\\xa0here.\\n\\nfor column in data.select_dtypes(include='object'):\\r\\n    if data[column].nunique() < 10:\\r\\n        sns.countplot(y=column, data=data)\\r\\n        plt.show()\\n\\n\\n\\n\\nCount plots by author\\n\\n\\nWhat to look out for:\\n\\nSparse classes which have the potential to affect a model’s performance.\\nMistakes in labeling of the classes, for example 2 exact classes with minor spelling differences.\\n\\nWe note that\\xa0Regionname\\xa0has some sparse classes which might need to be merged or re-assigned during modeling.\\n\\xa0\\nGrouping and segmentation\\n\\xa0\\nSegmentation allows us to cut the data and observe the relationship between categorical and numeric features.\\n\\xa0\\n8. Segment the target variable by categorical features.\\nHere, we will compare the target feature,\\xa0Price, between the various classes of our main categorical features\\xa0(Type,\\xa0Method\\xa0and\\xa0Regionname)\\xa0and see how the\\xa0Price\\xa0changes with the classes.\\nWe use the\\xa0Seaborn boxplot\\xa0which plots the distribution of\\xa0Price\\xa0across the classes of categorical features.\\xa0This\\xa0tutorial, from where I borrowed the Image below, explains the boxplot’s features clearly. The dots at both ends represent outliers.\\n\\n\\nImage from\\xa0www.geekeforgeeks.org\\n\\n\\nAgain, I used a\\xa0for loop\\xa0to plot a boxplot of each categorical feature with\\xa0Price.\\n\\nfor column in data.select_dtypes(include=’object’):\\r\\n if data[column].nunique() < 10:\\r\\n sns.boxplot(y=column, x=’Price’, data=data)\\r\\n plt.show()\\n\\n\\n\\n\\nBox plots by author\\n\\n\\nWhat to look out for:\\n\\nwhich classes most affect the target variables.\\n\\nNote how the\\xa0Price\\xa0is still sparsely distributed among the 3 sparse classes of\\xa0Regionname\\xa0seen earlier, strengthening our case against these classes.\\nAlso note how the\\xa0SA\\xa0class (the least frequent\\xa0Method\\xa0class) commands high prices, almost similar prices of the most frequently occurring class\\xa0S.\\n\\xa0\\n9. Group numeric features by each categorical feature.\\nHere we will see how all the other numeric features, not just\\xa0Price, change with each categorical feature by summarizing the numeric features across the classes. We use the\\xa0Dataframe’s groupby\\xa0function to group the data by a category and calculate a metric (such as\\xa0mean,\\xa0median,\\xa0min,\\xa0std,\\xa0etc) across the various numeric features.\\nFor only the 3 categorical features with less than 10 classes, we group the data, then calculate the\\xa0mean\\xa0across the numeric features. We use\\xa0display()\\xa0which results to a cleaner table than\\xa0print().\\n\\nfor column in data.select_dtypes(include='object'):\\r\\n    if data[column].nunique() < 10:\\r\\n        display(data.groupby(column).mean())\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWe get to compare the\\xa0Type,\\xa0Method\\xa0and\\xa0Regionname\\xa0classes across the numeric features to see how they are distributed.\\n\\xa0\\nRelationships between numeric features and other numeric features\\n\\xa0\\n10. Correlations matrix for the different numerical features\\nA\\xa0correlation\\xa0is a value between -1 and 1 that amounts to how closely values of two separate features move simultaneously. A\\xa0positive\\xa0correlation means that as one feature increases the other one also increases, while a\\xa0negative\\xa0correlation means one feature increases as the other decreases. Correlations close to 0 indicate a\\xa0weak\\xa0relationship while closer to -1 or 1 signifies a\\xa0strong\\xa0relationship.\\n\\n\\nImage from\\xa0edugyan.in\\n\\n\\nWe will use\\xa0df.corr()\\xa0to calculate the\\xa0correlations\\xa0between the numeric features and it returns a DataFrame.\\n\\ncorrs = data.corr()\\r\\ncorrs\\n\\n\\n\\n\\n\\nThis might not mean much now, so let us plot a heatmap to visualize the correlations.\\n\\xa0\\n11. Heatmap of the correlations\\nWe will use a\\xa0Seaborn heatmap\\xa0to plot the grid as a rectangular color-coded matrix. We use\\xa0sns.heatmap(corrs, cmap=’RdBu_r’,annot=True).\\nThe\\xa0cmap=‘RdBu_r’\\xa0argument tells the heatmap what colour palette to use. A high positive correlation appears as\\xa0dark red\\xa0and a high negative correlation as\\xa0dark blue. Closer to white signifies a weak relationship. Read\\xa0this\\xa0nice tutorial for other color palettes.\\xa0annot=True\\xa0includes the values of the correlations in the boxes for easier reading and interpretation.\\n\\nplt.figure(figsize=(10,8))\\r\\nsns.heatmap(corrs, cmap='RdBu_r', annot=True)\\r\\nplt.show()\\n\\n\\n\\n\\nHeatmap by author\\n\\n\\nWhat to look out for:\\n\\nStrongly correlated features; either dark red (positive) or dark blue(negative).\\nTarget variable; If it has strong positive or negative relationships with other features.\\n\\nWe note that\\xa0Rooms,\\xa0Bedrooms2,\\xa0Bathrooms, and Price have strong positive relationships. On the other hand, Price, our target feature, has a slightly weak\\xa0negative\\xa0correlation with\\xa0YearBuilt\\xa0and an even weaker negative relationship with\\xa0Distance\\xa0from CBD.\\nIn this article, we explored the Melbourne dataset and got a high-level understanding of the structure and its features. At this stage, we do not need to be 100% comprehensive because in future stages we will explore the data more elaborately. You can get the full code on Github\\xa0here. I will be uploading the dataset’s cleaning concepts soon.\\n\\xa0\\nBio: Susan Maina is passionate about data, machine learning enthusiast, writer at Medium.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nPowerful Exploratory Data Analysis in just two lines of code\\nPandas Profiling: One-Line Magical Code for EDA\\nStatistical and Visual Exploratory Data Analysis with One Line of Code\",\n",
       " 'Sponsored Post.\\nBy Jim Elliott, Samsung Semiconductor\\n\\nOver the past six months the world has been focused on the singular goal of developing treatments, vaccines, and containment strategies but what no one expected was how the tech world would rise to the challenge presented by Covid-19. While front line responders and essential workers put their lives on the line, researchers and scientists turned to artificial intelligence (AI) for answers. In mere months, emerging technologies have been rolled out and leveraged to full effect to vastly boost computing power, dramatically increase access to high-performance computing (HPC), and accelerate research by orders of magnitude. AI and HPC are being leveraged to not only assess and develop treatments but also to create potential vaccines, manage shutdowns and reopenings, analyze and enable access to digital medical records, and even to help develop better face masks.\\nSamsung Semiconductor technology has played a particularly essential role in the fight against Covid-19. Samsung technology powers many of the most innovative programs and AI platforms that are helping scientists conduct research and achieve breakthroughs at a speed that would have been impossible just a few years ago.\\nOne prominent example of Samsung tech in action is the Covid-19 High Performance Computing Consortium. This consortium is a public-private initiative of IBM, the White House Office of Science and Technology Policy, and the US Department of Energy that is providing free access to HPC for groups researching and fighting the coronavirus. The consortium harnesses supercomputing resources from private companies, national laboratories, universities, and others to create an ever-expanding HPC system that (as of this writing) boasts 4.2 million CPU cores, 43,000 GPUs, and 600TB of Samsung’s new HBM2 high-bandwidth memory, providing 430 petaflops of processing power. This massive compute power enables researchers studying COVID-19 to get the answers they need in hours or days instead of weeks or months.\\n\\nSource: COVID-19 High Performance Computing Consortium\\n\\n\\xa0\\nOne of many exciting projects utilizing the consortium’s HPC system is the Aaron Diamond AIDS Research Center (ADARC) at Columbia University. ADARC is studying neutralizing antibodies from convalescent COVID-19 donors in order to develop SARS-CoV-2 neutralizing antibodies that can be used in therapeutics such as monoclonal antibody treatments or \"passive\" immunization. Another research team utilizing the HPC system is led by Ryan Wang at Northeastern University. Wang’s team is studying how human contacts and voluntary contact tracing can impact the spread of COVID-19 using complex simulations. Their research might provide important insight into how to most effectively implement contact tracing in order to contain the spread of the virus.\\n\\nSource: The Lancet\\n\\n\\xa0\\nAnother advanced AI system that leverages Samsung tech is Nvidia’s HBM2-equipped DGX A100. This system, which utilizes 320GB of Samsung’s new 2.4Gbps HBM2, consolidates the power of a 5-petaflop data center onto a single platform. The very first DGX A100 unit began operation at the Argonne National Laboratory in May of 2020 and is being used to conduct COVID-19 research.\\nOther Nvidia GPUs incorporating Samsung tech are also being used at Oxford Nanopore Technologies in the UK, where they are helping researchers sequence entire virus genomes in just 7 hours, at Plotly for real-time infection rate analysis, and at the National Institutes of Health for COVID-19 classification and reconstruction of virus spike protein structures.\\n\\nSource: bioRxiv\\n\\n\\xa0\\nAt Oak Ridge National Laboratory, Nvidia V100 GPUs powered by Samsung HBM2 are being used to analyze a billion potential drug compounds per day. By early March, Oak Ridge was able to identify 77 different small-molecule drug compounds that are likely to bind to the virus’ spike protein and potentially inhibit its entry into host cells.\\nAt the same time, Lawrence Livermore National Laboratory is upgrading its ‘Corona’ supercomputer using AMD MI50 GPUs which are also equipped with Samsung HBM2. The upgrade will almost double the supercomputer’s peak computer power and will be used for molecular modeling in support of COVID-19 research, potentially resulting in new designs for both antibodies and small molecules for therapeutics.\\n\\nWhile the list of Covid-19 research projects leveraging Samsung tech is seemingly endless, the impact of semiconductor technology is being felt well beyond the laboratory. For example, the pandemic is also creating demand for high-bandwidth video streaming for virtual interaction between patients and frontline healthcare personnel. By providing server DRAM solutions with greater bandwidth and higher memory density per server, Samsung is helping to meet those needs as well.\\nThe Covid-19 pandemic is an ongoing crisis that is presenting an extreme challenge to both our medical and research communities. I’m proud of the way Samsung and the semiconductor industry as a whole have risen to the challenge of supporting caregivers and researchers by rapidly rolling out the next generation of advanced technologies. As a result, lives have been saved, and the impact of the pandemic will, hopefully, be blunted.\\n\\nFor the time being, the pandemic is still raging and being complacent is not an option. It\\'s also become clear that the risk of future such events is high—that this could happen all over again in 10, 20, 30 years or more. Our industry must continue to push boundaries and accelerate the rate of progress so that next time we can be more prepared.\\nImagine a future in which AI can help not only predict but also prevent pandemics. And should future pandemics emerge, envision being ahead of the game with broad-spectrum protease inhibitors and other small molecule drugs, or potentially having universal vaccines for coronaviruses, Ebola, and influenzas on the ready to deploy at the first hint of an outbreak. As semiconductor technology continues to develop, these aspirations might well become reality. It is my belief that Samsung Semiconductor Inc. and the semiconductor industry on the whole can play a pivotal role in building a more prepared and more resilient world.\\nTo learn more, please visit Samsung Semiconductor.',\n",
       " 'Sponsored Post.\\n\\n\\xa0\\nSQream calls on data practitioners to voice their analytics struggles in a video contest, and get a $50 Amazon gift card. \\nIn telecom, finance, healthcare, and virtually every other industry, organizations are experiencing an explosion of information. Big Data is becoming an outdated term as Massive Data takes hold. In this new environment, organizations are looking for ways to translate their most valuable asset into tangible business benefit. From better and faster decision-making to help enterprises face growing competition, tapping into their treasure troves of data is the key to successful business continuity and growth.\\nYet as data grows to massive proportions, so do the challenges experienced by data practitioners in their daily struggle to access, prepare and analyze it. As data stores reach terabytes and petabytes, SQL queries can take hours or days, and in many cases, time out or are stopped by IT. Each change in perspective requires hours of data preparation, leaving little time for exploration and discovery. Analytic reports reflect only a small percentage of available data, leaving out critical business insights. Legacy systems that were built for significantly smaller volumes of data are simply unable to handle modern-day workloads, and data professionals are left struggling to deliver the reports required by business stakeholders.\\nLike many people who are looking to regain control in these turbulent times - so are data professionals, who are beginning to demand better and faster access to their key asset. The explosion of data combined with the growing demands of the industry has set the Massive Data Revolution in motion.\\nIn light of the Revolution, SQream developed the leading data analytics acceleration platform for massive data, and is calling on data professionals to rise up and share their own massive data challenges in a 60-second video. Whether your struggles are around data preparation, ETL, SQL queries, or an inability to access and analyze enough of your data, SQream wants to hear about it from the source.\\nQualifying participants will receive an Amazon gift card, as well as the opportunity to have their voice, and greatest analytics challenges, heard by industry experts, fellow practitioners, and solution providers.\\n\"Data professionals have for too long been subject to severe limitations and unnecessary difficulties in their day-to-day work. Legacy systems are holding massive data hostage, with extremely long-running queries and the need for arduous data preparation standing in the way of critical insights,\" says David Leichner, CMO of SQream.\\n\"What was acceptable for data volumes that existed ten or even five years ago is no longer enough. With the rise of the Massive Data Revolution, we want data practitioners to take an active part in creating a new status quo. By creating an open conversation about the biggest industry challenges, we raise the bar on what the data community expects and demands of data analytics systems and processes.\"\\nProfessionals across the data pipeline, from CIOs and CDOs to DBAs, Data Analysts, Data Engineers, and Data Scientists are invited to share their massive data challenges from their own unique perspectives. Visit this page to learn more about the Massive Data Revolution Video Challenge, and be sure to submit your entry by December 16th.\\nTo learn how your organization can unleash the full power of its massive data, visit sqream.com.',\n",
       " 'By Ahmed Gad, KDnuggets Contributor.\\ncomments\\nTo evaluate object detection models like R-CNN and\\xa0YOLO, the\\xa0mean average precision (mAP)\\xa0is used. The mAP compares the ground-truth bounding box to the detected box and returns a score. The higher the score, the more accurate the model is in its detections.\\nIn my last article we looked in detail at the\\xa0confusion matrix, model accuracy, precision, and recall. We used the Scikit-learn library to calculate these metrics as well. Now we\\'ll extend our discussion to see how precision and recall are used to calculate the mAP.\\nHere are the sections covered in this tutorial:\\n\\nFrom Prediction Score to Class Label\\nPrecision-Recall Curve\\nAverage Precision (AP)\\nIntersection over Union (IoU)\\nMean Average Precision (mAP) for Object Detection\\n\\nLet\\'s get started.\\n\\xa0\\nFrom Prediction Score to Class Label\\n\\xa0\\nIn this section we\\'ll do a quick review of how a class label is derived from a prediction score.\\nGiven that there are two classes,\\xa0Positive\\xa0and\\xa0Negative, here are the ground-truth labels of 10 samples.\\n\\ny_true = [\"positive\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\"]\\n\\n\\xa0\\nWhen these samples are fed to the model it returns the following prediction scores. Based on these scores, how do we classify the samples (i.e. assign a class label to each sample)?\\n\\npred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3]\\n\\n\\xa0\\nTo convert the scores into a class label,\\xa0a threshold is used. When the score is equal to or above the threshold, the sample is classified as one class. Otherwise, it is classified as the other class. Let\\'s agree that a sample is\\xa0Positive\\xa0if its score is above or equal to the threshold. Otherwise, it is\\xa0Negative. The next block of code converts the scores into class labels with a threshold of\\xa00.5.\\n\\nimport numpy\\r\\n\\r\\npred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3]\\r\\ny_true = [\"positive\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\"]\\r\\n\\r\\nthreshold = 0.5\\r\\ny_pred = [\"positive\" if score >= threshold else \"negative\" for score in pred_scores]\\r\\nprint(y_pred)\\n\\n\\xa0\\n\\n[\\'positive\\', \\'negative\\', \\'positive\\', \\'positive\\', \\'positive\\', \\'positive\\', \\'negative\\', \\'negative\\', \\'negative\\', \\'negative\\']\\n\\n\\xa0\\nNow both the ground-truth and predicted labels are available in the\\xa0y_true\\xa0and\\xa0y_pred\\xa0variables. Based on these labels, the\\xa0confusion matrix, precision, and recall\\xa0can be calculated.\\n\\nr = numpy.flip(sklearn.metrics.confusion_matrix(y_true, y_pred))\\r\\nprint(r)\\r\\n\\r\\nprecision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\\r\\nprint(precision)\\r\\n\\r\\nrecall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\\r\\nprint(recall)\\n\\n\\xa0\\n\\n# Confusion Matrix (From Left to Right & Top to Bottom: True Positive, False Negative, False Positive, True Negative)\\r\\n[[4 2]\\r\\n [1 3]]\\r\\n\\r\\n# Precision = 4/(4+1)\\r\\n0.8\\r\\n\\r\\n# Recall = 4/(4+2)\\r\\n0.6666666666666666\\n\\n\\xa0\\nAfter this quick review of calculating the precision and recall, in the next section we\\'ll discuss creating the precision-recall curve.\\n\\xa0\\nPrecision-Recall Curve\\n\\xa0\\nFrom the definition of both the precision and recall given in\\xa0Part 1, remember that the higher the precision, the more confident the model is when it classifies a sample as\\xa0Positive. The higher the recall, the more positive samples the model correctly classified as\\xa0Positive.\\n\\xa0\\nWhen a model has high recall but low precision, then the model classifies most of the positive samples correctly but it has many false positives (i.e. classifies many\\xa0Negative\\xa0samples as\\xa0Positive). When a model has high precision but low recall, then the model is accurate when it classifies a sample as\\xa0Positive\\xa0but it may classify only some of the positive samples.\\n\\xa0\\nDue to the importance of both precision and recall, there is a\\xa0precision-recall curve\\xa0the shows the tradeoff between the precision and recall values for different thresholds. This curve helps to select the best threshold to maximize both metrics.\\nThere are some inputs needed to create the precision-recall curve:\\n\\nThe ground-truth labels.\\nThe prediction scores of the samples.\\nSome thresholds to convert the prediction scores into class labels.\\n\\nThe next block of code creates the\\xa0y_true\\xa0list to hold the ground-truth labels, the\\xa0pred_scores\\xa0list for the prediction scores, and finally the\\xa0thresholds\\xa0list for different threshold values.\\n\\nimport numpy\\r\\n\\r\\ny_true = [\"positive\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"negative\", \"negative\"]\\r\\n\\r\\npred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3, 0.7, 0.5, 0.8, 0.2, 0.3, 0.35]\\r\\n\\r\\nthresholds = numpy.arange(start=0.2, stop=0.7, step=0.05)\\n\\n\\xa0\\nHere are the thresholds saved in the\\xa0thresholds\\xa0list. Because there are 10 thresholds, 10 values for precision and recall will be created.\\n\\n[0.2, \\r\\n 0.25, \\r\\n 0.3, \\r\\n 0.35, \\r\\n 0.4, \\r\\n 0.45, \\r\\n 0.5, \\r\\n 0.55, \\r\\n 0.6, \\r\\n 0.65]\\n\\n\\xa0\\nThe next function named\\xa0precision_recall_curve()\\xa0accepts the ground-truth labels, prediction scores, and thresholds. It returns two equal-length lists representing the precision and recall values.\\n\\nimport sklearn.metrics\\r\\n\\r\\ndef precision_recall_curve(y_true, pred_scores, thresholds):\\r\\n    precisions = []\\r\\n    recalls = []\\r\\n    \\r\\n    for threshold in thresholds:\\r\\n        y_pred = [\"positive\" if score >= threshold else \"negative\" for score in pred_scores]\\r\\n\\r\\n        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\\r\\n        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\\r\\n        \\r\\n        precisions.append(precision)\\r\\n        recalls.append(recall)\\r\\n\\r\\n    return precisions, recalls\\n\\n\\xa0\\nThe next code calls the\\xa0precision_recall_curve()\\xa0function after passing the three previously prepared lists. It returns the\\xa0precisions\\xa0and\\xa0recalls\\xa0lists that hold all the values of the precisions and recalls, respectively.\\n\\nprecisions, recalls = precision_recall_curve(y_true=y_true, \\r\\n                                             pred_scores=pred_scores,\\r\\n                                             thresholds=thresholds)\\n\\n\\xa0\\nHere are the returned values in the\\xa0precisions\\xa0list.\\n\\n[0.5625,\\r\\n 0.5714285714285714,\\r\\n 0.5714285714285714,\\r\\n 0.6363636363636364,\\r\\n 0.7,\\r\\n 0.875,\\r\\n 0.875,\\r\\n 1.0,\\r\\n 1.0,\\r\\n 1.0]\\n\\n\\xa0\\nHere is the list of \\xa0values in the\\xa0recalls\\xa0list.\\n\\n[1.0,\\r\\n 0.8888888888888888,\\r\\n 0.8888888888888888,\\r\\n 0.7777777777777778,\\r\\n 0.7777777777777778,\\r\\n 0.7777777777777778,\\r\\n 0.7777777777777778,\\r\\n 0.6666666666666666,\\r\\n 0.5555555555555556,\\r\\n 0.4444444444444444]\\n\\n\\xa0\\nGiven the two lists of equal lengths, it is possible to plot their values in a 2D plot as shown below.\\n\\nmatplotlib.pyplot.plot(recalls, precisions, linewidth=4, color=\"red\")\\r\\nmatplotlib.pyplot.xlabel(\"Recall\", fontsize=12, fontweight=\\'bold\\')\\r\\nmatplotlib.pyplot.ylabel(\"Precision\", fontsize=12, fontweight=\\'bold\\')\\r\\nmatplotlib.pyplot.title(\"Precision-Recall Curve\", fontsize=15, fontweight=\"bold\")\\r\\nmatplotlib.pyplot.show()\\n\\n\\xa0\\nThe precision-recall curve is shown in the next figure. Note that as the recall increases, the precision decreases. The reason is that when the number of positive samples increases (high recall), the accuracy of classifying each sample correctly decreases (low precision). This is expected, as the model is more likely to fail when there are many samples.\\n\\nThe precision-recall curve makes it easy to decide the point where both the precision and recall are high. According to the previous figure, the best point is\\xa0(recall, precision)=(0.778, 0.875).\\nGraphically deciding the best values for both the precision and recall might work using the previous figure because the curve is not complex. A better way is to use a metric called the\\xa0f1\\xa0score, which is calculated according to the next equation.\\n\\nThe\\xa0f1\\xa0metric measures the balance between precision and recall. When the value of\\xa0f1\\xa0is high, this means both the precision and recall are high. A lower\\xa0f1\\xa0score means a greater imbalance between precision and recall.\\nAccording to the previous example, the\\xa0f1\\xa0is calculated according to the code below. According to the values in the\\xa0f1\\xa0list, the highest score is\\xa00.82352941. It is the 6th element in the list (i.e. index 5). The 6th elements in the\\xa0recalls\\xa0and\\xa0precisions\\xa0lists are\\xa00.778\\xa0and\\xa00.875, respectively. The corresponding threshold value is\\xa00.45.\\n\\nf1 = 2 * ((numpy.array(precisions) * numpy.array(recalls)) / (numpy.array(precisions) + numpy.array(recalls)))\\n\\n\\xa0\\n\\n[0.72, \\r\\n 0.69565217, \\r\\n 0.69565217, \\r\\n 0.7,\\r\\n 0.73684211,\\r\\n 0.82352941, \\r\\n 0.82352941, \\r\\n 0.8, \\r\\n 0.71428571, 0\\r\\n .61538462]\\n\\n\\xa0\\nThe next figure shows, in blue, the location of the point that corresponds to the best balance between the recall and the precision. In conclusion, the best threshold to balance the precision and recall is\\xa00.45\\xa0at which the precision is\\xa00.875\\xa0and the recall is\\xa00.778.\\n\\nmatplotlib.pyplot.plot(recalls, precisions, linewidth=4, color=\"red\", zorder=0)\\r\\nmatplotlib.pyplot.scatter(recalls[5], precisions[5], zorder=1, linewidth=6)\\r\\n\\r\\nmatplotlib.pyplot.xlabel(\"Recall\", fontsize=12, fontweight=\\'bold\\')\\r\\nmatplotlib.pyplot.ylabel(\"Precision\", fontsize=12, fontweight=\\'bold\\')\\r\\nmatplotlib.pyplot.title(\"Precision-Recall Curve\", fontsize=15, fontweight=\"bold\")\\r\\nmatplotlib.pyplot.show()\\n\\n\\xa0\\n\\nAfter the precision-recall curve is discussed, the next section discusses how to calculate the\\xa0average precision.\\n\\xa0\\nAverage Precision (AP)\\n\\xa0\\nThe\\xa0average precision (AP)\\xa0is a way to summarize the precision-recall curve into a single value representing the average of all precisions. The AP is calculated according to the next equation. Using a loop that goes through all precisions/recalls, the difference between the current and next recalls is calculated and then multiplied by the current precision. In other words, the AP is the weighted sum of precisions at each threshold where the weight is the increase in recall.\\n\\nIt is important to append the recalls and precisions lists by 0 and 1, respectively. For example, if the\\xa0recalls\\xa0list is0.8,0.60.8,0.6, then it should have 0 appended to be0.8,0.6,0.00.8,0.6,0.0. The same happens for the\\xa0precisions\\xa0list but have 1 rather than 0 appended (e.g.0.8,0.2,1.00.8,0.2,1.0).\\nGiven that both\\xa0recalls\\xa0and\\xa0precisions\\xa0are NumPy arrays, the previous equation is modeled according to the next Python line.\\n\\nAP = numpy.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\\n\\n\\xa0\\nHere is the complete code that calculates the AP.\\n\\nimport numpy\\r\\nimport sklearn.metrics\\r\\n\\r\\ndef precision_recall_curve(y_true, pred_scores, thresholds):\\r\\n    precisions = []\\r\\n    recalls = []\\r\\n    \\r\\n    for threshold in thresholds:\\r\\n        y_pred = [\"positive\" if score >= threshold else \"negative\" for score in pred_scores]\\r\\n\\r\\n        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\\r\\n        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\\r\\n        \\r\\n        precisions.append(precision)\\r\\n        recalls.append(recall)\\r\\n\\r\\n    return precisions, recalls\\r\\n\\r\\ny_true = [\"positive\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"negative\", \"negative\"]\\r\\npred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3, 0.7, 0.5, 0.8, 0.2, 0.3, 0.35]\\r\\nthresholds=numpy.arange(start=0.2, stop=0.7, step=0.05)\\r\\n\\r\\nprecisions, recalls = precision_recall_curve(y_true=y_true, \\r\\n                                             pred_scores=pred_scores, \\r\\n                                             thresholds=thresholds)\\r\\n\\r\\nprecisions.append(1)\\r\\nrecalls.append(0)\\r\\n\\r\\nprecisions = numpy.array(precisions)\\r\\nrecalls = numpy.array(recalls)\\r\\n\\r\\nAP = numpy.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\\r\\nprint(AP)\\n\\n\\xa0\\nThis is all about the average precision. Here is a summary of the steps to calculate the AP:\\n\\nGenerate the\\xa0prediction scores\\xa0using the model.\\nConvert the\\xa0prediction scores\\xa0to\\xa0class labels.\\nCalculate the\\xa0confusion matrix.\\nCalculate the\\xa0precision\\xa0and\\xa0recall\\xa0metrics.\\nCreate the\\xa0precision-recall curve.\\nMeasure the\\xa0average precision.\\n\\nThe next section talks about the\\xa0intersection over union (IoU)\\xa0which is how an object detection generates the prediction scores.\\n\\xa0\\nIntersection over Union (IoU)\\n\\xa0\\nTo train an object detection model, usually, there are 2 inputs:\\n\\nAn image.\\nGround-truth bounding boxes for each object in the image.\\n\\nThe model predicts the bounding boxes of the detected objects. It is expected that the predicted box will not match exactly the ground-truth box. The next figure shows a cat image. The ground-truth box of the object is in red while the predicted one is in yellow. Based on the visualization of the 2 boxes, is the model made a good prediction with a high match score?\\nIt is difficult to subjectively evaluate the model predictions. For example, someone may conclude that there is a 50% match while someone else notices that there is a 60% match.\\n\\nImage\\xa0without labels from\\xa0Pixabay\\xa0by\\xa0susannp4\\n\\n\\xa0\\nA better alternative is to use a quantitative measure to score how the ground-truth and predicted boxes match. This measure is the\\xa0intersection over union (IoU). The IoU helps to know if a region has an object or not.\\nThe IoU is calculated according to the next equation by dividing the area of intersection between the 2 boxes by the area of their union. The higher the IoU, the better the prediction.\\n\\nThe next figure shows 3 cases with different IoUs. Note that the IoUs at the top of each case are objectively measured and may differ a bit from the reality but it makes sense.\\nFor case A, the predicted box in yellow is so far from being aligned on the red ground-truth box and thus the IoU score is\\xa00.2\\xa0(i.e. there is only a 20% overlap between the 2 boxes).\\nFor case B, the intersection area between the 2 boxes is larger but the 2 boxes are still not aligned well and thus the IoU score is\\xa00.5.\\nFor case C, the coordinates of the 2 boxes are so close and thus their IoU is\\xa00.9\\xa0(i.e. there is a 90% overlap between the 2 boxes).\\nNote that the IoU is 0.0 when there is a 0% overlap between the predicted and ground-truth boxes. The IoU is\\xa01.0\\xa0when the 2 boxes fit each other 100%.\\n\\nTo calculate the IoU for an image, here is a function named\\xa0intersection_over_union(). It accepts the following 2 parameters:\\n\\ngt_box: Ground-truth bounding box.\\npred_box: Predicted bounding box.\\n\\nIt calculates the intersection and union between the 2 boxes in the\\xa0intersection\\xa0and\\xa0union\\xa0variables, respectively. Moreover, the IoU is calculated in the\\xa0iou\\xa0variable. It returns all of these 3 variables.\\n\\ndef intersection_over_union(gt_box, pred_box):\\r\\n    inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]\\r\\n    inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]\\r\\n\\r\\n    inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]\\r\\n    inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]\\r\\n\\r\\n    intersection = inter_box_w * inter_box_h\\r\\n    union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection\\r\\n    \\r\\n    iou = intersection / union\\r\\n\\r\\n    return iou, intersection, union\\n\\n\\xa0\\nThe bounding box passed to the function is a list of 4 elements which are:\\n\\nThe x-axis of the top-left corner.\\nThe y-axis of the top-left corner.\\nWidth.\\nHeight.\\n\\nHere are the ground-truth and predicted bounding boxes of the car image.\\n\\ngt_box = [320, 220, 680, 900]\\r\\npred_box = [500, 320, 550, 700]\\n\\n\\xa0\\nGiven that the image is named\\xa0cat.jpg, here is the complete that draws the bounding boxes over the image.\\n\\nimport imageio\\r\\nimport matplotlib.pyplot\\r\\nimport matplotlib.patches\\r\\n\\r\\ndef intersection_over_union(gt_box, pred_box):\\r\\n    inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]\\r\\n    inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]\\r\\n\\r\\n    inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]\\r\\n    inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]\\r\\n\\r\\n    intersection = inter_box_w * inter_box_h\\r\\n    union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection\\r\\n    \\r\\n    iou = intersection / union\\r\\n\\r\\n    return iou, intersection, union\\r\\n\\r\\nim = imageio.imread(\"cat.jpg\")\\r\\n\\r\\ngt_box = [320, 220, 680, 900]\\r\\npred_box = [500, 320, 550, 700]\\r\\n\\r\\nfig, ax = matplotlib.pyplot.subplots(1)\\r\\nax.imshow(im)\\r\\n\\r\\ngt_rect = matplotlib.patches.Rectangle((gt_box[0], gt_box[1]),\\r\\n                                       gt_box[2],\\r\\n                                       gt_box[3],\\r\\n                                       linewidth=5,\\r\\n                                       edgecolor=\\'r\\',\\r\\n                                       facecolor=\\'none\\')\\r\\n\\r\\npred_rect = matplotlib.patches.Rectangle((pred_box[0], pred_box[1]),\\r\\n                                         pred_box[2],\\r\\n                                         pred_box[3],\\r\\n                                         linewidth=5,\\r\\n                                         edgecolor=(1, 1, 0),\\r\\n                                         facecolor=\\'none\\')\\r\\nax.add_patch(gt_rect)\\r\\nax.add_patch(pred_rect)\\r\\n\\r\\nax.axes.get_xaxis().set_ticks([])\\r\\nax.axes.get_yaxis().set_ticks([])\\n\\n\\xa0\\nThe next figure shows the image with the bounding boxes.\\n\\nTo calculate the IoU, just call the\\xa0intersection_over_union()\\xa0function. Based on the bounding boxes, the IoU score is\\xa00.54.\\n\\niou, intersect, union = intersection_over_union(gt_box, pred_box)\\r\\nprint(iou, intersect, union)\\n\\n\\xa0\\n\\n0.5409582689335394 350000 647000\\n\\n\\xa0\\nThe IoU score\\xa00.54\\xa0means there is a 54% overlap between the ground-truth and predicted bounding boxes. Looking at the boxes, someone may visually feel it is good enough to conclude that the model detected the cat object. Someone else may feel the model is not yet accurate as the predicted box does not fit the ground-truth box well.\\nTo objectively judge whether the model predicted the box location correctly or not, a\\xa0threshold\\xa0is used. If the model predicts a box with an IoU score greater than or equal to the\\xa0threshold, then there is a high overlap between the predicted box and one of the ground-truth boxes. This means the model was able to detect an object successfully. The detected region is classified as\\xa0Positive\\xa0(i.e. contains an object).\\nOn the other hand, when the IoU score is smaller than the threshold, then the model made a bad prediction as the predicted box does not overlap with the ground-truth box. This means the detected region is classified as\\xa0Negative\\xa0(i.e. does not contain an object).\\n\\nLet\\'s have an example to clarify how the IoU scores help to classify a region as an object or not. Assume the object detection model is fed by the next image where there are 2 target objects with their ground-truth boxes in red and the predicted boxes are in yellow.\\nThe next code reads the image (given it is named\\xa0pets.jpg), draws the boxes, and calculates the IoU for each object. The IoU for the left object is\\xa00.76\\xa0while the other object has an IoU score of\\xa00.26.\\n\\nimport matplotlib.pyplot\\r\\nimport matplotlib.patches\\r\\nimport imageio\\r\\n\\r\\ndef intersection_over_union(gt_box, pred_box):\\r\\n    inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]\\r\\n    inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]\\r\\n\\r\\n    inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]\\r\\n    inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]\\r\\n\\r\\n    intersection = inter_box_w * inter_box_h\\r\\n    union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection\\r\\n    \\r\\n    iou = intersection / union\\r\\n\\r\\n    return iou, intersection, union, \\r\\n\\r\\nim = imageio.imread(\"pets.jpg\")\\r\\n\\r\\ngt_box = [10, 130, 370, 350]\\r\\npred_box = [30, 100, 370, 350]\\r\\n\\r\\niou, intersect, union = intersection_over_union(gt_box, pred_box)\\r\\nprint(iou, intersect, union)\\r\\n\\r\\nfig, ax = matplotlib.pyplot.subplots(1)\\r\\nax.imshow(im)\\r\\n\\r\\ngt_rect = matplotlib.patches.Rectangle((gt_box[0], gt_box[1]),\\r\\n                                       gt_box[2],\\r\\n                                       gt_box[3],\\r\\n                                       linewidth=5,\\r\\n                                       edgecolor=\\'r\\',\\r\\n                                       facecolor=\\'none\\')\\r\\n\\r\\npred_rect = matplotlib.patches.Rectangle((pred_box[0], pred_box[1]),\\r\\n                                         pred_box[2],\\r\\n                                         pred_box[3],\\r\\n                                         linewidth=5,\\r\\n                                         edgecolor=(1, 1, 0),\\r\\n                                         facecolor=\\'none\\')\\r\\nax.add_patch(gt_rect)\\r\\nax.add_patch(pred_rect)\\r\\n\\r\\ngt_box = [645, 130, 310, 320]\\r\\npred_box = [500, 60, 310, 320]\\r\\n\\r\\niou, intersect, union = intersection_over_union(gt_box, pred_box)\\r\\nprint(iou, intersect, union)\\r\\n\\r\\ngt_rect = matplotlib.patches.Rectangle((gt_box[0], gt_box[1]),\\r\\n                                       gt_box[2],\\r\\n                                       gt_box[3],\\r\\n                                       linewidth=5,\\r\\n                                       edgecolor=\\'r\\',\\r\\n                                       facecolor=\\'none\\')\\r\\n\\r\\npred_rect = matplotlib.patches.Rectangle((pred_box[0], pred_box[1]),\\r\\n                                         pred_box[2],\\r\\n                                         pred_box[3],\\r\\n                                         linewidth=5,\\r\\n                                         edgecolor=(1, 1, 0),\\r\\n                                         facecolor=\\'none\\')\\r\\nax.add_patch(gt_rect)\\r\\nax.add_patch(pred_rect)\\r\\n\\r\\nax.axes.get_xaxis().set_ticks([])\\r\\nax.axes.get_yaxis().set_ticks([])\\n\\n\\xa0\\nGiven that the IoU threshold is 0.6, then only the regions with IoU scores greater than or equal to 0.6 are classified as\\xa0Positive\\xa0(i.e. having objects). Thus, the box with IoU score 0.76 is Positive while the other box with IoU of 0.26 is\\xa0Negative.\\n\\nImage without Labels from\\xa0hindustantimes.com\\n\\n\\xa0\\nIf the threshold changed to be\\xa00.2\\xa0rather than 0.6, then both predictions are\\xa0Positive. If the threshold is\\xa00.8, then both predictions are\\xa0Negative.\\nAs a summary, the IoU score measures how close is the predicted box to the ground-truth box. It ranges from 0.0 to 1.0 where 1.0 is the optimal result. When the IoU is greater than the threshold, then the box is classified as\\xa0Positive\\xa0as it surrounds an object. Otherwise, it is classified as\\xa0Negative.\\nThe next section shows how to benefit from the IoUs to calculate the mean average precision (mAP) for an object detection model.\\n\\xa0\\nMean Average Precision (mAP) for Object Detection\\n\\xa0\\nUsually, the object detection models are evaluated with different IoU thresholds where each threshold may give different predictions from the other thresholds. Assume that the model is fed by an image that has 10 objects distributed across 2 classes. How to calculate the mAP?\\nTo calculate the mAP, start by calculating the AP for each class. The mean of the APs for all classes is the mAP.\\nAssuming that the dataset used has only 2 classes. For the first class, here are the ground-truth labels and predicted scores in the\\xa0y_true\\xa0and\\xa0pred_scores\\xa0variables, respectively.\\n\\ny_true = [\"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\"]\\r\\n\\r\\npred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.75, 0.2, 0.8, 0.3]\\n\\n\\xa0\\nHere are the\\xa0y_true\\xa0and\\xa0pred_scores\\xa0variables of the second class.\\n\\ny_true = [\"negative\", \"positive\", \"positive\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\"]\\r\\n\\r\\npred_scores = [0.32, 0.9, 0.5, 0.1, 0.25, 0.9, 0.55, 0.3, 0.35, 0.85]\\n\\n\\xa0\\nThe list of IoU thresholds starts from 0.2 to 0.9 with 0.25 step.\\n\\nthresholds = numpy.arange(start=0.2, stop=0.9, step=0.05)\\n\\n\\xa0\\nTo calculate the AP for a class, just feed its\\xa0y_true\\xa0and\\xa0pred_scores\\xa0variables to the next code.\\n\\nprecisions, recalls = precision_recall_curve(y_true=y_true, \\r\\n                                             pred_scores=pred_scores, \\r\\n                                             thresholds=thresholds)\\r\\n\\r\\nmatplotlib.pyplot.plot(recalls, precisions, linewidth=4, color=\"red\", zorder=0)\\r\\n\\r\\nmatplotlib.pyplot.xlabel(\"Recall\", fontsize=12, fontweight=\\'bold\\')\\r\\nmatplotlib.pyplot.ylabel(\"Precision\", fontsize=12, fontweight=\\'bold\\')\\r\\nmatplotlib.pyplot.title(\"Precision-Recall Curve\", fontsize=15, fontweight=\"bold\")\\r\\nmatplotlib.pyplot.show()\\r\\n\\r\\nprecisions.append(1)\\r\\nrecalls.append(0)\\r\\n\\r\\nprecisions = numpy.array(precisions)\\r\\nrecalls = numpy.array(recalls)\\r\\n\\r\\nAP = numpy.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\\r\\nprint(AP)\\n\\n\\xa0\\nFor the first class, here is its precision-recall curve. Based on this curve, the AP is\\xa00.949.\\n\\nThe precision-recall curve of the second class is shown below. Its AP is\\xa00.958.\\n\\nBased on the APs of the 2 classes (0.949 and 0.958), the mAP of the object detection model is calculated according to the next equation.\\n\\nBased on this equation, the mAP is\\xa00.9535.\\n\\nmAP = (0.949 + 0.958)/2 = 0.9535\\n\\n\\xa0\\n\\xa0\\nConclusion\\n\\xa0\\nThis tutorial discussed how to calculate the mean average precision (mAP) for an object detection model. We started by discussing how to convert a prediction score to a class label. Using different thresholds, a precision-recall curve is created. From that curve, the average precision (AP) is measured.\\nFor an object detection model, the threshold is the intersection over union (IoU) that scores the detected objects. Once the AP is measured for each class in the dataset, the mAP is calculated.\\n\\xa0\\nBio: Ahmed Gad received his B.Sc. degree with excellent with honors in information technology from the Faculty of Computers and Information (FCI), Menoufia University, Egypt, in July 2015. For being ranked first in his faculty, he was recommended to work as a teaching assistant in one of the Egyptian institutes in 2015 and then in 2016 to work as a teaching assistant and a researcher in his faculty. His current research interests include deep learning, machine learning, artificial intelligence, digital signal processing, and computer vision.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nEvaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall\\nWorking With The Lambda Layer in Keras\\nHow to Create Custom Real-time Plots in Deep Learning',\n",
       " 'comments\\nBy Olga Chernytska, Senior Machine Learning Engineer\\nNative PyTorch and TensorFlow augmenters have a big disadvantage – they cannot simultaneously augment an image and its segmentation mask, bounding box, or keypoint locations. So there are two options – either write functions on your own or use third-party libraries. I tried both, and the second option is just better\\xa0🙂\\n\\xa0\\nWhy Albumentations?\\n\\xa0\\nAlbumentations\\xa0was the first library that I’ve tried, and I’ve stuck with it, because:\\n\\nIt is open-source,\\nIntuitive,\\nFast,\\nHas more than\\xa060 different augmentations,\\nWell-documented,\\nAnd, what is most important, can simultaneously augment an image and its segmentation mask, bounding box, or keypoint locations.\\n\\nThere are two more similar libraries –\\xa0imgaug\\xa0and\\xa0Augmentor. Unfortunately, I cannot provide any comparison, as I haven’t tried them yet. Till this moment Albumentations was just enough.\\n\\xa0\\nShort Tutorial\\n\\xa0\\nIn this short tutorial, I’ll show how to augment images for segmentation and object detection tasks – easily with few lines of code.\\nIf you’d like to follow this tutorial:\\n\\nInstall Albumentations. I really recommend checking if you have the latest version, as older ones may be buggy. I use version ‘1.0.0’ and it works fine.\\nDownload a test image with labels below. It is just a random image from\\xa0COCO dataset. I modified it a bit and stored it in the format required by Albumentations. This library accepts images as NumPy arrays, segmentation masks as NumPy arrays, and bounding boxes as lists.\\n\\nDownload\\nLet’s load the image, its binary pixel-wise segmentation mask, and a bounding box. The bounding box is defined as a 4-element list – [x_min, y_min, width, height].\\n\\nimport pickle\\r\\nimport numpy as np\\r\\nimport matplotlib.pyplot as plt\\r\\nimport matplotlib.patches as patches\\r\\n\\r\\n# load data\\r\\nwith open(\"image_data.pickle\", \"rb\") as handle:\\r\\n    image_data = pickle.load(handle)\\r\\n\\r\\nimage = image_data[\"image\"]\\r\\nmask = image_data[\"mask\"]\\r\\nbbox = image_data[\"bbox_coco\"]\\r\\n\\r\\n# visualize data\\r\\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\\r\\nax[0].imshow(image)\\r\\nax[0].set_title(\"Image\")\\r\\nax[1].imshow(image)\\r\\nbbox_rect = patches.Rectangle(\\r\\n    bbox[:2], bbox[2], bbox[3], linewidth=2, edgecolor=\"r\", facecolor=\"none\"\\r\\n)\\r\\nax[1].add_patch(bbox_rect)\\r\\nax[1].imshow(mask, alpha=0.3, cmap=\"gray_r\")\\r\\nax[1].set_title(\"Image + BBox + Mask\")\\r\\nplt.show()\\n\\n\\nAfter loading and visualizing the image, you should get this:\\nImage. The output when running code for image and its labels visualization.\\nSegmentation mask is visualized as a transparent black-white image (1 is black, ‘horse’).\\n\\n\\xa0\\nMask Augmentation for Segmentation. And now we can start with Albumentations. Transformations here are defined very similarly to PyTorch and TensorFlow (Keras API):\\n\\nDefine transformation via combining several augmentations using a Compose object.\\nEach augmentation has argument `p`, the probability to be applied, and additionally augmentations-specific arguments, like `width` and `height` for RandomCrop.\\nUse defined transformation as a function to augment the image and its mask. This function returns a dictionary with keys – `image` and `mask`.\\n\\nBelow is the code on how to augment the image (and its mask) with random 256×256 crop (always) and horizontal flip (only in 50% cases).\\n\\nimport albumentations as A\\r\\n\\r\\n# define augmentation\\r\\ntransform = A.Compose([\\r\\n    A.RandomCrop(width=256, height=256, p=1), \\r\\n    A.HorizontalFlip(p=0.5),\\r\\n])\\r\\n\\r\\n# augment and visualize images\\r\\nfig, ax = plt.subplots(2, 3, figsize=(15, 10))\\r\\nfor i in range(6):\\r\\n    transformed = transform(image=image, mask=mask) \\r\\n    ax[i // 3, i % 3].imshow(transformed[\"image\"])\\r\\n    ax[i // 3, i % 3].imshow(transformed[\"mask\"], alpha=0.3, cmap=\"gray_r\")\\r\\nplt.show()\\n\\n\\nAs a result, you should get something like this. Your augmented images will be different, as Albumentations produces random transformations. For a detailed tutorial on mask augmentation refer to\\xa0original documentation.\\nImage. The output when running code for simultaneous image and mask augmentation.\\nSegmentation mask is visualized as a transparent black-white image (1 is black, ‘horse’)\\n\\n\\xa0\\nBounding Boxes Augmentation for Object Detection. It is similar to augmentation for segmentation masks, however:\\n\\nAdditionally, define `bbox_params`, where specify the format of the bounding box and argument for bounding box classes. `coco` means bounding box in COCO dataset format – [x_min, y_min, width, height]. And argument `bbox_classes` will be used later to pass classes for bounding boxes.\\n`transform` accepts bounding boxes as a list of lists. Additionally, it requires bounding box classes (as a list) even if there is a single bounding box in the image.\\n\\nBelow is the code that does RandomCrop and HorizonalFrip simultaneously for the image and its bounding box.\\n\\n# define augmentation\\r\\ntransform = A.Compose([\\r\\n    A.RandomCrop(width=256, height=256, p=1),\\r\\n    A.HorizontalFlip(p=0.5),\\r\\n], bbox_params=A.BboxParams(format=\\'coco\\', label_fields=[\"bbox_classes\"]))\\r\\n\\r\\n# augment and visualize images\\r\\nbboxes = [bbox] #`transform` accepts bounding boxes as a list of lists.\\r\\nbbox_classes = [\"horse\"]\\r\\n\\r\\nfig, ax = plt.subplots(2, 3, figsize=(15, 10))\\r\\nfor i in range(6):\\r\\n    transformed = transform(\\r\\n        image=image, \\r\\n        bboxes=bboxes, \\r\\n        bbox_classes=bbox_classes\\r\\n    )\\r\\n    ax[i // 3, i % 3].imshow(transformed[\"image\"])\\r\\n    trans_bbox = transformed[\"bboxes\"][0]\\r\\n    bbox_rect = patches.Rectangle(\\r\\n        trans_bbox[:2],\\r\\n        trans_bbox[2],\\r\\n        trans_bbox[3],\\r\\n        linewidth=2,\\r\\n        edgecolor=\"r\",\\r\\n        facecolor=\"none\",\\r\\n    )\\r\\n    ax[i // 3, i % 3].add_patch(bbox_rect)\\r\\nplt.show()\\n\\n\\nAnd here are the results. In case you need some specific bounding box augmentations – refer to the\\xa0original documentation.\\nImage. The output when running code for simultaneous image and bounding box augmentation.\\n\\n\\xa0\\nSimultaneous augmentation of multiple targets.\\xa0Besides allowing to simultaneously\\xa0augment several masks\\xa0or\\xa0several bounding boxes, Albumentations has a feature to simultaneously augment different types of labels, for instance, a mask and a bounding box.\\nWhen calling a `transform` simply give it everything you have:\\n\\n# define augmentation\\r\\ntransform = A.Compose([\\r\\n    A.RandomCrop(width=256, height=256, p=1),\\r\\n    A.HorizontalFlip(p=0.5),\\r\\n], bbox_params=A.BboxParams(format=\\'coco\\', label_fields=[\"bbox_classes\"]))\\r\\n\\r\\n# augment and visualize images\\r\\nbboxes = [bbox]\\r\\nbbox_classes = [\"horse\"]\\r\\n\\r\\nfig, ax = plt.subplots(2, 3, figsize=(15, 10))\\r\\nfor i in range(6):\\r\\n    transformed = transform(\\r\\n        image=image, \\r\\n        mask=mask, \\r\\n        bboxes=bboxes, \\r\\n        bbox_classes=bbox_classes\\r\\n    )\\r\\n    ax[i // 3, i % 3].imshow(transformed[\"image\"])\\r\\n    trans_bbox = transformed[\"bboxes\"][0]\\r\\n    bbox_rect = patches.Rectangle(\\r\\n        trans_bbox[:2],\\r\\n        trans_bbox[2],\\r\\n        trans_bbox[3],\\r\\n        linewidth=2,\\r\\n        edgecolor=\"r\",\\r\\n        facecolor=\"none\",\\r\\n    )\\r\\n    ax[i // 3, i % 3].add_patch(bbox_rect)\\r\\n    ax[i // 3, i % 3].imshow(transformed[\"mask\"], alpha=0.3, cmap=\"gray_r\")\\r\\nplt.show()\\n\\n\\nYour result will look like in the image below. And here is\\xa0more detailed documentation on that.\\nImage. The output when running code for a simultaneous image, segmentation mask, and bounding box augmentation.\\nSegmentation mask is visualized as a transparent black-white image (1 is black, ‘horse’).\\n\\n\\xa0\\nAnd More.\\xa0Albumentations has much more features available, such as augmentation for\\xa0keypoints\\xa0and\\xa0AutoAugment. And it includes about\\xa060 different augmentation types\\xa0– literally for any task you need.\\n\\xa0\\nCompatibility with PyTorch & TensorFlow\\n\\xa0\\nMost likely you are going to use Albumentations as a part of PyTorch or TensorFlow training pipeline, so I’ll briefly describe how to do it.\\nPyTorch. When\\xa0creating a Custom dataset, define Albumentations transform in the `__init__` function and call it in the `__getitem__` function. PyTorch models require input data to be tensors, so make sure you add `ToTensorV2` as the last step when defining `transform` (a trick from\\xa0one of Albumentations tutorials).\\n\\nfrom torch.utils.data import Dataset\\r\\nfrom albumentations.pytorch import ToTensorV2\\r\\n\\r\\nclass CustomDataset(Dataset):\\r\\n    def __init__(self, images, masks):\\r\\n        self.images = images  # assume it\\'s a list of numpy images\\r\\n        self.masks = masks  # assume it\\'s a list of numpy masks\\r\\n        self.transform = A.Compose([\\r\\n            A.RandomCrop(width=256, height=256, p=1),\\r\\n            A.HorizontalFlip(p=0.5),\\r\\n            ToTensorV2(),\\r\\n        ])\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.images)\\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        image = self.images[idx]\\r\\n        mask = self.masks[idx]\\r\\n        transformed = self.transform(image=image, mask=mask)\\r\\n        transformed_image = transformed[\"image\"]\\r\\n        transformed_mask = transformed[\"mask\"]\\r\\n        return transformed_image, transformed_mask\\n\\n\\nTensorFlow\\xa0(Keras API) also allows creating\\xa0Custom datasets, similar to PyTorch. So define Albumentations transform in the `__init__` function and call it in the `__getitem__` function. Pretty simple, isn’t it?\\n\\nfrom tensorflow import keras\\r\\n\\r\\nclass CustomDataset(keras.utils.Sequence):\\r\\n    def __init__(self, images, masks):\\r\\n        self.images = images\\r\\n        self.masks = masks\\r\\n        self.batch_size = 1\\r\\n        self.img_size = (256, 256)\\r\\n        self.transform = A.Compose([\\r\\n            A.RandomCrop(width=256, height=256, p=1), \\r\\n            A.HorizontalFlip(p=0.5),\\r\\n        ])\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.images) // self.batch_size\\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        \"\"\"Returns a batch of samples\"\"\"\\r\\n        i = idx * self.batch_size\\r\\n        batch_images = self.images[i : i + self.batch_size]\\r\\n        batch_masks = self.masks[i : i + self.batch_size]\\r\\n        batch_images_stacked = np.zeros(\\r\\n            (self.batch_size,) + self.img_size + (3,), dtype=\"uint8\"\\r\\n        )\\r\\n        batch_masks_stacked = np.zeros(\\r\\n            (self.batch_size,) + self.img_size, dtype=\"float32\"\\r\\n        )\\r\\n        for i in range(len(batch_images)):\\r\\n            transformed = self.transform(\\r\\n                image=batch_images[i], \\r\\n                mask=batch_masks[i]\\r\\n            )\\r\\n            batch_images_stacked[i] = transformed[\"image\"]\\r\\n            batch_masks_stacked[i] = transformed[\"mask\"]\\r\\n        return batch_images_stacked, batch_masks_stacked\\n\\n\\nThat’s it! Hope this tutorial encouraged you to try Albumentations next time you are working on segmentation, object detection or keypoint localization task. Let me know if it did!\\n\\xa0\\nBio: Olga Chernytska is a Senior Machine Learning Engineer in a large Eastern European outsourcing company; was involved in various data science projects for top US, European and Asian companies; main specialization and interest is Deep Computer Vision.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nThe only Jupyter Notebooks extension you truly need\\nHow to create an interactive 3D chart and share it easily with anyone\\nExtraction of Objects In Images and Videos Using 5 Lines of Code',\n",
       " \"comments\\nBy Orhan G. Yalçın, AI Researcher\\nWARNING:\\xa0Do not confuse this article with “Mastering TensorFlow Tensors in 5 Easy Steps”!\\n\\xa0\\nIf you are reading this article, I am sure that we share similar interests and are/will be in similar industries. So let’s connect via\\xa0Linkedin! Please do not hesitate to send a contact request!\\xa0Orhan G. Yalçın — Linkedin\\n\\nFigure 1. Photo by\\xa0Crissy Jarvis\\xa0on\\xa0Unsplash\\n\\xa0\\n\\n\\xa0\\nIn this tutorial, we will focus on\\xa0TensorFlow Variables. After the tutorial, you will be able to create, update, and manage\\xa0TensorFlow Variables\\xa0effectively. As usual, our tutorial will deliver code examples with detailed explanations as well as conceptual explanations. We will master\\xa0TensorFlow Variables\\xa0in 5 easy steps:\\n\\nStep 1: Definition of Variables\\xa0→A Brief Introduction, Comparison with Tensors\\nStep 2: Creation of Variables\\xa0→ Instantiating tf.Variable Objects\\nStep 3:\\xa0Qualifications of Variables\\xa0→ Characteristics and Features\\nStep 4: Operations with Variables\\xa0→ Basic Tensor Operations, Indexing, Shape Manipulation, and Broadcasting\\nStep 5: Hardware Selection for Variables\\xa0→ GPUs, CPUs, TPUs\\n\\nFasten your belts, and let’s start!\\n\\xa0\\nDefinition of Variables\\nIn this\\xa0step, we will briefly cover what Variables are and understand the difference between plain Tensor objects and Variable objects.\\n\\xa0\\nA Brief Introduction\\nA TensorFlow Variable is the preferred object type representing a shared and persistent state that you can manipulate with any operation, including TensorFlow models. Manipulation refers to any value or parameter update. This characteristic is the most distinguishing feature of Variables compared to\\xa0tf.Tensor\\xa0objects. TensorFlow Variables are recorded as\\xa0tf.Variable\\xa0objects. Let’s make a brief comparison between\\xa0tf.Tensor\\xa0and\\xa0tf.Variable\\xa0objects to understand their similarities and differences.\\n\\nFigure 2. Variable Values can be Updated (Figure by Author)\\n\\xa0\\n\\n\\xa0\\nComparison with Tensors\\nSo, the most important difference between Variables and Tensors is\\xa0mutability. The values in a Variable object can be updated (e.g., with the\\xa0assign()\\xa0function) as opposed to Tensors.\\n“The values of tensor objects cannot be updated, and you can only create a new Tensor object with the new values.”\\nVariable objects are mainly used to store model parameters, and since these values are constantly updated during training, using Variables, instead of Tensors, is a necessity rather than a choice.\\nThe shape of a Variable object can be updated with the\\xa0reshape()\\xa0instance function just like the shape of a Tensor object. Since Variable objects are built on top of Tensor objects, they have common attributes such as\\xa0.shape\\xa0and\\xa0.dtype. But, Variables also have unique attributes such as\\xa0.trainable,.device, and\\xa0.name\\xa0attributes that the Tensors do not have.\\n\\nFigure 3. A Tensorflow Variable is actually a wrapper around a TensorFlow Tensor with additional features (Figure by Author)\\n\\xa0\\n\\n\\xa0\\nLet’s see how we can create\\xa0tf.Variable\\xa0objects!\\n\\xa0\\nCreation of Variables\\nWe can instantiate (i.e., create)\\xa0tf.Variableobjects with the\\xa0tf.Variable()\\xa0function. The tf.Variable() function accepts different data types as parameter such as integers, floats, strings, lists, and\\xa0tf.Constant\\xa0objects.\\nBefore showing different Variable object examples with these different data types, I want you to\\xa0start a new Google Colab notebook\\xa0and import TensorFlow library with the following code:\\n\\nNow, we can start creating\\xa0tf.Variable\\xa0objects.\\n1 — We can pass a\\xa0tf.constant()\\xa0object as the\\xa0initial_value:\\n\\n2 — We can pass a single integer as the\\xa0initial_value:\\n\\n3 — We can pass a list of integers or floats as the\\xa0initial_value:\\n\\n4 — We can pass a single string as the\\xa0initial_value:\\n\\n5 — We can pass a list of strings as the\\xa0initial_value:\\n\\nAs you can see, there are several data types that th etf.Variable()\\xa0function accepts as the\\xa0initial_value\\xa0argument. Now let’s take a look at the characteristics and features of variables.\\n\\xa0\\nQualifications of Variables\\nEvery Variable must have some properties such as value, name, uniform data type, shape, rank, size, and more. In this section, we will see what these properties are and how we can view these properties in a Colab notebook.\\n\\xa0\\nValue\\nEvery Variable must specify an\\xa0initial_value. Otherwise, TensorFlow raises an error and says that\\xa0Value Error: initial_value must be specified.\\xa0Therefore, make sure that you pass on an\\xa0initial_valueargument when creating Variable objects. To be able to view a Variable’s values, we can use the\\xa0.value()\\xa0function as well as the\\xa0.numpy()\\xa0function. See the example below:\\n\\n\\nOutput:\\r\\nThe values stored in the variables:\\r\\ntf.Tensor( [[1. 2.]  \\r\\n            [1. 2.]], shape=(2, 2), dtype=float32)The values stored in the variables:\\r\\n[[1. 2.]\\r\\n[1. 2.]]\\n\\n\\xa0\\n\\xa0\\nName\\nName is a Variable attribute which helps developers to track the updates on a particular variable. You can pass a\\xa0name\\xa0argument while creating the Variable object. If you don’t specify a name, TensorFlow assigns a default name, as shown below:\\n\\n\\nOutput:\\r\\nThe name of the variable:  Variable:0\\n\\n\\xa0\\n\\xa0\\nDtype\\nEach Variable must have a uniform data type that it stores. Since there is a single type of data stored for every Variable, you can also view this type with the\\xa0.dtype\\xa0attribute. See the example below:\\n\\n\\nOutput:\\r\\nThe selected datatype for the variable:  <dtype: 'float32'>\\n\\n\\xa0\\n\\xa0\\nShape, Rank, and Size\\nThe shape property shows the size of each dimension in the form of a list. We can view the shape of the Variable object with the .shape attribute. Then, we can view the number of dimensions that a Variable object has with the\\xa0tf.size()\\xa0function. Finally, Size corresponds to the total number of elements a Variable has. We need to use the\\xa0tf.size()\\xa0function to count the number of elements in a Variable. See the code below for all three properties:\\n\\n\\nOutput:\\r\\nThe shape of the variable:  (2, 2)\\r\\nThe number of dimensions in the variable: 2\\r\\nThe number of dimensions in the variable: 4\\n\\n\\xa0\\n\\xa0\\nOperations with Variables\\nThere are several basic operations you can easily conduct with math operators and TensorFlow functions. On top of what we covered in\\xa0Part 2 of this tutorial series, you may also use the following math operators for Variable operations.\\n\\xa0\\nBasic Tensor Operations\\n\\xa0\\n\\nFigure 4. You May Benefit from Basic Math Operators (Figure by Author)\\n\\xa0\\n\\n\\xa0\\nAddition and Subtraction:\\xa0We can conduct addition and subtraction with\\xa0+\\xa0and\\xa0—\\xa0signs.\\n\\n\\nAddition by 2:\\r\\ntf.Tensor( [[3. 4.]  [3. 4.]], shape=(2, 2), dtype=float32)Substraction by 2:\\r\\ntf.Tensor( [[-1.  0.]  [-1.  0.]], shape=(2, 2), dtype=float32)\\n\\n\\xa0\\nMultiplication and Division:\\xa0We can conduct multiplication and division with\\xa0*\\xa0and\\xa0/\\xa0signs.\\n\\n\\nMultiplication by 2:\\r\\ntf.Tensor( [[2. 4.]  [2. 4.]], shape=(2, 2), dtype=float32)Division by 2:\\r\\ntf.Tensor( [[0.5 1. ]  [0.5 1. ]], shape=(2, 2), dtype=float32)\\n\\n\\xa0\\nMatmul and Modulo Operations: Finally, you can also do matmul and modulo operations with\\xa0@\\xa0and\\xa0%\\xa0signs:\\n\\n\\nMatmul operation with itself:\\r\\ntf.Tensor( [[3. 6.]  [3. 6.]], shape=(2, 2), dtype=float32)Modulo operation by 2:\\r\\ntf.Tensor( [[1. 0.]  [1. 0.]], shape=(2, 2), dtype=float32)\\n\\n\\xa0\\nThese are elementary examples, but they can be extended into complex calculations, which creates the algorithms that we use for deep learning applications.\\n\\nNote: These operators also work on regular Tensor objects.\\n\\n\\xa0\\n\\xa0\\nAssignment, Indexing, Broadcasting, and Shape Manipulation\\n\\xa0\\nAssignment\\nWith the\\xa0tf.assign()\\xa0function, you may assign new values to a Variable object without creating a new object. Being able to assign new values is one of the advantages of Variables, where value reassignment is required. Here is an example of reassignment of values:\\n\\n\\nOutput:\\r\\n...array([[  2., 100.],\\r\\n          [  1.,  10.]],...\\n\\n\\xa0\\n\\xa0\\nIndexing\\nJust as in Tensors, you may easily access particular elements using index values, as shown below:\\n\\n\\nOutput:\\r\\nThe 1st element of the first level is: [1. 2.]\\r\\nThe 2nd element of the first level is: [1. 2.]\\r\\nThe 1st element of the second level is: 1.0\\r\\nThe 3rd element of the second level is: 2.0\\n\\n\\xa0\\n\\xa0\\nBroadcasting\\nJust as with Tensor objects, when we try to do combined operations using multiple Variable objects, the smaller Variables can stretch out automatically to fit larger Variables, just as NumPy arrays can. For example, when you attempt to multiply a scalar Variable with a 2-dimensional Variable, the scalar is stretched to multiply every 2-dimensional Variable element. See the example below:\\n\\n\\ntf.Tensor([[ 5 10]\\r\\n           [15 20]], shape=(2, 2), dtype=int32)\\n\\n\\xa0\\n\\xa0\\nShape Manipulation\\nJust as in Tensor objects, you can reshape Variable objects as well. For the reshape operation, we can use the\\xa0tf.reshape()\\xa0function. Let's use the\\xa0tf.reshape()\\xa0function in code:\\n\\n\\ntf.Tensor( [[1.]\\r\\n            [2.]\\r\\n            [1.]\\r\\n            [2.]], shape=(4, 1), dtype=float32)\\n\\n\\xa0\\n\\xa0\\nHardware Selection for Variables\\nAs you will see in the upcoming Parts, we will accelerate our model training with GPUs and TPUs. To be able to see what type of device (i.e., processor) our variable is processed with, we can use\\xa0.device\\xa0attribute:\\n\\n\\nThe device which process the variable:   /job:localhost/replica:0/task:0/device:GPU:0\\n\\n\\xa0\\nWe can also set which device should process a particular calculation with the\\xa0tf.device()\\xa0function by passing the device name as an argument. See the example below:\\n\\n\\nOutput:\\r\\nThe device which processes the variable a: /job:localhost/replica:0/task:0/device:CPU:0The device which processes the variable b: /job:localhost/replica:0/task:0/device:CPU:0The device which processes the calculation: /job:localhost/replica:0/task:0/device:GPU:0\\n\\n\\xa0\\nEven though you will not have to set this manually while training a model, there might be circumstances where you have to choose a device for a particular calculation or data processing work. So, beware of this option.\\n\\xa0\\nCongratulations\\nWe have successfully covered the basics of TensorFlow’s Variable objects.\\nGive yourself a pat on the back!\\n\\xa0\\nThis should give you a lot of confidence since you are now much more informed about the main mutable Variable object type used for all kinds of operations in TensorFlow.\\nIf this is your first post, consider starting from\\xa0Part 1 of this tutorial series:\\nBeginner’s Guide to TensorFlow 2.x for Deep Learning Applications\\nUnderstanding the TensorFlow Platform and What it has to Offer to a Machine Learning Expert\\nor\\xa0check out Part 2:\\nMastering TensorFlow Tensors in 5 Easy Steps\\nDiscover how the building blocks of TensorFlow works at the lower level and learn how to make the most of Tensor…\\n\\xa0\\nSubscribe to the Mailing List for the Full Code\\nIf you would like to have access to full code on Google Colab and the rest of my latest content, consider subscribing to the mailing list:\\n\\nSlide to SubscribeFinally, if you are interested in more advanced applied deep learning tutorials, check out some of my other articles:\\nImage Classification in 10 Minutes with MNIST Dataset\\nUsing Convolutional Neural Networks to Classify Handwritten Digits with TensorFlow and Keras | Supervised Deep Learning\\nImage Noise Reduction in 10 Minutes with Convolutional Autoencoders\\nUsing Deep Convolutional Autoencoders to Clean (or Denoise) Noisy Images with the help of Fashion MNIST | Unsupervised…\\nImage Generation in 10 Minutes with Generative Adversarial Networks\\nUsing Unsupervised Deep Learning to Generate Handwritten Digits with Deep Convolutional GANs using TensorFlow and the…\\nFast Neural Style Transfer in 5 Minutes with TensorFlow Hub & Magenta\\nTransferring the van Gogh’s Unique Style to Photos with Magenta’s Arbitrary Image Stylization Network and Deep Learning\\nBio: Orhan G. Yalçın is an AI Researcher in the legal domain. He is a qualified lawyer with business development and data science skills, and has previously worked as a legal trainee for Allen & Overy on capital markets, competition, and corporate law matters.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nMastering TensorFlow Tensors in 5 Easy Steps\\nDeploying Trained Models to Production with TensorFlow Serving\\nPruning Machine Learning Models in TensorFlow\",\n",
       " 'comments\\nBy Anji Velagana, Freelance Content Writer\\n\\nIt\\'s essential to understand data warehousing depending on your requirements and business. Many organizations struggle in selecting the data warehouse that suits them. Hence, people are opting for the BigQuery/Snowflake course to understand data warehousing. Here, we are going to compare the two topmost data warehouses: BigQuery and Snowflake.\\nLet\\'s move right into knowing them.\\n\\xa0\\nWhat is BigQuery?\\n\\xa0\\nBigQuery is a serverless and fully managed data warehouse that can enable a scalable analysis of the data. It is referred to as \"Platform-as-a-Service (PaaS).\" By using ANSI SQL, BigQuery supports querying. An enterprise data warehouse is used to solve problems using Google infrastructure processing power by enabling super-fast SQL.\\nSignificance of BigQuery\\nIt is very hard to manage the data spread across the applications as the business grows. Also, it isn\\'t easy to analyze the data within meaningful insight systems. The precious engineering sources are often deployed in setting up a centralized data store. With BigQuery, the developers can focus on essential activities like analyzing business-critical data. The REST API of BigQuery enables businesses in building mobile front-ends and App Engine-based dashboards.\\n\\xa0\\nWhat is Snowflake?\\n\\xa0\\nIt is a cloud-based data warehousing company founded in 2012. Snowflake offers analytics services, cloud-based data storage services. In general, Snowflake is termed as a \"Data warehouse as a service.\" The data cloud of Snowflake is powered by the advanced data platform with Software-as-a-Service (SaaS). It enables data processing, storage, and analytics solutions that are faster, easier, and more flexible than traditional offerings.\\nSignificance of Snowflake\\nSnowflake has an architecture of multi-cluster shared data. The architecture separates their compute and storage layer. This can help them in scaling up automatically as per the demand without impacting their performance. Micro-partitioning is featured in Snowflake architecture that is able to manage both structured and semi-structured data. Hence, they are able to manage Parque, JSON and so on. Within the Snowflake. One of the vital points is that Snowflake can be delivered as a service. Besides, it is extremely easy to use with zero management. After the data migration into Snowflake, everything is taken care of, and there\\'s no requirement to prune, index, etc., allowing the users to focus more on the value in the data.\\n\\xa0\\nWhich is better, BigQuery or Snowflake?\\n\\xa0\\nIf you want to know which is better among BigQuery and Snowflake, let’s know their characteristics.\\nPricing\\nFor computing resources, Snowflake uses a time-based pricing model, in which the users charge for execution time. Whereas, BigQuery uses a model of query-based pricing for computing the resources, in which the users charge for data. BigQuery storage is less expensive than Snowflake storage.\\nArchitecture\\nThe architecture of Snowflake is a shared-nothing database and a hybrid traditional shared-disk architecture. Also, Snowflake uses the central data repository from all the compute nodes for persisted data. Similarly to shared-nothing architectures, Snowflake processes all the queries by using Massively Parallel Processing (MPP) to compute the clusters.\\nPerformance\\nBoth BigQuery and Snowflake perform well under different load levels. One must run the benchmarks by using his own data. Also, they can be used to handle the workloads of many companies with good performance. In terms of raw speed, on an average of 10.74 seconds, the Snowflake edged out BigQuery. It is a known factor that BigQuery clocked at 14.32 seconds per query. As per the benchmarks of independent third-party, the performance of Snowflake is better than the performance of BigQuery.\\nEase of Use\\nBoth BigQuery and Snowflake are user-friendly in nature when it comes to the case ease of use. Snowflake has received a 9.2 rating on the G2 business software review website.\\nScalability\\nOften, Snowflake allows the users to scale their storage and compute resources independently. It includes workload monitoring and automatic performance tuning to improve query times. Whereas, Bigquey handles scalability questions under the hood entirely. BigQuery can automatically provide the compute resources. It makes it very easy to process the data in just a few minutes.\\nSecurity\\nBoth BigQuery and Snowflake use AES encryption on the data to support customer-managed keys. Besides, they depend on the roles to provide access to the resources. Often, Snowflake allows federated user access through SAML 2.0 compliant vendors and Microsoft Active Directory (ADFS). BigQuery also allows federated user access by using Active Directory. Snowflake provides granular permissions for views, schemas, procedures, tables, and other objects.\\nMaintenance and Management\\nBoth BigQuery and Snowflake require low maintenance because automated management is going on in the background. This can imply in Snowflake that queries are optimized and tuned in the background sometimes. Since the platform of Bigquery is serverless, the customers are hardly aware of the considerations.\\n\\xa0\\nConclusion\\n\\xa0\\nI hope you found the detailed insights of BigQuery and Snowflake. Now, you are aware of both and choose to pick as per your needs. Still, you have any queries regarding BigQuery and Snowflake, feel free to comment in the below section.\\n\\xa0\\nBio: Anji Velagana is a B.Tech graduate turned Freelance Content Writer, pursuing a Masters in Journalism & Mass Communication with passion.\\n\\xa0\\nRelated:\\n\\nETL in the Cloud: Transforming Big Data Analytics with Data Warehouse Automation\\nApache Spark on Dataproc vs. Google BigQuery\\nCloud Data Warehouse is The Future of Data Storage',\n",
       " 'comments\\nBy Michael Grogan, Data Science Consultant\\n\\n\\nSource: Photo by\\xa0geralt\\xa0from\\xa0Pixabay\\n\\n\\xa0\\nFor someone who originally comes from an economics background, it might seem quite strange that I would spend some time building models that can predict\\xa0weather patterns.\\nI often questioned it myself — but there is a reason for it.\\xa0Temperature patterns are one of the easiest time series to forecast.\\n\\xa0\\nTime Series Components\\n\\xa0\\nWhen a time series is decomposed — or broken into its individual elements — a series consists of the following components:\\n\\nTrend:\\xa0The general direction of the time series over a significant period of time\\nSeasonality:\\xa0Patterns that frequently repeat themselves in a time series\\nRandom:\\xa0Random fluctuations in a time series\\n\\nWhen one thinks about it — the components of temperature data are very pronounced.\\nIn most parts\\xa0of the Northern Hemisphere at least, the general trend is for temperature to rise as one heads into the summer months, with a decreasing trend towards the winter months.\\nFor instance, here is the mean temperature fluctuations for Dublin Airport, Ireland from 2015–2018, sourced from\\xa0Met Éireann:\\n\\n\\nSource: RStudio\\n\\n\\xa0\\nDecomposing this time series visually reveals the following:\\nTrend\\n\\n\\nSource: RStudio\\n\\n\\xa0\\nSeasonal\\n\\n\\nSource: RStudio\\n\\n\\xa0\\nRandom\\n\\n\\nSource: RStudio\\n\\n\\xa0\\nAs we can see in the graph above, the seasonal patterns show clear evidence of a yearly cycle. For instance, an autocorrelation function reveals a strong correlation in temperature data every 12 months. This makes sense, as it stands to reason that temperatures in January will show the most correlation with recorded January temperatures in other years. Same when comparing temperatures across July, and so on.\\n\\n\\nSource: RStudio\\n\\n\\xa0\\nAn intuitive understanding of these components allows for a better appreciation of how they apply across other time series.\\nFor instance, the seasonal pattern for air passenger numbers (at least before COVID-19) has been a higher incidence of air passengers in the summer months, with lower passengers on the whole for winter. Here is an example of passenger number fluctuations which was generated using data from\\xa0San Francisco Open Data:\\n\\n\\nSource: Jupyter Notebook Output\\n\\n\\xa0\\nDomain knowledge is also important in discerning the components of a time series. For instance, a data scientist who specialises in analysis of the energy markets would intuitively know that commercial electricity consumption tends to follow a weekly, rather than a yearly seasonal pattern. i.e. consumption tends to peak on days of high usage such as Monday, while decreasing significantly over the weekend.\\n\\xa0\\nCan A Time Series Be Forecasted In The First Place?\\n\\xa0\\nAll too often, those who are new to time series analysis will attempt to forecast a series with a lot of inherent randomness present in the data.\\nFor instance, stock prices tend to follow a very stochastic (or random) pattern. These time series are often driven by\\xa0cyclicality\\xa0rather than seasonality, whereby the peaks and troughs in the time series do not occur at specified intervals.\\nAs such, while the overall trend may give a longer-term view of the stock’s direction — it still remains a lot harder to forecast the time series outright, as the patterns in the time series often do not repeat themselves.\\nThat said, all too often — one might attempt to forecast a stock price using a model such as ARIMA without fully taking the time to understand the components of that time series. I’m also guilty of having made this mistake in the past.\\nAdditionally, it is noteworthy that temperature data cannot be influenced by human intervention. However, many time series can (including stock prices), and as such, past data cannot account for these interventions.\\nUsing a separate example, suppose I were to attempt to use\\xa0Community Mobility data from Google\\xa0to try and forecast mobility trends for a major city in six months time.\\nSuch a forecast would make no sense — as it is completely dependent on factors external to the time series itself, i.e. government lockdowns, COVID-19 circulation, etc.\\n\\xa0\\nConclusion\\n\\xa0\\nStrictly speaking, you don’t have to start with weather patterns when predicting a time series. However, you should start with a set of data that is easy to forecast and has predictable trend and seasonality patterns.\\nThe big mistake that people make is in trying to forecast a time series that has a lot of inherent randomness baked in. Not only will you be unable to generate credible forecasts on such data, but terms such as\\xa0autocorrelation function\\xa0will make no intuitive sense to you.\\nPredicting a time series that is not influenced by external factors (temperature patterns being one of the very few) will allow you to better understand\\xa0why\\xa0factors such as autocorrelation, stationarity, and others are of theoretical relevance. Indeed, when it does come time to predict something more complex such as sales data — you will be better equipped to 1) understand the theoretical workings of the model and 2) the advantages and disadvantages of the use of different time series models with respect to the data.\\nDisclaimer: This article is written on an “as is” basis and without warranty. It was written with the intention of providing an overview of data science concepts, and should not be interpreted as professional advice. The findings and interpretations in this article are those of the author and are not endorsed by or affiliated with any third-party mentioned in this article.\\n\\xa0\\nBio: Michael Grogan is a Data Science Consultant. He posesses expertise in time series analysis, statistics, Bayesian modeling, and machine learning with TensorFlow.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nWorking With Time Series Using SQL\\nWhy Automated Feature Selection Has Its Risks\\nRejection Sampling with Python',\n",
       " 'By Jesus Rodriguez, Intotheblock.\\ncomments\\n\\n\\nSource:\\xa0https://smilegate.ai/en/2020/12/07/facebook-rebel/\\n\\n\\xa0\\n\\nI recently started a new newsletter focus on AI education. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:\\n\\n\\n\\n\\n\\xa0\\nPoker has been considered by many the core inspiration for the formalization of game theory. John von\\xa0Neuman was reportedly an avid poker fan and use many analogies of the card game while creating the foundation of game-theory. With the advent of artificial intelligence(AI) there have been many attempts to master different forms of poker, most of them with very limited results. Last year, researchers from Facebook and Carnegie Mellon University astonishing the AI world by unveiling\\xa0Pluribus, an AI agent that beat elite human professional players in the most popular and widely played poker format in the world: six-player no-limit Texas Hold’em poker. Since then, a question that has hunted AI researchers is whether the skills acquired by models like Pluribus can be used in other imperfect information games. A few days ago, Facebook again used poker as the inspiration for\\xa0Recursive Belief-based Learning (ReBeL), a reinforcement learning model that is able to master several imperfect-information games.\\nThe inspiration from ReBeL comes from DeepMind’s AlphaZero. After setting up new records in the Go game with the development of AlphaGo, DeepMind expanded iits efforts to other perfect-information games such as Chess, or Shogi. The result was AlphaZero, a reinforcement agent that was able to master all these games from scratch. Of course, recreating the magic of AlphaZero in imperfect-information games like poker entails a different level of complexity.\\nGames like poker in which players keep their cards secret represent a major obstacle for reinforcement learning + search algorithms. Most of these techniques assume that each player’s action has a fixed value regardless of the probability of that action being executed. For instance, in chess, a good move is good regardless of whether is played or not. Now let’s think about a game like poker in which the players bluff all the time. In many scenarios, the value of a bluff action diminishes the more its used as the opponents can adjust their strategy to it. How could we possibly leverage reinforcement learning + search methods across many imperfect-information games.\\n\\xa0\\nEnter ReBeL\\n\\xa0\\nThe idea behind ReBeL is SO SIMPLE as it is clever. If AlphaZero showed success with reinforcement learning + search strategies in perfect-information games, then why not to transform imperfect-information games to perfect-information equivalents? I know, I know, it sounds too good to be true but let’s look at an example.\\nLet’s imagine a simplified version of poker in which a single card is dealt to each player which can then choose between three actions: fold, call or raise. Now consider a variation of this game in which the cards are not dealt to the players directly but, instead, they can be seen only by an third-party referee. Instead of taking an action directly, the players will announce how likely they are to take a specific action given the current hand. The referee will take an action based on the player’s analysis. In terms of strategy, this modified game is identical to the original game with the difference that it contains no private information. Instead, the modified game can be considered a continuous-state perfect-information game.\\n\\n\\nSource:\\xa0https://ai.facebook.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/\\n\\n\\xa0\\nThe transformation of an imperfect-information game to a perfect-information environment, opens the door to utilizing the same techniques that worked for AlphaZero. The main challenge at this point is efficiency, as the search space is fairly larger than most perfect-information games. To address this, ReBeL uses an optimization technique known as counterfactual regret minimization (CFR) to improve the efficiency of the search.\\nFacebook evaluated ReBeL in two games: heads-up no-limit Texas Hold’em and Liar’s Dice with very strong performance in both.\\nReBeL represents an important milestone in order to use reinforcement learning + search in order to generically solve imperfect-information games. There are still plenty of challenges including the fact that it knows the rules of the game in advance which is not the case of many real world scenarios. Facebook also open sourced the implementation of the Liar’s Dice game to allow the research community to improve in these ideas.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nFacebook Open Sourced New Frameworks to Advance Deep Learning Research\\nMicrosoft and Google Open Sourced These Frameworks Based on Their Work Scaling Deep Learning Training\\nRemembering Pluribus: The Techniques that Facebook Used to Master World’s Most Difficult Poker Game',\n",
       " \"comments\\nBy Craig Smith, Eye on AI\\n\\nEditor's note: This is a transcript of a conversation between Craig Smith and Geoff Hinton, from an episode of the Eye on AI podcast. You can also find a video version of the interview, as well as an audio version, including a scrolling transcript with speed controls.\\n\\n\\xa0\\nCRAIG: Hi, I'm Craig Smith, and this is Eye on AI.\\nThis week I speak to Geoff Hinton, who has lived at the outer reaches of machine learning research since an aborted attempt at a carpentry career a half century ago. After that brief dogleg, he came back into line with his illustrious ancestors, George Boole, the father of Boolean logic and George Everest, British surveyor general of India and eponym of the world's tallest mountain. Geoff is one of the pioneers of deep learning and shared the 2018 Turing award with colleagues, Yoshua Bengio, and Yann LeCun. A year earlier, he had introduced capsule networks, an alternative to convolutional neural networks that take into account the pose of objects in a 3D world, solving the problem in computer vision, in which elements of an object change their position when viewed from different angles.\\nHe has been largely silent since then, and I'm delighted to have him on the podcast.\\nWe began like so many of us do today trying to get the teleconferencing system to work. I hope you find the conversation as engrossing as I did.\\nCRAIG: I don't think I need to introduce you or that you need to introduce yourself. I do want to sort of recap what's gone on in the last year. It's been quite a year. Capsule networks had sort of faded from view, at least from the layman's point of view, and resurfaced at NeurIPS last December with your introduction of stacked capsule autoencoders.\\nThen, in February at the AAAI conference, you talked about capsule networks as key to unsupervised learning. And in April you revived the idea of backpropagation as a learning function in the brain with the introduction of neural gradient representation by activity differences or NGRADs,\\nGEOFF: I think it would be better if we started with capsules and we do three different topics.\\nWe do capsules.\\nCRAIG: Okay.\\nGEOFF: Then we do SimCLR. And then we do the NGRAD stuff.\\nCRAIG: Okay. Can you talk about your new capsule idea? Not new - a year old or longer now - but how that has influenced your research?\\nGEOFF: Okay. So, several things have changed and more things are changing right now.\\nSo originally capsules, we used supervised learning and we thought it would be easy to get things working like that, even though I don't really believe in supervised learning. And last year we switched to unsupervised learning and we also switched to using set transformers.\\nSo what capsules are trying to do is recognize whole objects by recognizing their parts and the relationships between the parts.\\nSo, if you see something that might be an eye, and you see something that might be a nose, the possible eye could say where the face should be and the possible nose could say where the face should be. And if they agree on where the face should be, then you say, 'Hey, they're in the right relation to make a face, so we'll instantiate a face. We'll activate the face capsule.'\\nSo, there's various problems with that. One is the issue of whether you try and train it supervised or unsupervised, and it's going to be much better to use unsupervised because then you don't need labels. But the other problem, which we overcame with stacked capsule autoencoders, is that if you see say a circle in a line drawing, you don't know whether it's a left eye or a right eye or the front wheel of a car or the back wheel of a car.\\nAnd so, it has to vote for all sorts of objects it might be a part of, and so, you know, if it's the back wheel of a car, it knows roughly where the car should be and it can vote for, 'there should be a car there.' But of course, it might not be that. It might be a doorknob or it might be a left eye. And so, it makes lots and lots of votes.\\nAnd now what happens is every higher-level capsule gets a huge cloud of votes, nearly all of which are wrong. But one way to try and rectify that is to say, well, if any other capsule likes the vote, if any other capsule can make use of that vote, to make be part of this object, then route the vote there and don't route it to me.\\nAnd so that was the idea of dynamic routing; that you try and get all the bad votes to go to the places where they're good votes.\\nThat's complicated to make it work. The alternative which we use in stacked capsule autoencoders s is to say, if you discover parts, suppose you discovered a circle and a triangle and a rectangle, you don't really know what they're parts of. There's many, many things they could be parts of. So, what you want to do is have them interact with each other a bit, and use the spatial relations between them to allow each part to become more confident by what kind of part it is. So, if you're a circle and there's a triangle in the right relative position to be a nose, if you're a left eye, then you get more confident than you’re a left eye. And that's what transformers are very good at.\\nTransformers have a representation of, in the case of language, a word fragment. So, it might be the fragment 'may,' which happens to be a whole word. And they don't know whether that's a modal, like would and should, or whether it's a month, like June and July. And so, what they do is, the representation of that fragment interacts with the representations of other fragments. And if there's another fragment in the sentence, for example, June, then the representation for May gets more month-like. Whereas if there's another fragment that's, would or should, it gets more modal like. And after a few layers of that, the fragments have been disambiguated. That is, you know, much better what each fragment is meant to be.\\nSo, in language, that means you have a contextually sensitive representation of the word which s disambiguated between different meanings. In vision, if you have something like a circle, you'd like to know whether that circle is an eye, or the wheel of a car. And you can do that without yet creating a face or a car, by this interaction between parts. And in stacked capsule autoencoders, that's what we do.\\nWe take the first level of parts and they all interact with each other. So, they become more confident about what kind of a part they are. Well, once they are more confident about what kind of a part they are, then they vote for what whole they might be a part of. And that way they can have far more specific, confident votes.\\nSo, they don't make lots of crazy votes. Once you become convinced that a circle is probably a left eye, it doesn't vote for being the back wheel of a car. That means you've got far fewer votes to deal with it and so it's far easier to find the clusters. We made it so instead of trying to learn by supervision, by giving it labels, it learned to create a whole that was good at reconstructing the parts.\\nAnd so that's unsupervised learning.\\nCRAIG: At some point you need to connect it to language.\\nGEOFF: Yeah. So, all the learning in stacked capsule autoencoders, almost all the learning, is unsupervised. That is, you have these parts, and you recognize these parts, which are sort of templates that occur a lot – it’s a little bit more complicated than that, but - and then you recognize wholes that are combinations of these parts.\\nAnd the objective function is to find wholes that are good at reconstructing the parts, in particular, find wholes so if I tell you the pose of the whole thing, you can tell me the pose of the part. Like if I tell you there's a small face at 45 degrees in the bottom right-hand corner of the image, you can tell me that there should be a nose of 45 degrees that's even smaller in the bottom right hand corner of the image. So, the whole can predict the parts. And I didn't need any labels for that.\\nCRAIG: Right.\\nGEOFF: Now once you've done that, once you've got these wholes, you can then learn what they're called. So then supervised learning consists of taking wholes and learning their names. But you're not learning to recognize them when you're doing that, you're just learning what things you can already recognize are called, much like a little child learns to recognize cows and sheep. It doesn't know that cows are called cows and sheep are called sheep, and that's why it needs its mother to tell it. But its mother is not the one who tells it how to tell the difference between a cow and a sheep.\\nCRAIG: Yeah. Would this kind of unsupervised learning, in a larger system, also be able to make assumptions or inferences about relationships between objects or, or the laws of physics, for example?\\nGEOFF: Those are two somewhat different questions.\\nCRAIG: Yeah.\\nGEOFF: In the long run, we'd like it to do that, but let's return to the laws of physics later on when we talk about SimCLR.\\nOkay. for now, he recognizes objects by seeing parts in the correct relationships. And you recognize scenes by seeing objects in the correct relationships. In a scene the relationships between objects are typically somewhat looser, but yes, it can do that. It can recognize that objects are related in the right way to make a particular kind of a scene.\\nCRAIG: SimCLR [Simple framework for Contrastive Learning of visual Representations] came up later in the year. Can you talk about SimCLR and how that relates?\\nGEOFF: So that's a different learning algorithm. That's different in many ways. It's not, for example, focusing on the problem of dealing with viewpoint equivariance, that is, as the viewpoint changes, you get a representation that changes so that you can cope with viewpoint easily.\\nThat's not the primary goal of SimCLR. What SimCLR is doing is saying, 'I want to learn to represent a patch of an image in such a way that other patches of the same image have similar representations.' So, what you do is you take a crop of an image and then you take another crop of the same image.\\nAnd you say, we're going to have a neural net that converts those crops into a vector representation, pattern of neural activities. And we want those patterns to be similar if the crops came from the same image and different, if they came from different images. If you just say, make them similar, that's easy.\\nYou just make all of the vectors be identical. The trick is you have to make them similar if they came from the same image and different if they came from different images. And so that's called contrastive learning. And Ting Chen in the Google lab in Toronto, with some help from others of us, made that work extremely well.\\nHe wasn't the originator of the idea. In fact, the first comes from work I did with Sue Becker in 1993 or 92, and then later work I did in 2002. But we never really made it work well for images and other people revived the idea in 2018 and got contrastive learning working for crops of images. And then Ting Chen made it work considerably better, and that made people sit up. And so, what happens is once you've got this representation of a patch of an image, or, this neural net that can convert a patch from an image into a representation, such that you get similar representations with two patches coming from the same image, then you can use those representations to try and recognize what the objects are in the image. And that stage is supervised learning, but that doesn't require a deep net.\\nSo, the idea is you do unsupervised learning by using this deep net to try and get the same representation or very similar representations for two different patches of the same image. And different representations for patches of different images. After you've used the deep net to do that, so Ting uses a ResNet, which is a [inaudible] kind of deep net, after you've done that, you then just directly learn to turn those representations with no extra hidden layers into class labels. So that's called a linear classifier.\\nIt doesn't have hidden layers in it. And he does remarkably well. So, a linear classifier based on those representations that we've got by pure unsupervised learning with no knowledge of the labels can do as well now on ImageNet as a supervised method, provided for the unsupervised learning, we use a bigger ResNet.\\nIf you use a standard sized ResNet on ImageNet, you get a certain error rate and we can get pretty much the same error rate by using a bigger ResNet training it entirely unsupervised with no knowledge of labels. And then on top of the representations we extract, training a linear classifier.\\nCRAIG: And in that training, in one of the things I read, you talked about using augmented data.\\nGEOFF: Yes. It's very important when you do this. You can think of the two different crops as different ways of getting representations of the same image, but it's perhaps the major thing you do, but you also have to do things like mess with the color balance. So, for example, if I give you two different crops from the same image you can often recognize that they're from the same image by looking at the relative distribution of red, green, and blue - the color histogram.\\nAnd we don't want it doing that. So, to stop it cheating like that, you take two different crops of the same image and on one of the crops you change the color balance. And now it can't recognize that they're the same just by using the color distribution. And those are the two most important ones, doing different crops and changing the color balance.\\nCRAIG: Yeah. Is that augmentation something that the data scientist does in the data prep? It's not part of the model, the model doesn't automatically augment the data.\\nGEOFF: Well, it's not part of the data prep really, as you're training on the data, you'll, you'll get an image, you'll take two different crops of the image, and then you will augment those crops. You'll change the color balance.\\nCRAIG: Right.\\nGEOFF: So, you can't really think of it as modifying the data so much as given an image, you then get these crops with modified color balance, and you can modify all sorts of other things like orientation and stuff like that.\\nCRAIG: And that sounds very similar, from a layman's point of view, to what Yann LeCun is doing with video, where, where he, he, he takes a video and, and tries to predict what the next frame will be, in an unsupervised manner.\\nAm I wrong in that?\\nGEOFF: Well, it's not the same as trying to predict the next frame of a video. It Is the same, however, as trying to extract your representation from the next frame, that's easily predicted by the representation you extracted from the current frame. So that's contrastive learning. You can do contrastive learning for videos.\\nAnd you can say, you're really asking the question, ‘Did these two frames come from the same video?’ And that's a bit like asking, ‘did these two crops come from the same image,’ and you can use the same contrastive learning techniques for that.\\nCRAIG: Yeah. And then at AAAI, when you were talking on the stage with Yann and Yoshua Bengio, you talked about capsule networks as a form of unsupervised learning that has promise going forward. This SimCLR is another method. Are they related or can they be blended in making unsupervised methods more powerful?\\nGEOFF: They're somewhat different approaches at present. You could clearly try and combine them.\\nWe're not doing that at present. Yeah.\\n\\n\\xa0\\nCRAIG: In Nature. there was a paper, I believe it was in Nature, about sort of reviving the idea of back propagation as a function of learning in the brain. And you introduced this idea of, of NGRADs, neural gradient representation by activity differences. Can you talk about that?\\nGEOFF: Neuroscientists have been very skeptical about whether the brain can do anything like back propagation.\\nWell, one of the big problems has been, how does the brain communicate gradients? Because in back propagation, you need to change your weight in proportion to the gradient of the error with respect to that weight, whatever your error function is. And the idea is that you represent an error by the rate of change in neural activity.\\nAnd that's nice because it can have both signs, that is, neural activity can be going up or it can be going down, so you can represent both signs of error. And it also implies that the learning rule, which uses a gradient, is going to be something called spike timing dependent plasticity. That is when you change your synapse strength, you're going to change it in proportion to the error derivative.\\nAnd that means you're going to want to change it in proportion to the rate of change of the post synaptic activity. It is going to be the presynaptic activity times the rate of change of the post synaptic activity. And that's called spike timing dependent plasticity, which they found in the brain. And in fact, I've been suggesting for a long time that we use activity differences.\\nI had a paper with James McClelland in 1987, suggesting that temporal differences of activity be used as error derivatives. And that was actually before spike timing dependent plasticity had been discovered. By 2005, I got interested in activity differences again. And much more recently people have managed to make that work quite well.\\nI'm still somewhat skeptical. I think the brain could do back prop if it wanted to that way. It's a little clumsy and I'm now skeptical because I think back prop is too good an algorithm for the brain. So, the brain is actually dealing with a very different problem from what most neural nets are dealing with.\\nMost of the neural nets want to get a lot of knowledge represented in a modest number of parameters, like only a billion parameters, for example. For brain, that's a tiny number of parameters. That's the number of parameters you're having a cubic millimeter of brain, roughly. So, we have trillions and trillions of parameters.\\nBut, we don't have many training examples. We only live for like a billion seconds or 2 billion seconds.\\nAnd so, we, we don't get much experience and we've got a huge number of parameters and neural nets mostly were in the other regime. They get lots of training and they don't have many parameters. Now, if you've got lots and lots of parameters and not much training data, what you want to do is somewhat different from backpropagation, I think.\\nSo, I got very interested in the idea that there is one way of making this activity difference method work nicely; of trying to generate agreement between a top down representation and a bottom up representation. So, the idea is, you have, say, some hierarchy of parts. You look at an image, you instantiate parts at different levels.\\nAnd then from the high-level parts, you top down predict the low-level parts. And what you'd like to see is agreement between the top-down prediction, which depends on a larger context, and the bottom up extraction of a part, which depends on a smaller context. So, from some local region of the image you extract a part; from many of those parts, you predict a whole; from the whole, you now, top-down predict the individual parts. But those predictions of the parts have used more information because they're based on the whole, it got to see more.\\nAnd what you want is, agreement between the top-down prediction and the bottom up extraction of part representation. And, you want it to be significant agreement, so what you really want is on the same image, they agree, but on different images they disagree. So, if you take the parts from one image and the top-down predictions or another image, they should disagree.\\nAnd that's contrastive learning as in SimCLR. But it also suggests a learning algorithm for the brain that is somewhat different from back prop. And I got very excited. It's not quite as efficient as back prop, but it's much easier to put into a brain because you don't need to go backwards through many layers.\\nYou just need to compare a top-down prediction with a bottom up prediction. I call it back relaxation. And, over many times steps, it will get information backwards, but it won't get information backwards in one trial. And back propagation sends information all the way backwards through a multi-layer net on a single presentation of an image and back relaxation just gets it back one layer each time, and it needs multiple presentations of the same image to get it back all the way.\\nSo, I got really interested in back relaxation and whether that might explain how the brain was doing this learning of multi-layer nets. But then I discovered that sort of pure greedy bottom up learning did just about as well. I hadn't done the controls carefully enough. The bottom up algorithm that I introduced in 2006 actually worked as well as this back relaxation.\\nAnd that was a huge disappointment to me. I still want to go back and see if I can make back relaxation work better than greedy bottom up.\\nCRAIG: I see, and thus the June tweet.\\nGEOFF: Yeah, that's when I discovered that back relaxation doesn't work any better than greedy bottom up learning.\\nCRAIG: Is the assumption that, that the brain is so efficient that even if greedy bottom up can do it on its own that there wouldn't be this top-down function, or is it possible that that top down function exists as a, as a, an optimizer or something?\\nGEOFF: Well, you'd like this top-down prediction - and making it agree with the bottom up extraction - you'd like that to be better than just training a stack of autoencoders, one layer at a time. Otherwise it's not worth doing and training a stack of autoencoders, one hidden layer at a time, turns out to be pretty good.\\nAnd what's happened recently in these big neural nets is, deep learning really got going in about 2006 when we discovered that if you train stacks of autoencoders or restricted Boltzmann machines, one, one hidden layer at a time, and then you fine tune it, it works very well.\\nAnd that got neural nets going again. People then did things like speech. And vision on ImageNet, where, they said you don't need the pre-training. You don't need to train these stacks of autoencoders. You can just train the whole thing supervised.\\nAnd that was fine for a while. But then when they got even bigger data sets and even bigger networks, people have gone back to this unsupervised pre-training. So that's what Bert is doing. Bert is unsupervised pre-training. And GPT-3 uses unsupervised pre-training. And that is important now. So, there was this on again, off again, where there was supervised learning and then I introduced unsupervised pre-training and then people said, 'Oh, but we don't need that. We just use supervised learning.' But now they're back to saying, 'Oh, but we do need some unsupervised learning.'\\nCRAIG: Right.\\nGEOFF: But the unsupervised learning algorithms are now getting more sophisticated.\\nCRAIG: Yeah. and again, the, the SimCLR is, at least as it relates to computer vision, is one method. The stacked capsule autoencoders is another method, and there may be others still. The learning and the brain, you know, I had a long conversation, about a year ago with Rich Sutton about temporal difference learning. And there is a view that, that that algorithm is, describes what's happening in lower brain function. and what you're talking about is cortex learning, and, it, at what point do they - are they completely different systems?\\nGEOFF: Yes. The big successes of computational neuroscience have been taking the work that Rich Sutton and others did on temporal differences and relating it to experimental studies on the brain and dopamine. Peter Dayan in particular, was very important in showing the relationship between this theoretical learning algorithm and what's actually going on in the brain. But that's for reinforcement learning.\\nAnd I think reinforcement learning is kind of the icing on the cake. Most of the learning is going to be unsupervised learning. You have to learn how the world works and you don't want to learn how the world works by using reinforcement signals. You don't want to learn to do vision by stubbing your toe all the time. You want to learn to do vision some other way.\\nCRAIG: Yeah. This is giving you further insight into, into learning in the brain. I remember that that was really your initial impulse in getting involved in all this study.\\nGEOFF: Yeah. My main goal in life has been to understand how the brain works, and all of this technology that's come out of attempts to understand how the brain works, aren't really how the brain works. It's useful spinoff. But it's not what I was really after.\\nCRAIG: Is that all part of one general stream that you're pursuing that's headed to a particular goal?\\nGEOFF:  It's like this. if your research has been around for a while, you have a number of, kind of, deep intuitions about how things should be, and then you have particular projects that are like particular instances that combine those intuitions. And often projects that seem quite separate, eventually merge. But for now, the work on capsules is somewhat different.\\nAlthough all three of them could merge together. That is, if we, if we can get the idea of top-down predictions and bottom up predictions agreeing in a contrastive sense, that is, they agree well for the same image and they're very different for different images, that will fit in with stacked capsule autoencoders.\\nBut it will also, it's also an example of contrastive learning as in SimCLR. It may also explain how the brain can learn multi-layer nets. So obviously I would like to, I'd like to have one solution to everything. That's what everybody always wants. it's just, you have to be more realistic and get parts of this. You can't get the whole thing all at once.\\nCRAIG: Yeah. With the rise of transformers in models like GPT-3 and now, in capsule networks, which is primarily computer vision, there's kind of a convergence between computer vision and natural language processing. How do you see that convergence progressing? And, and those are the two principle components of consciousness, if I'm not wrong. So are we working towards, a model that can perceive the world, an AI model that can perceive the world that's closer to a human perception in that it blends...\\nGEOFF: One of the big motivations of capsule s was that it would be, it would have representations more like the representations we use.\\nSo, a classic example is, if you see a square rotated through 45 degrees, you have two completely different ways of perceiving that one is as a tilted square and the other is there's an upright diamond. And what you know about it is totally different, depending on which representation you use. Now, convolutional nets don't have two different representations of that.\\nThey just have one representation of that. To get two different representations, you need something that imposes a frame of reference. And a very strong feature of our perception is that we impose frames of reference on things and understand them relative to those imposed frames. And if you get someone to impose a different frame, they'll understand things quite differently.\\nThat was one of the big motivations for capsules. It's also one for computer graphics. So, in computer graphics, you represent a house with a particular coordinate frame. And then relative to that coordinate frame, you know, where the windows and the door are.\\nAgain, that's the kind of representation we need to get into neural nets, if neural nets are going to get more like us at representing objects. At present deep neural nets are very good at doing classification where they do it a completely different way from people. So, they're relying much more on things like texture.\\nAnd they can see all sorts of complex textures that we aren't sensitive to. And that's why you get these adversarial examples where two things look totally different to us, but very similar to a neural net and vice versa.\\nCRAIG: Google has just filed for a patent on capsule networks. Is that, because of the successes of stacked capsule autoencoders?\\nGEOFF: You know, I don't know all the motivations for filing the patent, but I think the main motivation, which is true for most of the patent filings Google does, is protective in the sense they don't want other people to sue them for using stuff they developed. And so, Google really, isn't interested in making its money out of patents. It's interesting in make is making its money out of having great products.\\nAnd it doesn't want to be prevented from using its own research and its great products. And the patent laws have changed in such a way that the first to file - it's not the first to invent, it's the first person to file it. And so, you have to file patents, just protectively.\\nCRAIG: There's this paper, that that's, under review right now, transformers for image recognition at scale.\\nDoes that relate at all to this use of transformers in capsule networks?\\nGEOFF: Yes, it does a bit. So, what it is showing is that the kind of interactions between parts that work so well in things like Bert, for words, where you're getting word fragments to interact, also works when you're getting representations of patches of images to interact.\\nAnd it's also what's happening in stacked capsule autoencoders, where we have a set transformer that's getting the representations of parts to interact with one another and become refined. But then in stacked capsule autoencoders, we then jump to high level representations and we're doing it all unsupervised.\\nWhereas in the paper with the 16 by 16 patches, they're training it supervised to perform classification. They're not training it unsupervised. So somewhat different. But this general trend of, extract some pieces and then get them to interact so you get clearer about what the pieces are, which is what transformers do, that seems to be a very good way to go about doing, building layers of representation.\\nCRAIG: Yeah. I'm going to ask a question that I'll probably cut out because it's going to sound. Ignorant, but in, in transformers, both, in capsule networks, or in natural language processing models like Bert or GPT-3, it relies on, massive, parameters, right? Billions of parameters.\\nGEOFF: Actually, less parameters than convolutional neural nets, but...\\nCRAIG: Okay. but, but it, it goes out into the world of the internet in this case and, and ingests all of this. And then it looks to me like it's a kind of search it's going out and, and, and finding, something that matches a representation.\\nAnd then fills it in with, with what's already out there. Is that wrong?\\nGEOFF: That's wrong. Yeah. You can go and find the closest thing. I mean, if you give it a story to complete, you can find the closest match on the web. And it'll do completions that are nothing like the closest match on the web. Basically, it's taking all this information in this data it's observed and it's boiling it down into these parameters that allow it to produce similar stuff, but not by matching to particular instances it's seen already.\\nCRAIG: And in the same way, capsule networks are creating a new representation\\nGEOFF: Yeah, and capsules networks should be able to deal with a new view of the same object.\\nCRAIG: Yeah. So where is your research going now? I mean, on these three streams or are there other streams?\\nGEOFF: My main interest always been unsupervised learning because I think that's what most human learning is. I'm interested in developing capsules further and in things like SimCLR,\\nI'm also interested in making distillation work better. So, the idea of distillation is you have a great big model and you've trained it on data and it's extracted the regular patterns in the data and got them into its parameters.\\nAnd now you want to train a much smaller model that will be as good as the big model, almost as good as a big model, but you couldn't have trained directly on the data. And so, we see this all over the place. So, insects are like this. The way insects, roughly speaking, the way most insects work is they have one stage that's just about extracting nutrients from the environment and that's called the larva.\\nOkay. And it's just an eating machine. And this great fat, ugly grub, or a caterpillar for a butterfly, just gets fat. That's its role in life. And then it basically gets turned into a soup. And out of that soup, you build the adult, which may look nothing like the larva. I mean, a caterpillar and a butterfly are very different things.\\nAnd they're optimized for different things. So, the larva is optimized for sucking nutrients out of the environment. And then the butterfly is optimized for traveling around and mating. And those are very different activities from sucking nutrients out of the environment. Now butterflies also get nutrients out of the environment, but they're not machines for doing that like larva. You also see it in mining - a nice Canadian example. So, if you want gold, first you take a chunk of the earth, then you convert it to pay dirt. and you have one way of doing that. And then you take the pay dirt and you heat it up very hot to try and get the gold out. I think that's how it works.\\nAnd the same for data mining. So, you've got a big set of data. And you you'd like to end up with a small agile model that can look at a new example and tell you what class it is, for example, but the kinds of models that are good at sucking structure out of the data are not necessarily the same as the models that are going to be small and agile and easy to use on your cell phone for making the right decisions.\\nAnd so, the idea is you use one kind of model for sucking structure to the data, a great big model. Once you suck the structure of the data, you get the great big model to train a small model. And it turns out the big model is much better at training a small model than just the raw data. And it's like an apprenticeship. Or like the way science works. Once scientists have done their research and figured out how things work, they can then teach school kids how things work. So, sort of any smart school kid can learn Newton's mechanics. But not any smart schoolkid could have invented Newton's mechanics. Once Newton invented it, which is kind of tricky, you can then explain it quite well. And you can, you can instill a model of it in a school kid. And so, the idea of distillation is we use great big neural networks for getting structure out of the data and then much smaller, more agile networks for actually using what we discovered. And it's now being quite widely used.\\nIt's used in Bert, for example, to get more agile networks. But I think there's probably ways of making it much better. And that's another thing I'm working on.\\nCRAIG: Yeah. on, capsule networks in the, in the, AAAI talk. You all agreed, you and Yann LeCun and Yoshua Bengio, on the, on the, on the direction of your research in unsupervised learning and a lot of what Yann LeCun has been doing with video sounds similar to what you talk about both with capsule networks, and SimCLR.\\nGEOFF: Yann and I share a lot of intuitions. We, we, we worked together for a while. We have a very similar view of the world.\\nCRAIG: So, can you talk about how, again, at some point all of these ideas will converge? How his research relates to your research, particularly his research on video?\\nGEOFF: Our goals are very similar and the methods are quite similar.\\nAnd as we start applying SimCLR-like methods to video, they're going to get even more similar. So, this idea of contrastive representation learning seems to be very powerful and Yann's exploiting it. Ting Chen made it work really well for static images. And we're now trying to extend that to video, but we're trying to extend it using attention, which is going to be very important for video because you can't possibly process everything in a video at a high resolution.\\nCRAIG: Yeah. Yeah. And, and that's interesting, when, when you relate, this, machine learning to learning in the brain and certainly attention is, is, critical. Yeah. and can you talk a little bit about how, these models, even if they're not using the algorithms that you think are operating in the brain, how they are analogous to, to human learning. I mean, there's this huge amount of unsupervised learning that goes on with, as you were saying with capsules that at the end, there's a little bit of supervised learning that that puts labels to representation.\\nGEOFF: Let me just clarify that the first few versions of capsules we did were all using supervised learning because we thought that would make life easier, even though that's not really what we believed in, but now we're doing unsupervised learning and it works better.\\nAnd it's, it's ideologically far more satisfactory.\\nCRAIG: Yeah. but in, in the unsupervised capsule networks, at the, at the end, you connected it to language through ...\\nGEOFF: That's just to show that it's learned something sensible. I mean, obviously you want to connect to language. There's very nice work going on at Google now in robotics, where they're using deep learning for getting robot arms to do things, to manipulate things. But they're also interfacing it with language. So, you can tell a robot what to do, and the robot can also tell you what it's doing. And that seems very important. It also seems that if the robot can tell you what it's doing, like, you know, it's opening the drawer.\\nThe objections of people like Gary Marcus have to natural language processing, saying it doesn't really understand what's going on. But you know, if it says I'm opening the drawer and I'm taking out a block and he opens his drawer and takes out a block, it's very hard to say it doesn't understand what's going on.\\nCRAIG: we mentioned at the beginning the laws of physics, learning the laws of physics, and you don't need language to learn the laws of physics. You do need a linguistic interface, to, to look at a tree and a car and be able to identify them as a tree in a car.\\nCan you talk about learning, something like the laws of physics that doesn't require language to be attached to it, but nonetheless, there's learning that takes place?\\nGEOFF: Yeah. At high school you may learn the laws of physics. We learn sort of common-sense physics. So, we learn, you know, if you throw something up it comes down again and if we're good, we learn how to throw a basketball so it goes through the hoop. And that's a very impressive skill cause you're throwing it from like 20 feet away and you have to get it right to a few inches. That's an amazing thing to be able to do.\\nAnd we, we don't learn that by being told how to do it. We don't run that using language at all. We learn it from people who say trial and error, but we're understanding how the world works just by observing the world. also, by acting in the world. So just passively observing the world will allow you to understand it, but it's not nearly as good as acting in the world.\\nAnd in fact, if you think about perception for robots or wander around an act in the world, it changes your view of high perception should work. So, if you're just taking images or videos and just passively processing them, it doesn't make you think about attention. But as soon as you have a robot, that's moving around in the world, it's got to decide what to look at.\\nAnd the sort of primary question in vision is where should I look next? And that's been sort of widely ignored by people that just process static images. Attention is crucial when it's sort of central to how human vision works.\\nCRAIG: Can you sum up a little bit, everyone likes to hear about convergence of all of these things, you know, convergence of, of computer vision with, natural language processing, convergence of unsupervised learning wit. supervised learning and reinforcement learning. is, is that beyond what, what you're really focused on? because you're focused on, on, the basic research, not necessarily, building models that pulls it all together.\\nGEOFF: Let me just say something about supervised learning versus unsupervised learning, because, it sounds like a very simple distinction. but actually, it's very confusing. So, if you ask. When, when a kid's mother says that's a cow, we tend to think of it in machine learning as the mother supplied a label.\\nBut what's really happening is this. The child has some sensory input and the child is getting a correlation between the visual sensory input and the auditory sensory input. Now the top level, the auditory thing gives you the word cow and the visual thing gives you whatever your visual is and you learn they go together. But actually, supervision when you actually get it in reality, it's just another correlation and there's, so it's all about complex correlations in the sensory input, called supervised and unsupervised learning.\\nAnd then there's correlations with payoffs. And that's reinforcement learning, but I think the correlations with payoffs don't have enough structure in them for you to do most of the learning. So most of the learning's unsupervised.\\nCRAIG: Okay, well, let's, let's leave it there. I really appreciate it. And, it's been a fascinating conversation and I'll edit it down to be a coherent on my side.\\nGEOFF: Bye for now.\\nCRAIG: Yeah, bye-bye. That's it for this week's podcast. I want to thank Geoff for his time. If you want to learn more about the episode today, you can find a scrolling transcript on our website, www.eye-on.ai.\\n\\xa0\\nRelated:\\n\\n12 Deep Learning Researchers and Leaders\\nSome Musings on Capsule Networks and DLPaper2Code\\n2020: A Year Full of Amazing AI Papers — A Review\",\n",
       " \"comments\\nBy Moez Ali, Founder & Author of PyCaret\\n\\n\\nPhoto by Ben White on Unsplash\\n\\n\\xa0\\nPyCaret\\n\\xa0\\nPyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is an end-to-end machine learning and model management tool that speeds up the machine learning experiment cycle and makes you more productive.\\nIn comparison with the other open-source machine learning libraries, PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few words only. This makes experiments exponentially fast and efficient.\\nOfficial:\\xa0https://www.pycaret.org\\nDocs:\\xa0https://pycaret.readthedocs.io/en/latest/\\nGit:\\xa0https://www.github.com/pycaret/pycaret\\n\\xa0\\n👉 compare_models does more than what you think\\n\\xa0\\nWhen we had released\\xa0version 1.0 of PyCaret in Apr 2020,\\xa0compare_models\\xa0function was comparing all the models in the library to return the averaged cross-validated performance metrics. Based on which you would use\\xa0create_model\\xa0to train the best performing model and get the trained model output that you can use for predictions.\\nThis behavior was later changed in version 2.0.\\xa0compare_models\\xa0now returns the best model based on the\\xa0n_select\\xa0parameter which by default is set to 1 which means that it will return the best model (by default).\\n\\n\\ncompare_models(n_select = 1)\\n\\n\\xa0\\nBy changing the default\\xa0n_select\\xa0parameter to 3, you can get a list of top 3 models. For example:\\n\\n\\ncompare_models(n_select = 3)\\n\\n\\xa0\\nThe returned objects are trained models, you really don’t need to call\\xa0create_model\\xa0again to train them. You can use these models to generate diagnostic plots or to even use them for predictions, if you would like. For example:\\n\\n\\npredict_model function\\n\\n\\xa0\\n👉You think you are limited to scikit-learn models only\\n\\xa0\\nWe recieve a lot of requests to include non\\xa0scikit-learn\\xa0models in the model library. Many people don’t realize that you are not limited to the default models only.\\xa0create_model\\xa0function also accepts untrained model object in addition to the ID’s of models available in the model library. As long as your object is compatible with\\xa0scikit-learn\\xa0fit/predict API, it will work just fine. For example, here we have trained and evaluated\\xa0NGBClassifier\\xa0from\\xa0ngboost\\xa0library by simply importing untrained NGBClassifier:\\n\\n\\ncreate_model with external models\\n\\n\\xa0\\nYou can also pass the untrained models in the\\xa0include\\xa0parameter of the\\xa0compare_models\\xa0and it will just work normally.\\n\\n\\ncompare_models with untrained object\\n\\n\\xa0\\nNotice that include parameters include ID’s for three untrained model from the model library i.e. Logistic Regression, Decision Tree and K Neighbors and one untrained object from ngboost library. Also, notice that the index represents the position of the model entered in the include parameter.\\n\\xa0\\n👉You don’t know about the pull( ) function\\n\\xa0\\nAll training functions (create_model, tune_model, ensemble_model, etc.) in PyCaret displays a score grid but it doesn’t return the score grid. Hence you cannot store the score grid in an object like pandas.DataFrame. However, there is a function called\\xa0pull\\xa0that allows you to do that. For example:\\n\\n\\npull function with create_model\\n\\n\\xa0\\nThis will also work for holdout score grid when you use\\xa0predict_model\\xa0function.\\n\\n\\npull function with predict_model\\n\\n\\xa0\\nNow that you can access metrics as pandas.DataFrame, you can do wonders. For example, you can create a loop to train a model with different parameters and create a comparison table with this simple code:\\n\\n\\ncreate_model and pull function\\n\\n\\xa0\\n👉 You think PyCaret is a black-box, it is not.\\n\\xa0\\nAnother common confusion is that all the preprocessing is happening behind the scenes and is not accessible to users. As such, you cannot audit what happens when you ran the\\xa0setup\\xa0function. This is not True.\\nThere are two functions in PyCaret\\xa0get_config\\xa0and\\xa0set_config\\xa0that allows you to access and change everything in the background, from your training set to the random state of your model. You can check the documentation of\\xa0get_config\\xa0function by simply calling\\xa0help(get_config)\\xa0to see which variables are accessible to you:\\n\\n\\nhelp(get_config)\\n\\n\\xa0\\nYou can access the variable by calling it inside the\\xa0get_config\\xa0function. For example to access\\xa0X_train\\xa0transformed dataset, you will write this:\\n\\n\\nget_config(‘X_train’)\\n\\n\\xa0\\nYou can use the\\xa0set_config\\xa0function to change the environment variables. With what you know so far about\\xa0pull, get_config,\\xa0and\\xa0set_config\\xa0function,\\xa0you can create some pretty sophisticated workflows. For example, you can resample holdout set\\xa0N times\\xa0to evaluate averaged performance metrics instead of relying on one holdout set:\\n\\nimport numpy as np\\r\\nXtest = get_config('X_test')\\r\\nytest = get_config('y_test')AUC = []for i in np.random.randint(0,1000,size=10):\\r\\n    Xtest_sampled = Xtest.sample(n = 100, random_state = i)\\r\\n    ytest_sampled = ytest[Xtest_sampled.index]\\r\\n    set_config('X_test', Xtest_sampled)\\r\\n    set_config('y_test', ytest_sampled)\\r\\n    predict_model(dt);\\r\\n    AUC.append(pull()['AUC'][0])>>> print(AUC)[Output]: [0.8182, 0.7483, 0.7812, 0.7887, 0.7799, 0.7967, 0.7812, 0.7209, 0.7958, 0.7404]>>> print(np.array(AUC).mean())[Output]: 0.77513\\n\\n\\n\\xa0\\n👉You are not logging your experiments\\n\\xa0\\nIf you are not logging your experiments, you should start logging them now. Whether you want to use MLFlow backend server or not, you should still log all your experiments. When you perform any experiment, you generate a lot of meta data which is impossible to keep track of manually.\\nPyCaret’s logging functionality will generate a nice, light-weight, easy to understand excel spreadsheet when you use\\xa0get_logs\\xa0function. For example:\\n\\n# loading dataset\\r\\nfrom pycaret.datasets import get_data\\r\\ndata = get_data('juice')# initializing setup\\r\\nfrom pycaret.classification import *\\r\\ns = setup(data, target = 'Purchase', silent = True, log_experiment = True, experiment_name = 'juice1')# compare baseline models\\r\\nbest = compare_models()# generate logs\\r\\nget_logs()\\n\\n\\n\\n\\nget_logs()\\n\\n\\xa0\\nIn this very short experiment we have generated over 3,000 meta data points (metrics, hyperparameters, runtime, etc.). Imagine how you would have manually kept track of these datapoints? Perhaps, it’s not practically possible. Fortunately, PyCaret provides a simple way to do it. Simply set\\xa0log_experiment\\xa0to True in the\\xa0setup\\xa0function.\\n\\xa0\\nThere is no limit to what you can achieve using the lightweight workflow automation library in Python. If you find this useful, please do not forget to give us ⭐️ on our\\xa0GitHub repo.\\nTo hear more about PyCaret follow us on\\xa0LinkedIn\\xa0and\\xa0Youtube.\\nTo learn more about all the updates in PyCaret 2.2, please see the\\xa0release notes\\xa0or read this\\xa0announcement.\\n\\xa0\\nImportant Links\\n\\xa0\\nUser Guide\\nDocumentation\\nOfficial Tutorials\\nExample Notebooks\\nOther Resources\\n\\xa0\\nWant to learn about a specific module?\\n\\xa0\\nClick on the links below to see the documentation and working examples.\\nClassification\\nRegression\\nClustering\\nAnomaly Detection\\nNatural Language Processing\\nAssociation Rule Mining\\n\\xa0\\nBio: Moez Ali is a Data Scientist, and is Founder & Author of PyCaret.\\nOriginal. Reposted with permission.\\nRelated:\\n\\n5 Things You Don’t Know About PyCaret\\nDeploy a Machine Learning Pipeline to the Cloud Using a Docker Container\\nGitHub is the Best AutoML You Will Ever Need\",\n",
       " \"comments\\nBy Sandeep Uttamchandani, Ph.D., Both a Product/Software Builder (VP of Engg) & Leader in operating enterprise-wide Data/AI initiatives (CDO)\\n\\n\\nImage by Tumisu from Pixabay\\n\\n\\xa0\\nML model training is the most time-consuming and resource-expensive part of the overall model-building journey. Training by definition is iterative, but somewhere during the iterations, mistakes seep into the mix. In this article, I share the ten deadly sins during ML model training — these are the most common as well as the easiest to overlook.\\n\\xa0\\nTen Deadly Sins of ML Model Training\\n\\xa0\\n\\n1. Blindly increasing the number of epochs when the model is not converging\\n\\xa0\\nDuring model training, there are scenarios when the loss-epoch graph keeps bouncing around and does not seem to converge irrespective of the number of epochs. There is no silver bullet as there are multiple root causes to investigate — bad training examples, missing truths, changing data distributions, too high a learning rate. The most common one I have seen is bad training examples related to a combination of anomalous data and incorrect labels.\\n\\xa0\\n2.\\xa0Not shuffling the training dataset\\n\\xa0\\nSometimes there are scenarios where the model seems to be converging, but suddenly the loss value increases significantly, i.e., loss value reduces and then increases significantly with epochs. There are multiple reasons for this kind of exploding loss. The most common one I have seen is outliers in the data that are not evenly distributed/shuffled in the data. Shuffling, in general, is an important step including for patterns where the loss is showing a repeating step function behavior.\\n\\xa0\\n3.\\xa0In multiclass classification, not prioritizing specific per-class metrics accuracy\\n\\xa0\\nFor multiclass prediction problems, instead of tracking just the overall classification accuracy, it is often useful to prioritize the accuracy of specific classes and iteratively work on improving the model class by class. For instance, in classifying different forms of fraudulent transactions, focus on increasing the recall of specific classes (such as foreign transactions) based on business needs.\\n\\xa0\\n4.\\xa0Assuming specificity will lead to lower model accuracy\\n\\xa0\\nInstead of building a generic model, imagine building a model for a specific geographic region or specific user persona. Specificity will make the data more sparse but can lead to better accuracy for those specific problems. It is important to explore the specificity and sparsity trade-off during tuning.\\n\\xa0\\n5.\\xa0Ignoring prediction bias\\n\\xa0\\nPrediction bias is the difference between the average of predictions and the average of labels in the dataset. Prediction bias serves as an early indicator of model issues. A big nonzero prediction bias is indicative of a bug somewhere in the model. There’s an interesting\\xa0Facebook paper\\xa0in the context of ad CTR. Typically, the bias is useful to measure across prediction buckets.\\n\\xa0\\n6.\\xa0Calling it a success just on model accuracy numbers\\n\\xa0\\nAccuracy of 95% means 95 of 100 predictions were correct. Accuracy is a flawed metric with a class imbalance in the dataset. Instead investigate deeply into metrics, such as precision/recall and how it correlates to overall user metrics such as spam detection, tumor classification, etc.\\n\\xa0\\n7.\\xa0Not understanding the impact of regularization lambda\\n\\xa0\\nLambda is a key parameter in striking the balance between simplicity and training-data fit. High lambda → simple model → possibly underfitting. Low lambda → complex model → potential overfitting your data (won’t be able to generalize to new data). The ideal value of lambda is one that generalizes well to previously unseen data: data-dependent and requires analysis.\\n\\xa0\\n8. Using the same test set over and over\\n\\xa0\\nThe more the same data is used for parameter and hyperparameter settings, the lesser confidence that the results will actually generalize. It is important to collect more data and keep adding to the test and validation sets.\\n\\xa0\\n9. Not paying attention to initiation value in neural networks\\n\\xa0\\nGiven non-convex optimization in NN,\\xa0initialization matters.\\n\\xa0\\n10. Assuming wrong labels always need to be fixed\\n\\xa0\\nWhen wrong labels are detected, it is tempting to jump in and get them fixed. It is important to first analyze misclassified examples for the root cause. Oftentimes, errors due to incorrect labels may be a very small percentage. There might be a bigger opportunity to better train for specific data slices that might be the predominant root cause.\\n\\xa0\\nTo summarize, avoiding these mistakes puts you significantly ahead of most other teams. Incorporate these as a checklist in your process.\\n\\xa0\\nBio: Sandeep Uttamchandani, Ph.D.: Data + AI/ML -- Both a Product/Software Builder (VP of Engg) & Leader in operating enterprise-wide Data/AI initiatives (CDO) | O'Reilly Book Author | Founder - DataForHumanity (non-profit)\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Determine if Your Machine Learning Model is Overtrained\\nWrite and train your own custom machine learning models using PyCaret\\nHow to break a model in 20 days — a tutorial on production model analytics\",\n",
       " 'By Gaurav Menghani, Software Engineer at Google AI.\\ncomments\\n\\nIn the previous parts (Part 1 & Part 2), we discussed why efficiency is important for deep learning models to achieve high-performance models that are pareto-optimal, as well as the focus areas for efficiency in Deep Learning. Let us now dive deeper into examples of tools and techniques that fall in these focus areas.\\n\\xa0\\nCompression Techniques\\n\\xa0\\nCompression techniques, as mentioned earlier, are generic techniques that can help achieve a more efficient representation of one or more layers in a neural network, with a possible quality trade-off. The efficiency could come from improving one or more of the footprint metrics, such as model size, inference latency, training time required for convergence, etc., in exchange for as little quality loss as possible. Often the model could be over-parameterized. In such cases, these techniques help improve generalization on unseen data as well.\\nPruning: One of the popular compression techniques is Pruning, where we prune unimportant network connections, hence making the network sparse. LeCun et al. [1], in their paper titled \"Optimal Brain Damage\", trimmed the number of parameters (connections between layers) in their neural network by a factor of four while increasing both the inference speed and generalization.\\n\\nAn illustration of pruning in neural networks.\\nA similar approach was followed by the Optimal Brain Surgeon work (OBD) by Hassibi et al. [2] and by Zhu et al. [3]. These methods take a network that has been pre-trained to reasonable quality and then iteratively prune the parameters that have the lowest saliency score, which measures the importance of a particular connection, such that the impact on the validation loss is minimized. Once pruning concludes, the network is fine-tuned with the remaining parameters. The process is repeated until the network is pruned to the desired level.\\nAmongst the various works on pruning, the differences occur in the following dimensions:\\n\\nSaliency: This is the heuristic for determining which connection should be pruned. This can be based on second-order derivatives [1, 2] of the connection weight with respect to the loss function, the magnitude of the connection weight [3], and so on.\\nUnstructured v/s Structured: The most flexible way of pruning is unstructured (or random) pruning, where all given parameters are treated equally. In structured pruning, parameters are pruned in blocks of size > 1 (such as pruning row-wise in a weight matrix or pruning channelwise in a convolutional filter (example: [4,5]). Structured pruning allows easier leveraging of inference-time gains in size and latency since these blocks of pruned parameters can be intelligently skipped for storage and inference.\\n\\n\\nUnstructured vs. Structured Pruning of a weight matrix, respectively.\\n\\nDistribution: One could set a pruning budget that is the same for each layer, or it could be allocated on a per-layer basis [6]. The intuition being that certain layers are more amenable to pruning than others. For example, often, the first few layers are already small enough that they cannot tolerate significant sparsity [7].\\nScheduling: Yet additional criteria are how much to prune and when? Do we want to prune an equal number of parameters every round [8], or do we prune at a higher pace in the beginning and gradually slow down [9]?\\nRegrowth: In some cases, the network is allowed to regrow the pruned connections [9], such that the network constantly operates with the same percentage of connections pruned.\\n\\nIn terms of practical usage, structured pruning with a meaningful block size can help improve latency. Elsen et al. [7] construct sparse convolutional networks that outperform their dense counterparts by 1.3 - 2.4× with ≈ 66% of the parameters while retaining the same Top-1 accuracy. They do this via their library to convert from the NHWC (channels-last) standard dense representation to a special NCHW (channels-first) ‘Block Compressed Sparse Row’ (BCSR) representation which is suitable for fast inference using their fast kernels on ARM devices, WebAssembly etc. [10]. Although they also introduce some constraints on the kinds of sparse networks that can be accelerated. Overall, this is a promising step towards practical improvements in footprint metrics with pruned networks.\\nQuantization:\\xa0 Quantization is another popular compression technique. It exploits the idea that almost all the weights of a typical network are in 32-bit floating-point values, and if we are okay with losing some model quality (accuracy, precision, recall, etc.), we can store these values in a lower precision format (16-bit, 8-bit, 4-bit, etc.).\\nFor example, when a model is persisted, we can map the minimum value in a weight matrix to 0 and the maximum value to 2b-1 (where b is the number of bits of precision), and linearly extrapolate all values between them to an integer value. Often, this might be sufficient for the purposes of reducing model size. For example, if b = 8, we are mapping the 32-bit floating-point weights to 8-bit unsigned integers. This would lead to a 4x reduction in space. When doing inference (computing model predictions), we can recover a lossy representation of the original floating-point value (due to the rounding error), using the quantized value and the min & max floating-point values of the array. This step is referred to as Weight Quantization since we are quantizing the model’s weights.\\n\\nMapping continuous high-precision values to discrete low-precision integer values. Source\\nThe lossy representation and the rounding error might be okay for larger networks with built-in redundancy due to a large number of parameters but might lead to a drop in accuracy for smaller networks, which would likely be sensitive to these errors.\\nWe can solve this issue (in an experimental manner) by simulating the rounding behavior of weight quantization during the training. We do this by adding nodes in the model training graph that quantize and dequantize the activations and weight matrices, such that the training-time inputs to a neural network operation look identical to what they would have during the inference stage. Such nodes are referred to as Fake Quantization nodes. Training in such a manner makes the networks more robust to the behavior of quantization in inference mode. Note that we are doing Activation Quantization along with Weight Quantization during training now. This step of training-time simulated quantization is described in detail by Jacob et al. and Krishnamoorthi et al. [11,12]\\n\\nOriginal model training graph and the graph with fake quantization nodes. Source\\nSince both weights and activations are run in simulated quantized mode, that means all layers receive inputs that could be represented in lower-precision, and after the model is trained, it should be robust enough to do the math operations directly in lower-precision. As an example, if we train the model to replicate quantization in the 8-bit domain, the model can be deployed to do the matrix multiplication and other operations with 8-bit integers.\\nOn resource-constrained devices (such as mobile, embedded, and IoT devices), 8-bit operations can be sped up between 1.5 - 2x using libraries like GEMMLOWP [13] which rely on hardware support for such acceleration such as the Neon intrinsics on ARM processors [14]. Further, frameworks such as Tensorflow Lite enable their users to directly use quantized operations without having to bother about the lower-level implementations.\\n\\nTop-1 Accuracy vs. Model Latency for models with and without quantization. Source\\nApart from Pruning and Quantization, there are other techniques like Low-Rank Matrix Factorization, K-Means Clustering, Weight-Sharing etc. which are also actively being used for model compression [15].\\nOverall, we saw that compression techniques could be used to reduce a model’s footprint (size, latency, etc.) while trading off some quality (accuracy, precision, recall, etc.) in return.\\n\\xa0\\nLearning Techniques\\n\\xa0\\nDistillation: As mentioned earlier, Learning techniques try to train a model differently in order to obtain a better performance. For example, Hinton et al. [16], in their seminal work, explored how smaller networks can be taught to extract dark knowledge from larger models/ensembles of larger models. They use a larger teacher model to generate soft labels on existing labeled data.\\nThe soft labels assign a probability to each possible class instead of hard binary values in the original data. The intuition is that these soft labels capture the relationship between the different classes from which the model can learn. For example, a truck is more similar to a car than to an apple, which the model might not be able to learn directly from hard labels. The student network learns to minimize the cross-entropy loss on these soft labels, along with the original ground-truth hard labels. The weights of each of these loss functions can be scaled based on the results from experimentation.\\n\\nDistillation of a smaller student model from a larger pre-trained teacher model.\\nIn the paper, Hinton et al. [16] were able to closely match the accuracy of a 10-model ensemble for a speech recognition task with a single distilled model. There are other comprehensive studies [17,18] demonstrating the significant improvements in model quality of smaller models. As an example, Sanh. et al. [18] were able to distill a student model that retains 97% of the performance of BERT-Base while being 40% smaller and 60% faster on CPU.\\nData Augmentation: Typically for large models and complex tasks, the more data you have, the higher the chances of improving your model’s performance. However, getting high-quality labeled data is often both slow and expensive since they usually require a human in the loop. Learning from this data which has been labeled by humans, is referred to as supervised learning. It works very well when we have the resources to pay for the labels, but we can and should do better.\\nData Augmentation is a nifty way of improving the performance of the model. Usually, it involves making transformations to your data, such that it does not require re-labeling (label-invariant transformations). For example, if you were teaching your neural network to classify an image to contain a dog or a cat, rotating the image would not change the label. Other transformations could be horizontal/vertical flipping, stretching, cropping, adding Gaussian noise, etc. Similarly, if you were detecting the sentiment of a given piece of text, introducing a typo would likely not change the label.\\nSuch label-invariant transformations have been used across the board in popular deep learning models. They are especially handy when you have a large number of classes and/or few examples for certain classes.\\n\\nSome common types of data augmentations. Source\\nThere are other transformations such as Mixup [19], which mix inputs from two different classes in a weighted manner and treat the label to be a similarly weighted combination of the two classes. The idea is that the model should be able to extract out features that are relevant for both classes.\\nThese techniques are really introducing data efficiency to the pipeline. It is not very different from teaching a kid to identify real-life objects in different contexts.\\nSelf-Supervised Learning: There is rapid progress in an adjacent area, where we can learn generic models that completely skip the need for labels for extracting meaning out of data. With methods like contrastive learning [20], we can train a model such that it learns a representation for the input, such that similar inputs would have similar representations, while unrelated inputs should have very dissimilar representations. These representations are n-dimensional vectors (embeddings) which can then be useful as features in other tasks where we might not have enough data to train models from scratch. We can view the first step of using unlabeled data as pre-training and the next step as fine-tuning.\\n\\nContrastive Learning. Source\\nThis two-step process of pre-training on unlabeled data and fine-tuning on labeled data has also gained rapid acceptance in the NLP community. ULMFiT [21] pioneered the idea of training a general-purpose language model, where the model learns to solve the task of predicting the next word in a given sentence.\\nThe authors found that using a large corpus of preprocessed but unlabeled data such as the WikiText-103 (derived from English Wikipedia pages) was a good choice for the pre-training step. This was sufficient for the model to learn general properties about the language. The authors found that fine-tuning such a pre-trained model for a binary classification problem required only 100 labeled examples (as compared to 10,000 labeled examples otherwise).\\n\\nRapid convergence with ULMFiT. Source\\n\\nHigh-level approach of pre-training on a large corpus and fine-tuning on the relevant dataset. Source\\nThis idea was also explored in BERT models, where the pre-training steps involve learning a\\xa0 bi-directional masked language model, such that the model has to predict a missing word in the middle of a sentence.\\nOverall, learning techniques help us improve model quality without impacting the footprint. This could be used for improving model quality for deployment. If the original model quality was satisfactory, you could also exchange the newfound quality gains for improving model size and latency by simply reducing the number of parameters in your network until you go back to the minimum viable model quality.\\nIn our next part, we will continue to go over examples of tools and techniques that fit in the remaining three focus areas. Also, feel free to go over our survey paper that explores this topic in detail.\\n\\xa0\\nReferences\\n[1] Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal brain damage. In Advances in neural information processing systems. 598–605.\\n[2] Babak Hassibi, David G Stork, and Gregory J Wolff. 1993. Optimal brain surgeon and general network pruning. In IEEE international conference on neural networks. IEEE, 293–299.\\n[3] Michael Zhu and Suyog Gupta. 2018. To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings. OpenReview.net. https://openreview.net/forum?id=Sy1iIDkPM\\n[4] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. 2017. Structured pruning of deep convolutional neural networks. ACM Journal on Emerging Technologies in Computing Systems (JETC) 13, 3 (2017), 1–18.\\n[5] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2016. Pruning Filters for Efficient ConvNets. In ICLR (Poster).\\n[6] Xin Dong, Shangyu Chen, and Sinno Jialin Pan. 2017. Learning to prune deep neural networks via layer-wise optimal brain surgeon. arXiv preprint arXiv:1705.07565 (2017).\\n[7] Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan. 2020. Fast sparse convnets. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 14629–14638.\\n[8] Song Han, Jeff Pool, John Tran, and William J Dally. 2015. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626 (2015).\\n[9] Tim Dettmers and Luke Zettlemoyer. 2019. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840 (2019).\\n[10] XNNPACK. https://github.com/google/XNNPACK\\n[11] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713\\n[12] Raghuraman Krishnamoorthi. 2018. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv (Jun 2018). arXiv:1806.08342 https://arxiv.org/abs/1806.08342v1\\n[13] GEMMLOWP. https://github.com/google/gemmlowp\\n[14] Arm Ltd. 2021. SIMD ISAs | Neon – Arm Developer. https://developer.arm.com/architectures/instruction-sets/simdisas/neon [Online; accessed 3. Jun. 2021].\\n[15] Rina Panigrahy. 2021. Matrix Compression Operator. https://blog.tensorflow.org/2020/02/matrix-compressionoperator-tensorflow.html\\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015).\\n[17] Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, and Matt Richardson. 2016. Do deep convolutional nets really need to be deep and convolutional? arXiv preprint arXiv:1603.05691 (2016).\\n[18] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).\\n[19] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2017. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017).\\n[20] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597–1607.\\n[21] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146 (2018).\\nBio: Gaurav Menghani is a Staff Software Engineer at Google Research where he leads research projects geared towards optimizing large machine learning models for efficient training and inference on devices ranging from tiny microcontrollers to Tensor Processing Unit (TPU)-based servers. His work has positively impacted > 1 Billion of active users across YouTube, Cloud, Ads, Chrome, etc. He is also an author of an upcoming book with Manning Publication on Efficient Machine Learning. Before Google, Gaurav worked at Facebook for 4.5 years and has contributed significantly to Facebook’s Search system and large-scale distributed databases. He has an M.S. in Computer Science from Stony Brook University.\\ntwitter.com/GauravML\\nlinkedin.com/in/gauravmenghani\\nwww.gaurav.im\\nRelated:\\n\\nHigh Performance Deep Learning, Part 1\\nHigh-Performance Deep Learning: How to train smaller, faster, and better models – Part 2\\n5 Challenges to Scaling Machine Learning Models',\n",
       " 'By Hadrien Jean, Machine Learning Scientist.\\ncomments\\n\\n\\xa0\\nBasis and Change of Basis\\n\\xa0\\nOne way to understand eigendecomposition is to consider it as a change of basis. You’ll learn in this article what is the basis of a vector space.\\nYou’ll see that any vector of the space are linear combinations of the basis vectors and that the number you see in vectors depends on the basis you choose.\\nFinally, you’ll see how to change the basis using change of basis matrices.\\nIt is a nice way to consider matrix factorization as eigendecomposition or Singular Value Decomposition. As you can see in Chapter 09 of\\xa0Essential Math for Data Science, with eigendecomposition, you choose the basis such that the new matrix (the one that is similar to the original matrix) becomes diagonal.\\n\\xa0\\nDefinitions\\n\\xa0\\nThe\\xa0basis\\xa0is a coordinate system used to describe vector spaces (sets of vectors). It is a reference that you use to associate numbers with geometric vectors.\\nTo be considered as a basis, a set of vectors must:\\n\\nBe linearly independent.\\nSpan the space.\\n\\nEvery vector in the space is a unique combination of the basis vectors. The dimension of a space is defined to be the size of a basis set. For instance, there are two basis vectors in\\xa0ℝ2\\xa0(corresponding to the\\xa0x\\xa0and\\xa0y-axis in the Cartesian plane), or three in\\xa0ℝ3.\\nAs shown in section 7.4 of\\xa0Essential Math for Data Science, if the number of vectors in a set is larger than the dimensions of the space, they can’t be linearly independent. If a set contains fewer vectors than the number of dimensions, these vectors can’t span the whole space.\\nAs you saw, vectors can be represented as arrows going from the origin to a point in space. The coordinates of this point can be stored in a list. The geometric representation of a vector in the Cartesian plane implies that we take a reference: the directions given by the two axes\\xa0x\\xa0and\\xa0y.\\nBasis vectors\\xa0are the vectors corresponding to this reference. In the Cartesian plane, the basis vectors are orthogonal unit vectors (length of one), generally denoted as\\xa0i\\xa0and\\xa0j.\\n\\nFigure 1: The basis vectors in the Cartesian plane.\\n\\xa0\\nFor instance, in Figure 1, the basis vectors\\xa0i\\xa0and\\xa0j\\xa0point in the direction of the\\xa0x-axis and\\xa0y-axis respectively. These vectors give the standard basis. If you put these basis vectors into a matrix, you have the following identity matrix (for more details about identity matrices, see 6.4.3 in\\xa0Essential Math for Data Science):\\n\\nThus, the columns of\\xa0I2 span\\xa0ℝ2. In the same way, the columns of\\xa0I3 span\\xa0ℝ3\\xa0and so on.\\n\\nOrthogonal basis\\nBasis vectors can be orthogonal because orthogonal vectors are independent. However, the converse is not necessarily true: non-orthogonal vectors can be linearly independent and thus form a basis (but not a standard basis).\\n\\nThe basis of your vector space is very important because the values of the coordinates corresponding to the vectors depend on this basis. By the way, you can choose different basis vectors, like in the ones in Figure 2 for instance.\\n\\nFigure 2: Another set of basis vectors.\\n\\xa0\\nKeep in mind that vector coordinates depend on an implicit choice of basis vectors.\\n\\xa0\\nLinear Combination of Basis Vectors\\n\\xa0\\nYou can consider any vector in a vector space as a linear combination of the basis vectors.\\nFor instance, take the following two-dimensional vector\\xa0v:\\n\\n\\nFigure 3: Components of the vector\\xa0v.\\n\\xa0\\nThe components of the vector\\xa0v\\xa0are the projections on the\\xa0x-axis and on the\\xa0y-axis (vx\\xa0and\\xa0vy, as illustrated in Figure 3). The vector\\xa0v\\xa0corresponds to the sum of its components:\\xa0v = vx + vy, and you can obtain these components by scaling the basis vectors:\\xa0vx = 2i and\\xa0vy = −0.5j. Thus, the vector\\xa0v\\xa0shown in Figure 3 can be considered as a linear combination of the two basis vectors\\xa0i\\xa0and\\xa0j:\\n\\n\\xa0\\nOther Bases\\n\\xa0\\nThe columns of identity matrices are not the only case of linearly independent columns vectors. It is possible to find other sets of\\xa0n\\xa0vectors linearly independent in\\xa0ℝn.\\nFor instance, let’s consider the following vectors in\\xa0ℝ2:\\n\\nand\\n\\nThe vectors\\xa0v\\xa0and\\xa0w\\xa0are represented in Figure 4.\\n\\nFigure 4: Another basis in a two-dimensional space.\\n\\xa0\\nFrom the definition above, the vectors\\xa0v\\xa0and\\xa0w\\xa0are a basis because they are linearly independent (you can’t obtain one of them from combinations of the other) and they span the space (all the space can be reached from the linear combinations of these vectors).\\nIt is critical to keep in mind that, when you use the components of vectors (for instance\\xa0vx\\xa0and\\xa0vy, the\\xa0x\\xa0and\\xa0y\\xa0components of the vector\\xa0v), the values are relative to the basis you chose. If you use another basis, these values will be different.\\nYou can see in Chapter 09 and 10 of\\xa0Essential Math for Data Science\\xa0that the ability to change the bases is fundamental in linear algebra and is key to understand eigendecomposition or Singular Value Decomposition.\\n\\xa0\\nVectors Are Defined With Respect to a Basis\\n\\xa0\\nYou saw that to associate geometric vectors (arrows in the space) with coordinate vectors (arrays of numbers), you need a reference. This reference is the basis of your vector space. For this reason, a vector should always be defined with respect to a basis.\\nLet’s take the following vector:\\n\\nThe values of the\\xa0x\\xa0and\\xa0y\\xa0components are respectively 2 and -0.5. The standard basis is used when not specified.\\nYou could write\\xa0Iv\\xa0to specify that these numbers correspond to coordinates with respect to the standard basis. In this case\\xa0i\\xa0is called the\\xa0change of basis matrix.\\n\\nYou can define vectors with respect to another basis by using another matrix than\\xa0i.\\n\\xa0\\nLinear Combinations of the Basis Vectors\\n\\xa0\\nVector spaces (the set of possible vectors) are characterized in reference to a basis. The expression of a geometrical vector as an array of numbers implies that you choose a basis. With a different basis, the same vector\\xa0v\\xa0is associated with different numbers.\\nYou saw that the basis is a set of linearly independent vectors that span the space. More precisely, a set of vectors is a basis if every vector from the space can be described as a finite linear combination of the components of the basis and if the set is linearly independent.\\nConsider the following two-dimensional vector:\\n\\nIn the\\xa0ℝ2\\xa0Cartesian plane, you can consider\\xa0v\\xa0as a linear combination of the standard basis vectors\\xa0i\\xa0and\\xa0j, as shown in Figure 5.\\n\\nFigure 5: The vector\\xa0v\\xa0can be described as a linear combination of the basis vectors\\xa0i\\xa0and\\xa0j.\\n\\xa0\\nBut if you use another coordinate system,\\xa0v\\xa0is associated with new numbers. Figure 6 shows a representation of the vector\\xa0v\\xa0with a new coordinate system (i′ and\\xa0j′).\\n\\nFigure 6: The vector\\xa0v\\xa0with respect to the coordinates of the new basis.\\n\\xa0\\nIn the new basis,\\xa0v\\xa0is a new set of numbers:\\n\\n\\xa0\\nThe Change of Basis Matrix\\n\\xa0\\nYou can use a\\xa0change of basis matrix\\xa0to go from a basis to another. To find the matrix corresponding to new basis vectors, you can express these new basis vectors (i′\\xa0and\\xa0j′) as coordinates in the old basis (i\\xa0and\\xa0j).\\nLet’s take again the preceding example. You have:\\n\\nand\\n\\nThis is illustrated in Figure 7.\\n\\nFigure 7: The coordinates of the new basis vectors with respect to the old basis.\\n\\xa0\\nSince they are basis vectors,\\xa0i′\\xa0and\\xa0j′\\xa0can be expressed as linear combinations of\\xa0i\\xa0and\\xa0j.:\\n\\n\\nLet’s write these equations under the matrix form:\\n\\nTo have the basis vectors as columns, you need to transpose the matrices. You get:\\n\\nThis matrix is called the change of basis matrix. Let’s call it\\xa0CC:\\n\\nAs you can notice, each column of the change of basis matrix is a basis vector of the new basis. You’ll see next that you can use the change of basis matrix\\xa0CC\\xa0to convert vectors from the output basis to the input basis.\\n\\nChange of basis vs linear transformation\\xa0\\nThe difference between change of basis and linear transformation is conceptual. Sometimes it is useful to consider the effect of a matrix as a change of basis; sometimes you get more insights when you think of it as a linear transformation. Either you move the vector or you move its reference. This is why rotating the coordinate system has an inverse effect compared to rotating the vector itself. For eigendecomposition and SVD, both of these views are usually taken together, which can be confusing at first. Keeping this difference in mind will be useful throughout the end of the book. The main technical difference between the two is that change of basis must be invertible, which is not required for linear transformations.\\n\\n\\xa0\\nFinding the Change of Basis Matrix\\n\\xa0\\nA change of basis matrix maps an input basis to an output basis. Let’s call the input basis\\xa0B1\\xa0with the basis vectors\\xa0i\\xa0and\\xa0j, and the output basis\\xa0B2\\xa0with the basis vectors\\xa0i′\\xa0and\\xa0j′. You have:\\n\\nand\\n\\nFrom the equation of the change of basis, you have:\\n\\nIf you want to find the change of basis matrix given\\xa0B1\\xa0and\\xa0B2, you need to calculate the inverse of\\xa0B1\\xa0to isolate\\xa0C:\\n\\nIn words, you can calculate the change of basis matrix by multiplying the inverse of the input basis matrix (, which contains the input basis vectors as columns) by the output basis matrix (, which contains the output basis vectors as columns).\\n\\nConverting vectors from the output to the input basis\\xa0\\nBe careful, this change of basis matrix allows you to convert vectors from\\xa0\\xa0to\\xa0\\xa0and not the opposite. Intuitively, this is because moving an object is the opposite to moving the reference. Thus, to go from\\xa0\\xa0to\\xa0, you must use the inverse of the change of basis matrix\\xa0.\\n\\nNote that if the input basis is the standard basis (), then the change of basis matrix is simply the output basis matrix:\\n\\n\\nInvertible Change of Basis Matrix\\nSince the basis vectors are linearly independent, the columns of\\xa0C\\xa0are linearly independent, and thus, as stated in section 7.4 of\\xa0Essential Math for Data Science,\\xa0C\\xa0is invertible.\\n\\n\\xa0\\nExample: Changing the Basis of a Vector\\n\\xa0\\nLet’s change the basis of a vector\\xa0v, using again the geometric vectors represented in Figure 6.\\n\\xa0\\nNotation\\n\\xa0\\nYou’ll change the basis of\\xa0v\\xa0from the standard basis to a new basis. Let’s denote the standard basis as\\xa0\\xa0and the new basis as\\xa0. Remember that the basis is a matrix containing the basis vectors as columns. You have:\\n\\nand\\n\\nLet’s denote the vector\\xa0v\\xa0relative to the basis\\xa0\\xa0as\\xa0:\\n\\nThe goal is to find the coordinates of\\xa0v\\xa0relative to the basis\\xa0, denoted as\\xa0.\\n\\nSquare bracket notation\\nTo distinguish the basis used to define a vector, you can put the basis name (like\\xa0) in subscript after the vector name enclosed in square brackets. For instance,\\xa0\\xa0denotes the vector\\xa0v\\xa0relative to the basis\\xa0, also called the\\xa0representation\\xa0of\\xa0v\\xa0with respect to\\xa0\\n\\n\\xa0\\nUsing Linear Combinations\\n\\xa0\\n\\n\\xa0\\n\\n\\xa0\\nLet’s code this:\\n\\nv_B1 = np.array([2, 1])\\r\\nB_2 = np.array([\\r\\n    [0.8, -1.3],\\r\\n    [1.5, 0.3]\\r\\n])\\r\\n\\r\\nv_B2 = np.linalg.inv(B_2) @ v_B1\\r\\nv_B2\\r\\n\\n\\n\\n\\narray([ 0.86757991, -1.00456621])\\r\\n\\n\\n\\nThese values are the coordinates of the vector\\xa0v\\xa0relative to the basis\\xa0. This means that if you go to\\xa00.86757991i′−1.00456621j′\\xa0you arrive to the position (2, 1) in the standard basis, as illustrated in Figure 6.\\n\\xa0\\nConclusion\\n\\xa0\\nUnderstanding the concept of basis is a nice way to approach matrix decomposition (also called matrix factorization), like eigendecomposition or singular value decomposition (SVD). In these terms, you can think of matrix decomposition as finding a basis where the matrix associated with a transformation has specific properties: the factorization is a change of basis matrix, the new transformation matrix, and finally the inverse of the change of basis matrix to come back into the initial basis (more details in Chapter 09 and 10 of\\xa0Essential Math for Data Science).\\n\\xa0\\nBio: Hadrien Jean is a machine learning scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog (hadrienj.github.io).\\nOriginal. Reposted with permission.\\nRelated:\\n\\nEssential Math for Data Science: Scalars and Vectors\\nEssential Math for Data Science: Introduction to Matrices and the Matrix Product\\nEssential Math for Data Science: Information Theory',\n",
       " \"comments\\nBy Mehmet Suzen, Theoretical Physicist | Research Scientist.\\n\\nImage source: Wikipedia.\\nEdsger Dijkstra\\xa0was a Dutch theoretical physicist turned computer scientist and probably one of the most influential earlier pioneers in the field. He had deep insight into what computer science is and a well-founded notion of how it should be taught in academics. In this post, we extrapolate his ideas into data science. We develop something called the\\xa0Dijkstra principle for data science that is driven by his ideas on what does computer science entail.\\n\\xa0\\nComputer Science and Astronomy\\n\\xa0\\nAstronomy\\xa0is not about telescopes. Indeed, it is about how the universe works and how its constituent parts are interacting. Telescopes, either being optical or radio observations or similar detection techniques, are merely tools to practice and do investigation for astronomy. A formed analogy goes into computer science as well, and this is the quote from Dijkstra:\\nComputer science is no more about\\xa0computers than astronomy is about telescopes.\\xa0 -\\xa0Edsger Dijkstra\\n\\nDijkstra in Zurich, 1984 (Wikipedia).\\xa0\\nThe idea of Computer Science not being about computers is rather strange at first glance. However, what Dijkstra had in mind are abstract mechanisms and mathematical constructs that\\xa0one can map real problems to and solve as a computer science problem, such as\\xa0graph algorithms. Though Computer Science had many subfields, its inception can be considered as being rooted in\\xa0applied mathematics.\\n\\xa0\\nDijkstra principle for data science\\n\\xa0\\nBy using Dijkstra's approach now, we are in a position to formulate a\\xa0principle for data science.\\nData science is no more about data than computer science is about computers.\\xa0- Dijkstra principle for data science\\nThis sounds absurd. If data science is not about data, then what is it about? Apart from the definition of data science as an emergent field, as an amalgamation of multiple fields from statistics to high-performance computing, \\xa0the idea that data not being the core\\xa0tenant\\xa0of data science implies the practice does not aim at data itself rather a higher purpose. Data is used similar to a telescope in astronomy, and the purpose is to reveal the\\xa0empirical truths about the\\xa0representations\\xa0that the data conveys. There are no unique ways to\\xa0achieve this purpose.\\n\\xa0\\nConclusion\\n\\xa0\\nSuch a Dijkstra principle for data science\\xa0would be very helpful in\\xa0understanding the data science practice as being\\xa0not data-centric, contrary to the mainstream dogma, rather as\\xa0a\\xa0science-centric practice\\xa0with the data being the primary tool to leverage, using\\xa0a multitude\\xa0of techniques. The implication is that machine learning is a secondary tool on top of data in practicing data science. This attitude would help causality play a\\xa0major role in shifting modern data science forward.\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nData Science: (not) the preferred nomenclature\\nData Science Books You Should Start Reading in 2021\\nHow to frame the right questions to be answered using data\",\n",
       " 'Sponsored Post.\\n\\niMerit is a leader in providing high-quality data for training data sets. In our latest blog post, we bring you the latest in recognising and handling edge cases in your ML systems. If you wish to talk to an expert and learn more about conquering your edge cases, please contact us.\\xa0\\nThrough training, machine learning (ML) systems learn to recognize patterns by associating inputs (e.g., pixel values, audio samples, text) to categories (e.g., object identities, words, sentiments). This association can be thought of as dividing the multi-dimensional space of possible inputs into regions representing categories, the regions being defined by decision boundaries. When, during testing or operation, an input falls beyond the edge of the region assigned to its category, the input will be mis-categorized. We call this an edge case.\\nEdge cases occur for three basic reasons:\\n\\nBias – the ML system is too ‘simple’\\nVariance – the ML system is too ‘inexperienced’\\nUnpredictability – the ML system operates in an environment full of surprises.\\n\\nHow do we recognize these edge cases situations, and what can we do about them?\\n\\xa0\\nBias\\n\\xa0\\nBias shows up when an ML system cannot achieve good performance on its training data set. This is an indication that the architecture of the ML system, its model, does not have a structure that can represent the nuances in the training data. A very simple example is shown below:\\n\\nThis chart shows the linear decision boundary implemented by a two-input, single-layer, two-neuron neural network. Having been trained to try to distinguish the purple from the yellow dots, it cannot do a good job because its simple linear decision boundary is inadequate to separate the classes.\\n\\nIf we make the ML system a bit more complex by adding a pre-processing layer to transform the original inputs to polar coordinates, the purple and yellow dots line up differently in the new input space presented to the linear part of the ML system, and we get a decision boundary that looks like this. This more complex ML system performs well on the training data.\\n“The structure of the layers themselves can also make a big difference.”\\nOften the additional complexity needed to overcome bias in an ML system can be accomplished simply by adding processing units and layers. However, the structure of the layers themselves can also make a big difference. The ImageNet visual database has been used since 2010 to benchmark the performance of object recognition ML systems. The best performance on this data set jumped 10.8 percentage points in 2012, when Convolutional Neural Networks replaced earlier architectures that used pre-processing and Support Vector Machines, essentially a more sophisticated version of the simple ML system shown above.\\n\\xa0\\nVariance\\n\\xa0\\nWhen an ML system achieves good performance on its training data, but performs poorly in testing, the problem is often that the training data set is too small to adequately reflect the range of variability in the ML system’s operational environment.\\n\\nThe primary way to reduce edge cases caused by variance is to gather more training data.\\nFor example, the object recognition system shown here does fine with adults, but apparently doesn’t have much experience with children.\\nEdge cases caused by variance issues is a large problem for automatic facial recognition systems. While these systems can be highly accurate in recognizing smartphone users, for example, they can produce inaccurate and troubling results when used for law enforcement. In a 2018 test, 28 members of the US Congress were falsely matched to mug shots in a database of 25,000 criminals.\\nThe primary way to reduce edge cases caused by variance is to gather more training data. Studies have shown that ML system performance consistently improves as training data set size increases, even for data sets that are already very large.\\nIn some cases, data augmentation can be used to increase the size of training data sets, by making small changes to the original data to represent variations such as object rotation, lighting, or pose. However, it is particularly important to ensure that training data is representative of important but possibly rare situations encountered in the operational environment, such as the appearance of children or extreme weather conditions.\\n\\xa0\\nUnpredictability\\n\\xa0\\nMachine learning relies on finding regular patterns in input data. There is always statistical variation in the data, but with an appropriate architecture and enough training data, an ML system can often find enough data regularity (achieve small enough bias and variance), to make reliable decisions and minimize edge cases.\\n\\nHowever autonomous driving is an example of an environment that presents ML systems with such a high degree of variability that the possible situations are virtually endless. Training data can be gathered through many millions of miles of driving without encountering important edge cases.\\xa0 For example, in 2018 a woman was struck and killed by a self-driving car, because the ML system had never been trained to recognize and respond to a jaywalker with a bicycle.\\nHandling the virtually unlimited number of edge cases encountered in the unpredictable autonomous driving application is a particularly difficult challenge. Two avenues are being pursued to deal with this problem. One approach is to develop ‘checker’ ML systems, specifically trained to recognize dangerous or questionable systems and take overriding action to maintain safety.\\nAnother approach is to train autonomous vehicle ML systems to better handle edge cases. One way is to use virtual road training environments to expand the range of training situations. Another way uses adversarial training, the technology famous for creating ‘deep fakes’,\\xa0 to greatly expand training sets, allowing ML systems to better handle novelty.\\n\\xa0\\nOvercoming Edge Cases\\n\\xa0\\nThe table below summarizes how to recognize and deal with the edge cases resulting from ML bias, variance, and unpredictability:\\n\\n\\nCulprit\\nHow it Shows Up\\nWhat to Try\\n\\n\\nBias\\nPoor training performance\\nBetter ML system model\\n\\n\\nVariance\\nPoor test performance\\nMore training data\\n\\n\\nUnpredictability\\nOperational surprises\\nSimulation Adversarial training\\n\\n\\n\\nKey takeaways\\n\\nEnsure your training and test data sets are large and diverse enough, (and of course accurately labeled!), to expose weaknesses in ML design and to sufficiently characterize the diversity of the operational environment\\nFor operational environments with high unpredictability, consider augmenting training data with examples generated through simulation and adversarial training.\\nMonitor operations and respond to the inevitable appearance of new edge cases.',\n",
       " \"comments\\nBy Thomas Richardson, Behavioural scientist at U. of Manchester.\\n\\nThe joys and stresses of too much data. Photo is me!\\nResearchers these days often have access to a lot of data. You might be analysing\\xa0The European Social Survey with hundreds of variables and tens of thousands of respondents. Or perhaps you collected tons of data yourself; You might do a study where participants do a huge battery of tests, leaving you with heaps of data per person.\\nHow do you go about analysing this beautiful mess?\\n\\xa0\\nWell, I’ll tell you what\\xa0not\\xa0to do:\\n\\xa0\\nDive in headfirst and start correlating and plotting and building multilevel models but then realise that you might have to control for some variables and you get a result, but then it goes away with covariates B and C but not when you also add A, and then you find something seemingly robust, but you’ve tested like 15 things by now so maybe it's a false positive, so you learn about cross-validation and what the hell am I even doing here and when did it become Friday?!?\\nBut seriously, sometimes we have more data than we know what to do with. There are worse problems, I’ll admit, but this has been a problem for me during my PhD. I’ve lost entire days digging around in big datasets. Leaving aside the problem of p-hacking (I generally try to validate any results found on a new dataset, and so should you!), this can really eat into your time.\\nYou can also get suckered into this when your goal is to “get to know the data inside and out.” We often hear statisticians recommending that we do more exploratory data analysis, plot our data etc. This is valuable, but it’s not necessary to know every corner of the European Social Survey. It’s often not productive to see how a model is influenced by 10 different covariates that are barely related to the dependent variable. You have to draw the line somewhere and say “enough”!\\n\\nGraph by the author.\\nI’m a big fan of the idea that constraints on our choice can be liberating. If that sounds paradoxical, these great articles [here and\\xa0here] will explain it. I’ve found that when it comes to data, more options aren’t always better.\\n\\xa0\\nSolutions\\n\\xa0\\n1. Be question-driven\\nBefore looking at your data, decide what your\\xa0primary questions\\xa0are. These are the questions you will set out to answer, and after, you will stop (at least until you’ve written the paper/chapter). What are the questions that, once answered, could be publishable? Not to be too cynical, but in academia, papers are the name of the game. Papers aren’t usually a collection of interesting observations stapled together. They are focused, and you should be too.\\nDo all the analysis required to answer a paper-sized question in a satisfactory manner (i.e., by checking the robustness of the result, ruling out alternative explanations),\\xa0and then stop.\\xa0All other questions in the data are unimportant or less important. You can always come back to them later.\\n2. Discuss the data with colleagues or lab members\\nTalking them through your data and analyses will clarify it in your mind. The idea of gaining a deeper understanding of something through explaining it to others is often called the\\xa0Feynman technique, and I’m a big fan. They may be able to advise what in the data is interesting and what isn’t.\\nRemember, you’re trying to produce knowledge that is valuable to your field. Therefore, it makes sense to ask a few members of said field if they think your analysis is valuable.\\n3. Keep detailed notes of what you try\\nWith so many t-tests and regression models flying about, you might end up forgetting you ran some analysis previously and end up running it again! Also, sometimes models don’t yield insight in isolation but in combination with other models.\\nFor example, you run model A, and predictor A1 shows an effect that you don’t understand. So you see if A1 correlates with a few other things, and that might explain it. You build model B and eureka!\\xa0Now record what you did. Interpret each test before moving onto the next one. Don’t just go running tests willy-nilly without thinking about what they mean.\\xa0Running tests for the hell of it is a great way to find false positives. If you keep notes and interpret what’s going on as you go, every test truly does teach you something about the data.\\n4. Set a time limit on analysis\\nSet yourself an amount of time to do it in. Deadlines, even self-imposed ones, are great for clarifying the mind. Having years to do a PhD is not an excuse to waste time. Any data stuff not done after this time will have to wait for another day. It’s not a big deal if you miss something important, as your supervisor/colleagues/reviewers on the eventual paper will find them. You can even come back to the dataset and take a few more days on it later when you’ve got some more time. This also has the advantage of returning to the data refreshed. Give yourself a time limit. Don’t just wander leisurely through the data.\\n5. Pre-registration\\nConstrain what analyses you’ll do in a formal way. Write down what analyses you’ll do before you look at the data.\\xa0Aspredicted is a good tool to help you with this. Pre-registration is brilliant for many other reasons: give it a Google,\\xa0read this or ask your local Open Science Methods Police for more information (If you’re at the University of Manchester, that may well be me!).\\n\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nThe question that makes your data project more valuable\\nHow to frame the right questions to be answered using data\\nPowerful Exploratory Data Analysis in just two lines of code\",\n",
       " 'By Gregory Piatetsky, KDnuggets.  \\n\\nHere are the most popular July 2021 stories on KDnuggets. The top story, \"Data Scientists and ML Engineers Are Luxury Employees\" touched upon the same topic as our recent poll - the demand for Data Scientists and related professions, but our poll results show that the majority of KDnuggets readers remain confident that Data Scientists will not go extinct in 10 years. \\r\\nRather, KDnuggets readers want to improve their skills, and two very popular posts about it were Advice for Learning Data Science from Google\\'s Director of Research and 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist. Check the other top stories below!\\r\\n\\r\\nStories in green font are also the winners of KDnuggets Top Blog Rewards for July.\\r\\n\\nMost Viewed - Platinum Badge (>32,000 UPV)\\n\\n Data Scientists and ML Engineers Are Luxury Employees, by Adrien Biarnes (*)\\r\\n Top 6 Data Science Online Courses in 2021, by Natassha Selvaraj (*)\\r\\n\\n\\nMost Viewed - Gold Badge (>16,000 UPV)\\n\\n Advice for Learning Data Science from Google\\'s Director of Research, by Benjamin Obi Tayo (*)\\r\\n 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist, by Tessa Xie\\r\\n Pandas not enough? Here are a few good alternatives to processing larger and faster data in Python, by DaurEd (*)\\r\\n A Learning Path To Becoming a Data Scientist, by Sara Metwalli (*)\\r\\n Geometric foundations of Deep Learning, by Michael Bronstein, Joan Bruna, Taco Cohen, and PV (*)\\r\\n GitHub Copilot Open Source Alternatives, by Matthew Mayo\\r\\n\\n\\nMost Viewed - Silver Badge (> 8,000 UPV)\\n\\n How Can You Distinguish Yourself from Hundreds of Other Data Science Candidates?, by Tirthajyoti Sarkar (*)\\r\\n 5 Python Data Processing Tips & Code Snippets, by Matthew Mayo (*)\\r\\n Design patterns in machine learning, by Agoston Torok (*)\\r\\n How to Get Practical Data Science Experience to be Career-Ready, by Terence Shin (*)\\r\\n\\n\\n\\n\\nMost Shared - Platinum Badge (>1400 shares)\\n\\n Data Scientists and ML Engineers Are Luxury Employees, by Adrien Biarnes (*)\\r\\n Relax! Data Scientists will not go extinct in 10 years, but the role will change, by Gregory Piatetsky (*)\\r\\n Pandas not enough? Here are a few good alternatives to processing larger and faster data in Python, by DaurEd (*)\\r\\n\\n\\nMost Shared - Gold Badge (>700 shares)\\n\\n Why and how should you learn \"Productive Data Science\"?, by Tirthajyoti Sarkar\\r\\n Become an Analytics Engineer in 90 Days, by Tuan Nguyen\\r\\n Not Only for Deep Learning: How GPUs Accelerate Data Science & Data Analytics, by Kevin Vu (*)\\r\\n 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist, by Tessa Xie\\r\\n GitHub Copilot: Your AI pair programmer - what is all the fuss about?, by Matthew Mayo\\r\\n\\n\\nMost Shared - Silver Badge (>400 shares)\\n\\n 11 Important Probability Distributions Explained, by Terence Shin\\r\\n Geometric foundations of Deep Learning, by Michael Bronstein, Joan Bruna, Taco Cohen, and PV\\r\\n A Learning Path To Becoming a Data Scientist, by Sara Metwalli\\r\\n A Brief Introduction to the Concept of Data, by Angelica Lo Duca (*)\\r\\n Top 6 Data Science Online Courses in 2021, by Natassha Selvaraj\\r\\n\\n\\r\\n(*) indicates that badge added or upgraded based on these monthly results.\\r\\n\\nMost Shareable (Viral) Blogs\\r\\nAmong the top blogs, here are the blogs with the highest ratio of shares/unique views, which suggests that people who read it really liked it. \\r\\n\\n Relax! Data Scientists will not go extinct in 10 years, but the role will change, by Gregory Piatetsky\\r\\n The Brutal Truth About Data Science, by Prad Upadrashta\\r\\n Streamlit Tips, Tricks, and Hacks for Data Scientists, by Kaveh Bakhtiyari\\r\\n Not Only for Deep Learning: How GPUs Accelerate Data Science & Data Analytics, by Kevin Vu\\r\\n 5 Mistakes I Wish I Had Avoided in My Data Science Career, by Tessa Xie',\n",
       " 'comments\\nBy Jacqueline Nolis, Head of Data Science at Saturn Cloud\\n\\nThe talk this blog post was based on.\\n\\xa0\\nA common moment when training a neural network is when you realize the model isn’t training quickly enough on a CPU and you need to switch to using a GPU. A less common, but still important, moment is when you realize that even a large GPU is too slow to train a model and you need further options.\\nOne option is to connect multiple GPUs together across multiple machines so they can work as a unit and train a model more quickly. For most of the time, I’ve been using neural networks with the idea that connecting GPUs seemed outrageously difficult and maybe even only feasible for engineering teams trying to show off training large models. Thankfully, it turns out multi-GPU model training across multiple machines is pretty easy with Dask. This blog post is about my first experiment in using multiple GPUs with Dask and the results.\\nTo see how to use Dask with GPUs to more quickly train a model, I first needed a model to try training. I decided to lean on my old friend, a neural network trained on Seattle pet license data, to generate realistic sounding pet names. I had previously used this neural network as\\xa0an example in R, so switching it to Python and Dask seemed fun. I decided to use PyTorch for the framework, but still with a multi-layer LSTM to capture the patterns in the words.\\n\\nSome adorable names generated for adorable pets.\\n\\xa0\\nThis whole blog post can be run for free on the\\xa0Saturn Cloud Hosted free tier\\xa0using the free Jupyter server and Dask clusters.\\n\\xa0\\nModel architecture\\n\\xa0\\n\\xa0\\nTo train a neural network to generate realistic text, I needed the data to be properly formatted. Specifically, I want to predict each character in a name (as opposed to word in a sentence), so the base data was the beginning sequences of characters in text and the target data was next letter in the sequence of text. For example, to train a model on the pet name “SPOT” I would need to train it that a first letter is “S”, after “S” is “P,” after “SP”, is “O”, and so on. I used blanks used as filler and a special stop character indicate the name was complete. This was be made into a matrix as shown below (and if the data is too long I’d just truncate the earlier letters):\\n\\n\\nX_1\\nX_2\\nX_3\\nX_4\\nX_5\\nY\\n\\n\\n(blank)\\n(blank)\\n(blank)\\n(blank)\\n(blank)\\nS\\n\\n\\n(blank)\\n(blank)\\n(blank)\\n(blank)\\nS\\nP\\n\\n\\n(blank)\\n(blank)\\n(blank)\\nS\\nP\\nO\\n\\n\\n(blank)\\n(blank)\\nS\\nP\\nO\\nT\\n\\n\\n(blank)\\nS\\nP\\nO\\nT\\n(stop)\\n\\n\\n\\xa0\\nEach character was then 1-hot encoded (so given a dictionary value like A=1, B=2, etc and converted into vectors of 0s with one 1). With that, I got a 3-dimensional binary matrix as input to the model and 2-dimensional binary matrix as the target.\\nThe network was four LSTM layers, plus a dense layer with a node for each character in the dictionary. The four LSTM layers would find the patterns within the text, and the dense mapped the results to each character-level prediction. For a single GPU, I would train the model using a single function:\\n\\ndef train():\\r\\n    device = torch.device(0) # Set GPU as the device\\r\\n    dataset = OurDataset(pet_names, device=device)\\r\\n    loader = DataLoader(dataset, batch_size=batch_size, \\r\\n                        shuffle=True, num_workers=0)\\r\\n    model = Model()\\r\\n    model = model.to(device)\\r\\n    criterion = nn.CrossEntropyLoss()\\r\\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\\r\\n    for epoch in range(num_epochs):\\r\\n        dataset.permute()\\r\\n        for i, (batch_x, batch_y) in enumerate(loader):\\r\\n            optimizer.zero_grad()\\r\\n            batch_y_pred = model(batch_x)\\r\\n            loss = criterion(batch_y_pred.transpose(1, 2), batch_y)\\r\\n            loss.backward()\\r\\n            optimizer.step()\\r\\n        time = datetime.datetime.now().isoformat()\\r\\n        print(f\"{time} - epoch {epoch} - loss {loss.item()}\")\\r\\n    return model\\r\\n\\n\\n(Full code for the final model can be found on this\\xa0Saturn Cloud GitHub repo.)\\n\\xa0\\nTraining with multiple GPUs\\n\\xa0\\n\\xa0\\nThe basic idea to train with multiple GPUs is to use PyTorch’s Distributed Data Parallel (DDP) function. DDP will let multiple models being trained at the same time pass parameters between each other after each batch and compute the gradient concurrently. Thus, if you have three devices training a model with DDP, it’s equivalent to training a single model with a batch size that’s three times as large. Since the devices are all training concurrently, it’s theoretically possible that in this scenario, the model could be trained 3x faster than a model on a single device with a batch size 3x larger. In practice, this won’t be the case because of latency between the models when they communicate. But still, the results can be pretty good.\\nTo turn my training code into a multi-GPU set up, I needed to make a few adjustments. First, I needed to add the PyTorch DDP wrapper around the model in the training future (and again the full code is available on\\xa0GitHub):\\n\\nmodel = Model()\\r\\nmodel = model.to(device) # Was in single-GPU scenario\\r\\nmodel = DDP(model) # Added for multi-GPU scenario\\r\\n\\n\\n\\xa0\\nNext, any place in my training function where I might only want a single machine to do something (like write the output at the end of an Epoch), I needed to wrap in a “only have one worker do this” logical statement. I used worker number\\xa00\\xa0as the one to run these commands.\\n\\nworker_rank = int(torch.distributed.get_rank())\\r\\nif worker_rank == 0:\\r\\n   # ...\\r\\n\\n\\n\\xa0\\nThen, I used dask.delayed to have multiple versions of the training function running concurrently. After adding a\\xa0@dask.delayed\\xa0decorator above the training function, I used\\xa0dask_pytorch_ddp\\xa0as a simpler wrapper around the functions to run them:\\n\\nfrom dask_pytorch_ddp import dispatch\\r\\nfutures = dispatch.run(client, train)\\r\\n\\n\\n\\xa0\\nFinally, to log the model results rather than having to coordinate where results are being saved within the Dask cluster, I used\\xa0Weights and Biases\\xa0as a place to store results and models.\\n\\xa0\\nComparing results\\n\\xa0\\n\\xa0\\nFor the analysis, I decided to try adjusting a number of parameters and see how a version with multiple-GPUs compared to a single GPU. Note that for simplicity in comparing, in the single GPU case I still used the DDP-based code but only with one machine (rather than removing that entirely)\\n\\nNetworked GPUs\\xa0- 1, 4, or 16\\nTraining batch size, per GPU\\xa0- 1024, 4096, or 16384\\nModel learning rate\\xa0- 0.001, 0.004, or 0.016\\n\\nThen, I used a Saturn Cloud Dask cluster to iterate though each of the combinations. Note that you have to be careful when comparing the results of the different experiments.\\xa0For an equivalent comparison, if the number of networked GPUs went up by 4x you need to lower the training size per GPU to 1/4th the value.\\xa0I, uh, may have spent hours trying to figure out what was wrong before I realized my comparisons weren’t equivalent. With that, here are the results:\\n\\nAs you can see, using more GPUs can dramatically reduce the amount of time it takes to train the model, while still providing the same quality of model accuracy.\\nHere is a graph of the same data visualized in a different way–instead of showing the curves over the epochs of the training run, this is the amount of time it takes before the model hits a reasonable level of accuracy. You can see that increasing the number of GPUs by 4x can give you a nearly 4x increase in speed.\\n\\n\\xa0\\nA few notes about these pretty amazing results. First, these techniques work best when your models need larger batch sizes because at that point. multiple GPUs aren’t just speeding up your results, but also you can avoid out of memory errors. So, if your problem is sufficiently simple, you might not get as much value out of using multiple GPUs, although there should still be a speed increase. Although, if the time it takes for your GPUs to communicate is high, then that can reduce the value of this too. Additionally, besides having multiple machines all coordinating GPU model training, one other option you have is to train with multiple GPUs on the same machine with resources like V100 graphics cards. That can be done using only DDP–having Dask communicate locally could still work, but would probably be redundant.\\nBesides using multiple GPUs with Dask to train the same model in a coordinated way, you could use Dask to concurrently train models with different sets of parameters. This allows you to explore a parameter space much faster–you could go faster still with tools like Dask-ML for fast hyper-parameter tuning.\\nLastly, just to see the fruits of all this labor, below is a set of pet names the model generated (and you can generate them yourself\\xa0here).\\n[\\'Karfa\\', \\'Samalpidells\\', \\'Avexen\\', \\'Ejynsp\\', \\'Charlie\\', \\'Kooie\\', \\'Winnundie\\', \\'Wunla\\', \\'Scarlie A\\', \\'Fli\\', \\'Argio\\', \\'Kucka\\', \\'Karlor\\', \\'Litr\\', \\'Bheneng\\', \\'Gelo\\', \\'Cangalantynes\\', \\'Mop\\', \\'Nimi\\', \\'Hary\\', \\'Cassm Tur\\', \\'Ullyne\\', \\'Hollylof\\', \\'Sopa\\', \\'Ghugy\\', \\'Bigde\\', \\'Nucha\\', \\'Mudser\\', \\'Wridney\\', \\'Mar Bas\\', \\'Rugy Bi\\', \\'Roba\\', \\'Ruedel\\', \\'Henrie\\', \\'Sharlia Hu\\', \\'Yellaj\\', \\'Balil\\']\\n\\xa0\\nBio: Jacqueline Nolis has over a decade of experience in using data science to solve business problems, and has lead data science, machine learning, and AI projects at Microsoft, Adobe, Airbnb, and T-Mobile, and has helped build data science teams from scratch.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nPandas not enough? Here are a few good alternatives to processing larger and faster data in Python\\nGPU-Powered Data Science (NOT Deep Learning) with RAPIDS\\nIntroducing Packed BERT for 2x Training Speed-up in Natural Language Processing',\n",
       " \"comments\\nBy Anupam Chugh, iOS Developer | Writer @Medium | BITS Pilani.\\n\\nPhoto by\\xa0Paul Hanaoka\\xa0on\\xa0Unsplash.\\nA lot happened in the machine learning community during the past year. Here’s a tour through the most popular and trending open-source research projects, demos, and prototypes. It ranges from photo editing to NLP to training models with “no-code,” and I hope they inspire you to build incredible AI-powered products this year. You can also find additional machine learning projects here.\\n\\xa0\\n1. Background Matting v2\\n\\xa0\\nBackground Matting v2\\xa0takes a cue from the popular\\xa0The World is Your Green Screen\\xa0open-source project and showcases how to remove or change the background in real-time. It gives better performance (30fps at 4K and 60fps at FHD) and can be used with Zoom, the popular videoconferencing app.\\nThe technique uses an additional captured frame of the background and uses it in recovering the alpha matte and the foreground layer. To process high-resolution images in real-time, two neural networks are used.\\nThis project is certainly helpful if you’re looking to remove a person from a video whilst preserving the background.\\n\\nDemo\\n\\xa0\\n2. SkyAR\\n\\xa0\\nHere’s another\\xa0amazing project\\xa0that does a video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles.\\nBased on Pytorch, the project partially adapts code from the\\xa0pytorch-CycleGAN-and-pix2pix\\xa0project, and it leverages sky matting, motion estimation through optical flow, and image blending to provide artistic backgrounds on videos in real-time.\\nThe above open-source has incredible potential in movies and video games such as adding fake rain/sunny weather/etc.\\n\\nSource\\n\\xa0\\n3. AnimeGAN v2\\n\\xa0\\nCartoonifying photos is always a fun machine learning project. Isn’t it?\\nThis project, AnimeGANv2, is the improved version of AnimeGAN. Specifically, it combines neural style transfer and generative adversarial network (GAN) to accomplish the task while also ensuring that the generation of high-frequency artifacts is prevented.\\n\\nSource\\n\\xa0\\n4. txtai\\n\\xa0\\nAI-refined search engines and QA Chatbots are always the need of the hour. And that’s exactly what this\\xa0project\\xa0does.\\nBy using\\xa0sentence-transformers,\\xa0transformers, and\\xa0faiss,\\xa0txtai\\xa0builds an AI-powered engine for contextual search and extractive question-answering.\\nEssentially,\\xa0txtai\\xa0supports building text indices to perform similarity searches and create extractive question-answering based systems.\\n\\nSource\\n\\xa0\\n5. Bringing-Old-Photos-Back-to-Life\\n\\xa0\\nNext up, we have\\xa0Microsoft’s latest photo restoration project\\xa0that auto-fixes damaged photos.\\nSpecifically, it restores old photos that suffer from complex degradation through a deep learning implementation in PyTorch by leveraging scratch detection, face enhancements, and other techniques.\\nAs per their\\xa0research paper: “We train two variational autoencoders (VAEs) to respectively transform old photos and clean photos into two latent spaces. And the translation between these two latent spaces is learned with synthetic paired data. This translation generalizes well to real photos because the domain gap is closed in the compact latent space. Besides, to address multiple degradations mixed in one old photo, we design a global branch with a partial nonlocal block targeting the structured defects, such as scratches and dust spots, and a local branch targeting the unstructured defects, such as noises and blurriness.”\\nThe model certainly outperforms the conventional state of the art methods, as evident in the below demo:\\n\\nSource\\n\\xa0\\n6. Avatarify\\n\\xa0\\nDeepfake projects have taken the machine learning and AI community by storm.\\xa0This project\\xa0shows a classic example of that by letting you create photo-realistic avatars in real-time video conferencing apps.\\nBasically, it uses the\\xa0First Order Model\\xa0to do a motion extraction from the video and apply it to the target avatar image by using optical flow. In doing so, you can generate avatars on a virtual camera or even animate classic paintings. From Elon Musk to Mona Lisa, you can impersonate anyone for fun!\\n\\nSource\\n\\xa0\\n7. Pulse\\n\\xa0\\nHere’s an AI model that showcases how to generate a realistic face image from a low-resolution one.\\nPULSE, which stands for Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models, provides an alternative formulation of the super-resolution problem based on creating realistic SR images that also downscale back correctly.\\n\\nSource\\n\\xa0\\n8. pixel2style2pixel\\n\\xa0\\nBased on the\\xa0research paper\\xa0“Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation,” this\\xa0project\\xa0uses a Pixel2Pixel framework, and it’s designed to address a wide range of image-to-image tasks using the same architecture in order to avoid any possible locality bias.\\nBased on a novel encoder network, this network can be trained to align a face image to a frontal pose, conditional image synthesis, and create super-resolution images.\\nFrom generating almost real-life people out of cartoonist pics to converting sketches or face segmentations to photo-realistic images, there are so many things that you can do with this.\\n\\nSource\\n\\xa0\\n9. igel\\n\\xa0\\nIt could be due to budget issues or a lack of a clear vision, but finding people with relevant machine learning expertise is always a challenge for startups. More so since the field is always a work in progress.\\nHence, there has been a surge in no-code machine learning platforms lately, with the likes of Google and Apple releasing their own toolsets to quickly train models.\\nThis delightful open-source machine learning project does just that by allowing you to train/fit, test, and use models without writing code. While the GUI drag and drop version is still a work in progress, there’s a lot that you can achieve with the command line tools of this project:\\n\\n//train or fit a model\\r\\nigel fit -dp 'path_to_your_csv_dataset.csv' -yml 'path_to_your_yaml_file.yaml'\\r\\n\\r\\n//evaluate\\r\\nigel evaluate -dp 'path_to_your_evaluation_dataset.csv'\\r\\n\\r\\n//predict\\r\\nigel predict -dp 'path_to_your_test_dataset.csv'\\r\\n\\n\\n\\xa0\\nThere’s also a single command\\xa0igel experiment\\xa0to combine all the phases: train, evaluate, and predict. For more details, refer to the\\xa0documentation here.\\n\\nSource\\n\\xa0\\n10. Pose Animator\\n\\xa0\\nLast but not least, we have a web animation tool. Basically,\\xa0this project\\xa0uses PoseNet and FaceMesh landmark results to bring an SVG vector image to life by leveraging some TensorFlow.js models.\\nYou can animate your own designs or skeleton images in the following way:\\n\\nSource\\n\\xa0\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\n2020: A Year Full of Amazing AI Papers — A Review\\n20+ Machine Learning Datasets & Project Ideas\\nAll Machine Learning Algorithms You Should Know in 2021\",\n",
       " 'comments\\nBy Isabelle Flückiger, Senior Executive | International Advisor | Speaker | Thought Leader | Learning Leadership | Lecturer | Startup Advisor\\n\\n\\nImage by\\xa0Steve Buissinne\\xa0from\\xa0Pixabay\\n\\n\\xa0\\nData science is a success. Thousands of students around the globe sign up for online courses or even a data science master program.\\nThe data science field is a very competitive market, especially to get one of the (supposed) dream jobs at one of the big tech companies. The positive news is that you have it in your hand to gain a competitive advantage for such a position by preparing yourself adequately.\\nOn the other hand, there are\\xa0(too) many MOOCs, master programs, bootcamps, blogs, videos and data science academies. As a beginner, you feel lost. Which course should I attend? What topics should I learn? On what methods do I need to focus? What tool and programming language must I study?\\nThe truth is that every data scientist has her/his individual journey and is biased towards that learning path. So, without knowing you, it is difficult to say what’s the best approach for you.\\nBut there are common mistakes made over and over again by all data scientists. Even when knowing them, you will not avoid them altogether, but eventually, stop earlier in doing them and find faster back to the road to success.\\nBased on my 20+ years of experience in data science, leading teams up to 150 people, and still giving lectures on a part-time basis at one of the leading global universities, I summarized for you the core mistakes to avoid reaching your dream faster.\\nThe mistakes are given in the order of the learning progress as a beginner data scientist.\\n\\xa0\\n#1 Investing too much time in assessing all the different types and options of courses available before you finally start — or eventually never start\\n\\xa0\\nI know that you are overwhelmed by all the courses, and you try not to make any mistakes. You want to invest your time and money effectively and select the right approach which promises the fastest and best success.\\nUnfortunately, there is no immediate success like in any technical and scientific field, and for the best possible success, you will not have any comparison.\\nThe fact is that today, all established platforms, academies, and institutes have good courses. So, do not overthink and over-analyze the courses. Be brave and choose one, complete that course and then select another one.\\nThe most crucial aspect is starting and doing. You cannot make a mistake here because you neither know your journey nor how it would have been different when choosing another one. No one can tell you that. Period.\\nIt is also important to realize that learning is circular and not linear. Taking one data science course does not exclude that you are taking another one.\\nI do still data science, machine learning, and AI training after all my years of experience. In every still so “simply” beginner course, I detect a new aspect and a new view on the topic. And this is exactly what finally makes a high-demanded data scientist. It’s to understand all the different perspectives on a topic.\\n\\xa0\\n#2 You want to learn too many methods and tools at once instead of learning and understanding the methods one by one\\n\\xa0\\nMany aspiring data scientists think that having as many as possible methods mentioned in the CV help to get a job faster. But the contrary is true. When applying for a job and you started only six months ago with data science for every recruiter, it’s clear that it is buzzword dropping with no substance behind it.\\nIf we look at regression models, there are many books only about regression. There are more than 50 regression types, and each comes with different preconditions. So, only have “regression” in your CV does not say anything. Also, regression models are still the most important models for applications and to set the basis of understanding for data science in general.\\nYou must understand what is solved by a method; what are the assumptions; what do the parameters mean; what are pitfalls; and so on, and so forth.\\nBased on the CV and how the knowledge of regression is described, every experienced recruiter — or today, the algorithms behind the process — can identify the depth of your understanding.\\nIt is better to have in-depth knowledge and experience in only a handful of methods than knowing many with no substance.\\n\\xa0\\n#3 You code everything from the beginning because you think this helps you to program better and faster\\n\\xa0\\nWhen starting coding, people think they must quickly begin coding and re-programming as many algorithms as possible. Also, here you should focus on understanding a few and not on quantity.\\nFirst, you need to understand the prerequisites of coding: linear algebra, mathematical induction, discrete mathematics, geometry — yes, this is the strength of the excellent programmers but often forgotten by data scientists, statistics and probability theory, calculus, Boolean algebra, and graph theory.\\nI did not become better and faster by coding more. I got good at programming by understanding the mathematical basis, reviewing the code of others, and run and test them on different data and problems.\\nYes, coding is essential, but more important is to understand the (good) architecture of code. And this can only be learned by reviewing other code.\\nA fact is that code becomes more and more a commodity, and there are even no-code tools. The differentiator will not be anymore between the ones that can code and those that cannot, but the ones that understand its architecture and those that do not.\\nI show you another example: I assume you have already used TensorFlow. But do you understand what it is? What does it do? And why it is called “TensorFlow”? Do you know what a tensor is? Not just the mechanical calculation of a tensor product, but what does it mean geometrically?\\n\\xa0\\n#4 By learning the theory, you think you know everything but miss enough practical experience\\n\\xa0\\nLearning data science is try and error. Only when you make as much experience as possible, making all the errors and resolving them, you get a deeper understanding.\\nThe theory is okay and vital. You need an understanding of the fundamentals.\\nUnfortunately, in practice, it rarely works like in theory. On the contrary, it often works precisely in a way, as you have learned you should not do it.\\nSo, you must start from the beginning with practical examples. Often, you will not feel ready to do practical work: not enough knowledge of the basics or not enough programming experience.\\nBut I strongly advise: start at the beginning even though you do not feel ready to do exercises. It has not to be a daylong or week-long project. A small 1–2 hours project is enough.\\nYou can either start with a no-code tool like RapidMiner or KNIME or take somebody else’s code and apply it. E.g. take a simple sentiment analysis code and use it to Tweets or product description. Then you can start to alter the code for other examples and compare the results.\\nWhen you learned talking as a small child, you started with single words or expressions of two or three words. And step by step, you built up a feeling for the language. It is the same with practical experience in data science.\\nPro tip: Learning is circular. So, store your work. Later you can come back, improve it, move it to GitHub, and add visualizations with Tableau.\\n\\xa0\\n#5 You think that certifications are a competitive advantage to get a data science job\\n\\xa0\\nCertifications are okay. There are many voices out there that tell you that you should not do certifications. But they can serve as a motivation, and finally, they show officially your progress and your eagerness to learn. I still do certificates. There is nothing wrong with it, and when you invest time, it is legitimate to have it.\\nBut it is not a differentiator in the market. The fact is that there are thousands of people that have the same certifications. So, to have a competitive advantage, you must go beyond that.\\nFor example, a student of mine approached me for support for an internship opportunity in the finance field. He wanted to apply what he has learned and get to know the culture and cooperation within a data science team. I could place him with a bank, and he writes a semester thesis from that. Yes, it is stressful to do the study and the internship and semester thesis in parallel. But it will give him an invaluable competitive advantage for job offers.\\n\\xa0\\n#6 You worry about the opinion of other people instead of building your own opinion based on facts\\n\\xa0\\nMost aspiring data scientists worry about the opinion of other data scientists. And the more arguments they hear, the more confused they are. Even though confusion is required to the path of clarity, it should not remain a steady state.\\nEach data scientist is an individual with her/his experience, learning and career path and opinion. I am used to saying, “if you have two data scientist in a room, you have at least four different opinions.”\\nIt is good to take opinions as inspiration and as a guide to search for information, but not as the information itself.\\nSearch for hard facts. Draw your logical conclusions, validate, and update them again. This is an important skill to progress successfully in your data science career.\\n\\xa0\\n#7 Not caring about business and domain knowledge\\n\\xa0\\nMany data scientists think they can apply the methods to every problem and industry, but I can tell you that’s wrong from more than 20 years of experience.\\nToo often, I saw data scientists presenting findings to the business people, and the reaction was, “oh, we know this already. What we need is ‘why that happens’ and ‘how to solve it.’ Or, in the worst case, ‘this is absolute nonsense because this is not how our business works.’ Boom!\\nIt is more important to have domain knowledge than knowing all the sexist and fanciest methods. A data scientist is solving a business problem, not a technical problem. By solving a business problem, you bring value to the company’s business, and you have only so much value as the value of your solution. You do this successfully when you know the business.\\nI worked in many different industries. Each time before I even started to engage with the business, I read a lot about the industry.\\n\\nI started with Wikipedia, learned the big picture and about the companies\\nI looked up the annual reports and investor relations information of the top 10 companies in an industry\\nI read all the news articles of the last few years about this industry and companies\\nI contacted my LinkedIn contacts who work in this industry\\n\\nOnly then, I started to interact with the business.\\nHalf of your learning should contain the development of industry and business knowledge.\\n\\xa0\\n#8 You are not studying and learning on a consistent and ongoing basis\\n\\xa0\\nIt is very easy to be distracted or give up early because you do not understand the topic. Learning data science is a marathon and not a sprint. So, it is essential to build up a routine to study ongoing and consistent. Like in marathon training, you train in small units but daily.\\nAlso, as written before, learning is circular. Having once studied a topic does not mean that you have mastered it.\\nLet me give you an example. In the mathematical finance lectures, I had to learn many limit theorems. The exam went excellent, and I was convinced that I understand them. But seven years later, when I had to review code for the valuation of complex structure financial products, the scales fell from my eyes, and I realized that I did not understand it until that moment of code review.\\nSo, book daily, or at least weekly, a few hours to learn. It does not matter whether you are an aspiring or already a senior data scientist.\\nThe learning should consist of new data science topics, already learned topics but from another perspective, e.g. another course or book, new technologies and technology trends, industry and business knowledge, data visualization and data storytelling, and applications to data.\\nIt adds layer and layer of understanding, and in the job interview, you will be able to give convincing answers by presenting the holistic view from different perspectives.\\n\\xa0\\n#9 No storytelling with the data\\n\\xa0\\nIn a data science job, you will primarily communicate your findings to non-technical people, notably, the people from the business. And the business is financing your job. Without their commitment, your job and the data science team would not exist.\\nYour job is to bring value to the business. It is not to apply fancy methods only for the sake of application.\\nA friend of mine is the data science lead of a global bank. When they are hiring data scientists, they send them two weeks in advance a dataset and ask for a 20 minutes presentation. No further input is given. They want to see the storytelling. They are not interested in the methods applied — except a candidate would tell absolute nonsense about the methods used. What they want to see is, first, the framing of the business problem and why it is important to solve. Second, what should be solved and last, how it is solved, and the result in a business context. “This is the most important work we do the whole day. A candidate must not be perfect in that but show that she/he has understood what is important in our job.”\\nSo, learn data storytelling — there are even free courses about that — and learn visualization of data in a business context.\\n\\xa0\\n#10 Learning on your own without interactions with the data science community\\n\\xa0\\nMany people think they can learn data science through their own hard work. All the other data scientists are seen as competitors, and one is reluctant to exchange knowledge.\\nBut living in your world where you only read and learn based on your selection is highly biased, and many perspectives on a topic or method are missing. Further, the open discourse about a topic and gaining experience in argumentation is missing — a skill needed by any data scientist.\\nAny experienced recruiter knows after one or two questions if you are a one-person show or if you have a vivid network that helps you to gain knowledge exponentially. This benefits the company and increases your market value and demand.\\nSo, it’s crucial to develop a network. This can be done by attending bootcamps, hackathons, and Meetup meetings.\\nNow, you know theoretically what you should avoid.\\nAny of these mistakes is a potential showstopper for your data science job.\\nI know that you still will make several of these mistakes. I am not different. It is in human nature to think that “I am different” — even though the data says the contrary. But the awareness of these potential mistakes will help you to re-adjust your path faster and thus become more effectively a demanded data scientist.\\n\\xa0\\nDo you like my story? Here you can find more.\\n\\xa0\\nHands-On Step-By-Step Guidance to Grow Your Job Opportunities\\nHow to leverage Meetup meetings strategically to get your dream data science job\\n\\xa0\\nThe Ultimate Guide on the Data Science MicroMasters Programs on edX 2021\\nWhich of the 6 programs should you choose?\\n\\xa0\\nThe Top Technology Trends and Their Impact on Data Science, Machine Learning and AI\\nAn action plan for you and your career\\n\\xa0\\nBio: Isabelle Flückiger is a Senior Executive with international C-level advisory experience in end-to-end digital, data and new technology transformation projects, with key industry experience in banking, insurance, chemicals, utilities and pharma/life sciences.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Land a Data Analytics Job in 6 Months\\nTop 10 Data Science Projects for Beginners\\nData Science is Not Becoming Extinct in 10 Years, Your Skills Might',\n",
       " 'comments\\nBy James Briggs, Data Scientist\\n\\n\\nBERT, but in Italy — image by author\\n\\n\\xa0\\nMany of my articles have been focused on BERT — the model that came and dominated the world of natural language processing (NLP) and marked a new age for language models.\\nFor those of you that may not have used transformers models (eg what BERT is) before, the process looks a little like this:\\n\\npip install transformers\\nInitialize a pre-trained transformers model —\\xa0from_pretrained.\\nTest it on some data.\\nMaybe\\xa0fine-tune the model (train it some more).\\n\\nNow, this is a great approach, but if we only ever do this, we lack the understanding behind creating our own transformers models.\\nAnd, if we cannot create\\xa0our own transformer models — we must rely on there being a pre-trained model that fits our problem, this is not always the case:\\n\\n\\nA few comments asking about non-English BERT models\\n\\n\\xa0\\nSo in this article, we will explore the steps we must take to build our own transformer model — specifically a further developed version of BERT, called RoBERTa.\\n\\xa0\\nAn Overview\\n\\xa0\\n\\xa0\\nThere are a few steps to the process, so before we dive in let’s first summarize what we need to do. In total, there are four key parts:\\n\\nGetting the data\\nBuilding a tokenizer\\nCreating an input pipeline\\nTraining the model\\n\\nOnce we have worked through each of these sections, we will take the tokenizer and model we have built — and save them both so that we can then use them in the same way we usually would with\\xa0from_pretrained.\\n\\xa0\\nGetting The Data\\n\\xa0\\n\\xa0\\nAs with any machine learning project, we need data. In terms of data for training a transformer model, we really are spoilt for choice — we can use almost any text data.\\n\\n\\nVideo walkthrough for downloading OSCAR dataset using HuggingFace’s datasets library\\n\\n\\xa0\\nAnd, if there’s one thing that we have plenty of on the internet — it’s unstructured text data.\\nOne of the largest datasets in the domain of text scraped from the internet is the OSCAR dataset.\\nThe OSCAR dataset boasts a huge number of different languages — and one of the clearest use-cases for training from scratch is so that we can apply BERT to some less commonly used languages, such as Telugu or Navajo.\\nUnfortunately, the only language I can speak with any degree of competency is English — but my girlfriend is Italian, and so she — Laura, will be assessing the results of our Italian-speaking BERT model — FiliBERTo.\\nSo, to download the Italian segment of the OSCAR dataset we will be using HuggingFace’s\\xa0datasets\\xa0library — which we can install with\\xa0pip install datasets. Then we download OSCAR_IT with:\\n\\nLet’s take a look at the\\xa0dataset\\xa0object.\\n\\nGreat, now let’s store our data in a format that we can use when building our tokenizer. We need to create a set of plaintext files containing just the\\xa0text\\xa0feature from our dataset, and we will split each\\xa0sample\\xa0using a newline\\xa0\\\\n.\\n\\nOver in our\\xa0data/text/oscar_it\\xa0directory we will find:\\n\\n\\nThe directory containing our plaintext OSCAR files\\n\\n\\xa0\\n\\xa0\\nBuilding a Tokenizer\\n\\xa0\\n\\xa0\\nNext up is the tokenizer! When using transformers we typically load a tokenizer, alongside its respective transformer model — the tokenizer is a key component in the process.\\n\\n\\nVideo walkthrough for building our custom tokenizer\\n\\n\\xa0\\nWhen building our tokenizer we will feed it all of our OSCAR data, specify our vocabulary size (number of tokens in the tokenizer), and any special tokens.\\nNow, the RoBERTa special tokens look like this:\\n\\nSo, we make sure to include them within the\\xa0special_tokens\\xa0parameter of our tokenizer’s\\xa0train\\xa0method call.\\n\\nOur tokenizer is now ready, and we can save it file for later use:\\n\\nNow we have two files that define our new\\xa0FiliBERTo\\xa0tokenizer:\\n\\nmerges.txt\\xa0— performs the initial mapping of text to tokens\\nvocab.json\\xa0— maps the tokens to token IDs\\n\\nAnd with those, we can move on to initializing our tokenizer so that we can use it as we would use any other\\xa0from_pretrained\\xa0tokenizer.\\n\\xa0\\nInitializing the Tokenizer\\n\\xa0\\n\\xa0\\nWe first initialize the tokenizer using the two files we built before — using a simple\\xa0from_pretrained:\\n\\nNow our tokenizer is ready, we can try encoding some text with it. When encoding we use the same two methods we would typically use,\\xa0encode\\xa0and\\xa0encode_batch.\\n\\nFrom the encodings object\\xa0tokens\\xa0we will be extracting the\\xa0input_ids\\xa0and\\xa0attention_mask\\xa0tensors for use with FiliBERTo.\\n\\xa0\\nCreating the Input Pipeline\\n\\xa0\\n\\xa0\\nThe input pipeline of our training process is the more complex part of the entire process. It consists of us taking our raw OSCAR training data, transforming it, and loading it into a\\xa0DataLoader\\xa0ready for training.\\n\\n\\nVideo walkthrough of the MLM input pipeline\\n\\n\\xa0\\n\\xa0\\nPreparing the Data\\n\\xa0\\n\\xa0\\nWe’ll start with a single sample and work through the preparation logic.\\nFirst, we need to open our file — the same files that we saved as\\xa0.txt\\xa0files earlier. We split each based on newline characters\\xa0\\\\n\\xa0as this indicates the individual samples.\\n\\nThen we encode our data using the\\xa0tokenizer\\xa0— making sure to include key parameters like\\xa0max_length,\\xa0padding, and\\xa0truncation.\\n\\nAnd now we can move onto creating our tensors — we will be training our model through masked-language modeling (MLM). So, we need three tensors:\\n\\ninput_ids\\xa0— our\\xa0token_ids\\xa0with ~15% of tokens masked using the mask token\\xa0<mask>.\\nattention_mask\\xa0— a tensor of\\xa01s and\\xa00s, marking the position of ‘real’ tokens/padding tokens — used in attention calculations.\\nlabels\\xa0— our\\xa0token_ids\\xa0with\\xa0no\\xa0masking.\\n\\nIf you’re not familiar with MLM, I’ve explained it\\xa0here.\\nOur\\xa0attention_mask\\xa0and\\xa0labels\\xa0tensors are simply extracted from our\\xa0batch. The\\xa0input_ids\\xa0tensors require more attention however, for this tensor we mask ~15% of the tokens — assigning them the token ID\\xa03.\\n\\nIn the final output, we can see part of an encoded\\xa0input_ids\\xa0tensor. The very first token ID is\\xa01\\xa0— the\\xa0[CLS]\\xa0token. Dotted around the tensor we have several\\xa03\\xa0token IDs — these are our newly added\\xa0[MASK]\\xa0tokens.\\n\\xa0\\nBuilding the DataLoader\\n\\xa0\\n\\xa0\\nNext, we define our\\xa0Dataset\\xa0class — which we use to initialize our three encoded tensors as PyTorch\\xa0torch.utils.data.Dataset\\xa0objects.\\n\\nFinally, our\\xa0dataset\\xa0is loaded into a PyTorch\\xa0DataLoader\\xa0object — which we use to load our data into our model during training.\\n\\xa0\\nTraining the Model\\n\\xa0\\n\\xa0\\nWe need two things for training, our\\xa0DataLoader\\xa0and a model. The\\xa0DataLoader\\xa0we have — but no model.\\n\\n\\n\\n\\xa0\\nInitializing the Model\\n\\xa0\\n\\xa0\\nFor training, we need a raw (not pre-trained)\\xa0BERTLMHeadModel. To create that, we first need to create a RoBERTa config object to describe the parameters we’d like to initialize FiliBERTo with.\\n\\nThen, we import and initialize our RoBERTa model with a language modeling (LM) head.\\n\\n\\xa0\\nTraining Preparation\\n\\xa0\\n\\xa0\\nBefore moving onto our training loop we need to set up a few things. First, we set up GPU/CPU usage. Then we activate the training mode of our model — and finally, initialize our optimizer.\\n\\n\\xa0\\nTraining\\n\\xa0\\n\\xa0\\nFinally — training time! We train just as we usually would when training via PyTorch.\\n\\nIf we head on over to Tensorboard we’ll find our loss over time — it looks promising.\\n\\n\\nLoss / time — multiple training sessions have been threaded together in this chart\\n\\n\\xa0\\n\\xa0\\nThe Real Test\\n\\xa0\\n\\xa0\\nNow it’s time for the real test. We set up an MLM pipeline — and ask Laura to assess the results. You can watch the video review at 22:44 here:\\n\\n\\n\\nWe first initialize a\\xa0pipeline\\xa0object, using the\\xa0\\'fill-mask\\'\\xa0argument. Then begin testing our model like so:\\n\\n“ciao\\xa0come\\xa0va?”\\xa0is the right answer! That’s as advanced as my Italian gets — so, let’s hand it over to Laura.\\nWe start with\\xa0“buongiorno, come va?”\\xa0— or\\xa0“good day, how are you?”:\\n\\nThe first answer, “buongiorno, chi va?” means “good day, who is there?” — eg nonsensical. But, our second answer is correct!\\nNext up, a slightly harder phrase,\\xa0“ciao, dove ci incontriamo oggi pomeriggio?”\\xa0— or\\xa0“hi, where are we going to meet this afternoon?”:\\n\\nAnd we return some more positive results:\\n\\n✅ \"hi, where do we see each other this afternoon?\"\\r\\n✅ \"hi, where do we meet this afternoon?\"\\r\\n❌ \"hi, where here we are this afternoon?\"\\r\\n✅ \"hi, where are we meeting this afternoon?\"\\r\\n✅ \"hi, where do we meet this afternoon?\"\\n\\n\\xa0\\nFinally, one more, harder sentence,\\xa0“cosa sarebbe successo se avessimo scelto un altro giorno?”\\xa0— or “what would have happened if we had chosen another day?”:\\n\\nWe return a few good more good answers here too:\\n\\n✅ \"what would have happened if we had chosen another day?\"\\r\\n✅ \"what would have happened if I had chosen another day?\"\\r\\n✅ \"what would have happened if they had chosen another day?\"\\r\\n✅ \"what would have happened if you had chosen another day?\"\\r\\n❌ \"what would have happened if another day was chosen?\"\\n\\n\\xa0\\nOverall, it looks like our model passed Laura’s tests — and we now have a competent Italian language model called FiliBERTo!\\nThat’s it for this walkthrough of training a BERT model from scratch!\\nWe’ve covered a lot of ground, from getting and formatting our data — all the way through to using language modeling to train our raw BERT model.\\nI hope you enjoyed this article! If you have any questions, let me know via\\xa0Twitter\\xa0or in the comments below. If you’d like more content like this, I post on\\xa0YouTube\\xa0too.\\nThanks for reading!\\n\\xa0\\n\\n70% Off! Natural Language Processing: NLP With Transformers in Python\\n\\nTransformer models are the de-facto standard in modern NLP. They have proven themselves as the most expressive…\\n\\xa0\\n*All images are by the author except where stated otherwise\\n\\xa0\\nBio: James Briggs is a data scientist specializing in natural language processing and working in the finance sector, based in London, UK. He is also a freelance mentor, writer, and content creator. You can reach the author via email (jamescalam94@gmail.com).\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Apply Transformers to Any Length of Text\\nUnderstanding BERT with Hugging Face\\nTopic Modeling with BERT',\n",
       " 'By Jesus Rodriguez, Intotheblock.\\ncomments\\n\\n\\nSource:\\xa0https://arxiv.org/pdf/2101.03961.pdf\\n\\n\\xa0\\n\\nI recently started a new newsletter focus on AI education and\\xa0already has over 50,000 subscribers. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:\\n\\n\\n\\xa0\\nOpenAI’s GPT-3 is, arguably , the most famous deep learning models created in the last few years. One of the things that impresses the most about GPT-3 is its size. In some context, GPT-3 is nothing but GPT-2 with a lot of more parameters. With 175 billion parameters, GPT-3 was about four times bigger than its largest predecessor.\\nKnowing that, how would you then feel about a model that is 6 times larger than GPT-3? This is precisely what a team from Google Research achieved with their novel\\xa0Switch Transformer architecture. The new model features an unfathomable 1.6 trillion parameters which makes it effectively\\xa0six times larger than GPT-3.\\n1.6 trillion parameters is certainly impressive but that’s not the most impressive contribution of the Switch Transformer architecture. With this new model, Google is essentially unveiling a method that maximize the parameter count of a transformer model in a simple and computationally efficient way. Transformer models like GPT-3 are not only huge but also computationally expensive which limits its adoption in mainstream scenarios.\\nThe key modification of the Switch Transformer architecture is based on introducing a Mixture of Experts (MoE) routing layer that facilitates learning sparse models instead of a super large dense model. This is not as confusing as it reads so let me try to explain. Typical transformer architectures are composed by the famous attention layer followed by a dense feed forward network. Among other things, that dense layer is responsible for the large cost of training transformer models. Google’s Switch Transformer proposes replacing that layer with what they called a Switch FFN layer. That layer processes the input tokens and decides which smaller feed forward network should process it. The Switch FFN layer includes three main benefits:\\n\\nThe router computation is very small as it only routes to a single expert.\\nThe capacity of each expert network can remain manageable.\\nThe router implementation is super simple.\\n\\n\\n\\nSource:\\xa0https://arxiv.org/pdf/2101.03961.pdf\\n\\n\\xa0\\nWith the new optimizations, Google was able to train a Switch Transformer model to an astonishing 1.6 trillion parameters! The training speed improved to up seven times compared to previous architectures.\\nMiraculously, the Switch Transformer release has managed to remain under the radar. Somehow, it reminds me of the original BERT paper that trigger the whole transformer movement. However, if the hype behind GPT-3 is any indication of what’s next to come, keep an eye for new milestones using the Switch Transformer.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nMicrosoft Uses Transformer Networks to Answer Questions About Images With Minimum Training\\nOpenAI Releases Two Transformer Models that Magically Link Language and Computer Vision\\nTop 5 Artificial Intelligence (AI) Trends for 2021',\n",
       " 'comments\\nBy Ahmad Bin Shafiq, Machine Learning Student.\\n\\n\\xa0\\nPlotly\\n\\xa0\\nPlotly is a famous library used for creating interactive plotting and dashboards in Python. Plotly is also a\\xa0company, that allows us to host both online and offline data visualisatoins.\\nIn this article, we will be using offline plotly to visually represent data in the form of different geographical maps.\\nInstalling Plotly\\n\\npip install plotly\\r\\npip install cufflinks\\r\\n\\r\\n\\n\\n\\xa0\\nRun both commands in the command prompt to install\\xa0plotly\\xa0and\\xa0cufflinks\\xa0and all of their packages on our local machine.\\nChoropleth Map\\nChoropleth maps are popular thematic maps used to represent statistical data through various shading patterns or symbols on predetermined geographic areas (i.e., countries). They are good at utilizing data to easily represent the variability of the desired measurement across a region.\\nHow does a Choropleth map works?\\nChoropleth Maps\\xa0display divided geographical areas or regions that are colored, shaded, or patterned in relation to a data variable. This provides a way to visualize values over a geographical area, which can show variation or patterns across the displayed location.\\nUsing Choropleth with Python\\nHere, we will be using a\\xa0dataset\\xa0of power consumption of different countries throughout the world in 2014.\\nOkay, so let’s get started.\\nImporting libraries\\n\\nimport plotly.graph_objs as go \\r\\nfrom plotly.offline import init_notebook_mode,iplot,plot\\r\\ninit_notebook_mode(connected=True)\\r\\n\\r\\nimport pandas as pd\\r\\n\\r\\n\\n\\n\\xa0\\nHere,\\xa0init_notebook_mode(connected=True)\\xa0connects Javascript to our notebook.\\nCreating/Interpreting our DataFrame\\n\\ndf = pd.read_csv(\\'2014_World_Power_Consumption\\')\\r\\ndf.info()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nHere we have 3 columns, and all of them have 219 non-null entries.\\n\\ndf.head()\\r\\n\\r\\n\\n\\n\\nCompiling our data into dictionaries\\n\\ndata = dict(\\r\\n        type = \\'choropleth\\',\\r\\n        colorscale = \\'Viridis\\',\\r\\n        locations = df[\\'Country\\'],\\r\\n        locationmode = \"country names\",\\r\\n        z = df[\\'Power Consumption KWH\\'],\\r\\n        text = df[\\'Country\\'],\\r\\n        colorbar = {\\'title\\' : \\'Power Consumption KWH\\'},\\r\\n      )\\r\\n\\r\\n\\n\\n\\xa0\\ntype = ’choropleth\\': defines the type of the map, i.e., choropleth in this case.\\ncolorscale = ‘Viridis\\': displays a color map (for more color scales, refer\\xa0here).\\nlocations = df[\\'Country\\']: add a list of all countries.\\nlocationmode = \\'country names’: as we have country names in our dataset, so we set location mode to ‘country names’.\\nz: list of integer values that display the power consumption of each state.\\ntext = df[\\'Country\\']: displays a text when hovering over the map for each state element. In this case, it is the name of the country itself.\\ncolorbar = {‘title’ : ‘Power Consumption KWH’}: a dictionary that contains information about the right sidebar. Here, colorbar contains the title of the sidebar.\\n\\nlayout = dict(title = \\'2014 Power Consumption KWH\\',\\r\\n              geo = dict(projection = {\\'type\\':\\'mercator\\'})\\r\\n             )\\r\\n\\r\\n\\n\\n\\xa0\\n\\nlayout\\xa0— a\\xa0Geo object that can be used to control the appearance of the base\\xa0map\\xa0onto which data is plotted.\\n\\nIt is a nested dictionary that contains all the relevant information about how our map/plot should look like.\\nGenerating our plot/map\\n\\nchoromap = go.Figure(data = [data],layout = layout)\\r\\niplot(choromap,validate=False)\\r\\n\\r\\n\\n\\n\\xa0\\n\\nCool! Our choropleth map for the ‘2014 World Power Consumption’ has been generated, and from the above, we can see that each country displays its name and its power consumption in kWh when hovering over each element on the map. The more concentrated the data is in one particular area, the deeper the shade of color on the map. Here ‘China’ has the largest power consumption, and so its color is deepest.\\n\\xa0\\nDensity Maps\\n\\xa0\\nDensity mapping\\xa0is simply a way to show where points or lines may be concentrated in a given area.\\nUsing Density Maps with Python\\nHere, we will be using a worldwide\\xa0dataset\\xa0of earthquakes and their magnitudes.\\nOkay, so let’s get started.\\nImporting libraries\\n\\nimport plotly.express as px\\r\\nimport pandas as pd\\r\\n\\r\\n\\n\\n\\xa0\\nCreating/Interpreting our DataFrame\\n\\ndf = pd.read_csv(\\'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv\\')\\r\\ndf.info()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nHere, we have 4 columns, and all of them have 23412 non-null entries.\\n\\ndf.head()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nPlotting our data\\n\\nfig = px.density_mapbox(df, lat=\\'Latitude\\', lon=\\'Longitude\\', z=\\'Magnitude\\', radius=10,\\r\\n                        center=dict(lat=0, lon=180), zoom=0,\\r\\n                        mapbox_style=\"stamen-terrain\")\\r\\nfig.show()\\r\\n\\r\\n\\n\\n\\xa0\\nlat=\\'Latitude\\': takes in the Latitude column of our data frame.\\nlon=\\'Longitude\\': takes in the Longitude column of our data frame.\\nz: list of integer values that display the magnitude of the earthquake.\\nradius=10: sets the radius of influence of each point.\\ncenter=dict(lat=0, lon=180): sets the center point of the map in a dictionary.\\nzoom=0: sets map zoom level.\\nmapbox_style=\\'stamen-terrain\\': sets the base map style. Here, \"stamen-terrain\" is the base map style.\\nfig.show(): displays the map.\\nMap\\n\\nGreat! Our Density map for the ‘Earthquakes and their magnitudes’ has been generated, and from the above, we can see that it covers all the territories that suffered from the earthquake, and also shows the magnitude of the earthquake of every region when we\\xa0hover\\xa0over it.\\nGeographical plotting using plotly can be a bit challenging sometimes due to its various formats of data, so please refer to this\\xa0cheat sheet\\xa0for all types of syntaxes of\\xa0plotly\\xa0plots.\\n\\xa0\\n\\xa0\\nRelated:\\n\\nExploring the Impact of Geographic Information Systems\\nTop 10 Data Visualization Tools for Every Data Scientist\\n7 Techniques to Visualize Geospatial Data',\n",
       " 'By Matthew Mayo, KDnuggets.\\ncomments\\nTo the chagrin of absolutely no one, 2020 is finally drawing to a close. It has been a rollercoaster of a year, one defined almost exclusively by the COVID-19 pandemic. But other things have happened, including in the fields of AI, data science, and machine learning as well. To that end, it\\'s time for KDnuggets annual year end expert analysis and predictions. This year we posed the question:\\nWhat were the main developments in AI, Data Science, Machine Learning Research in 2020 and what key trends do you see for 2021?\\nLast year\\'s noted main developments and predictions included continued advancements in many research areas, NLP in particular. While there can be debate as to whether 2020\\'s big NLP advancement was as formidable as some may have originally thought (or continue to think), there is no doubt that there was a continued and intense focus on NLP research in 2020. It should not be difficult to surmise that this continues into 2021 as well.\\nTopics such as ethics and diversity were taking center stage in 2019, and this past year they stayed there. There seems to have been a transition from thinking of diversity and ethics and related subjects as periphery concerns in machine learning to viewing them as core considerations alongside technology. Let\\'s hope this trend continues into 2021 and beyond.\\nWhat did our panel come up with as the main developments of 2020, and what do they see as the most likely key trends for 2021? Our group this year consists of Dan Becker, Pedro Domingos, Ajit Jaokar, Ines Montani, Brandon Rohrer, Dipanjan Sarkar, Rosaria Silipo, Rachael Tatman, and Daniel Tunkelang. More so than any other year, we thank our contributors for taking the time out of their busy schedules in these tumultuous times to share their insights with our readers.\\nThis is the first in a series of 3 such posts over the coming week. While they will be split up into research, technology, and industry, there is considerable and understandable overlap between these disciplines, and as such we recommend you check out all 3 as they are published.\\n\\n\\xa0 \\nWithout further delay, here are the 2020 key trends and 2021 predictions from this year\\'s group of experts.\\n\\xa0\\nDan Becker (@dan_s_becker) is Founder of Decision AI, and previously founded Kaggle Learn.\\nML research followed some established themes this year:\\n\\nTransformers: GPT-3 received the most attention of any development this year, and it shows the continued evolution of Transformer models trained on huge corpora. We also saw first successful experiments to use transformers for computer vision, which was historically dominated by convolutional networks.\\nGenerative Models: Research like Vid2Player shows computer-generated video at a level of quality beyond what we\\'ve seen in the past. The social impact of generative models will be huge and hard to predict.\\nReinforcement Learning: I saw less attention to RL in 2020 than I saw in the previous couple years. But the transfer learning across tasks in One Policy To Rule Them All looks hugely promising. I expect this to be less important than GPT-3 over the next couple years, but likely far more important over a longer time horizon. Most people don\\'t realize the huge impact RL will have once it works more reliably.\\n\\n2021:\\n\\nProbabilistic Programming and Bayesian Models: We\\'ve seen a lot of experimentation in new probabilistic programming languages. This reminds me of the experimentation I saw in deep learning frameworks 5 years ago. So I hope probabilistic programming is a key trend in 2021, though it will also require more education for users to take advantage of the new tools.\\nGPT-4: As more people experiment with GPT-3, I think we\\'ll find it falls a little short of most practical usefulness... Extrapolating from recent trends, GPT-4 will be much better and will likely cross that threshold of practical usefulness.\\nGPUs for structured data: The NVIDIA RAPIDS team is creating data science tools that promise a sudden speedup beyond anything we\\'ve seen in the last decade. My sense is that this software isn\\'t yet ready for prime time, but that could come in 2021.\\nAutoML becomes uninteresting: Most data scientists are still tuning parameters through ad hoc experimentation. It\\'s a matter of time until we all use more automatic solutions, and the time may come in the next year.\\nReinforcement learning becomes practically useful: This is what I\\'m most excited about. Conventional machine learning is focused on prediction, but few data scientists optimize the decision layer that translates those predictions into real-world business decisions. This has resulted in models that are accurate without being useful. We\\'ll see a mindset shift in 2021 to use models for optimal decision-making in complex environments.\\n\\n\\xa0\\nPedro Domingos (@pmddomingos) is a Professor in the Dept. of Computer Science & Engineering, University of Washington.\\nTo my mind, the main developments in 2020 were the emergence of graph neural networks and neuro-symbolic AI as major research directions. I think in 2021 we\\'ll see the latter subsume the former: GNNs are a limited form of relational learning, and before long we\\'ll have neuro-symbolic approaches that accomplish all that GNNs do, and then some. After this, where you turn the dial of representational power for specific applications is mainly the usual matter of overfitting control and scalability. At the high end, how far neuro-symbolic AI gets us toward human-level AI is the trillion dollar question.\\n\\xa0\\nAjit Jaokar (@AjitJaokar) is the course director of the \"Artificial Intelligence: Cloud and Edge implementations\" course at the University of Oxford, and is an entrepreneur.\\n2020 was the year of COVID but also of tech. AI matured through MLOps deployments. The Cloud platforms (ex: AWS, Azure, GCP) continue to drive innovation in all areas of AI including AI on Edge devices. I expect to see much more innovation in this space after the acquisition of ARM by Nvidia.\\nIn the AI world, the big trend was NLP (GPT-3 and other models). For 2021, the real question is – would few shot learner models (like GPT-3) change the way models are built? Instead of the traditional sequence of building a model from data reflecting a problem, we could flip it. We can think of just a forward pass with very large models i.e. Model – Problem – Inference. Of course, we need a massive pre-trained model like GPT-3. If this trend does take off – it will be transforming for AI over the next two years.\\nIn 2021, traditional machine learning models could become a commodity in the sense that everyone would be using some form of basic ML or DL. So, we could shift from data science to decision science. The output of data science is a model with a performance metrics (for example accuracy). With decision science, we could take this further by suggesting actions and execute these actions. That means algorithms like reinforcement learning could be a part of 2021 and beyond\\n\\xa0\\nInes Montani (@_inesmontani) is a software developer working on Artificial Intelligence and Natural Language Processing technologies, and the co-founder of Explosion.\\n2020 has been an extraordinary year and while we\\'ve seen many exciting advancements in the field, the most important developments in my opinion have been about consolidation rather than revolution. In previous years, the technology was evolving so quickly that for many companies, it was wise to wait. That calculation has changed now, and there\\'s a much better understanding of what projects are likely to succeed. Building prototypes and applying machine learning to business problems has never been easier but what remains challenging is closing the gap between prototyping and shipping successful projects into production. In 2021, we\\'ll likely keep seeing more focus on the whole lifecycle of a machine learning project: from prototype to production, and from iterative development to ongoing maintenance and monitoring.\\n\\xa0\\nBrandon Rohrer (@_brohrer_) is Principal Data Scientist at iRobot and Instructor at End-to-End Machine Learning.\\nConvolutional and recurrent neural networks are beginning to show they can’t solve every problem as well as we would like. Two papers this year sum up this trend. The Hardware Lottery (https://arxiv.org/abs/2009.06489v1) describes how much serendipity can be involved in which algorithms rise to prominence and become entrenched as an industry standard. And the tour de force paper Underspecification Presents Challenges for Credibility in Modern Machine Learning (https://arxiv.org/abs/2011.03395) casts a harsh light how we have been evaluating models and measuring progress. These are good things. In 2021, if we choose to, we can invest in exploration and in solving new families of problems.\\nAlso, because we’ve been left no choice, we’ve sprung to develop tools and practices for remote instruction, distributed teams, and asynchronous work. The machine learning research environment of 2020 would be unrecognizable to our 2019 selves. In 2021 I predict the quality and quality of online instruction and collaboration will double.\\n\\xa0\\nDipanjan Sarkar is a Data Science Lead at Applied Materials, a Google Developer Expert in Machine Learning, a published author, and consultant.\\nBased on my prediction last year, 2020 has rightly been the year of NLP, with transformers paving the way to solve tough problems like question answering, search and translation with ease. Explainable AI has also started moving out of the ‘inflated expectations’ Gartner Hype Cycle phase, with a lot of practical implementations being available and used to explain complex models across diverse problems and data.\\nFor 2021, I am sure we are going to see the advent of powerful, yet efficient models especially for both vision and natural language processing. We have already seen progress with efficient transformer models like DistilBERT, Reformer and Performer. Deep Learning frameworks like TensorFlow are focusing on ML for mobile and IoT devices with TFLite and TF.js with edge and on-device computing being in-demand.\\nI also foresee more progress in areas pertaining to unsupervised and self-supervised learning in the field of deep learning with methodologies like SimCLR, SimSiam and SwAV which have achieved huge success in pre-training models to give a better performance during the adaptation phase. Last, but not the least, low-code automated machine learning platforms and responsible AI are two other areas to watch out for as we can definitely expect some interest advancements there.\\n\\xa0\\nRosaria Silipo (@DMR_Rosaria) is Principal Data Scientist at KNIME.\\nIn this strange year of 2020, given the uncertainty about the future, attention has focused on getting data science solutions ready to work and produce results: safe deployment, application monitoring, and secure solutions. This trend will probably continue in 2021. \\nDeployment remains the critical phase in a data science project, where all unnoticed mistakes from the previous steps resurface. So, in addition to the classic enterprise features, we are starting to feel the need for productionizing an application from within the training  environment to avoid needless mistakes during the transfer.\\nSome focus in 2021 will also be on the interpretation of the data analysis process, especially in the life sciences via machine learning interpretability (MLI) or eXplainable AI (XAI) techniques for black-box models.\\nOn a side note, I really suspect that if the COVID-isolation persists in many countries around the world, the number of books about machine learning and artificial intelligence will skyrocket.\\n\\xa0\\nRachael Tatman (@rctatman) is Developer Advocate at Rasa working on natural language processing.\\nI know a lot of folks would probably consider GPT-3 to be a major new development in NLP this year, but I\\'d consider it a pretty straightforward extension of existing NLP methods on a scale that\\'s utterly impractical for the vast majority of NLP applications. What I personally find far more exciting is the growing trend of focusing on small, efficient models that still perform well. The first SustainNLP workshop (https://sites.google.com/view/sustainlp2020/home) is a great example of this. From a research perspective, I think finding ways to get really excellent model performance with limited data and compute resources is going to be a huge challenge in the field, but also really rewarding. \\n\\xa0\\nDaniel Tunkelang is an independent consultant specializing in search, discovery, and ML/AI.\\nIn 2020, AI has continued to improve incrementally. We\\'ve seen iterations on transformer-based models for natural language understanding and generation, the most notable being OpenAI\\'s GPT-3. Autonomous vehicles continue to be almost ready for mainstream use. More broadly, AI has moved from being a buzzword to a critical capability for companies in all industries.\\nMeanwhile, 2020 has been dominated by the Covid-19 pandemic. While AI has played a part in the fight against the virus, what\\'s been more interesting is how, because of the pandemic, most people working on or studying machine learning are doing so from home. If the mainstream acceptance of remote work and education persists after the pandemic -- which seems likely -- then we can look forward to two competing trends. On one hand, AI expertise will become truly global rather than tied to specific hubs. On the other hand, technology powerhouses will recruit talent globally, at the expense of smaller regional firms.\\nAnd yet, even as remote work drives the globalization of AI, the growing conflict between the United States and China splinters it. It seems likely that we will spend the next decade in an AI arms race.\\nps. On November 30, the submission deadline for this post, DeepMind researchers announced that their AlphaFold system has solved the Critical Assessment of protein Structure Prediction (CASP) grand challenge by predicting protein folding grand challenge with revolutionary accuracy and speed. It\\'s too early to digest this announcement, but this may indeed turn out to be the biggest AI breakthrough of 2020.\\n\\xa0\\nRelated:\\n\\nAI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020\\nMachine Learning & AI Main Developments in 2018 and Key Trends for 2019\\nAI, Analytics, Machine Learning, Data Science, Deep Learning Technology Main Developments in 2019 and Key Trends for 2020',\n",
       " 'Sponsored Post.\\n\\nDeveloping an AI or a ML model is not a child’s play. It requires lot of knowledge and skills with enriched experience to make the model work successfully in multiple scenarios.\\n\\nAdditionally, you need high-quality computer vision training data especially to train your visual perception based AI model. The most crucial stage in AI development is acquiring & collecting the training data and using this data while training the models. \\nAny mistake while training your model will not only makes your model perform inaccurately but also could be disastrous while making crucial business decisions, especially in certain areas such as Healthcare or Self Driving Cars.\\nWhile training the AI model, multi-stage activities are performed to utilize the training data in the best manner, so that outcomes are satisfying. So, here are the 6 common mistakes you need to understand to make sure your AI model is successful. \\n\\xa0\\n#1 Using Unverified and Unstructured Data\\n\\xa0\\nThe use of unverified & unstructured data is one of the most common mistakes machine learning engineers do in AI developments. The unverified data might have errors such as duplication, conflicting data, lack of categorization, data conflict, errors and other data issues that could create anomalies during the training process.\\nHence, before you use the data for your machine learning training, carefully examine your raw data set and eliminate the unwanted or irrelevant data, helping your AI model work with better accuracy.\\n\\xa0\\n#2 Using the Already Used Data to Test Your Model \\n\\xa0\\nOne should avoid re-using the data that has already been used to test the model. Hence, such mistakes should be avoided. For example, if someone has already learned something and has applied that knowledge to their area of work; using the same learnings on another area of work could lead to one being biased and repetitive in inferencing.\\nSimilarly, in machine learning, the same logic applies, AI can learn with the bulk of datasets to predict the answers accurately. Using the same training data on Models or AI based applications could lead the model to be biased and derive results which are the resultant of their previous learning. Hence, while testing the capabilities of your AI model, it is very important to test using the new datasets that were not used earlier for machine learning training. \\n\\xa0\\n#3 Using the Insufficient Training Data Sets\\n\\xa0\\nTo make your AI model successful you need to use the right training data so that it can predict with highest level of accuracy. Lack of sufficient data for training is one of the primary reasons behind the failure of the model.\\nHowever, depending on the type of AI model or industries, the fields of requirement of training data is varied. For deep learning, you need more quantitative as well as qualitative datasets to make sure it can work with the high precision. \\n\\xa0\\n#4 Making Sure Your AI Model is Unbiased\\n\\xa0\\nIt is not possible to develop an AI model that can give a hundred per cent accurate results in various scenarios. Just like humans, machines could also be biased due to various factors such as age, gender, orientation, and income level etc., which could affect the results one way or another. Hence, you need to minimize this by using statistical analysis to find how each personal factor is affecting the data and AI training data in process.\\n\\xa0\\n#5 Relying on AI Model Learning Independently\\n\\xa0\\nThough, you need experts to train in your AI model, using a huge amount of training datasets. But if AI is using the repetitive machine learning process that needs to be considered while training such models.\\nHere, as a machine learning engineer, you need to make sure that your AI model is learning with the right strategy. To ensure this you must frequently check the AI training process and its results at regular intervals to get the best outcomes.\\nHowever, while developing the machine learning AI, you need to keep asking yourself important questions such as; is your data sourced from a trustworthy reliable source? Does your AI cover a wide demographic and is there anything else affecting the results?\\n\\xa0\\n#6 Not Using the Properly Labelled Datasets\\n\\xa0\\nTo achieve the winning streak while developing an AI model through machine learning you need a well-defined strategy. This will not only help you to get the best outcomes but also to make the machine learning models more reliable among the end-users.\\nThough, mentioned above are the key points you need to keep in mind while training your model. But training data accurately with highest level of precision is highly crucial in making the AI successful and work with the best level of accuracy in various scenarios. If your data is not properly labelled it will affect the performance of the model.\\nIf your machine learning model is Computer Vision-oriented, to get the right training data, image annotation is the precise technique to create such datasets. Getting the right labelled data is another challenge for AI companies while training the model. But there are many companies offering data labeling for machine learning and AI',\n",
       " 'By Derrick Mwiti, Data Scientist.\\ncomments\\nTensorFlow has a large ecosystem of libraries and extensions. If you’re a developer, you can easily add them into your ML work without having to build new functions.\\nIn this article, we will explore some of the TensorFlow extensions that you can start using right away.\\nTo start, let’s check out domain-specific pre-trained models from TensorFlow Hub.\\nLet’s get to it!\\n\\xa0\\nTensorFlow Hub\\n\\xa0\\nTensorFlow Hub is a repository with hundreds of trained and ready-to-use models. You can find models for:\\n\\nnatural language processing\\nobject detection\\nimage classification\\nstyle transfer\\nvideo action detection\\nsound classification\\npitch recognition\\n\\nTo use a model, you first need to identify it at\\xa0tfhub.dev. You’re going to need to check its documentation. For example, here are instructions to load this\\xa0ImageNet classification model.\\n\\nmodel = tf.keras.Sequential([\\r\\n    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/inception_v1/classification/4\")\\r\\n])\\n\\n\\nModels can be used as they are, or you can fine-tune them. The model’s documentation offers instructions on how to do this.\\nFor example, we can fine-tune the above model by passing ‘trainable=True’ to ‘hub.kerasLayer’.\\n\\nhub.KerasLayer(\"https://tfhub.dev/google/imagenet/inception_v1/classification/4\",\\r\\n               trainable=True, arguments=dict(batch_norm_momentum=0.997))\\n\\n\\n\\xa0\\nTensorFlow Model Optimization Toolkit\\n\\xa0\\nThis is a collection of tools that you can use to optimize models for execution and deployment.\\nWhy is this important?\\n\\nit reduces the latency of models on mobile devices,\\nit reduces the cost of cloud, because models become small enough for edge device deployment.\\n\\nOptimizing models might lead to a reduction in accuracy. Depending on the problem, you’ll need to decide if a slightly less accurate model is worth the advantage of model optimization.\\nOptimization can be applied to pre-trained models from tfhub.dev, as well as your own trained models. You can also download optimized models from tfhub.dev.\\nOne of the techniques for model optimization is pruning. In this technique, unnecessary values in the weight tensor are eliminated. This results in smaller models, with accuracy that’s very close to the baseline model.\\nThe first step in pruning a model is to define the pruning parameters.\\nSetting a sparsity of 50% means that 50% of the weights will be zeroed. The ‘PruningSchedule’ is responsible for controlling\\xa0pruning\\xa0during training.\\n\\nfrom tensorflow_model_optimization.sparsity.keras import ConstantSparsity\\r\\npruning_params = {\\r\\n    \\'pruning_schedule\\': ConstantSparsity(0.5, 0),\\r\\n    \\'block_size\\': (1, 1),\\r\\n    \\'block_pooling_type\\': \\'AVG\\'\\r\\n}\\n\\n\\nAfter that, you can prune the entire model using the above parameters.\\n\\nfrom tensorflow_model_optimization.sparsity.keras import prune_low_magnitude\\r\\nmodel_to_prune = prune_low_magnitude(\\r\\n    keras.Sequential([\\r\\n        tf.keras.layers.Dense(128, activation=\\'relu\\', input_shape=(X_train.shape[1],)),\\r\\n        tf.keras.layers.Dense(1, activation=\\'relu\\')\\r\\n    ]), **pruning_params)\\n\\n\\nAn alternative is to use quantization aware training that uses lower-precision, for example 8-bit instead of 32-bit float.\\n\\nimport tensorflow_model_optimization as tfmot\\r\\nquantize_model = tfmot.quantization.keras.quantize_model\\r\\nq_aware_model = quantize_model(model)\\n\\n\\nAt this point, you’ll have a model that’s quantization aware, but not yet quantized.\\nAfter you compile and train the model, you can create the quantized model using the TFLite Converter.\\n\\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\\r\\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\\r\\n\\r\\nquantized_tflite_model = converter.convert()\\n\\n\\nYou can also quantize\\xa0certain layers\\xa0of the model.\\nThe other model optimization strategy is\\xa0weight clustering. In this technique, the number of unique weight values is reduced.\\n\\xa0\\nTensorFlow Recommenders\\n\\xa0\\nTensorFlow Recommenders (TFRS) is a library for building recommender system models.\\nYou can use it for preparing data, formulating the model, training, evaluation, and deployment. This\\xa0Notebook\\xa0contains a full example of how to use TFRS.\\n\\xa0\\nTensorFlow Federated\\n\\xa0\\nTensorFlow Federated (TFF) is an open-source library for machine learning on decentralized data. In federated learning, devices can collaboratively learn from a shared model.\\nThe model will be trained on a server using proxy data. Each device will then download the model and improve it using the data on that device.\\nWhat’s good about this approach is that sensitive user data is never uploaded to the server. One way this has been used is in\\xa0phone keyboards.\\nTensorFlow Federated is made up of two layers:\\n\\nFederated Learning (FL) API\\nFederated Core (FC) API\\n\\nUsing the Federated Learning (FL) API, developers can apply federated training and evaluation on existing TensorFlow models.\\nThe Federated Core (FC) API is a system of low-level interfaces for writing federated algorithms.\\nIf you’re interested, check out official\\xa0TensorFlow Federated tutorials\\xa0to learn more.\\n\\xa0\\nTensorFlow Graphics\\n\\xa0\\nTo build more efficient neural network architectures, you can insert differentiable graphic layers.\\nModeling geometric priors and constraints to neural networks leads to architectures that can be trained more robustly and efficiently.\\nThe combination of computer graphics and computer vision lets us use unlabelled data in machine learning problems. Tensorflow Graphics provides a suite of differentiable graphics, geometry layers and 3D viewer functionalities.\\nHere’s an example of the output produced by a code snippet from\\xa0the official docs.\\n\\nimport numpy as np\\r\\nimport tensorflow as tf\\r\\nimport trimesh\\r\\n\\r\\nimport tensorflow_graphics.geometry.transformation as tfg_transformation\\r\\nfrom tensorflow_graphics.notebooks import threejs_visualization\\r\\n\\r\\n# Download the mesh.\\r\\n!wget https://storage.googleapis.com/tensorflow-graphics/notebooks/index/cow.obj\\r\\n# Load the mesh.\\r\\nmesh = trimesh.load(\"cow.obj\")\\r\\nmesh = {\"vertices\": mesh.vertices, \"faces\": mesh.faces}\\r\\n# Visualize the original mesh.\\r\\nthreejs_visualization.triangular_mesh_renderer(mesh, width=400, height=400)\\r\\n# Set the axis and angle parameters.\\r\\naxis = np.array((0., 1., 0.))  # y axis.\\r\\nangle = np.array((np.pi / 4.,))  # 45 degree angle.\\r\\n# Rotate the mesh.\\r\\nmesh[\"vertices\"] = tfg_transformation.axis_angle.rotate(mesh[\"vertices\"], axis,\\r\\n                                                        angle).numpy()\\r\\n# Visualize the rotated mesh.\\r\\nthreejs_visualization.triangular_mesh_renderer(mesh, width=400, height=400)\\n\\n\\n\\n\\xa0\\nTensorFlow Privacy\\xa0\\n\\xa0\\nThis library is for training machine learning models with training data privacy. Some of the\\xa0tutorials\\xa0provided for this include:\\n\\ntraining a language model with differential privacy\\na convolutional neural network on MNIST with differential privacy\\n\\nDifferential privacy is expressed using\\xa0epsilon and delta.\\n\\xa0\\nTensor2tensor\\n\\xa0\\nThis is a library of models and datasets aimed at making deep learning more accessible and accelerate research in machine learning.\\n\\xa0\\nTensorFlow Probability\\n\\xa0\\nAccording to the official docs:\\n“TensorFlow Probability is a library for probabilistic reasoning and statistical analysis in TensorFlow”\\nYou can use the library to encode domain knowledge, but it also has:\\n\\nsupport for many probability distributions\\ntools for building deep probabilistic models\\nvariational inference and Markov chain Monte Carlo\\noptimizers such as Nelder-Mead, BFGS, and SGLD\\n\\nHere’s an example model based on the Bernoulli distribution:\\n\\nmodel = tfp.glm.Bernoulli()\\r\\ncoeffs, linear_response, is_converged, num_iter = tfp.glm.fit(\\r\\n    model_matrix=features[:, tf.newaxis],\\r\\n    response=tf.cast(labels, dtype=tf.float32),\\r\\n    model=model)\\n\\n\\n\\xa0\\nTensorFlow Extended (TFX)\\n\\xa0\\nTensorFlow Extended (TFX) is a platform that you can use to bring your machine learning pipeline to production.\\nPlus, using\\xa0TensorFlow’s ModelServer\\xa0lets you use a RESTful API to access your model.\\nAssuming you have it installed and configured, the server can be started by running:\\n\\n$ tensorflow_model_server -- rest_api_port=8000 \\r\\n                               -- model_config_file=models.config \\r\\n                               -- model_config_file_poll_wait_seconds=300\\n\\n\\nThe API will be available on port 8000 on localhost. Setting up this server requires some knowledge of server administration.\\n\\xa0\\nTensorBoard\\n\\xa0\\nTensorBoard\\xa0is TensorFlow’s open-source visualization toolkit. You can use it as a callback in your model training in order to track the process. It can be used to track various metrics such as log loss and accuracy. TensorBoard also provides several tools that can be used for experimentation.\\xa0 You can use it to:\\n\\nvisualize images\\ncheck model weights and biases\\nvisualize the architecture of the model\\nsee the performance of your application via profiling\\n\\njust to mention a few.\\nNote:\\xa0As an alternative, you can also track and visualize model training runs,\\xa0 and version your models in\\xa0Neptune.\\nFor instance, here is how you can log your Keras experiments using Neptune.\\n\\nPARAMS = {\\'lr\\': 0.01, \\'epochs\\': 10}\\r\\nneptune.create_experiment(\\'model-training-run\\', params=PARAMS)\\r\\n\\r\\nmodel.fit(x_train, y_train,\\r\\n          epochs=PARAMS[\\'epochs\\'],\\r\\n          callbacks=[NeptuneMonitor()])\\r\\n\\r\\nneptune.log_artifact(\\'model.h5\\')\\n\\n\\n\\nSee Neptune TensorFlow/Keras integration\\n\\xa0\\nTensorFlow Agents\\n\\xa0\\nThis library can be used for designing, implementing, and testing reinforcement learning algorithms. It provides modular components that are extensively tested. Components can be modified and extended.\\nThis\\xa0notebook\\xa0shows how to train a\\xa0DQN (Deep Q Networks)\\xa0agent on the Cartpole environment. The initialization code looks like this:\\n\\nimport tensorflow as tf\\r\\nfrom tf_agents.networks import q_network\\r\\nfrom tf_agents.agents.dqn import dqn_agent\\r\\n\\r\\nq_net = q_network.QNetwork(\\r\\n  train_env.observation_spec(),\\r\\n  train_env.action_spec(),\\r\\n  fc_layer_params=(100,))\\r\\n\\r\\nagent = dqn_agent.DqnAgent(\\r\\n  train_env.time_step_spec(),\\r\\n  train_env.action_spec(),\\r\\n  q_network=q_net,\\r\\n  optimizer=optimizer,\\r\\n  td_errors_loss_fn=common.element_wise_squared_loss,\\r\\n  train_step_counter=tf.Variable(0))\\r\\n\\r\\nagent.initialize()\\n\\n\\n\\xa0\\nFinal thoughts\\n\\xa0\\nIn this article, we explored several libraries that can be used to extend TensorFlow’s functionalities. Try using the code snippets I provided to familiarize yourself with the tools.\\nWe talked about:\\n\\nusing pre-trained models from TensorFlow Hub,\\noptimizing your models using TensorFlow Model Optimization Toolkit,\\nbuilding recommenders using TensorFlow Recommenders,\\ntraining models on decentralized data using TensorFlow Federated,\\ntraining in private mode with TensorFlow Privacy.\\n\\nAnd that’s quite a lot, so choose one of these to start with, and go through the list to see if any tools fit your machine learning workflow.\\n\\xa0\\nBio: Derrick Mwiti is a data scientist who has a great passion for sharing knowledge. He is an avid contributor to the data science community via blogs such as Heartbeat, Towards Data Science, Datacamp, Neptune AI, KDnuggets just to mention a few. His content has been viewed over a million times on the internet. Derrick is also an author and online instructor. He also trains and works with various institutions to implement data science solutions as well as to upskill their staff. You might want to check his Complete Data Science & Machine Learning Bootcamp in Python course.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nThe Best Machine Learning Frameworks & Extensions for Scikit-learn\\nTop Python Libraries for Data Science, Data Visualization & Machine Learning\\nTop Python Libraries for Deep Learning, Natural Language Processing & Computer Vision',\n",
       " 'comments\\nBy Hubert Baniecki, Research Software Engineer at MI2DataLab.\\n\\nI will showcase how straightforward and convenient it is to explain a\\xa0tensorflow\\xa0predictive model using the\\xa0dalex\\xa0Python package. The introduction to this topic can be found in\\xa0Explanatory Model Analysis: Explore, Explain, and Examine Predictive Models.\\nFor this example, we will use the\\xa0data from the\\xa0World Happiness Report\\xa0and predict the happiness scored according to economic production, social support, etc., for any given country.\\n\\n\\nData from the World Happiness Report (Kaggle.com).\\nLet’s first train the basic\\xa0tensorflow\\xa0model incorporating the experimental normalization layer for a better fit.\\n\\nThe next step is to create a\\xa0dalex\\xa0Explainer object, which takes model and data as input.\\n\\nNow, we are ready to explain the model using various methods: model level methods explain the global behavior, while predict level methods focus locally on a single observation from the data. We can start by evaluating model performance.\\n\\n\\nModel performance for the happiness regression task.\\nWhich features are the most important? Let’s compare the two methods, one of which is implemented in the\\xa0shap\\xa0package.\\n\\n\\n\\nComparison of the two Feature Importance methods, one of which is implemented in the shap package.\\nWhat are the continuous relationships between variables and predictions? We use Partial Dependence profiles, which point out that not always the more, the better.\\n\\n\\nPartial Dependence profiles display the continuous relationships between variables and predictions.\\nWhat about the residuals? These plots are useful to visualize where the model is wrong.\\n\\n\\nResidual diagnostics can help assess weaknesses in our model.\\nOne can be more curious about the variable attributions for a specific country,\\n\\n\\nVariable attributions for Poland.\\nor several countries to compare the results.\\n\\n\\nVariable attributions for multiple countries.\\nShould you be interested in surrogate approximation, there is a possibility to produce the\\xa0lime\\xa0package explanations using the unified interface.\\n\\n\\nSurrogate approximation - an explanation from the lime package.\\nFinally, if an interpretable model is needed, we can approximate the black-box with an easy-to-understand decision tree.\\n\\n\\nDecision tree trained on predicted values of the black-box model.\\nI hope that this journey brought you some happiness as it is accessible and user-friendly to explain predictive models nowadays. Of course, there are more explanations, results, and plots in the\\xa0dalex\\xa0toolkit. We prepared various resources listed in the\\xa0package README\\xa0.\\nThe code for this piece is available at\\xa0http://dalex.drwhy.ai/python-dalex-tensorflow.html.\\n\\xa0\\n\\npip install tensorflow dalex shap statsmodels lime scikit-learn\\r\\n\\n\\n\\xa0\\n:)\\nOriginal. Reposted with permission.\\n\\xa0\\nBio: Hubert Baniecki is a\\xa0Research Software Engineer, developing R & Python tools for Explainable AI, and researching ML in the context of interpretability and human-model interaction.\\nRelated:\\n\\nInterpretability, Explainability, and Machine Learning – What Data Scientists Need to Know\\nExplaining the Explainable AI: A 2-Stage Approach\\nExplainability: Cracking open the black box, Part 1',\n",
       " 'By AI21 Labs \\n\\nWe are thrilled to announce the launch of AI21 Studio, our new developer platform where you can use our state-of-the-art Jurassic-1 language models to build your own applications and services. Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers. AI21 Studio is currently in open beta, allowing anyone to sign up and immediately start querying Jurassic-1 using our API and interactive web environment.\\nFrom a technical perspective, Jurassic-1 Jumbo enjoys a slight size advantage relative to GPT-3 (excess 3B parameters), but also, it introduces several conceptual novelties into this arena of huge language models. The depth-to-width ratio of Jurassic-1 Jumbo’s core Transformer architecture was optimized for its size -- It is shallower (76 vs 96 layers) and wider (13824 vs 12288 hidden dimension) than GPT-3, with more computational parameters per layer over fewer layers. This modification is aimed at maximizing the expressivity of the network, following theoretical insights published in the last NeurIPS. From a practical perspective, a shallower and wider network allows more parallelization between compute operations, reducing latency. Moreover, the Jurassic-1 models utilize a unique 250,000-token vocabulary which is not only much larger than most existing vocabularies (5x or more), but also the first to include multi-word tokens such as expressions, phrases, and named entities. Because of this, Jurassic-1 needs fewer tokens to represent a given amount of text, thereby improving computational efficiency and further reducing latency. Check out the white paper for more technical details, as well as a thorough evaluation of our models.\\xa0\\nIn order to help developers scale their applications beyond a proof-of-concept and efficiently serve production-scale traffic, AI21 Studio allows developers to train custom versions of Jurassic-1 models. Training a custom model is easy and requires as few as 50-100 training examples. Once trained, your custom model is served in AI21 Studio and immediately available for your exclusive use.\\nWe created AI21 Studio to democratize access to cutting-edge AI technology. Using Jurassic-1 within AI21 Studio, you can quickly build text-based applications that rival those being dreamed up in the world’s biggest labs, even if you have no prior experience. We’ve been using AI21 Studio internally to power our own applications, and it has propelled our product development immensely. Now it’s your turn.\\nGET STARTED NOW\\nTo learn more, visit our blog posts highlighting different use-cases for Jurassic-1 and demonstrating how to bootstrap a custom model in AI21 Studio or read the AI21 Studio documentation.',\n",
       " 'comments\\nBy Emeli Dral, CTO and Co-founder of Evidently AI\\n\\n\\nImage by Author\\n\\n\\xa0\\nWhen we analyze machine learning model performance, we often focus on a single quality metric. With regression problems, this can be MAE, MAPE, RMSE, or whatever fits the problem domain best.\\nOptimizing for a single metric absolutely makes sense during training experiments. This way, we can compare different model runs and can choose the best one.\\nBut when it comes to solving a real business problem and putting the model into production, we might need to know a bit more.\\xa0How well does the model perform on different user groups? What types of errors does it make?\\nIn\\xa0this post, I will present an approach to evaluating the regression model performance in more detail.\\n\\xa0\\nRegression errors: too much or too little?\\n\\xa0\\nWhen we predict a continuous variable (such as price, demand, and so on), a common-sense definition of error is simple: we want the model predictions to be as close to actual as possible.\\nIn practice, we might care not only about the absolute error value but also other criteria. For example, how well we catch the trend, if there is a correlation between the predicted and actual value — and what is the sign of our error, after all.\\nUnderestimating and overestimating the target value might have different business implications.\\xa0Especially if there is some business logic on top of the model output.\\nImagine you are doing demand forecasting for a grocery chain. Some products are perishables, and delivering too much based on the wrong forecast would lead to waste. Overestimation has a clear cost to factor in.\\n\\n\\nImage by author. Source images from Unsplash:\\xa01,\\xa02.\\n\\n\\xa0\\nIn addition to classic error analysis, we might want to track this error skew\\xa0(the tendency to over- or underestimate) and how it changes over time. It makes sense both when analyzing model quality during validation and in production monitoring.\\nTo explain this concept of analyzing the error bias, let’s walk through an example.\\n\\xa0\\nEvaluating the model performance\\n\\xa0\\nLet’s say we have a model that predicts the demand for city bike rentals. (If you want to play with this use case, this\\xa0Bike Demand Prediction dataset\\xa0is openly available).\\nWe trained a model, simulated the deployment, and compared its performance in “production” to how well it did on the training set.\\nIn practice, we need to know the ground truth for that. Once we learn the actual demand, we can calculate our model’s quality and estimate how far off we are in our predictions.\\nHere, we can see a major increase in error\\xa0between Reference performance in training and current Production performance.\\n\\n\\nScreenshot from the\\xa0Evidently\\xa0report.\\n\\n\\xa0\\nTo understand the quality better, we can look at the error distribution.\\xa0It confirms what we already know: the error increased. There is some bias towards overestimation, too.\\n\\n\\nScreenshot from the\\xa0Evidently\\xa0report.\\n\\n\\xa0\\nThings do not look ideal, and we want to dig deeper into what is going on.\\xa0As do our business stakeholders. Why do these errors happen? Where exactly? Will retraining help us improve the quality? Do we need to engineer new features or create further post-processing?\\nHere is an idea of how to explore it.\\n\\xa0\\nLooking at the edges\\n\\xa0\\nAggregate quality metrics show us the mean performance. However, these are the extreme cases that can often give us helpful information. Let us look directly there!\\nWe can group the predictions where we have high errors and learn something useful from them.\\nHow can we implement this approach?\\nLet’s take each individual prediction and calculate the error. Then, we create two groups based on the types of errors:\\n\\nOverestimation.\\xa0Cases where the model predicts the values that are higher than actual.\\nUnderestimation.\\xa0Cases where the model predicts the values that are lower than actual.\\n\\nLet us limit the size of each group by choosing only 5% of the most extreme examples with the largest error. This way, we have the top-5% of predictions where the model overestimates and the top-5% where the model underestimates.\\nThe rest 90% of predictions are the “majority.” The error in this group should be close to the mean.\\nThat is how we can visualize the proposed segments. That is a sort of situation we’d like to see: most of the predictions are close to the actual values. Analyzing outliers can bring meaningful insight.\\n\\n\\nImage by Author.\\n\\n\\xa0\\nHow can it be useful?\\nLet’s take a time series example.\\xa0If we built a great model and “learned” all the signal from the data, the error should be random. There should be no pattern. Except for a few likely outliers, the error would be close to the average in all groups. Sometimes slightly larger, sometimes smaller. But on average, about the same.\\nIf there is some useful signal in the data that can explain the error, the situation can look differently.\\xa0There can be a large error in specific groups. There can also be a clear skew towards under- or overestimation.\\nIn these cases, the error may be dependent on specific feature values.\\xa0What if we could find and describe the instances where it is higher than usual? That is precisely what we want to investigate!\\n\\xa0\\nSpotting the flaws\\n\\xa0\\nIn our case, we can see that the error both in the over- and underestimation groups are significantly higher than the one in the “majority” group.\\n\\n\\nScreenshot from the\\xa0Evidently\\xa0report.\\n\\n\\xa0\\nWe can then try to investigate and explore the new patterns.\\nTo do that, we look at the objects inside both 5%-groups and see what feature values correspond to them. Feature by feature, if we can.\\nOur goal is to identify if there is a relationship between the specific feature values and high error. To get deeper insight, we also distinguish between over- or under-estimation.\\nImagine that we predict healthcare costs and consistently over-estimate the price for patients of certain demographics? Or, the error is unbiased but large, and our model fails on a specific segment? That is a sort of insight we want to find.\\n\\n\\nImage by Author.\\n\\n\\xa0\\nWe can make a complex (and computationally heavy) algorithm to perform this search for underperforming segments. As a reasonable replacement, we can just do this analysis feature by feature.\\nHow can we do it? Let’s plot the feature distributions and our target demand and color-code the examples where we made high errors.\\nIn our bike demand prediction use case, we can already get some insights. If we plot the “humidity” feature, we can notice that our model now significantly overestimates the demand when the humidity values are between 60 and 80 (plotted to the right).\\nWe saw these values in our training dataset (plotted to the left), but the error was unbiased and similar on the whole range.\\n\\n\\nScreenshot from the\\xa0Evidently\\xa0report.\\n\\n\\xa0\\nWe can notice other patterns, too. For example, in temperature. The model also overestimates the demand when the temperature is above 30°C.\\n\\n\\nScreenshot from the\\xa0Evidently\\xa0report.\\n\\n\\xa0\\nWe can now suspect that something happened to the weather, and new related patterns emerged. In reality, we trained the model using the data from only cold months of the year. When it went to “production,” summer just started. With the new weather came new seasonal patterns that the model failed to grasp before.\\nThe good news is that by looking at these plots, we can see that there seems to be some useful signal in the data. Retraining our model on new data would likely help.\\n\\xa0\\nHow to do the same for my model?\\n\\xa0\\nWe implemented this approach in the\\xa0Evidently\\xa0open-source library. To use it, you should prepare your model application data as a pandas DataFrame, including model features, predictions, and actual (target) values.\\nThe library will work with a single DataFrame or two — if you want to compare your model performance in production with your training data or some other past period.\\n\\n\\nImage by Author.\\n\\n\\xa0\\nThe Regression performance report will generate a set of plots on model performance and an Error Bias table. The table helps explore the relations between the feature values and the error type and size.\\nYou can also quickly sort the features to find those where the “extreme” groups look differently from the “majority.” It helps identify the most interesting segments without manually looking at each feature one by one.\\nYou can read the full docs on\\xa0Github.\\n\\xa0\\nWhen is this useful?\\n\\xa0\\nWe believe this sort of analysis can be helpful more than once in your model lifecycle. You can use it:\\n\\nTo analyze the results of the model test. For example, once you validate your model an offline test or after A/B test or shadow deployment.\\nTo perform ongoing monitoring of your model in production.\\xa0You can do this at every run of a batch model or schedule it as a regular job.\\nTo decide on the model retraining.\\xa0Looking at the report, you can identify if it is time to update the model or if retraining would help.\\nTo debug models in production.\\xa0If the model quality fails, you can spot the segments where the model underperforms and decide how to address them. For example, you might provide more data for the low-performing segments, rebuild your model or add business rules on top of it.\\n\\nIf you want a practical example, here is a\\xa0tutorial\\xa0on debugging the performance of the machine learning model in production: “How to break a model in 20 days”.\\n\\xa0\\nBio: Emeli Dral is a Co-founder and CTO at Evidently AI where she creates tools to analyze and monitor ML models. Earlier she co-founded an industrial AI startup and served as the Chief Data Scientist at Yandex Data Factory. She is a co-author of the Machine Learning and Data Analysis curriculum at Coursera with over 100,000 students.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nA Machine Learning Model Monitoring Checklist: 7 Things to Track\\nMLOps: Model Monitoring 101\\nEvaluating Object Detection Models Using Mean Average Precision',\n",
       " 'Sponsored Post.\\nBy Armand McQueen\\n\\nOne of the big headaches in deep learning is that models take forever to train. As an ML engineer, waiting hours or days for training to complete makes iteratively improving your model a slow and frustrating process. You can speed up model training by using more GPUs, but this raises two challenges:\\n\\nDistributed training is a hassle because it requires changing your model code and dealing with DevOps headaches like server management, cluster scheduling, networking, etc.\\nUsing many GPUs at once can quickly cause your training costs to skyrocket, especially when using on-demand cloud GPUs.\\n\\nIn this blog post, we show how to accelerate fine-tuning the ALBERT language model while also reducing costs by using Determined’s built-in support for distributed training with AWS spot instances. Originally, ALBERT took over 36 hours to train on a single V100 GPU and cost $112 on AWS. With distributed training and spot instances, training the model using 64 V100 GPUs took only 48 minutes and cost only $47! That’s both a 46x performance improvement and a 58% reduction in cost!\\nBest of all, realizing these performance gains and cost reductions required nothing more than changing a few configuration settings. As we detail in the blog post, switching to distributed training and leveraging spot instances in Determined can be done without changing your model code, without needing to understand the details of using spot instances, and with no manual server wrangling required.\\nIn the full article, we show you how we fine-tuned ALBERT on the SQuAD 2.0 dataset (using the Huggingface implementation), and how to save money by training with Determined using Spot Instances. You can read the full article “ALBERT on Determined: Distributed Training with Spot Instances” on our blog, and see the experiment in the Determined repository here.\\xa0\\nTo learn more about Determined and how it can help make your training easier, faster and cheaper, check out our GitHub repo or hop on our community Slack.',\n",
       " 'comments\\nBy Hugo Bowne-Anderson, Head of Data Science Evangelism and VP of Marketing at Coiled\\nThe capability to scale large data analyses is growing in importance when it comes to data science and machine learning, and at a rapid rate. Fortunately, tools like Dask and Coiled are making it easy and fast for folks to do just that.\\nDask is a popular solution for scaling up analyses when working in the PyData Ecosystem and Python. This is the case because Dask is designed to parallelize any PyData library, and hence seamlessly works with the host of PyData tools.\\nScaling up your analysis to utilize all the cores of a sole workstation is the first step when starting to work with a large dataset.\\nNext, to leverage a cluster on the cloud (Azure, Google Cloud Platform, AWS, and so on) you might need to scale out your computation.\\nRead on and we will:\\n\\nUse pandas to showcase a common pattern in data science workflows,\\nUtilize Dask to scale up workflows, harnessing the cores of a sole workstation, and\\nDemonstrate scaling out our workflow to the cloud with Coiled Cloud.\\n\\nFind all the code here on github.\\nNote: Before you get started, it’s important to think about if scaling your computation is actually necessary. Consider making your pandas code more efficient before you jump in. With machine learning, you can measure if more data will result in model improvement by plotting learning curves before you begin.\\n\\xa0\\nPANDAS AND ETL: A COMMON PATTERN IN DATA SCIENCE\\n\\xa0\\nFirst, we’ll use pandas on an in-memory dataset to introduce a common data science pattern. This is a 700 MB subset of the NYC taxi dataset, which is about 10 GB in total.\\nWe want to see the scaling shine bright, so we picked a relatively boring workflow. Now we read in the data, massage it, create a feature, and save it to Parquet (not human-readable but vastly more efficient than CSV).\\n\\n# Import pandas and read in beginning of 1st file\\r\\nimport pandas as pd\\r\\ndf = pd.read_csv(\"data_taxi/yellow_tripdata_2019-01.csv\")\\r\\n\\r\\n# Alter data types for efficiency\\r\\ndf = df.astype({\\r\\n    \"VendorID\": \"uint8\",\\r\\n    \"passenger_count\": \"uint8\",\\r\\n    \"RatecodeID\": \"uint8\",\\r\\n    \"store_and_fwd_flag\": \"category\",\\r\\n    \"PULocationID\": \"uint16\",\\r\\n    \"DOLocationID\": \"uint16\",    \\r\\n})\\r\\n\\r\\n# Create new feature in dataset: tip_ratio\\r\\ndf[\"tip_ratio\"] = df.tip_amount / df.total_amount\\r\\n\\r\\n# Write to parquet\\r\\ndf.to_parquet(\"data_taxi/yellow_tripdata_2019-01.parq\")\\n\\n\\xa0\\nThis took roughly 1 minute on my laptop, a wait time for analysis we can tolerate (maybe).\\nNow we want to perform the same analysis on the dataset at large.\\n\\xa0\\nDASK: SCALING UP YOUR DATA SCIENCE\\n\\xa0\\nThe 10GB size of the data set is more than the RAM on my laptop, so we can’t store it in memory.\\nInstead we could write a for loop:\\n\\nfor filename in glob(\"~/data_taxi/yellow_tripdata_2019-*.csv\"):\\r\\n\\tdf = pd.read_csv(filename)\\r\\n\\t...\\r\\n\\tdf.to_parquet(...)\\n\\n\\xa0\\nHowever, the multiple cores on my laptop aren’t taken advantage of through this method, nor is this a graceful solution. Here comes Dask for single machine parallelism.\\nImporting several aspects of Dask, we’ll spin up a local cluster and launch a Dask client:\\n\\nfrom dask.distributed import LocalCluster, Client\\r\\ncluster = LocalCluster(n_workers=4)\\r\\nclient = Client(cluster)\\r\\nclient\\n\\n\\xa0\\nThen we import Dask DataFrame, lazily read in the data, and perform the ETL pipeline just as we did with pandas before.\\n\\nimport dask.dataframe as dd\\r\\n\\r\\ndf = dd.read_csv(\\r\\n\\t\"data_taxi/yellow_tripdata_2019-*.csv\",\\r\\n\\tdtype={\\'RatecodeID\\': \\'float64\\',\\r\\n   \\t\\'VendorID\\': \\'float64\\',\\r\\n   \\t\\'passenger_count\\': \\'float64\\',\\r\\n   \\t\\'payment_type\\': \\'float64\\'}\\r\\n)\\r\\n\\r\\n# Alter data types for efficiency\\r\\ndf = df.astype({\\r\\n\\t\"VendorID\": \"UInt8\",\\r\\n\\t\"passenger_count\": \"UInt8\",\\r\\n\\t\"RatecodeID\": \"UInt8\",\\r\\n\\t\"store_and_fwd_flag\": \"category\",\\r\\n\\t\"PULocationID\": \"UInt16\",\\r\\n\\t\"DOLocationID\": \"UInt16\",    \\r\\n})\\r\\n\\r\\n# Create new feature in dataset: tip_ratio\\r\\ndf[\"tip_ratio\"] = df.tip_amount / df.total_amount\\r\\n# Write to Parquet\\r\\ndf.to_parquet(\"data_taxi/yellow_tripdata_2019.parq\")\\n\\n\\xa0\\nTaking about 5 minutes on my laptop, we’ll call this tolerable (I guess). But, if we wanted to do something marginally more complex (which we commonly do!), this time would quickly increase.\\nIf I had access to a cluster on the cloud, now would be the time to utilize it!\\nBut first, let’s reflect on what we’ve just worked out:\\n\\nWe used a Dask DataFrame - a large, virtual dataframe divided along the index into various Pandas DataFrames\\nWe’re working on a local cluster, made of:\\n\\nA scheduler (which organizes and send the work / tasks to workers) and,\\nWorkers, which compute those tasks\\n\\nWe’ve launched a Dask client, which is “the user-facing entry point for cluster users.”\\n\\nIn short - the client lives wherever you are writing your Python code and the client talks to the scheduler, passing it the tasks.\\n\\n\\xa0\\n\\xa0\\nCOILED: SCALING OUT YOUR DATA SCIENCE\\n\\xa0\\nAnd now what we’ve been waiting for - it’s time to burst to the cloud. If you had access to cloud resources (like AWS) and knew how to configure Docker and Kubernetes containers, you could get a Dask cluster launched in the cloud. This would be time consuming, however.\\nEnter a handy alternative: Coiled, which we’ll use here. To do so, I\\'ve signed into Coiled Cloud (the Beta is currently free compute!), pip installed coiled, and authenticated. Feel free to follow along and do this yourself.\\n\\npip install coiled --upgrade\\r\\ncoiled login   # redirects you to authenticate with github or google\\n\\n\\xa0\\nWe then perform our necessary imports, spin up a cluster (takes roughly a minute), and instantiate our client:\\n\\nimport coiled\\r\\nfrom dask.distributed import LocalCluster, Client\\r\\ncluster = coiled.Cluster(n_workers=10)\\r\\nclient = Client(cluster)\\n\\n\\xa0\\nNext we import our data (this time from s3), and perform our analysis:\\n\\nimport dask.dataframe as dd\\r\\n\\r\\n# Read data into a Dask DataFrame\\r\\ndf = dd.read_csv(\\r\\n\\t\"s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv\",\\r\\n\\tdtype={\\r\\n    \\t\\'RatecodeID\\': \\'float64\\',\\r\\n   \\t\\'VendorID\\': \\'float64\\',\\r\\n   \\t\\'passenger_count\\': \\'float64\\',\\r\\n   \\t\\'payment_type\\': \\'float64\\'\\r\\n\\t},\\r\\n\\tstorage_options={\"anon\":True}\\r\\n)\\r\\n\\r\\n# Alter data types for efficiency\\r\\ndf = df.astype({\\r\\n\\t\"VendorID\": \"UInt8\",\\r\\n\\t\"passenger_count\": \"UInt8\",\\r\\n\\t\"RatecodeID\": \"UInt8\",\\r\\n\\t\"store_and_fwd_flag\": \"category\",\\r\\n\\t\"PULocationID\": \"UInt16\",\\r\\n\\t\"DOLocationID\": \"UInt16\",    \\r\\n})\\r\\n\\r\\n# Create new feature in dataset: tip_ratio\\r\\ndf[\"tip_ratio\"] = df.tip_amount / df.total_amount\\r\\n\\r\\n# Write to Parquet\\r\\ndf.to_parquet(\"s3://hugo-coiled-tutorial/nyctaxi-2019.parq\")\\n\\n\\xa0\\nHow long did this take on Coiled Cloud? 30 seconds. This is an order of magnitude less time than it took on my laptop, even for this relatively straightforward analysis.\\nIt’s easy to see the power of being able to do this set of analyses in a single workflow. We didn’t need to switch contexts or environments. Plus, it is straightforward to go back to using Dask from Coiled on my local workstation or pandas when we’re done. Cloud computing is great when needed, but can be a burden when it’s not. We just had an experience that was a lot less burdensome.\\n\\xa0\\nDO YOU NEED FASTER DATA SCIENCE?\\n\\xa0\\nYou can get started on a Coiled cluster for free right now. Coiled also handles security, conda/docker environments, and team management, so you can get back to doing data science and focus on your job. Get started today on Coiled Cloud.\\n\\xa0\\nBio: Hugo Bowne-Anderson Hugo Bowne-Anderson is Head of Data Science Evangelism and VP of Marketing at Coiled (@CoiledHQ, LinkedIn). He has extensive experience as a data scientist, educator, evangelist, content marketer, and data strategy consultant, in industry and basic research. He also has experience teaching data science at institutions such as Yale University and Cold Spring Harbor Laboratory, conferences such as SciPy, PyCon, and ODSC and with organizations such as Data Carpentry. He is committed to spreading data skills, access to data science tooling, and open source software, both for individuals and the enterprise.\\nRelated:\\n\\nMachine Learning in Dask\\nWhy and How to Use Dask with Big Data\\nK-means Clustering with Dask: Image Filters for Cat Pictures',\n",
       " 'By Kevin Vu, Exxact Corp.\\ncomments\\nAutograd: The Missing Machine Learning Library\\n\\xa0\\nWait, people use libraries other than TensorFlow and PyTorch?\\n\\xa0\\nAsk a group of deep learning practitioners for their programming language of choice and you’ll undoubtedly hear a lot about Python. Ask about their go-to machine learning library, on the other hand, and you’re likely to get a picture of a two library system with a mix of TensorFlow and PyTorch. While there are plenty of people that may be familiar with both, in general commercial applications in machine learning (ML) tend to be dominated by the use of TensorFlow, while research projects in artificial intelligence/ML\\xa0mostly use PyTorch. Although there’s significant convergence between the two libraries with the introduction of eager execution by default in\\xa0TensorFlow 2.0\\xa0released last year, and the availability of building static executable models using\\xa0Torchscript, most seem to stick to one or the other for the most part.\\nWhile the general consensus seems to be that you should pick TensorFlow for its better deployment and edge support if you want to join a company, and PyTorch for flexibility and readability if you want to work in academic research, there’s more to the world of AI/ML libraries than just PyTorch and TensorFlow. Just like there’s more to AI/ML than just deep learning. In fact, the gradients and tensor computations powering deep learning promise to have a wide-ranging impact in fields ranging from physics to biology. While we would bet that the so-called shortage of ML/AI researchers is exaggerated (and who wants to dedicate their most creative years to\\xa0maximizing ad engagement\\xa0and recommending more addictive newsfeeds?), we expect that the tools of differentiable programming will be increasingly valuable to a wide variety of professionals for the foreseeable future.\\n\\xa0\\nDifferentiable Computing is Bigger than Deep Learning\\n\\xa0\\nDeep learning, the use of many-layered artificial neural networks very loosely based on ideas about computation in mammalian brains, is well known for its impacts on fields like computer vision and natural language processing. We’ve also seen that many of the lessons in hardware and software developed alongside deep learning in the past decade (gradient descent, function approximation, and accelerated tensor computations) have found interesting applications in the absence of neural networks.\\nAutomatic differentiation and\\xa0gradient descent over the parameters of quantum circuits\\xa0offers meaningful utility for quantum computing in the era of Noisy Intermediate-Scale Quantum (NISQ) computing devices (i.e.\\xa0quantum computing devices that are available now). The penultimate step in\\xa0DeepMind’s impressive upset at the CASP13\\xa0protein folding prediction conference and competition used gradient descent applied directly over predicted amino acid positions, rather than a deep neural network as the Google Alphabet subsidiary is well known for. These are just a few examples of the power of differentiable programming unbound by the paradigm of artificial neurons.\\n\\nDeep learning can be categorized as a subspace of the more general differentiable programming. Deep neuroevolution refers to the optimization of neural networks by selection, without explicit differentiation or gradient descent.\\n\\xa0\\nDifferentiable programming is a broader programming paradigm that encompasses most of deep learning, excepting gradient-free optimization methods such as neuroevolution/evolutionary algorithms. Yann LeCun, Chief AI Scientist at Facebook, touted the possibilities of differentiable programming in a\\xa0Facebook post\\xa0(content\\xa0mirrored in a Github gist). To hear LeCun tell it, differentiable programming is little more than a rebranding of modern deep learning, incorporating dynamic definitions of neural networks with loops and conditionals.\\nI would argue that the consequences of widespread adoption of differentiable programming are closer to what Andrej Karpathy describes as\\xa0“Software 2.0”, although he also limits his discussion largely to neural networks. It’s reasonable to argue that software 2.0/differentiable programming is a broader paradigm in its entirety than either LeCun or Karpathy described. Differentiable programming represents a generalization beyond the constraint of neural networks as function approximators to facilitate gradient-based optimization algorithms for a wide range of systems. If there is a Python library that is emblematic of the simplicity, flexibility, and utility of differentiable programming it has to be Autograd.\\n\\xa0\\nCombining Deep Learning with Differentiable Programming\\n\\xa0\\nDifferentiating with respect to arbitrary physical simulations and mathematical primitives presents opportunities for solutions where deep neural networks are inefficient or ineffective. That’s not to say you should throw away all your deep learning intuition and experience. Rather, the most impressive solutions will combine elements of deep learning with the broader capabilities of differentiable programming, such as the work of\\xa0Degrave et al. 2018, whose authors combined a differentiable physics engine with a neural network controller to solve robotic control tasks.\\nEssentially they extended the differentiable parts of the environment beyond the neural network to include simulated robot kinematics. They could then backpropagate through the parameters of the robot environment into the neural network policy, speeding up the optimization process by about 6x to 8x in terms of sample efficiency. They chose to use\\xa0Theano\\xa0as their automatic differentiation library, which prevented them from differentiating through conditional statements, limiting the types of contact constraints they could implement. A differentiable physics simulator built with Autograd or even recent versions of PyTorch or Tensorflow 2.0, which support differentiating through dynamic branching, would have even more possibilities for optimizing a neural network robot controller,\\xa0e.g.\\xa0offering more realistic collision detection.\\nThe\\xa0universal approximation power\\xa0of deep neural networks makes them an incredible tool for problems in science, control, and data science, but sometimes this flexibility is more liability than utility, as anyone who has ever struggled with over-fitting can attest. As a famous quote from John von Neumann puts it: “With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.” (an actual demonstration of this concept can be found in “Drawing an elephant with 4 complex parameters” by Mayer\\xa0et al.\\xa0[pdf]).\\nIn modern machine learning practice, that means being careful not to mismatch your model to your dataset, a feat that for small datasets is all too easy to stumble into. In other words a big conv-net is likely to be overkill for many bespoke datasets with only a few hundred to a few thousand samples. In many physics problems, for example, it will be better to describe your problem mathematically and run gradient descent over the free parameters. Autograd is a Python package well suited to this approach, especially for Pythonicly-inclined mathematicians, physicists, and others who are well-practiced at describing problems at a low level with Python matrix and array computational package NumPy.\\n\\xa0\\nAutograd: Anything you can NumPy, you can differentiate\\n\\xa0\\nHere’s a simple example of what Autograd can do:\\n\\nimport autograd.numpy as np\\r\\nfrom autograd import elementwise_grad as egrad\\r\\n\\r\\nimport matplotlib.pyplot as plt\\r\\n\\r\\nx = np.linspace(-31.4,31.4, 256)\\r\\n\\r\\nsinc = lambda x: np.sin(x) / x\\r\\n\\r\\nplt.figure(figsize=(12,7))\\r\\n\\r\\nplt.title(“sinc function and derivatives”, fontsize=24)\\r\\n\\r\\nmy_fn = sinc\\r\\n\\r\\nfor ii in range(9):\\r\\n\\r\\n    plt.plot(x, my_fn(x), lw=3, label=”d{} sinc(x)/dx{}”.format(ii,ii))\\r\\n\\r\\n    plt.legend(fontsize=18)\\r\\n\\r\\n    plt.axis([-32, 32, -0.50, 1.2])\\r\\n\\r\\n    plt.savefig(“./sinc_grad{}.png”.format(ii))\\r\\n\\r\\n    my_fn = egrad(my_fn) \\n\\n\\xa0\\n\\nDifferentiation with Autograd. In this case Autograd was able to differentiate up to the 7th derivative before running into some numerical stability problems around x=0 (note the sharp olive green spike in the center of the figure).\\n\\xa0\\nAutograd is a powerful automatic differentiation library that makes it possible to differentiate native Python and NumPy code. Derivatives can be computed to an arbitrary order (you can take derivatives of derivatives of derivatives, and so on), and assigned to multiple arrays of parameters so long as the final output is a scalar (e.g. a loss function). The resulting code is\\xa0Pythonic, a.k.a. it is readable and maintainable, and it doesn’t require learning new syntax or style. That means we don’t have to worry about memorizing complex APIs like the contents of\\xa0torch.nn\\xa0or\\xa0tf.keras.layers, and we can concentrate on the details of our problem,\\xa0e.g.\\xa0translating mathematics into code. Autograd+NumPy is a mature library that is maintained but no longer developed, so there’s no real danger of future updates breaking your project.\\nYou\\xa0can\\xa0implement a neural network easily with Autograd, as the mathematical primitives of dense neural layers (matrix multiplication) and convolution (you can easily use Fourier transforms for this, or use\\xa0convolve2d\\xa0from scipy) have relatively fast implementations in NumPy. To try out a simple MLP demonstration on scikit-learn’s diminutive digits dataset, download this\\xa0Github gist, (you may also be interested in studying the\\xa0official example\\xa0in the autograd repository).\\nIf you copy the gist and run it in a local virtual environment you’ll need to\\xa0pip install\\xa0both\\xa0autograd, and\\xa0scikit-learn, the latter for its digits dataset. Once all set up, running the code should yield progress reports like the following:\\n\\nepoch 10, training loss 2.89e+02, train acc: 5.64e-01, val loss 3.94e+02, val accuracy 4.75e-01\\r\\ntotal time: 4.26, epoch time 0.38\\r\\n\\r\\nepoch 20, training loss 8.79e+01, train acc: 8.09e-01, val loss 9.96e+01, val accuracy 7.99e-01\\r\\n\\r\\ntotal time: 7.73, epoch time 0.33\\r\\n\\r\\nepoch 30, training loss 4.54e+01, train acc: 9.20e-01, val loss 4.55e+01, val accuracy 9.39e-01\\r\\n\\r\\ntotal time: 11.49, epoch time 0.35\\r\\n\\r\\n…\\r\\n\\r\\nepoch 280, training loss 1.77e+01, train acc: 9.99e-01, val loss 1.39e+01, val accuracy 9.83e-01\\r\\n\\r\\ntotal time: 110.70, epoch time 0.49\\r\\n\\r\\nepoch 290, training loss 1.76e+01, train acc: 9.99e-01, val loss 1.39e+01, val accuracy 9.83e-01\\r\\n\\r\\ntotal time: 115.41, epoch time 0.43\\n\\n\\xa0\\nThat’s a reasonably good result of 98.3% validation accuracy after just under two minutes of training. With a little tweaking of hyperparameters, you could probably push that performance to 100% accuracy or very near. Autograd handles this small dataset easily and efficiently (while Autograd and NumPy operations don’t run on the GPU, primitives like matrix multiply do take advantage of multiple cores). But if all you wanted to do was build a shallow MLP you could do so more quickly in terms of both development and computational time with a more mainstream and modern machine learning library.\\nThere is some utility in building simple models at a low-level like this where control is prioritized or as a learning exercise, of course, but if a small dense neural network was the final goal we’d recommend you stick to PyTorch or TensorFlow for brevity and compatibility with hardware accelerators like GPUs. Instead let’s dive into something a bit more interesting: simulating an optical neural network. The following tutorial does involve a bit of physics and a fair bit of code: if that’s not your thing feel free to skip ahead to the next section where we’ll touch on some of Autograd’s limitations.\\n\\xa0\\nSimulating an Optical Neural Network with Autograd\\n\\xa0\\nOptical neural networks (ONNs) are an old idea, with the scientific journal Applied Optics running special issues on the topic\\xa0in 1987\\xa0and again\\xa0in 1993. The concept has recently been revisited by academics (e.g.\\xa0Zuo\\xa0et al. 2019) and by startups such as\\xa0Optalysys,\\xa0\\xa0Fathom Computing, and\\xa0Lightmatter\\xa0and\\xa0Lightelligence, the last two of which were spun out of the same lab at MIT by co-authors on a\\xa0high-profile paper published in Nature.\\nLight is an attractive physical phenomenon for implementing neural networks due to the similarity in the mathematics used to describe both neural networks and optical propagation. Thanks to the\\xa0Fourier Transform property of lenses\\xa0and the\\xa0convolution property\\xa0of the Fourier transform, convolutional layers can be implemented with a perturbative element placed after 2 focal lengths and one lens away from an input plane (this is known as a\\xa04f correlator) while a matrix multiply can be implemented by placing the element 2 focal lengths and 1 lens from that. But this isn’t an optics lecture, it’s a coding tutorial, so let’s see some code!\\nTo install the necessary dependencies, activate your desired virtual environment with your environment manager of choice and use\\xa0pip\\xa0to install Autograd and scikit-image if you haven’t already.\\npip install autograd\\npip install scikit-image\\nWe’ll be simulating an optical system that essentially operates as a single-output generator, processing a flat input wavefront by passing it through a series of evenly-spaced phase images. To keep the tutorial relatively simple and the line count down, we will attempt to match only a single target image, shown below (you can download the image to your working directory if you want to follow along). After completing this simple tutorial, you may be inclined to experiment with building an optical classifier, autoencoder, or some other image transformation.\\n\\nNow for some Python, starting with importing the packages we’ll need.\\n\\nimport autograd.numpy as np\\r\\nfrom autograd import grad\\r\\n\\r\\nimport matplotlib.pyplot as plt \\r\\n\\r\\nimport time\\r\\n\\r\\nimport skimage\\r\\n\\r\\nimport skimage.io as sio \\n\\n\\xa0\\nWe’ll use the angular spectrum method to simulate optical propagation. This is a good method for near-field conditions where the aperture size of your lens or beam is similar to the propagation distance. The following function executes angular spectrum method propagation given a starting wavefront and its dimensions, wavelength of light, and propagation distance.\\n\\ndef asm_prop(wavefront, length=32.e-3, \\\\\\r\\nwavelength=550.e-9, distance=10.e-3):\\r\\n\\r\\n    if len(wavefront.shape) == 2:\\r\\n\\r\\n        dim_x, dim_y = wavefront.shape\\r\\n\\r\\n    elif len(wavefront.shape) == 3:\\r\\n\\r\\n        number_samples, dim_x, dim_y = wavefront.shape\\r\\n\\r\\n    else:\\r\\n\\r\\n        print(“only 2D wavefronts or array of 2D wavefronts supported”)\\r\\n\\r\\n    assert dim_x == dim_y, “wavefront should be square”\\r\\n\\r\\n    px = length / dim_x\\r\\n\\r\\n    l2 = (1/wavelength)**2\\r\\n\\r\\n    fx = np.linspace(-1/(2*px), 1/(2*px) – 1/(dim_x*px), dim_x)\\r\\n\\r\\n    fxx, fyy = np.meshgrid(fx,fx)\\r\\n\\r\\n    q = l2 – fxx**2 – fyy**2\\r\\n\\r\\n    q[q<0] = 0.0\\r\\n\\r\\n    h = np.fft.fftshift(np.exp(1.j * 2 * np.pi * distance * np.sqrt(q)))\\r\\n\\r\\n    fd_wavefront = np.fft.fft2(np.fft.fftshift(wavefront))\\r\\n\\r\\n    if len(wavefront.shape) == 3:\\r\\n\\r\\n        fd_new_wavefront = h[np.newaxis,:,:] * fd_wavefront\\r\\n\\r\\n        New_wavefront = np.fft.ifftshift(np.fft.ifft2(\\\\\\r\\n\\r\\n                fd_new_wavefront))[:,:dim_x,:dim_x]\\r\\n\\r\\n    else:\\r\\n\\r\\n        fd_new_wavefront = h * fd_wavefront\\r\\n\\r\\n        new_wavefront = np.fft.ifftshift(np.fft.ifft2(\\\\\\r\\n\\r\\n                fd_new_wavefront))[:dim_x,:dim_x]\\r\\n\\r\\n    return new_wavefront\\n\\n\\xa0\\nInstead of restricting our ONN to either convolution or matrix multiplication operations, we’ll propagate our beam through a series of evenly spaced phase object images. Physically, this is similar to shining a coherent beam of light through a series of thin, wavy glass plates, only in this case we’ll use Autograd to backpropagate through the system to design them so that they direct light from the input wavefront to match a given target pattern at the end. After passing through the phase elements, we’ll collect the light on the equivalent of an image sensor. This gives us a nice nonlinearity in the conversion from a complex field to real-valued intensity that we could use to build a more complex optical neural network by stacking several of these together.\\nEach layer is defined by passing through a series of phase images separated by short distances. This is described computationally as\\xa0 propagation over a short distance, followed by a thin phase plate (implemented as multiplication):\\n\\ndef onn_layer(wavefront, phase_objects, d=100.e-3):\\r\\n    for ii in range(len(phase_objects)):\\r\\n\\r\\n        wavefront = asm_prop(wavefront * phase_objects[ii], distance=d)\\r\\n\\r\\n    return wavefront\\n\\n\\xa0\\nThe key to training a model in Autograd is in defining a function that returns a scalar loss. This loss function can then be wrapped in Autograd’s\\xa0grad\\xa0function to compute gradients. You can specify which argument contains the parameters to compute gradients for the\\xa0argnum\\xa0argument to\\xa0grad, and remember that the loss function must return a single scalar value, not an array.\\n\\ndef get_loss(wavefront, y_tgt, phase_objects, d=100.e-3):\\r\\n    img = np.abs(onn_layer(wavefront, phase_objects, d=d))**2\\r\\n\\r\\n    mse_loss = np.mean( (img – y_tgt)**2 + np.abs(img-y_tgt) )\\r\\n\\r\\n    return mse_loss\\r\\n\\r\\nget_grad = grad(get_loss, argnum=2)\\n\\n\\xa0\\nFirst, let’s read in the target image and set up the input wavefront. Feel free to use a 64 by 64 image of your choosing, or download the grayscale smiley image from earlier in the article.\\n\\n# target image\\r\\ntgt_img = sio.imread(“./smiley.png”)[:, :, 0]\\r\\n\\r\\ny_tgt = 1.0 * tgt_img / np.max(tgt_img)\\r\\n\\r\\n# set up input wavefront (a flat plane wave with an 16mm aperture)\\r\\n\\r\\ndim = 128\\r\\n\\r\\nside_length = 32.e-3\\r\\n\\r\\naperture = 8.e-3\\r\\n\\r\\nwavelength = 550.e-9\\r\\n\\r\\nk0 = 2*np.pi / wavelength\\r\\n\\r\\npx = side_length / dim\\r\\n\\r\\nx = np.linspace(-side_length/2, side_length/2-px, dim)\\r\\n\\r\\nxx, yy = np.meshgrid(x,x)\\r\\n\\r\\nrr = np.sqrt(xx**2 + yy**2)\\r\\n\\r\\nwavefront = np.zeros((dim,dim)) * np.exp(1.j*k0*0.0)\\r\\n\\r\\nwavefront[rr <= aperture] = 1.0 \\n\\n\\xa0\\nNext, define the learning rate, propagation distance, and the model parameters.\\n\\nlr = 1e-3\\r\\ndist = 50.e-3\\r\\n\\r\\nphase_objects = [np.exp(1.j * np.zeros((128,128))) \\\\\\r\\n\\r\\n        for aa in range(32)]\\r\\n\\r\\nlosses = []\\n\\n\\xa0\\nIf you’re familiar with training neural networks with PyTorch or similar librarie\\u200bs, the training loop should look familiar. We call the gradient function we defined earlier (which is a function transformation of the function we wrote to calculate loss), and apply the resulting gradients to the parameters of our model. I found the model to get much better results by updating parameters (phase_objects) by only the phase of the gradient, rather than the raw complex gradient itself. The real-valued phase component of the gradient is accessed by using NumPy’s\\xa0np.angle, and it’s converted back into complex values by\\xa0np.exp(1.j * value).\\n\\nfor step in range(128):\\r\\n    my_grad = get_grad(wavefront, y_tgt, phase_objects, d=dist)\\r\\n\\r\\n    for params, grads in zip(phase_objects, my_grad):\\r\\n\\r\\n        params -= lr * np.exp( -1.j * np.angle(grads))\\r\\n\\r\\n     loss = get_loss(wavefront, y_tgt, phase_objects,d=dist)\\r\\n\\r\\n     losses.append(loss)\\r\\n\\r\\n     img = np.abs(onn_layer(wavefront, phase_objects))**2\\r\\n\\r\\n     print(“loss at step {} = {:.2e}, lr={:.3e}”.format(step, loss, lr))\\r\\n\\r\\n     fig = plt.figure(figsize=(12,7))\\r\\n\\r\\n     plt.imshow(img / 2.0, cmap=”jet”)\\r\\n\\r\\n     plt.savefig(“./smiley_img{}.png”.format(step))\\r\\n\\r\\n     plt.close(fig)\\r\\n\\r\\nfig = plt.figure(figsize=(7,4))\\r\\n\\r\\nplt.plot(losses, lw=3)\\r\\n\\r\\nplt.savefig(“./smiley_losses.png”)\\n\\n\\xa0\\nIf everything worked out you should see monotonically decreasing mean squared error loss and the code will save a series of figures depicting optical network’s output as it gets closer and closer to matching the target image.\\n\\nOptimization of the optical system attempting to match the target image. Each of the numbered images with a blue background is the model output at different training steps. Unsurprisingly for training with a single sample, the loss decreases smoothly over the course of training.\\n\\xa0\\nThat’s it! We’ve simulated an optical system acting as a single-output generator. If you have any trouble getting the code to run, try copying the code from\\xa0this Github gist\\xa0all in one go to prevent introducing typos.\\n\\xa0\\nAutograd Uses and Limitations\\n\\xa0\\nAutograd is a flexible automatic differentiation package that has influenced mainstream machine learning libraries in many ways. It’s not always easy to determine the ancestry of how different ideas influence one another in a rapidly developing space like machine learning. However, the imperative, define-by-run approach features prominently in Chainer, PyTorch, and to some extent TensorFlow versions after 2.0 that feature eager execution. According to\\xa0libraries.io\\xa0ten other Python packages depend on Autograd, including packages for\\xa0solving inverse kinematics,\\xa0sensitivity analysis, and\\xa0Gaussian processes. My personal favorite is the quantum machine learning package\\xa0PennyLane.\\nAutograd may not be as powerful as PyTorch or TensorFlow, and it doesn’t have implementations of all the latest deep learning tricks, but in some ways this can be an advantage during certain stages of development. There aren’t a lot of specialized APIs to memorize and the learning curve is particularly gentle for anyone who is familiar with Python and/or NumPy. It doesn’t have any of the bells and whistles for deployment or scaling, but it is simple and efficient to use for projects where control and customization is important. It’s particularly well-suited to mathematicians and physicists who need to translate abstract ideas from math to code to build arbitrary machine learning or optimization solutions at a low-level of implementation.\\nThe biggest con to using Autograd in our opinion is a lack of support for hardware acceleration. Perhaps there’s no better way to describe this drawback than the 4-year-long discussion on\\xa0this Github issue, which discusses various ways of introducing GPU support. If you worked your way through the optical neural network tutorial in this post you’ll have already noticed that running an experiment with even a modestly sized model could require a prohibitively high amount of computational time.\\xa0Computation speed with Autograd is enough of a drawback that we don’t actually recommend using it for projects much larger than the MLP or ONN generator demonstrations described above.\\nInstead, consider JAX, an Apache 2.0 licensed library developed by Google Brain researchers, including the Autograd developers. JAX combines hardware acceleration and just-in-time compilation for substantial speedups over native NumPy code, and in addition, JAX offers a set of function transformations for automatically parallelizing code. JAX can be slightly more complicated than a direct NumPy replacement with Autograd, but its powerful features can more than make up for that. We’ll compare JAX to Autograd as well as the popular PyTorch and TensorFlow in a future article.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nPyTorch for Deep Learning: The Free eBook\\nBatch Normalization in Deep Neural Networks\\nDeep Learning for Signal Processing: What You Need to Know',\n",
       " 'Sponsored Post.\\n\\nThe U.S. National Institute of Justice’s (NIJ) “Recidivism Forecasting Challenge” (the Challenge) aims to increase public safety and improve the fair administration of justice across the United States. The Challenge offers an opportunity for contestants to win prize money totaling over $700,000 for their development of a recidivism forecasting model using data provided by NIJ. The winning Challenge forecasts will be used to help improve recidivism rates, the likelihood of a past criminal offender to reoffend, and inform policies and practices.\\nIn accordance with priorities set by the U.S. Department of Justice, NIJ supports the research, development, and evaluation of strategies to reduce violent crime, and to protect police and other public safety personnel by reducing recidivism. Results from the Challenge will provide critical information to community corrections departments that may help facilitate more successful re-integration into society for previously incarcerated persons and persons on parole. \\nAs the research, development, and evaluation agency of the U.S. Department of Justice, NIJ invests in scientific research across diverse disciplines to serve the needs of the criminal justice community. NIJ seeks to use and distribute rigorous evidence to inform practice and policy; often relying on data analytic methods to do so. The Challenge aims to improve the ability to forecast recidivism using person- and place-based variables with the goal of improving outcomes for those serving a community supervision sentence. In addition to the Challenge data provided, NIJ encourages contestants to consider a wide range of potential supplemental data sources that are available to community corrections agencies to enhance risk determinations, including the incorporation of dynamic place-based factors along with the common static and dynamic risk factors. \\nThe Challenge will have three categories of contestants: students; individuals/small teams/businesses; and large businesses. NIJ will evaluate all entries on how accurately they forecast the outcome of recidivism. Recidivism is defined in this Challenge as an arrest for a new crime. To receive prize money, (114 total prizes available, up to 15 per contestant/team) winning applicants must provide a comprehensive document detailing the lessons learned about what variables did and did not matter to their final forecasting model and, when applicable, what type of models outperformed other models. Contestants are encouraged to provide additional intellectual property regarding specific techniques, weighting, or other sensitive decisions.\\nThe Challenge uses data from the State of Georgia about persons released from prison to parole supervision for the period January 1, 2013, through December 31, 2015. Contestants will submit forecasts (percent likelihoods) of whether individuals in the dataset recidivated within one year, two years, or three years after release. \\nThe final submission deadline is June 30, 2021, 11:59:59 pm ET.\\nNIJ expects that new and more nuanced information will be gained from the Challenge and help address high recidivism among persons under community supervision. Findings could directly impact the types of factors considered when evaluating risk of recidivism and highlight the need to support people in specific areas related to reincarceration. Additionally, the Challenge could provide guidance on gender specific considerations and strategies to account for racial bias during risk assessment.  \\nTo receive notices on NIJ’s Recidivism Challenge and data science-related resources, subscribe to email updates.',\n",
       " 'comments\\nBy Sharan Kumar Ravindran, Data Science Professional and Author\\n\\n\\nPhoto by\\xa0Glenn Carstens-Peters\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nEvery one of us needs a resume to showcase our skills and experience but how much effort are we putting into it to make it impactful. It is undeniable that resumes play a key role in our job application process. This article will explore some simple strategies to significantly improve the presentation as well as the content of data science resumes.\\n\\xa0\\nFirst, Why is it important to focus on the resume?\\n\\xa0\\nGetting a data science job is becoming very competitive, though the number of opportunities is historically high the number of people applying for these jobs is extremely high as well.\\nFor example, below is a screenshot of a job posting from LinkedIn, this job posting has a total of 1200+ views and if we consider approximately one-tenth apply for the job then it is a total of 120+ applications and this is just one way to apply for the job there would be people applying for this job from other sources, through references and directly as well and hence the total number of applications would be approximately 200+. The same logic would apply to any data science job position hence resume plays a critical role in getting shortlisted.\\n\\n\\nScreenshot from\\xa0LinkedIn\\n\\n\\xa0\\nIn this article, I am going to guide you on building a high-impact resume that can help you in getting shortlisted for the job application. The topics covered in this article are,\\n\\nBasic rules in resume preparation\\nCustomizing your resume and cover letter\\nGoogle’s X-Y-Z formula to make impactful statements\\nTools to help build a stunning resume\\n\\nIf you have a preference for video format,\\xa0check out here.\\n\\xa0\\nBasic rules in resume preparation\\n\\xa0\\nResume formatting\\n\\xa0\\nMost job applications accept resumes in both pdf and word format. But I would suggest you stick to the pdf version as this ensures the formatting is preserved, that is the recruiter sees the resume the same way you see it.\\n\\xa0\\nProfile summary\\n\\xa0\\nProfile summary is key to a resume, consider it as an elevator pitch. It should be persuasive and should cover information like who you are, what are your skills and strengths. This part of the resume will be the main driver for the first impression also in influencing the recruiter’s decision hence spend enough time to ensure it includes the key details about you.\\nMy people include career objectives at the beginning of the resume. I personally advocate removing the career objective from the resume and instead use that space for a better profile summary. Because most recruitment happens based on your achievements, strengths, and skill and not based on your aspirations. So make an intelligent decision and efficiently use the real-estate of your resume especially the start.\\n\\xa0\\nUse bullet points\\n\\xa0\\nMake sure the details you include in your resume are in bullet points, be it the profile summary or professional/project experience. It is very difficult to focus on a long paragraph and hence keeping it simple and in bullet point ensures better readability as demonstrated in the below screenshots.\\n\\n\\nProfile summary in one long paragraph\\n\\n\\xa0\\n\\n\\nProfile Summary in bullet points\\n\\n\\xa0\\nTry to restrict each bullet point to 2–3 lines and make key phrases bold as it helps while scanning through quickly.\\n\\xa0\\nConsistency in format\\n\\xa0\\nThe contents of the resume should be in a consistent format, the titles, subtitles, bullet points, and other texts in the resume should all be in a consistent format. Below are few things that will ensure consistency,\\n\\nPick one font and use it across the resume\\nTitles used in your resume like for highlighting Experience and Education should be in a consistent format. You can choose to use a bigger font but let it be consistent across the resume\\nIf your resume is more than one page then ensure the margin, alignments and spaces are consistent across all the pages\\nYou can choose to capitalize the first words in the titles but then let it be consistent across the resume\\n\\n\\xa0\\nAvoid typos\\n\\xa0\\nAlways check for typographical and grammatical errors as they might turn off the recruiter. While there is a good chance for the typographical and grammatical errors to go unnoticed but when caught they send out wrongs signals like,\\n\\nYou are not detailed enough to catch those mistakes\\nAs a data scientist communication is a key aspect and having spelling or grammatical mistakes is definitely not good\\nFirms are increasingly using automated tools to filter resumes, these tools most likely reject resume with typographical errors\\n\\n\\xa0\\nInclude contact details\\n\\xa0\\nYour contact details are important for the recruiter to contact you hence ensure that you double-check your details. Many people start editing their resumes based on their colleague’s or friend’s resumes in those cases ensure the hyperlinks are also edited when you edit the text. Like when you edit the email id ensure the email in the hyperlink is edited as well.\\n\\xa0\\nInclude links to your profile and portfolios\\n\\xa0\\nEnsure your resume has links to your LinkedIn profile, git repository, and other websites or profiles like Kaggle you would like to highlight to the recruiter\\n\\xa0\\nCustomizing your resume and cover letter\\n\\xa0\\nWhen you are competing with a lot of people on a job application, simple things like customization can be a differentiator and could help you in getting the immediate attention of the recruiter. When I say customizing your resume to the job posting it doesn’t mean completely re-writing your resume to every job you apply but just making a few minor tweaks to ensure your profile highlights the requirement and the expectation the job demands.\\nCustomizing your resume helps you in\\n\\nEnsuring that your resume is tailored to the job posting\\nEnsure your resume passes the automated keyword-based filtering\\nSends out a positive signal to the recruiter since you have done your groundwork\\n\\nThere are few components in your resume that you can customize, they are\\n\\nThe target job title in your resume, as well as the cover letter, should match the job posting\\nEnsure skills highlighted in your resume includes some of the skills requested in the job description\\nMake simple modifications to your profile summary to ensure the expectation mentioned in the job posting are addressed\\nIf you are applying for a job position away from your current city or you are applying for a job that demands traveling then explicitly mention in the resume or the cover letter your willingness to relocate or travel\\n\\n\\xa0\\nGoogle X-Y-Z formula to make impactful statements\\n\\xa0\\nThis is an amazing formula that helps to convert your accomplishments into a high-impact statement. It was first introduced by Laszlo Bock in his article\\xa0here. This is a very effective technique that can be used to write an impactful resume. This formula means,\\n\\n\\nAccomplished “X” as measured by “Y” by doing “Z”\\n\\n\\nI will use some simple examples to exactly explain how this formula can be applied in your data science resume.\\n\\xa0\\nExample 1:\\n“Built a recommendation system”\\nthis is a simple statement, not attractive at all because it doesn’t exactly mention the impact of the use case. We can try to improve it by including details of its impact by using below the statement,\\n“Built a recommendation system that increased revenue by 10%”\\nNow, this is much better than the previous statement but this can be further improved by using the Google X-Y-Z formula as below (X, Y, and Z of the formula are highlighted below)\\n“Built a\\xa0recommendation system (X)\\xa0that helped to increase the\\xa0revenue by 10% and improved the customer engagement (Y)on the platform by using\\xa0Collaborative Filtering Algorithm (Z)”\\n\\xa0\\nExample 2:\\n“Participated in a kaggle competition”\\nthis is again a simple statement which just says you participated in a kaggle competition but doesn’t talk about your performance hence this can be improved by including some details like below,\\n“Finished at 20th Place in a Kaggle competition”\\nThis is now better but we can make it more impactful by using the google X-Y-Z formula like,\\n“Participated in a\\xa0Kaggle competition (X)\\xa0and finished at\\xa020th position out of 1250 teams (Y)\\xa0by\\xa0working with 3 colleagues building an ensemble predictive model (Z)”\\nNow, use this formula to convert your accomplishments into more powerful statements.\\n\\xa0\\nTools to help build a stunning resume\\n\\xa0\\nThere are great tools out there to help you in building a stunning resume. Below are two of my favorites,\\n\\xa0\\nResume.io\\n\\xa0\\n\\nThis is a paid platform to build your resume and cover letter, though it doesn’t require any payment to use the platform and build a resume payment will be required for you to download the resume\\nThey have a lot of resume templates catering to several job categories to help you with getting started\\nWhile this platform supports several templates and has many options to make changes but some parts of the template remain rigid. But I personally feel its better this way as it makes you focus only on things that need attention and it also ensures consistency in your resume\\n\\n\\xa0\\nFlowcv.io\\n\\xa0\\n\\nThis is a free platform to build your resume and cover letter, if you wish to have more than one version of your resume then it is required to make a one-off payment\\nThis platform offers several configurations to make any changes to define how things should look. I feel having more options means more decision to make hence time-consuming and might lead to some inconsistencies\\n\\nThese tools are very helpful in creating stunning resumes.\\n\\xa0\\nTo stay connected\\n\\xa0\\n\\nIf you like this article and interested in similar ones,\\xa0follow me on Medium\\nI teach and talk about various data science topics on my YouTube Channel.\\xa0Subscribe to my channel here.\\nSign up to\\xa0my email list here\\xa0for more data science tips and to stay connected with my work\\n\\n\\xa0\\nBio: Sharan Kumar Ravindran (LinkedIn, GitHub) is a data science professional with over 10 years of experience and has authored two data science-related books.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nData careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space\\nUsing NLP to improve your Resume\\nHow Data Professionals Can Add More Variation to Their Resumes',\n",
       " 'By Terence Shin, Data Scientist | MSc Analytics & MBA student.\\ncomments\\n\\nPhoto by David Clode on Unsplash.\\nLearning data science can be overwhelming. There are hundreds of tools and resources out there, and it’s not always obvious what tools you should be focusing on or what you should learn.\\nThe short answer is that you should learn what you enjoy because data science offers a wide range of skills and tools. That being said, I wanted to share with you what I believe are the top 10 Python libraries that are most commonly used in data science.\\nWith that said, here are the Top 10 Python Libraries for Data Science.\\n\\xa0\\n1. Pandas\\n\\xa0\\nYou’ve heard the saying. 70 to 80% of a data scientist’s job is understanding and cleaning the data, aka data exploration and data munging.\\nPandas is primarily used for data analysis, and it is one of the most commonly used Python libraries. It provides you with some of the most useful set of tools to explore, clean, and analyze your data. With Pandas, you can load, prepare, manipulate, and analyze all kinds of structured data. Machine learning libraries also revolve around Pandas DataFrames as an input.\\nWhere to Learn Pandas\\n\\nKaggle Tutorial\\n75 Pandas Exercises with Solutions\\nPandas Practice Problems\\n\\n\\xa0\\n2. NumPy\\n\\xa0\\nNumPy is mainly used for its support for N-dimensional arrays. These multi-dimensional arrays are 50 times more robust compared to Python lists, making NumPy a favorite for data scientists.\\nNumPy is also used by other libraries such as TensorFlow for their internal computation on tensors. NumPy also provides fast precompiled functions for numerical routines, which can be hard to manually solve. To achieve better efficiency, NumPy uses array-oriented computations, so working with multiple classes becomes easy.\\nWhere to Learn NumPy\\n\\nNumPy.org\\nTutorialsPoint\\n\\n\\xa0\\n3. Scikit-learn\\n\\xa0\\nScikit-learn is arguably the most important library in Python for machine learning. After cleaning and manipulating your data with Pandas or NumPy, scikit-learn is used to build machine learning models as it has tons of tools used for predictive modelling and analysis.\\nThere are many reasons to use scikit-learn. To name a few, you can use scikit-learn to build several types of machine learning models, supervised and unsupervised, cross-validate the accuracy of models, and conduct feature importance.\\nWhere to Learn Scikit-learn\\n\\nScikit-learn tutorial\\nAn Introduction to Machine Learning (Udacity)\\n\\n\\xa0\\n4. Gradio\\n\\xa0\\n\\nImage taken by Gradio (with permission)\\nGradio lets you build and deploy web apps for your machine learning models in as little as three lines of code. It serves the same purpose as Streamlit or Flask, but I found it much faster and easier to get a model deployed.\\nGradio is useful for the following reasons:\\n\\nIt allows for further model validation. Specifically, it allows you to interactively test different inputs into the model.\\nIt’s a good way to conduct demos.\\nIt’s easy to implement and distribute because the web app is accessible by anyone through a public link.\\n\\nWhere to Learn Gradio\\n\\nGetting Started page\\n\\n\\xa0\\n5. TensorFlow\\n\\xa0\\nTensorFlow is one of the most popular libraries of Python for implementing neural networks. It uses multi-dimensional arrays, also known as tensors, which allows it to perform several operations on a particular input.\\nBecause it is highly parallel in nature, it can train multiple neural networks and GPUs for highly efficient and scalable models. This feature of TensorFlow is also called pipelining.\\nWhere to Learn TensorFlow\\n\\nTensorFlow’s website\\nKaggle’s Intro to Deep Learning\\nGoogle’s Intro to TensorFlow\\n\\n\\xa0\\n6. Keras\\n\\xa0\\nKeras is mainly used for creating deep learning models, specifically neural networks. It’s built on top of TensorFlow and Theano and allows you to build neural networks very simply. Since Keras generates a computational graph using back-end infrastructure, it is relatively slow compared to other libraries.\\nWhere to Learn Keras\\n\\nKeras’s Website\\nGoogle’s Intro to Keras\\n\\n\\xa0\\n7. SciPy\\n\\xa0\\nAs the name suggests, SciPy is mainly used for its scientific functions and mathematical functions derived from NumPy. Some useful functions which this library provides are stats functions, optimization functions, and signal processing functions. To solve differential equations and provide optimization, it includes functions for computing integrals numerically. Some of the applications which make SciPy important are:\\n\\nMulti-dimensional image processing\\nAbility to solve Fourier transforms, and differential equations\\nDue to its optimized algorithms, it can do linear algebra computations very robustly and efficiently\\n\\nWhere to Learn SciPy\\n\\nSciPy Website\\nGuru99 Tutorial\\n\\n\\xa0\\n8. Statsmodels\\n\\xa0\\nStatsmodels is a great library for doing hardcore statistics. This multifunctional library is a blend of different Python libraries, taking its graphical features and functions from Matplotlib, for data handling, it uses Pandas, for handling R-like formulas, it uses Pasty, and is built on NumPy and SciPy.\\nSpecifically, it’s useful for creating statistical models, like OLS, and also for performing statistical tests.\\nWhere to Learn Statsmodels\\n\\nStatsmodels: Getting Started\\n\\n\\xa0\\n9. Plotly\\n\\xa0\\nPlotly is definitely a must-know tool for building visualizations since it is extremely powerful, easy to use, and has a big benefit of being able to interact with the visualizations.\\nAlong with Plotly is Dash, which is a tool that allows you to build dynamic dashboards using Plotly visualizations. Dash is a web-based python interface that removes the need for JavaScript in these types of analytical web applications and allows you to run these plots online and offline.\\nWhere to Learn Plotly\\n\\nBeginner Visualizations in Plotly\\nAdvanced Visualizations in Plotly\\n\\n\\xa0\\n10. Seaborn\\n\\xa0\\nBuilt on the top of Matplotlib, Seaborn is an effective library for creating different visualizations.\\nOne of the most important features of Seaborn is the creation of amplified data visuals. Some of the correlations that are not obvious initially can be displayed in a visual context, allowing Data Scientists to understand the models more properly.\\nDue to its customizable themes and high-level interfaces, it provides well-designed and extraordinary data visualizations, hence making the plots very attractive, which can, later on, be shown to stakeholders.\\nWhere to Learn Seaborn\\n\\nElite Data Science\\nSeaborn User Guide\\n\\n\\xa0\\nRelated:\\n\\nMore Data Science Cheatsheets\\nAre You Still Using Pandas to Process Big Data in 2021? Here are two better options\\nData Science Learning Roadmap for 2021',\n",
       " 'Sponsored Post.\\n\\n\\xa0\\nAlthough data engineers and data scientists have overlapping skill sets, they fulfill different roles within the fields of big data and AI system development. Data scientists develop analytical models, while data engineers deploy those models in production. As such, data scientists focus primarily on analytics, and data engineers focus more heavily on programming.\\nTo launch your data career, you’ll need both theoretical knowledge and applied skills. Bootcamp programs like Springboard’s Data Science Career Track and Data Engineering Career Track can help make you job-ready through hands-on, project-based learning and one-on-one mentorship. Wondering which data career path is right for you? Read on to find out.\\xa0\\xa0\\n\\xa0\\nWhat Do Data Engineers Do?\\n\\xa0\\n\\xa0\\nData engineers create and maintain key data infrastructures like databases, data warehouses, and data pipelines. Data engineers also prepare data for production by converting raw, unstructured data into a structured format that can be analyzed and interpreted.\\xa0\\nThe work of data engineers is foundational to big data analytics. Data engineers construct data pipelines that capture data from users, SaaS platforms, and other data producers. Data pipelines process this data in real-time and store it in warehouses for analysis. This process is referred to as ETL (extract, transform, load).\\nThe responsibilities of a data engineer vary depending on organizational size. A data engineer at a small company might build data ecosystems and manage the entirety of the data flow, similar to a full-stack data scientist. At a mid-sized company, data engineers craft custom tools to support big data analytics. At large companies that handle large, complex volumes of data, data engineers often focus on optimizing ETL processes.\\xa0\\xa0\\xa0\\n\\xa0\\nWhat Do Data Scientists Do?\\n\\xa0\\n\\xa0\\nData scientists analyze and interpret data to solve business problems. Initially, data scientists explore data and conduct market research in order to formulate business questions around a specific trend or pain point. Data scientists must then frame business questions as data analytics problems.\\xa0\\nTo identify critical patterns within a data set, data scientists use advanced analytical techniques powered by machine learning and statistics. Data scientists build models to establish relationships between data objects. Predictive models forecast future events based on historical data, while prescriptive models recommend actionable changes in business strategy based on current and historical data.\\nData scientists must also interpret the results of their analyses to design data-driven business solutions. When data scientists present their findings to stakeholders, they must build a cohesive narrative that communicates the meaning of their results and how those results can inform business strategy.\\xa0\\n\\xa0\\nKey Data Engineering Skills\\xa0\\n\\xa0\\n\\xa0\\nData engineers need a robust software engineering foundation and must use programming knowledge to deploy models, build data pipelines, and orchestrate data warehousing solutions. Python, Java, and Scala are three of the top programming languages most commonly used by data engineers.\\xa0\\nData engineers must also be able to manipulate database management systems, which facilitate information storage and retrieval. Data engineers use SQL to build and manage relational database systems.\\xa0\\nData engineers also need to understand the basics of distributed systems and demonstrate fluency in Hadoop, which is a framework that enables distributed processing of vast data sets. A strong understanding of data APIs is also a must. Software applications use APIs to access and retrieve data, and data engineers build APIs in databases so that data scientists can query the data.\\xa0\\nFinally, data engineers use cursory machine learning knowledge to understand the needs of data scientists, deploy models in production more efficiently, and build improved data pipelines.\\xa0\\n\\xa0\\nKey Data Science Skills\\n\\xa0\\n\\xa0\\nData scientists have strong programming skills and a solid understanding of statistics. Python is known as the lingua franca of data science, and data scientists use this popular programming language to write code and use powerful Python-based tools. Data scientists also use R to manipulate data, implement machine learning algorithms, and conduct statistical analysis. Data scientists also use SQL to read, retrieve, and add data to databases.\\nMachine learning is also a key data science skill. Data scientists use algorithms to clean, categorize, and analyze large data sets. Machine learning combines computer science and statistics, and machine learning models help data scientists make data-driven predictions and recommendations.\\xa0\\nData scientists must also be well-versed in data visualization, which uses charts, graphics, maps and more to represent data to stakeholders. Data scientists must also be able to create coherent narratives that show how their findings impact an organization’s business goals.\\xa0\\n\\xa0\\nTop Tools for Data Engineers\\n\\xa0\\n\\xa0\\nData engineers need to be proficient with distributed processing technologies and tools used to work with data at scale. Top tools for data engineers include:\\xa0\\n\\nApache Hadoop and Apache Spark. Hadoop is a major big data tool that enables batch processing of vast datasets across servers. Spark is a data processing engine that enables stream processing.\\xa0\\nAmazon Web Services/Redshift. Data warehousing applications like AWS are built to show a long-range view of data over time.\\xa0\\nMicrosoft Azure. Data engineers use this cloud technology to build data analytics systems at scale.\\xa0\\nC++. Data engineers use this programming language to rapidly compute large data sets quickly in the absence of predefined algorithms.\\xa0\\n\\n\\xa0\\nTop Tools for Data Scientists\\xa0\\n\\xa0\\n\\xa0\\nData scientists need a strong command of analytical and data visualization tools, including:\\xa0\\n\\nTableau. This data viz software allows data scientists to create interactive visualizations.\\xa0 Tableau can manage large amounts of data and interface with multiple data sources.\\xa0\\nJupyter. This interactive computational notebook can be used for writing live code, cleaning data, data, viz, and more.\\nApache Hadoop. Hadoop can store large data sets and stream the data to applications like MapReduce, which handle data analytics.\\xa0\\nScikit-learn. This predominantly Python-based machine learning library offers features like data classification, regression, clustering, preprocessing, and more.\\xa0\\n\\n\\xa0\\nReady to launch your data career?\\xa0\\n\\xa0\\n\\xa0\\nIf you want to pivot into a career in data science or data engineering, Springboard’s Data Science Career Track or Data Engineering Career Track will give you the skills you need to get hired. Apply today',\n",
       " 'comments\\nBy Manjesh Gupta, Associate Manager - AI/Machine Learning at Virtusa.\\n\\nOur present human society is a product of millions of years of biological evolution and thousands of years of social evolution. Everything has a history. We make beliefs about people or things based on our accumulated knowledge. In such a scenario, it is quite natural that some of our beliefs are prejudiced because, at times, we do not have enough information. Gordon Allport defines “prejudice” as a “feeling, favorable or unfavorable, toward a person or thing, prior to, or not based on, actual experience.” It is often said that prejudices exist and will continue to exist. The real question is whether we as individuals or a society are willing to change our prejudiced beliefs when presented with counter-evidence. In 1953, Albert Einstein wrote in an essay, “Few people are capable of expressing with equanimity opinions which differ from the prejudices of their social environment. Most people are even incapable of forming such opinions.”\\nIn a social setting, these prejudiced beliefs manifest themselves as attitude or behavior, favorable or unfavorable, towards an individual or a group, based on their sex, gender, social class, race, ethnicity, language, political affiliation, sexuality, religion, and other personal characteristics. In such cases, generally, the group identity of an individual or sub-group takes precedence over the individual identity. We know that we behave in a prejudiced manner (which may not even be necessarily wrong at times).\\nDo AI algorithms reproduce this human behavior?\\nLet us examine a few cases.\\nIf you ask some of the natural language processing algorithms – “Man is to Computer Programmer as Woman is to ___________?” It may answer “Homemaker.”\\xa0The word-embeddings used in such algorithms are known to reflect gender (and other biases) for quite some time now.\\xa0This paper\\xa0examined the “word2vec” embeddings to show the presence of gender stereotypes. The paper also suggests a method to neutralize the bias.\\nThe\\xa0Gender Shades project\\xa0showed that Facial recognition systems from IBM, Microsoft, and Face++ are biased against women and “darker” subjects in terms of accuracy of recognition. These algorithms were, on average, around 15% less accurate for female and “darker” subjects. Recently,\\xa0an algorithm designed\\xa0to generate “high-definition” faces from pixelated images generated an output of a “white” person when a pixelated face of Barrack Obama was used as input.\\nIn an extremely alarming use case of artificial intelligence, an algorithm that suggested the risk of a person to commit a crime again\\xa0was found to be biased\\xa0against “black” people. This algorithm was being used by judicial systems in the United States. Recently, researchers from Harrisburg University\\xa0claim to have built a deep learning algorithm\\xa0that can predict if a person is a criminal based solely on a picture of their face. This paper was supposed to be published by Springer Nature. Researchers and experts from various fields joined a petition to stop its publication, saying that predicting \"criminality\" is not an exact science. Later,\\xa0Springer clarified\\xa0that they had already rejected the paper before the petition started.\\nSo, what causes AI algorithms to exhibit this behavior?\\nIn a way, the results of these algorithms hold a mirror to human society. They reflect and perhaps even amplify the issues already present. We know that these algorithms need data to learn. Their predictions are only as good as the data they are trained on and the goal they are set to achieve.\\nThe data needed to train these algorithms is huge (think millions and above). Suppose we are trying to develop an algorithm to identify cats and dogs from pictures. Not only do we need thousands of pictures of cats and dogs, but they should be labeled (say the cat is class 0 and dog is class 1) so that the algorithm can understand. We can download these images off the internet (the ethics of which is questionable), but still, they need to be labeled manually. Now, consider the complexity and effort required to correctly label a million images in one thousand classes. Often this labeling task is done by “cheap labor” who may or may not have the motivation to do it correctly, or they simply make mistakes.\\nAnother problem in the data set is that of class imbalance. Let’s say we used ten thousand images of dogs and only one thousand images of cats to train our above algorithm. This may not be the actual proportion of the cat and dog populations in the real world, and so the class cat is underrepresented. These problems are generally referred to as “dataset bias.”\\nThe second issue in AI algorithms is feature selection. They classify examples based on different features. Let us say we are making an algorithm to predict credit risk for incoming loan customers for a bank. We try to make an unbiased algorithm and not use features like race, gender, caste, ethnicity, etc., in the algorithm. However, we include ZIP code as a feature, which seems rather harmless. Many studies have shown that ZIP code can be used as a proxy for socioeconomic status. If we try to look carefully around us, we may find that certain ZIP codes have the majority of people of a particular class, color, ethnicity, caste, etc. In this case, even though we tried to develop an unbiased algorithm, we unintentionally introduced social bias by using ZIP code as a feature. At times, it may be difficult to identify which feature introduces unintentional social bias. Also, based on selected features, there may be a trade-off between algorithm accuracy and social bias introduced into the algorithm.\\nThird, human beings live in a time dimension. They do not necessarily stay the same their entire lives. A person born poor can become rich while another rich person may squander all their wealth. A convicted criminal may change to become a better person while another “role model” person may commit horrific crimes in the future. AI algorithms (perhaps, just like human beings) try to make sense of the future based upon the presumption that what happened in the past will somehow repeat. This may be true for some (or maybe a lot of) phenomena and people. However, it is certainly not true for all phenomena or all people. Sure it takes time for people and culture to change (for better or worse), but they never remain static.\\nThese AI algorithms are not aware of the context, their predictions, or the consequences of their prediction. At present, they are mostly dependent on human beings for training data and what exactly to derive from that data. Here is a brief\\xa0TED talk\\xa0which explains how AI algorithms learn.\\nThis social bias introduced in AI algorithms leads to loss of social and economic opportunity and dignity in many cases. Especially with the widespread use of such algorithms, it becomes critical to examine the problems with them. The debate on this topic often leads to much deeper questions. How do we define and create a “fair” and “balanced” dataset? How do we ensure that all people developing AI algorithms use fair and balanced datasets?\\xa0When a human being makes an error in judgment or action, they can be held accountable for it (or maybe not, or maybe it depends on how rich they are). Can we make an AI algorithm accountable for its errors, and how? What\\xa0problems should not be solved\\xa0using AI and should better be left to human judgment for now? These questions, among others, are tackled in the field of study called\\xa0Ethics of Artificial Intelligence.\\nPersonally, I believe that it is impossible to completely eliminate our prejudices, but as human beings, we should reflect deeply on how we can minimize them in ourselves and our creations.\\n[* The words \"bias,\" \"social bias,\" and \"prejudice\" in this article are used in a social sense and should not be confused with the mathematical \"bias\" of a machine learning algorithm.]\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Manjesh Gupta\\xa0has\\xa0nine years of experience across Artificial Intelligence, Machine Learning, Data Analytics, Research, IT Project Management, Stakeholder Management, Change Management, and Capacity Building, with technical ML/DL skills in\\xa0NLP, Risk Analytics, Computer Vision, and Time Series Forecasting.\\nRelated:\\n\\nTowards a Responsible and Ethical AI\\nHow to Create Unbiased Machine Learning Models\\nEthics, Fairness, and Bias in AI',\n",
       " 'By Ahmad Anis, Machine learning and Data Science Student.\\ncomments\\n\\nPhoto by\\xa0Harley-Davidson\\xa0on\\xa0Unsplash.\\nWhile working with pandas, if you have encountered a large dataset, then you might have thought of an alternative, especially when your machine is not strong. Pandas is really good for small/average-sized datasets, but as data gets bigger, it does not perform as well as it performs on simple and smaller datasets.\\nHere you can see the comparison of Pandas with another library\\xa0modin\\xa0on reading the dataset from a CSV file.\\n\\nSimilarly, a very common problem pandas users often go through is the dead jupyter kernel due to out of memory. The computations are expensive, and the CPU is not strong enough to handle those.\\nIn this article, you are going to learn about Vaex, a Python library that is similar to Pandas, how to install it, and some of its important functions that can help you in performing different tasks.\\n\\xa0\\nIntroduction to Vaex\\n\\xa0\\nVaex is a python library that is an out-of-core dataframe, which can handle up to 1 billion rows per second. 1 billion rows. Yes, you read it right, that too, in a second. It uses memory mapping, a zero-copy policy which means that it will not touch or make a copy of the dataset unless explicitly asked to. This makes it possible to work with datasets that are equal to the size of your hard drive. Vaex also uses lazy computations for the best performance and no memory wastage.\\nInstallation\\nYou can install vaex using\\xa0pip:\\n\\npip install vaex\\r\\n\\r\\n\\n\\n\\xa0\\nOr, if you use anaconda, you can use\\xa0conda\\xa0to install.\\n\\nconda install -c conda-forge vaex\\r\\n\\r\\n\\n\\n\\xa0\\nGetting Started\\nWe will import both vaex and pandas to compare the performance.\\n\\nimport vaex\\r\\nimport pandas as pd\\r\\n\\r\\n\\n\\n\\xa0\\nWe will create an artificial dataset and read it both using Pandas and Vaex to see the difference in the performance.\\n\\nimport numpy as np\\r\\nn_rows = 1000000\\r\\nn_cols = 20\\r\\ndf = pd.DataFrame(np.random.randint(100000000, 1000000000, size=(n_rows, n_cols)), columns=[‘c%d’ % i for i in range(n_cols)])\\r\\n\\r\\n\\n\\n\\xa0\\nThis dataset contains 1000,000 rows and 20 columns.\\nLet’s save it in a\\xa0csv\\xa0file.\\n\\ndf.to_csv(‘dataset.csv’, index=False)\\r\\n\\r\\n\\n\\n\\xa0\\nVaex can work well with both CSV files and hdf5 files. To read a CSV file, we can use\\xa0vaex.from_csv\\xa0function. This function can read a CSV file and optionally convert it to HDF5 format.\\nIf you are working with the jupyter notebook, you can use\\xa0%%time\\xa0magic command to check the execution time.\\n\\n%%time\\r\\nvaex_df = vaex.from_csv(‘dataset.csv’,convert=True, chunk_size=5_000)\\r\\n\\r\\n\\n\\n\\xa0\\nYou can check the execution time, \\xa0which is 15.8ms. If the CSV file is large, you can use\\xa0chunk_size\\xa0argument to read the file in chunks.\\n\\nYou can see that it is taking about 15.8 ms total to read the file, which is around 200 MBs.\\nThis has created an hdf5 file too. Let us read that using vaex.\\n\\n%%time\\r\\n\\r\\nvaex_df = vaex.open(‘dataset.csv.hdf5’)\\r\\n\\r\\n\\n\\n\\nYou can see that vaex has read the simple hdf5 file in 20.1 ms total with 19.4 ms CPU time.\\nLet us read the same file using pandas.\\n\\n%%time\\r\\n\\r\\ndf_test = pd.read_csv(\\'dataset.csv\\')\\r\\n\\r\\n\\n\\n\\nAnd you can see that it took pandas 2.19 s total to read the same file, which shows a big performance difference between vaex and pandas.\\nIf we add the same chunk size to both vaex and pandas, we can see that vaex is still very fast as compared to pandas.\\n\\nHere vaex read the data in 28.6 µs which is equal to 0.02 ms, whereas pandas read the same file in 4.41 ms total, which is a huge performance gap.\\n\\xa0\\nImportant Vaex Functions\\n\\xa0\\nFunctions related to Opening/Reading the dataset\\n(1) Open\\nUsing the Open function, you can open a dataframe by giving its path. You can convert your dataframe to other formats such as\\xa0hdf5\\xa0format using the Open function. This function can take a path or a list of paths to read the dataset. Some examples of it are given.\\n\\n>>> df = vaex.open(\\'table1.hdf5\\')\\r\\n>>> df = vaex.open(\\'datafile#*.csv\\', convert=\\'bigdata.hdf5\\')\\r\\n\\r\\n\\n\\n\\xa0\\n(2) Open Many Files\\nOften you have data distributed in many files, and you have to open them separately. Vaex provides\\xa0open_many\\xa0function, which can open the list of filenames, and return a DataFrame with all DataFrames concatenated at a fast speed.\\n\\nimport vaex\\r\\n\\r\\ndf_concat = vaex.open_many([\\'file1.csv\\', \\'file2.csv\\'])\\r\\n\\r\\n\\n\\n\\xa0\\n(3) Concat\\nConcat is a function in vaex that can concatenate the given list of data frames. For example:\\n\\nnew_df = vaex.concat([df1, df2, df3])\\r\\n\\r\\n\\n\\n\\xa0\\nThis will concatenate the list of data frames into a new dataframe.\\n(4) Creating a Dataframe using Pandas DataFrame\\nThis function is used to create an in-memory dataframe using a pandas dataframe.\\n\\nimport vaex\\r\\n\\r\\nimport pandas as pd\\r\\n\\r\\ndf = pd.read_csv(‘test.csv’) #pandas dataframe\\r\\n\\r\\ndf_vaex = vaex.from_pandas(df) #in-memory vaex df\\r\\n\\r\\n\\n\\n\\xa0\\n(5) DataFrame from Array\\nThis function takes in a list or an array and converts it into a vaex dataframe.\\n\\ntext = [‘Testing’, ‘lowercase’, ‘UPPERCASE’, ‘test_example’]\\r\\n\\r\\ndf = vaex.from_arrays(text=text)\\r\\n\\r\\n\\n\\n\\nImportant Functions related to Statistics and Aggregation\\n(1) Finding Correlation\\nOftentimes, you need to find the correlation, vaex provides an easy way to calculate the correlation.\\n\\ndf.correlation(x, y=None, binby=[], limits=None, shape=128, sort=False, sort_key=<ufunc \\'absolute\\'>, selection=False, delay=False, progress=None)\\r\\n\\r\\n\\n\\n\\xa0\\nHere vaex calculates the correlation between x and y, possibly on a grid defined by\\xa0binby. Here\\xa0x\\xa0and\\xa0y\\xa0are 2 expressions.\\nLet\\'s say you have a DataFrame, with 5 columns as follows.\\n\\nYou want to calculate the correlation based on the following expressions.\\nx = \"c0**2+c1**2+c2**2+c3**2\"\\xa0\\nand\\ny =\"-log(c4+c5)\",\\nthen it will calculate the correlation coefficient using the following formula cov[x,y]/(std[x]*std[y]) between x and y.\\n\\nvaex_df.correlation(x=\"c0**2+c1**2+c2**2+c3**2\", y=\"-log(c4+c5)\")\\r\\n\\r\\n\\n\\n\\xa0\\n(2) Count\\nThis function counts the number of Not Null values, or all values if the\\xa0expression\\xa0provided is\\xa0*\\xa0or\\xa0None.\\n\\nvaex_df.count(\\'*\\') #counts all values\\r\\n\\r\\n\\n\\n\\n\\nvaex_df.count() #counts not null values only\\r\\n\\r\\n\\n\\n\\n(3) Covariance\\nCreating a covariance matrix is another important trick to get statistical insights from a dataset. Using vaex, you can quickly create a covariance matrix by using\\xa0covar\\xa0function on a DataFrame.\\nYou need to provide two expressions, x and y, that will calculate the covariance cov[x,y] possibly on-grid defined by binby. This function will return a scalar if no binby is provided.\\n\\nvaex_df.covar(\"c0**2+c1**2+c2**2\", \\'-log(c4)\\', binby=\\'c4\\')\\r\\n\\r\\n\\n\\n\\nCovariance Matrix.\\n(4) Describe\\nJust like pandas, the\\xa0describe\\xa0function gives a statistical description of the dataset, including the count, mean, standard deviation, minimum and maximum value.\\n\\nvaex_df.describe()\\r\\n\\r\\n\\n\\n\\nSimilarly, there are functions related to min, max, std, median, mode, min-max, and other important statistical functions, which you can explore in the documentation\\xa0here.\\n(5) Groupby\\nVaex can also do fast group-by aggregations. The output of it is a vaex DataFrame. Let\\'s see an example.\\n\\ngender = [‘male’,’male’,’female’,’male’,’female’]\\r\\n\\r\\nweight = [95, 87, 74, 79, 65]\\r\\n\\r\\nvaex_df = vaex.from_arrays(gender=gender, weight=weight)\\r\\n\\r\\n\\n\\n\\nvaex_df\\nNow you can perform GroupBy operations on it.\\n\\nvaex_df.groupby(by=’gender’).agg({‘weight’:‘mean’})\\r\\n\\r\\n\\n\\n\\xa0\\nThis will group by on gender column and calculate the aggregation function mean on the weight column.\\n\\nYou can pass in multiple aggregate functions in a list for a single column. It will automatically give appropriate names to the resulting columns.\\n\\nvaex_df.groupby(by=’gender’).agg({‘weight’:[‘mean’,’std’]})\\r\\n\\r\\n\\n\\n\\nImportant Functions related to Data Cleaning\\n(1) Filling Missing Data\\nLet\\'s create a small dataset to demonstrate this function.\\n\\ntest = [1,2,3,4,5,np.nan,np.nan]\\r\\n\\r\\ntest_df = vaex.from_arrays(co=test)\\r\\n\\r\\n\\n\\n\\nYou can pass in any value in fillna to fill nan values with that value.\\n\\ntest_df.fillna(-1)\\r\\n\\r\\n\\n\\n\\nNotice that it is not in-place filling, so you might need to work on it. Another important thing is that no copy of the underlying data is made, but only a view/reference is made. The original columns will be renamed, and by default, they will be hidden columns. No data is lost.\\n(2) Missing Data Information\\nYou can use\\xa0ismissing(),\\xa0isna(), and\\xa0isnan()\\xa0functions for the information about missing data. The first function returns true where there are missing values (masked arrays), missing strings, or None. The second function returns a boolean expression indicating if the values are Not Available (missing or NaN). The third function returns an array where there are NaN values.\\n\\ntest_df.co.isna()\\r\\n\\r\\n\\n\\n\\n\\ntest_df.co.isnan()\\r\\n\\r\\n\\n\\n\\n\\ntest_df.co.ismissing()\\r\\n\\r\\n\\n\\n\\n(3) Dropping Missing Data\\nYou can drop missing data using\\xa0dropna(),\\xa0dropnan(),\\xa0dropmissing(), and\\xa0dropinf(). All of these functions perform similar tasks but they drop by using different functions. For example,\\xa0dropmissing()\\xa0function drops values uses ismissing filter function, and\\xa0dropna()\\xa0drops using the\\xa0isna()\\xa0filter function.\\n\\ntest_df.dropna()\\r\\n\\r\\n\\n\\n\\n\\ntest_df.dropnan()\\r\\n\\r\\n\\n\\n\\n\\ntest_df.dropmissing()\\r\\n\\r\\n\\n\\n\\nNotice that it will return a shallow copy of the original dataframe.\\n(4) Drop Columns\\nYou can drop the columns in a vaex dataframe using the\\xa0drop\\xa0function.\\n\\nvaex_df.drop(‘c0’) \\r\\n\\r\\n\\n\\n\\xa0\\nYou can pass in a single column or a list of the columns. It is also not in place, and you have to check the parameter. If the column is used in a virtual column, you can simply just True the check parameter, and it will hide it.\\nString Operations\\nVaex has a separate class for string functions\\xa0vaex.expression.StringOperations.\\xa0To use these in your dataset, let\\'s say you want to use it on a string column. All you have to do is to call\\xa0df.str.reqFunction()\\xa0this will apply that function on every row.\\nLet\\'s create a new testing dataframe to demonstrate the string functions.\\n\\nimport vaex\\r\\ntext = [‘Testing’, ‘lowercase’, ‘UPPERCASE’, ‘test_example’]\\r\\ndf = vaex.from_arrays(text=text)\\r\\n\\r\\n\\n\\n\\xa0\\n(1) Lower\\nThis function converts all the uppercase characters to lowercase characters.\\n\\ndf.text.str.lower()\\r\\n\\r\\n\\n\\n\\n(2) Contains\\nThis is an important function that takes in a keyword argument pattern, which can be a string to search in all rows or a regex pattern.\\n\\ndf.text.str.contains(pattern=’([A-Z])’, regex=True)\\r\\n\\r\\n\\n\\n\\nIt has returned True for all the rows that contain the matched regex. Alternatively, you can search only a single string.\\n\\ndf.text.str.contains(pattern=’ing’, regex=False)\\r\\n\\r\\n\\n\\n\\n(3) Endswith\\nThis function is used to check if the given strings end with the specific pattern or not. This pattern can be string or regex.\\n\\ndf.text.str.endswith(‘case’)\\r\\n\\r\\n\\n\\n\\n(4) Alpha Numeric Check\\nThis function\\xa0isalnum is used to check if all the characters in the string are alphanumeric or not. In our case, our last row has an underscore which is not an alphanumeric character.\\n\\ndf.text.str.isalnum()\\r\\n\\r\\n\\n\\n\\nThere are a lot of other functions which you can explore in the\\xa0official documentation.\\nFunctions related to plotting\\nVaex can quickly visualize a huge amount of dataset in no time. Most of the plots are done in 1 or 2 dimensions, and vaex can full fill most of the use cases.\\nThe simplest 1D plot can be shown using\\n\\nvaex_df.plot1d(vaex_df.c0)\\r\\n\\r\\n\\n\\n\\nWhen only given 1 argument i.e., column name, it will show the histogram of that column. We can do some complex visualization, that is to not plot the counts, but to use some other bin. In most cases, passing the\\xa0what=\\'<statistic>(<expression>)\\xa0where\\xa0<statistic>\\xa0is from one of the statistics methods mentioned in the API docs.\\n\\nvaex_df.plot1d(vaex_df.c0, what=’mean(c0)’)\\r\\n\\n\\n\\nSimilarly for 2D plotting, you can use\\n\\nvaex_df.plot(vaex_df.c0, vaex_df.c1\\r\\n\\n\\n\\nYou can explore other graphs and graph types in the\\xa0official documentation.\\n\\xa0\\nVirtual Columns\\n\\xa0\\nAnother important feature of Vaex is the virtual columns. Sometimes, any expression is stored as a column, and it is convenient to do so. Vaex gives virtual columns, which do not take any memory and are computed on the fly when needed. This virtual column is treated as the normal column.\\nLet’s create a dummy dataset.\\n\\n_rows = 10\\r\\n\\r\\nn_cols = 2\\r\\n\\r\\ndata=np.random.randint(100000000, 1000000000, size=(n_rows, n_cols))\\r\\n\\n\\ndata numpy array\\n\\xa0\\n\\ndf = vaex.from_arrays(d1=data[:, 0], d2=data[:, 1])\\r\\n\\n\\n\\nNow let\\'s say you want to create another column d3 that is the sum of d1 and d2 columns. You can do\\n\\ndf[\\'d3\\'] = df.d1+df.d2\\r\\n\\r\\n\\n\\n\\xa0\\nThis will create a virtual column d3 that is calculated on the run time when needed and will not occupy any memory.\\n\\nYou can confirm it by calling the info function on the dataframe.\\n\\ndf.info()\\r\\n\\n\\n\\n\\xa0\\nImportant Note\\n\\xa0\\nIf you are using Google Colab to use vaex, you might end up with an old IPython version error, so you need to update the IPython via the following command in the cell.\\n\\n!pip install ipython -U\\r\\n\\r\\n\\n\\n\\xa0\\nThen you need to restart the runtime.\\n\\xa0\\nEnding Notes\\n\\xa0\\nIn this article, you learned the basics of Vaex, which is a Python library used for the fast processing of big data and can be a good alternative to Pandas, especially for large datasets. Vaex has a lot of other functions and features, so I would definitely recommend checking out the official documentation.\\n\\xa0\\nRelated:\\n\\nThe secret to analysing large, complex datasets quickly and productively?\\nGood-bye Big Data. Hello, Massive Data!\\nHow to Speed Up Pandas with Modin',\n",
       " 'comments\\nBy Vidhi Chugh, Data Scientist\\n\\nSource\\n\\xa0\\nCovid has changed our lives in more ways than we could ever imagine. Working from home became a norm, something which was looked down upon when women used to seek this \\'LUXURY\\' in pre-covid times.\\nWith the booming \\'digital\\' world, networking events also found their way through online webinars. Specifically, in the context of women conferences, I often used to wonder why is there a need to call out women and organize separate events for them.\\nWhenever we talk about an all-inclusive world, why does women\\'s participation in \"general conferences\" not speak for itself? What stops women from demonstrating their work or giving voice to their beliefs in front of a wider audience?\\nWith all these thoughts lingering in my mind, I thought to attend these conferences and witness for myself.\\nI have heard the rationale of people from both sides, some I am sharing with you here. In the end, I will conclude with my opinion and takeaways after attending 3 such women-focused conferences.\\n\\xa0\\nWhy it all started? The points in favor:\\n\\xa0\\n\\xa0\\nVoice it out: General conferences are broadly seen as places where \"men are talking business\" while women are presumably amateurish and inept when they do the same. Women-focused conferences relieve them of this impression and give them the voice and freedom to share a wide range of topics e.g. how to maintain work-life balance, managing kids during the pandemic, receiving support from colleagues, a new business idea, or collaboration on the current project.\\nCutting edge skills: If the conference involves attendees from an industry that demands disruptive technical skills, often synonymously dominated by men, the talks also involve such skills that a lot of women are yet to pick up, making it a tough bet for their time (filled with family responsibilities).\\n\\xa0\\nIssues with women conferences:\\n\\xa0\\n\\xa0\\nWhy try to be mother nature: The gender difference should only stick to the point that mother nature has distinguished it to. And it should just stop there. Anything afterward is purely based on merit, dedication, and hard work. Why is it important to bring together women in one conference and teach them how to open up and ask for help, being assertive and confident?\\nWhy preaching? Above all, my question is why do we need to teach all grown-ups about what they already know? Why can\\'t we behave agnostic to the gender? No one can teach the whole society and pivot the deep-rooted biased lens we have been seeing the world with. It calls for a bottom-up approach where every individual feels the responsibility to promote and support equality in fighting the prejudices prevalent in the society\\nMarginalization: It is more of a vicious circle where the marginalized group is intended to be supported with such events but ends up getting marginalized further. Bring women and men at par, let the other side understand, discuss, support each other, and grow together.\\nSometimes, I wonder whether it is specific to a particular field or industry, but unfortunately, it is everywhere. Recently, I came across a post where a senior executive had to call out a finance conference with an all-male panel discussion.\\nThere is a fancy word for it manel i.e., male panel.\\n\\nSourced from author’s LinkedIn\\xa0network\\n\\xa0\\nMale Advocate: Men play a crucial role in promoting women\\'s rights and opportunities. Improving the condition of women in society and the workplace cannot be done with the other half (as they say), the men.\\nLinkedIn has introduced courses on helping women advance their careers. There is one course in particular that stands out \\'Becoming a Male Ally at Work\\'. The course helps the \\'gender in the majority to better understand the stereotypes that are holding women back.\\n\\xa0\\nWhat did I find after attending 3 women conferences?\\n\\xa0\\n\\xa0\\nWhile I can go on talking about how we, the society, can come together in solidarity and make it reach an upright state, this all might sound just too theoretical for you.\\nSo, let\\'s see a real example of how the power of women supporting each other can do wonders.\\nThis year, I attended 3 such conferences and where women presented remarkable work in Data Science. I found one thing consistent across these networks\\u200a-\\u200aleaders from the industry share their stories and tips to deal with difficult situations. Some of the advice that resonates with me:\\nBeing a yes (wo)man: Boundaries are just something that exists in our minds. Once you are past these self-imposed boundaries, the world of opportunities will open up for you. This fearless attitude of being a \\'Yes woman\\' has done wonders to me on both fronts, personally and professionally. I became more confident of doing my best to whatever came my way.\\nNetwork your way through: Make an ecosystem of women in data, connecting with like-minded people gives you a perspective of how to handle similar situations. It also helps you make wiser decisions without committing the same mistakes that others have done.\\nShare the same vocabulary: Understand the business requirements, concerns of the stakeholders, map it to how you can contribute, and voice it out. I believe that the organizations are appreciative of their associates that add value and do not let their gender, caste, or religion come in the way of retaining successful business contributors.\\nStop fighting the unconscious: It is important to acknowledge that there might be a subtle bias present in the subconscious of people around you. But we have to stop letting it come in the way of our vision. I am a working mother and know for sure the value of each hour that slips out of my hand. I would rather prefer to spend my time productively by upskilling myself. People can take anything but your knowledge away from you. Keep working on your knowledge bank and it will pave the right path for you in the long run.\\nWays to be productive: Women are good at multi-tasking (and we know that\\xa0:)). So, it is important that we use it to our advantage. Listening to podcasts, or technical courses while cooking or taking nature\\'s walk keeps me on my toes and lets me utilize my time judiciously. Another important aspect of networking comes from the right use of social media, I try to connect with the leaders in Data Science to keep myself adept with the latest technologies and algorithms.\\nNo golden rule: In the end, I would say that there is no recipe for success. Working on the \"try fast, fail fast\" formula proves to be good sometimes. So, do not be a victim of self-doubt and give a shot at everything that you dream of. When anyone tries to get you down, do not let it affect you. \\'Show-up to the adversities in the face\\'.\\nSo, with all the confidence I gained from attending these conferences, I implemented what I had learned, i.e., showing up. Yes, it has helped me get past the fear of self-doubt.\\nThough attending women in data science conferences has done good to me, my heart wishes for a day where conferences do not get a group-specific tag or label. That would be the day when the purpose of such conferences being conducted today would manifest into an inclusive and diverse world.\\nTill then, let us work on \"each one, help one\".\\n\\xa0\\nBio: Vidhi Chugh is inquisitive about the power of data, and is commonly referred to as a Data Scientist.\\nRelated:\\n\\nTowards a Responsible and Ethical AI\\n8 Women in AI Who Are Striving to Humanize the World\\nMore Resources for Women in AI, Data Science, and Machine Learning',\n",
       " 'comments\\nBy Pranjal Saxena, Data Scientist, Top Writer in Artificial Intelligence\\n\\n\\nPhoto by\\xa0Andrea Piacquadio\\xa0from\\xa0Pexels\\n\\n\\xa0\\nI have been in the data science field for the last half-decade when python programming came into the trend. Back then, in 2016, neural networks and deep learning were just some buzzy words. At that time, there was a hype about Google self-driving cars and reinforcement learning. But, most of the data science enthusiasts were not even aware of the working of neural networks.\\nToday in 2021, most companies are adopting a data science strategy to make more revenue by automating different scenarios and replacing dozens of IT people with a single data scientist who can automate the task of those IT people using various automating tools like BluePrism, UI Path, Python and machine learning algorithms.\\nThat’s why most of us are working hard to learn python, machine learning, analytics, deep learning. Why? Because there is an excellent value for the data scientist in the industries. And, also people are getting a good hike in their job data in the data science field.\\n\\n\\nBut, do you know in today’s time, these “automation tasks are being automated using another automation strategy?” The whole data science pipeline is being automated using a single tool.\\n\\n\\nIn 2019, data scientists used to spent days in data gathering, data cleaning, feature selection, but now we have many tools in the market that can do these tasks in a few minutes.\\nOn the other hand, we were trying different machine learning libraries like logistic regression, random forest, boosting machines, naive Bayes, and other data science libraries to a better model.\\nBut, today, we have tools like H2O, PyCaret, and many other cloud providers who can do the same model selection on the same data using the combination of other 30–50 machine learning libraries to give you the best machine learning algorithms for your data with least error.\\nThings are now getting change at a fast pace. And, we are anyhow losing our value because everyone will trust the tool that tries more than twenty machine learning algorithms to come up with better accuracy than us who tries only a couple of machine learning libraries to come up with less accuracy.\\n\\xa0\\nThe tough reality part\\n\\xa0\\nUntil now, we have discussed how some automation tools are doing well in the machine learning area. And these tools are doing well than us because where we are using limited machine learning algorithm knowledge. In contrast, these tools are using the combination of libraries to get more efficient results by automating the complete EDA process providing the best possible results in less time.\\nBut, what about the deep learning area where we have less command than the machine learning area and having limited processing power. There also we have a good amount of tools in the market. These tools invest a good amount of money in having the best processors.\\n\\n\\nDeep learning is all about more data, processing power, and a complex neural network that needs more processing power to provide more accurate results.\\n\\n\\nWhen we talk about deep learning, that is famous for handling unstructured data. And, 95% time, we work with images and test data here. Object detection, image segmentation, building chatbots, sentiment analysis, document similarity are the famous use cases.\\nBut, working on these use cases required knowledge of different deep learning algorithms like convolution neural network, recurrent neural network, U-Net, hourglass, YOLO, and many more models that need a good amount of processing power to process more data for better accuracy.\\nThe catch here is that when in today’s time in 2021, companies are investing a good amount of money in automating these complete pipeline workflows. And, we are busy understanding basic machine learning and deep learning model irrespective of the fact that we can’t afford high-end machines without any investors.\\nEach company is aware of this fact, so after five years, when these cloud-enabled data science tools will become more efficient and will be able to provide better accuracy in much less amount time, then why companies will invest in hiring us and not buying the subscription of those tools?\\n\\xa0\\nThe Ray of Hope\\n\\xa0\\nWhen all these things are going to automate, you might be thinking about the future of data science enthusiasts. Will, there be a shortage of jobs or will there be fewer hirings?\\nWell, things become easier when we think differently. It is true that companies will keep focusing on the automated workflow of machine learning. But, remember, no company wants to depend on another company for their work.\\nEach company aims to build their product so that instead of depending on others, they can build their automated system and then sell them in the market to earn more revenue. So, yes, there will be a need for data scientists who can help industries build automation systems that can automate the task of machine learning and deep learning.\\nAt last, we can say that the role of data scientists will be to automate the pipeline with optimized results. So, in the end, we will be automating the pipeline of machine learning workflow and let the automation decide the best features in the data and derive the best possible result using the best-curated algorithm.\\n\\xa0\\nFinal Thoughts\\n\\xa0\\nWe have seen how there will be a lack of data science jobs in the next five years because companies will be adopting the automated pipelines of data science. But, there will also be high demands for data scientists who can automate data science pipelines.\\nAs per my thought to automate those pipelines, we first need to understand machine learning algorithms to build a better automated system, which will eventually lead to more jobs.\\nWell, What are your thoughts? I would love to hear yours. I hope you liked the article. Stay connected for more related articles. I publish articles on real-time data science scenarios and their use cases.\\nThanks for the reading!\\n10 Python Tricks That Will Wow You\\nHandy features to improve your Python programming skills\\n\\xa0\\n15 Ultimate Daily Hacks for Every Programmer\\nYou don’t need to import TensorFlow to print “hello world”\\n\\xa0\\nFound this story interesting? Follow me (Pranjal) on Medium. If you want to reach out to me with private questions do connect me on\\xa0Linkedin. And, If you want to get more exciting articles on data science and technology directly to your mail then here is my free newsletter:\\xa0Pranjal’s Newsletter.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nData Scientist, Data Engineer & Other Data Careers, Explained\\nA Guide On How To Become A Data Scientist (Step By Step Approach)\\nThese Soft Skills Can Make or Break Your Data Science Career',\n",
       " 'comments\\nBy Bex T., Top Writer in AI\\n\\n\\nWeekly Awesome Tricks And Best Practices From Kaggle\\n\\n\\xa0\\nAbout This Project\\n\\xa0\\nKaggle is a wonderful place. It is a gold mine of knowledge for data scientists and ML engineers. There are not many platforms where you can find high-quality, efficient, reproducible, awesome codes brought by experts in the field all in the same place.\\nIt has hosted 164+ competitions since its launch. These competitions attract experts and professionals from around the world to the platform. As a result, there are many high-quality notebooks and scripts on each competition and for the massive amount of open-source datasets Kaggle provides.\\nAt the beginning of my data science journey, I would go to Kaggle to find datasets to practice my skills. Whenever I looked at other kernels I would be overwhelmed by the complexity of the code and immediately shy away.\\nBut now, I find myself spending a considerable amount of time reading other’s notebooks and making submissions to competitions. Sometimes, there are pieces that are worth spending your entire weekend on. And sometimes, I find simple but deadly effective code tricks and best practices that can only be learned by watching other pros.\\nAnd the rest is simple, my OCD practically forces me to spill out every single piece of data science knowledge I have. So here I am, writing the first edition of my ‘Weekly Awesome Tricks And Best Practices From Kaggle’. Throughout the series, you will find me writing about anything that can be useful during a typical data science workflow including code shortcuts related to common libraries, best practices that are followed by top industry experts on Kaggle, and so on, all learned by me during the past week. Enjoy!\\n\\xa0\\n1. Plotting Only the Lower Part of Correlation Matrix\\n\\xa0\\nA good correlation matrix can say a lot about your dataset. It is common to plot it to see the pairwise correlation between your features and the target variable. According to your needs, you can decide which features to keep and feed into your ML algorithm.\\nBut today, datasets contain so many features that it can be overwhelming to look at correlation matrices like this:\\n\\n\\nWeekly Awesome Tricks And Best Practices From Kaggle\\n\\n\\xa0\\nHowever nice, there is just too much information to take in. Correlation matrices are mostly symmetrical along the main diagonal, so they contain duplicate data. Also, the diagonal itself is useless. Let’s see how we can plot only the useful half:\\n\\n\\n\\n\\nThe resulting plot is much easier to interpret and free of distractions. First, we build the correlation matrix using the .corr method of the DataFrame. Then, we use\\xa0np.ones_like\\xa0function with\\xa0dtype\\xa0set to\\xa0bool\\xa0to create a matrix of True values with the same shape as our DataFrame:\\n\\nThen, we pass it to Numpy’s\\xa0.triu\\xa0function which returns a 2D boolean mask that contains False values for the lower triangle of the matrix. Then, we can pass it to Seaborn’s heatmap function to subset the matrix according to this mask:\\n\\nI also made a few additions to make the plot a bit nicer, like adding a custom color palette.\\n\\xa0\\n2. Include Missing Values in value_counts\\n\\xa0\\nA handy little trick with value_counts is that you can see the proportion of missing values in any column by setting\\xa0dropna\\xa0to False:\\n\\nBy determining the proportion of values that are missing, you can make a decision as to whether to drop or impute them. However, if you want to look at the proportion of missing values across all columns,\\xa0value_counts\\xa0is not the best option. Instead, you can do:\\n\\nFirst, find the proportions by dividing the number of missing values by the length of the DataFrame. Then, you can filter out columns with 0%, i. e. only choose columns with missing values.\\n\\xa0\\n3. Using Pandas DataFrame Styler\\n\\xa0\\nMany of us never realize the vast, untapped potential of pandas. An underrated and often overlooked feature of pandas is its ability to style its DataFrames. Using the\\xa0.style\\xa0attribute of pandas DataFrames, you can apply conditional designs and styles to them. As a first example, let’s see how you can change the background color depending on the value of each cell:\\n\\n\\n\\n\\nIt is almost a heatmap without using Seaborn’s heatmap function. Here, we are counting each combination of diamond cut and clarity using\\xa0pd.crosstab\\xa0Using the\\xa0.style.background_gradient\\xa0with a color palette, you can easily spot which combinations occur the most. From the above DataFrame only, we can see that the majority of diamonds are of ideal cut and the largest combination is with the ‘VS2’ type of clarity.\\nWe can even take this further by finding the average price of each diamond cut and clarity combination in crosstab:\\n\\n\\n\\n\\nThis time, we are aggregating diamond prices for each cut and clarity combination. From the styled DataFrame, we can see that the most expensive diamonds have ‘VS2’ clarity or premium cut. But it would be better if we could display the aggregated prices by rounding them. We can change that with\\xa0.style\\xa0too:\\n\\n\\n\\n\\nBy chaining\\xa0.format\\xa0method with a format string\\xa0{:.2f}, we are specifying a precision of 2 floating points.\\nWith\\xa0.style, your imagination is the limit. With a little bit of knowledge of CSS, you can build custom styling functions for your needs. Check out the official pandas\\xa0guide\\xa0for more information.\\n\\xa0\\n4. Configuring Global Plot Settings With Matplotlib\\n\\xa0\\nWhen doing EDA, you will find yourself keeping some settings of Matplotlib the same for all of your plots. For example, you might want to apply a custom palette for all plots, using bigger fonts for tick labels, changing the location of legends, using fixed figure sizes etc.\\nSpecifying each custom change to plots can be a pretty boring, repetitive and time-consuming task. Fortunately, you can use Matplotlib’s\\xa0rcParams\\xa0to set global configs for your plots:\\n\\nfrom matplotlib import rcParams\\n\\n\\nrcParams\\xa0is just a plain-old Python dictionary containing default settings of Matplotlib:\\n\\n\\n\\nYou can tweak pretty much every possible aspect of each individual plot. What I usually do and have seen others doing is set a fixed figure size, tick label font size, and a few others:\\n\\nYou can avoid a lot of repetition by setting these right after you import Matplotlib. See all the other available settings by calling rcParams.keys().\\n\\xa0\\n5. Configuring Global Settings of Pandas\\n\\xa0\\nJust like Matplotlib, pandas has global settings you can play around with. Of course, most of them are related to displaying options. The official user guide says that the entire options system of pandas can be controlled with 5 functions available directly from pandas namespace:\\n\\nget_option() / set_option() — get/set the value of a single option.\\nreset_option() — reset one or more options to their default value.\\ndescribe_option() — print the descriptions of one or more options.\\noption_context() — execute a code block with a set of options that revert to prior settings after execution.\\n\\nAll options have case-insensitive names and are found using regex under the hood. You can use\\xa0pd.get_option\\xa0to see what is the default behavior and change it to your liking using set_option:\\n\\n>>> pd.get_option(‘display.max_columns’)\\r\\n20\\n\\n\\nFor example, the above option controls the number of columns that are to be shown when there are many columns in a DataFrame. Today, the majority of datasets contain more than 20 variables and whenever you call\\xa0.head\\xa0or other display functions, pandas annoyingly puts ellipsis to truncate the result:\\n\\n>>> houses.head()\\n\\n\\n\\n\\n\\nI would rather see all columns by scrolling through. Let’s change this behavior:\\n\\n>>> pd.set_option(‘display.max_columns’, None)\\n\\n\\nAbove, I completely remove the limit:\\n\\n>>> houses.head()\\n\\n\\n\\n\\n\\nYou can revert back to the default setting with:\\n\\npd.reset_option(‘display.max_columns’)\\n\\n\\nJust like columns, you can tweak the number of default rows shown. If you set display.max_rows to 5, you won’t have to call\\xa0.head()\\xa0all the time:\\n\\n>>> pd.set_option(‘display.max_rows’, 5)>>> houses\\n\\n\\n\\n\\n\\nNowadays, plotly is becoming vastly popular, so it would be nice to set it as default plotting backed for pandas. By doing so, you will get interactive plotly diagrams whenever you call .plot on pandas DataFrames:\\n\\npd.set_option(‘plotting.backend’, ‘plotly’)\\n\\n\\nNote that you need to have plotly installed to be able to do this.\\n\\xa0\\nIf you don’t want to mess up default behavior or just want to change certain settings temporarily, you can use\\xa0pd.option_context\\xa0as a context manager. The temporary behavior change will only be applied to the code block that follows the statement. For example, if there are large numbers, pandas has an annoying habit of converting them to standard notation. You can avoid this temporarily by using:\\n\\n\\n\\n\\nYou can see the list of available options in the official pandas user guide.\\n\\xa0\\nBio: Bex T. is a Top Writer in AI, writing “I wish I found this earlier” posts about Data Science and Machine Learning.\\nOriginal. Reposted with permission.\\nRelated:\\n\\n8 Places for Data Professionals to Find Datasets\\n10 resources for data science self-study\\n10 Python Skills for Beginners',\n",
       " \"By Matthew Mayo, KDnuggets.\\ncomments\\nI probably don't need to tell you that Hugging Face — and in particular its Transformers library — has become a major power player in the NLP space. Transformers is full of SOTA NLP models which can be used out of the box as-is, as well as fine-tuned for specific uses and high performance. Hugging Face NLP tools don't stop there, however; its ecosystem includes numerous additional libraries, such as Datasets, Tokenizers, and Accelerate, and the 🤗 Model Hub.\\nHowever, with all of the massive and relentless advancements of natural language processing recently, keeping up with research breakthroughs and SOTA practices can be fraught with challenges. Hugging Face truly does provide a cohesive set of tools for practical NLP, but how does one keep up with both the most recent best practices and approaches to implementing NLP solutions and implementing these solutions with Hugging Face libraries?\\nWell, Hugging Face has a solution to this as well. Welcome to the free and (ad-free) Hugging Face course.\\n\\xa0\\n\\n\\xa0\\nThis course, taught by Matthew Carrigan, Lysandre Debut, and Sylvain Gugger, promises to cover a lot of ground:\\n\\nIntroduction -  it will provide an introduction to the 🤗 Transformers library — how Transformer models work; using models from the 🤗 Hub; fine-tuning models on datasets; and sharing your results.\\nDiving in -  the course covers the basics of 🤗 Datasets and 🤗 Tokenizers, allowing students to plot their approach to common NLP problems on their own.\\nAdvanced - more complex topics are covered — learn specialized architectures, for concerns such as memory efficiency and long sequences; gain exposure to more custom solutions; confidently solve complex NLP problems; and contribute to 🤗 Transformers.\\n\\n\\xa0\\n\\nA brief overview of the Hugging Face course (source)\\n\\xa0\\nWhat do you need to jump into this course? While prior PyTorch or TensorFlow knowledge is not necessary, though would certainly be helpful, course creators note 2 specific prerequisites:\\n\\nA good knowledge of Python\\nAn understanding of introductory deep learning, perhaps via a course such as those developed by fast.ai or deeplearning.ai\\n\\nI have not gone through the course to much extent yet; however, I have been hoping for and expecting something like this from Hugging Face for some time, and as such I will definitely be going through this material as soon as I can. \\nFrom what I have looked through, the course looks very well put together, with code, videos, notes, etc. I like the progression as well, getting students using the Transformers library right away via the Pipeline function, and then moving further down the layers from there. There is also setup help to get students up and running quickly with TensorFlow or PyTorch in either a local environment or using Colab, with all options fully supported throughout.\\n\\nOutput of the Transformer model is sent directly to the model head to be processed (source)\\n\\xa0\\nThere is a lot going on in the vast field of NLP, and there is a lot to keep up with. Hugging Face first set out to centralize and make widely-available the increasing number of Transformers, and these early efforts have made its subsequent ecosystem a leader in the field. Why not see how you can master these tools yourself? This will undoubtedly help you stay relevant and up on the SOTA in the ever-evolving field of NLP.\\n\\xa0\\nRelated:\\n\\nThe Essential Guide to Transformers, the Key to Modern SOTA AI\\nHow to Create and Deploy a Simple Sentiment Analysis App via API\\nGreat New Resource for Natural Language Processing Research and Applications\",\n",
       " 'By Gregory Piatetsky, KDnuggets.\\ncomments\\r\\nEarlier we published \\r\\n\\n AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021\\n Main 2020 Developments and Key 2021 Trends in AI, Data Science, Machine Learning Technology\\n\\r\\nHere is last part in our 2021 Predictions series - the predictions from the industry.  We received many submissions, and to keep this article size manageable, we limited this to 12 companies: \\r\\nAlluxio, Alteryx, Diamanti, Dremio, \\r\\nIndicative, Lexalytics, Luminoso, MathWorks,\\r\\nMobiDev, Qlik, SAS, and Splice Machine.\\r\\n\\n\\n\\nHaoyuan Li, Founder and CEO, Alluxio\\n\\n\"Containers\" everywhere for analytics and AI\\n\\n Containerized application deployments and Kubernetes have started to gain traction with enterprises increasingly moving away from traditional Hadoop based data lakes. While moving away, enterprises are realizing the benefit of abstracting the physical infrastructure while also adopting public clouds for agility. Vendor lock in is a concern but at the same time a uniform toolset across environments is a must to reduce spending on the expertise required to operate across environments, such as hybrid and multi-cloud. Container based deployments for compute abstraction alongside new abstraction services for storage anywhere, will be the solution of choice for enterprises moving off Hadoop.\\r\\n\\nConvergence of Machine Learning frameworks\\n\\n Companies of all sizes and at all stages are moving aggressively towards operationalizing machine learning efforts. There are several popular frameworks for model training, including TensorFlow and PyTorch, leading the game. Just like Apache Spark is considered a leader for data transformation jobs and Presto is emerging as the leading tech for interactive querying, 2021 will be the year we\\'ll see a front-runner dominate the broader model training space with PyTorch or TensorFlow as leading contenders.\\r\\n\\nAI & Analytics provided by the same platform (team)\\n\\n AI and Analytics capabilities were provided by different platforms / teams in the past. Over the years, we are seeing the platform is converging and the AI team is more focused on the algorithmic side, while AI & Analytics platform teams merged to provide the software infrastructure for both analytics and AI use cases.\\r\\n\\n\\n\\nAlan Jacobson, Chief Data and Analytics Officer, Alteryx\\n\\nUpskilling will play a bigger role in corporate boardrooms as well as in employees lives.\\n\\r\\nWhile it is always important for companies to offer training to employees, the fields of data science and digital transformation are challenging companies to break the mold and deliver new and constantly evolving  ways to upskill and deliver ROI. More and more, we\\'re going to see upskilling programs that help people learn and apply skills in real time. Hackathons are one example of how this is happening currently in many companies. We\\'re going to see an expansion of these and other on the job experiences that use real data and real problems with a goal of creating real value. Data science has evolved to the point where people don\\'t need to go back to college to learn, they\\'ll learn on the job or while at home by encountering new tools and technologies.  And with a huge shortage of those with analytic skills, many will start new jobs and careers based on the new skills.\\r\\n\\nThe \"analytic divide\" is going to get worse.\\n\\r\\nLike the much-publicized \"digital divide\" we\\'re also seeing the emergence of an \"analytic divide.\" Many companies were driven to invest in analytics due to the pandemic, while others have been forced to cut anything they didn\\'t view as critical to keep the lights on - and a proper investment in analytics was, for these organizations, analytics was on the chopping block. This means that the analytic divide will further widen in 2021, and this trend will continue for many years to come.  Without a doubt, winners and losers in every industry will continue to be defined by those that are leveraging analytics and those that are not.\\r\\n\\nAnalytics platforms and processes will increasingly outperform ad-hoc, siloed solving.\\n\\r\\nBusinesses are already starting to democratize data across the organization, arming more employees with real-time insights. I see this accelerating with both a cultural shift and a technology shift.  This trend will result in data gurus and citizen data scientists with deep domain knowledge increasingly joining forces as part of a more holistic and effective problem-solving process. \\r\\n\\n\\nBoris Kurktchiev, Field CTO, Diamanti\\n\\n Everyone thought that AI/ML was going to be the next big thing, but I think there is confusion around what AI/ML means. There\\'s a lot of confusion to the end user about how and why AI matters to them. We\\'ll see more advances in augmented technology to determine what the application of AI and ML means and how to use it to make technology better for the end user. \\r\\n Everyone wants hybrid cloud, and hybrid cloud relies on one thing: federated Kubernetes. This idea has been the twinkle in the developer community\\'s eye since 2015. 2021 is the year that we see a proper implementation of that to the point where organizations can truly have a hybrid cloud. Without federated Kubernetes, organizations must contend with disparate components living in different clouds but not able to truly integrate with one another.\\r\\n\\n\\n\\nTomer Shiran, co-founder of Dremio\\n\\nSeparation of Compute and Data Becomes the Default Choice\\n\\r\\nThe rise of cloud data lake storage (e.g., Amazon S3 and Azure Data Lake Storage) as the default bit bucket in the cloud, combined with the infinite supply and elasticity of cloud compute resources, has ushered in a new era in data analytics architectures. Just as applications have moved to microservice architectures, data itself is now able to fully exploit cloud capabilities. Data can be stored and managed in open source file and table formats such as Apache Parquet and Apache Iceberg, and accessed by decoupled and elastic compute engines such as Apache Spark (batch), Dremio (SQL) and Apache Kafka (streaming). With these advances data will, in essence, become its own tier, enabling us to rethink data architectures and leverage application design benefits for big data analytics.\\r\\n\\nThe Shine of the Cloud Data Warehouse Wears Off\\n\\r\\nThe cloud data warehouse vendors have leveraged the separation of storage from compute to deliver offerings with a lower cost of entry than traditional data warehouses, as well as improved scalability. However, the data itself isn\\'t separated from compute-it must first be loaded into the data warehouse, and can only be accessed through the data warehouse. This includes paying the data warehouse vendor to get the data into AND out of their system. So, while upfront expenses for a cloud data warehouse may be less, the costs at the end of the year are likely significantly higher than expected. By leveraging modern cloud data lake engines and open source table formats like Apache Iceberg, however, companies can now query data in the data lake directly without any degradation of performance, resulting in an extreme reduction in complex and costly data copies and movement.\\r\\n\\nData Privacy and Governance Kicks Into Another Gear in the United States\\n\\r\\nUsers are increasingly concerned about their online privacy making it much more likely that the United States will adopt regulations similar to Europe\\'s General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). This will require companies to double down on privacy and data governance in their data analytics infrastructure. Furthermore, companies will realize that data privacy and governance cannot be achieved with separate standalone tools, and instead must be implemented as an integral part of the analytics infrastructure. Because of this, data version control will become standard in cloud data lakes and open source technologies such as Project Nessie will enable companies to securely manage and govern data in an enterprise-wide platform. \\r\\n\\n\\nJeremy Levy, CEO of Indicative.\\n\\n As data professionals, we have a responsibility to the broader public. I think that within the next year we will see progress toward a code of ethics within the data analytics space, led by conscious companies who recognize the seriousness of potential abuses. Perhaps the US government will intervene and pass some version of its own GDPR, but I believe that technology companies will lead this charge. What Facebook has done with engagement data is not illegal, but we\\'ve seen that it can have deleterious effects on child development and on our personal habits. In the coming years, we will look back on the way companies used personal data in the 2010s and cringe in the way we do when we see people smoking on a plane in films from the 1960s.\\r\\n\\n\\n\\nLexalytics CEO Jeff Catlin and Chief Scientist Paul Barba:\\n\\n Data Annotation will become the next big \"side hustle\" in 2021. It\\'s already a common way to make an extra buck or two, but there\\'s been a race to the bottom in pricing, where annotations are largely sourced well below minimum wage in industrialized nations. However, as AI sees successes in industries requiring expertise, like health care or law, the demand for specialist knowledge will see the development of infrastructure for matching more lucrative annotation contracts to professionals.\\r\\n There will be more consolidation in the ML platform space. As AI became the \"it\" technology over the last few years, a bunch of AI infrastructure companies popped up and began peddling AI platforms to ease the task of building models for companies looking to leverage AI. While it sounds good on the surface, there is no identified business task being solved here, it\\'s simply more efficient use of technology, and that\\'s hard to sell.  It\\'s likely that the VCs who backed these plays will begin severing the cash lifelines in 2021.\\r\\n The improvements in deep learning models over the last 18 months means that NLP features that have been desired but unfulfilled will start showing results. These include better entity recognition which drives better normalization, which in turn drives generic relationship extraction. The advances in deep learning models make all of these possible.\\r\\n AI platforms will consolidate, but AI services will pick up the slack here.  Companies are becoming more accepting of 3rd party expertise in machine learning, and this is driving an increase in consulting services for ML.  This trend will continue and accelerate in 2021.\\r\\n Fake news detection will start showing dividends. Fake news detection is an incredibly hard problem, but a lot of very smart people are spending a lot of time working on it. The spread of misinformation will be notably lower by late 2021.\\r\\n\\n\\nRobyn Speer, Chief Science Officer at Luminoso\\n\\nDoing more to fight bias in AI\\n\\n In 2021, I really hope business will do more to fight AI bias in all its forms. If only it could be as simple as \"not training on biased data.\" But where is unbiased data going to come from? Any data that you collect in quantity reflects the biases of the world we live in. I recently discussed this in this Twitter thread.\\n I see four steps to fighting AI bias that happen at different stages of machine learning: Knowing the biases of our source data and how to account for them; applying de-biasing techniques, when appropriate, to counteract the ways that biases get baked into intermediate representations; ensuring that the results of machine learning are used in ways that are fair and transparent; and being responsive and accountable in cases where the system turns out to have flaws or unintended consequences.\\r\\n\\n\\n\\nJohanna Pingel and David Willingham, Deep Learning Project Managers at MathWorks\\n\\nA note on COVID-19: Investment in AI has not decreased \\n\\r\\nWe\\'d be remiss if we didn\\'t mention COVID-19, an unforeseen trend of 2020, which is expected to continue with us into 2021. Overall investment in AI-related projects has not decreased. While some heavily impacted industries have cut back in the near term, analysts report that these have been offset by those who increased their investment above what they had forecasted. Many are using this time to invest in upskilling remote learning, with AI themed courses amongst the top sought after by the engineering and scientific community, making them primed and ready to take on more AI projects in 2021.\\r\\n\\nAI aligns engineering, computer science, data science and IT direction\\n\\n Engineers will continue to work with data scientists using AI models to enhance existing applications or discover new innovative solutions to the projects they\\'re working on. However, creating a successful AI-based system is more than just developing a model. It requires model lifecycle management, which includes training, deploying, monitoring and updating the model for the system in which it resides. To do this efficiently these processes need to be automated, robust and well maintained. In 2021, engineers will augment their workflows to include:\\r\\n\\nModel explainability will reduce the aversion to AI within safety critical systems\\n\\n AI has long been considered a black box approach to modelling systems, and with it a fear that how it operates is largely unknown. As more explainability methods are being produced by research and more software vendor tools offer them, industry practitioners will more readily adopt AI innovations within their workflows.\\r\\n\\r\\nEngineers and scientists are beginning to understand why a model is making certain decisions and the limits at which a model can operate safely. They are running experiments to explain how a model operates in a variety of scenarios and using visualizations to understand the inner workings of a model when it doesn\\'t behave as it should. It\\'s driving innovation in the verification and validation of AI within safety critical systems, with automotive, aerospace and medical standards committees, such as EUROCAE and the FDA, working on the levels needed for certification.\\r\\n\\n\\nMaksym Tatariants, Data Science Engineer at MobiDev\\n\\r\\nGrowing adoption of Machine Learning in mainstream software, including mobile apps, defined 2020. Together with the hardware support, like Apple\\'s M1 chip, or Nvidia\\'s Ampere GPU architecture, the \"intelligent process automation\" will continue its growth in 2021.\\r\\n\\r\\nBesides, Edge AI has made a strong contribution to hardware evolution. There is definitely a clear focus on optimizing the latest neural networks for smartphones and IoT devices. Acceleration techniques, such as Automated Mixed Precision or TensorRT, are becoming available and easy-to-use. Thereby, they will help to perform edge computing in a better manner. As a result it will improve privacy and security of the user\\'s data.\\r\\n\\r\\n3D dimension research is another key trend: 3D reconstruction, pose estimation, and scene understanding. Although there\\'s still some lack of information, plenty of promising model architectures are appearing. The 2021 challenge is to learn how to work with 3D dimensions in real-time on consumer-grade hardware. \\r\\n\\n\\nDan Sommer, Senior Director, Global Market Intelligence Lead at Qlik\\n\\r\\nAccording to Gartner, by the end of 2024, 75% of enterprises will shift from piloting to operationalizing AI, driving a 5x increase in streaming data and analytics infrastructures. Having up-to-date and business ready data are more important than ever.\\r\\n\\r\\nSince the pandemic arrived, we\\'ve seen a surge in the need for real-time and up-to-date data. What is usually fairly stale - quarterly business forecasts, for example - is fleeting and mutable now. Alerts, data refreshes and forecasts will need to occur more often, with the freshest variables. On a macro level, we\\'ve seen disruptions to supply chains, with hospitals scrambling to procure PPE and consumers stockpiling toilet paper. In the case of PPE, we reacted to an actual shortage too slowly; with toilet paper, consumers broke the supply chain by assuming a shortage where none existed. Surges like these are accentuated in a crisis, and we have to build preparedness for them.\\r\\n\\n\\nKimberly Nevala, AI Strategic Advisor, SAS: \\r\\n\\n The Analytics \"Core\" Gets Reinforced. The pandemic upended expected business trajectories and exposed the weaknesses in machine learning systems dependent on large amounts of representative historical data, including well-bounded and reasonably predictable patterns. As a result, organizations will bolster investments in traditional analytics teams and techniques better suited to rapid data discovery and hypothesizing.\\r\\n Ethical AI Principles Cede to Responsible AI Practices. Organizations will move beyond ethics in principle to practical procedures to guide AI decision-making. This will include right-sizing AI governance and oversight to their specific industry, problem domain and level of maturity. Accountability for AI-enabled products and services will be placed with the product owner.  Bolstered by increased consumer awareness and agency, leading adopters will promote \"responsible AI\" practices as a value-added differentiator.\\r\\n\\nSarah Gates, Analytics Strategist, SAS: 2. The Year of ModelOps\\r\\n\\n Pressures created by COVID-19 have raised organizational awareness of and need for ModelOps - the holistic approach used to rapidly move mathematical models through the analytics lifecycle, delivering value and insights faster. For organizations wanting to accelerate their digital transformation and to rev up agility and competitiveness, ModelOps is the magic fairy dust that will make it possible.\\r\\n\\n\\n\\nMonte Zweben, CEO of Splice Machine.\\n\\n Feature Stores will be implemented as the #1 ML product in 2021 to operationalize Machine Learning\\r\\n Every commercial database will have ML features\\r\\n Cloud migrations accelerate 10x, causing a major land grab by AWS, Azure, and GCP\\r\\n Vendor lock-in becomes the #1 concern of cloud migrations as companies fear lock-in by the cloud provider equivalent to the control Oracle/IBM had\\r\\n Everyone will be talking about the democratization of machine learning and data science as companies break out of the model where there is a centralized data science silo holding everybody up - much like the \"web\" group was in the 2000\\'s - now every development team has web skills. The same will happen with ML\\r\\n Data lakes finally die and the re-emergence of the curated SQL data warehouse for structured data with associated cloud storage for unstructured data makes the dream of Big Data finally real \\r\\n\\n\\nRelated:\\n\\n AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021\\n Main 2020 Developments and Key 2021 Trends in AI, Data Science, Machine Learning Technology\\n Industry AI, Analytics, Machine Learning, Data Science Predictions for 2020',\n",
       " \"comments\\nBy John Lafleur, Co-Founder, Airbyte.io.\\nThere are quite a few decks already available online, and we found them all useful, each in different ways. That’s why we thought the deck we used for our seed round could be helpful to some companies, especially those that are open source or developer tools. For context: on that seed deck, we started working on Airbyte in the end of July and raised our seed round with Accel in December, only 5 months in. We’re assuming the deck was acceptable, as we raised the seed after only 13 days in the fundraising process.\\nWe will write a dedicated article about the process within a few days, so this article is really about the deck itself.\\n\\xa0\\nOur Deck\\n\\xa0\\nOur deck was pretty standard, though some might consider it a bit short. That’s why we always sent the appendixes along with the deck after our meetings.\\nThe structure and our pitch on each slide\\nHere are more details on our slides - why we included them and what we were saying during our pitches.\\n\\xa0\\n0. Cover\\n\\xa0\\n\\nAlthough you might only spend 10 seconds discussing the cover slide, it is important. It’s about showing that you have identified your positioning.\\n\\xa0\\n1. Industry Context\\n\\xa0\\n\\nThanks to Snowflake’s IPO and Segment’s acquisition, we knew that data infrastructure was hot from an investor's point of view. We decided to start with some context on the industry and how we fit into this ecosystem.\\nWhat we were saying:\\xa0\\nWhen you look at the data infrastructure industry, there is often a new category that emerges through a commercial product. Once the market matures, an open-source alternative gets created and ends up taking over the category. This behavior is often seen because data infrastructure requires privacy, security, and scale, which cloud-based solutions can’t offer as well as open-sourced ones. There are many examples, such as Kafka, Spark, and now DBT. We want to be the open-source solution for data integration.\\nYou might wonder why an open-source approach would also win the format for data integration; sometimes, a closed-source cloud-based approach works. This last sentence is a transition to the next slide.\\xa0\\n\\xa0\\n2. Problem We’re Solving\\n\\xa0\\n\\nWhat we were saying:\\nIn June and July, we started reaching out to 250 of Fivetran’s, StitchData’s, and Matillion’s customers. We ultimately managed to talk to 45 of them. We wanted to know whether an open-source approach would make sense to address data integration. What we learned is that a cloud-based closed-source solution will never be able to fully address the data integration problem. It has several inherent issues.\\n100% of the companies we talked to were using Fivetran, StitchData, or other solutions, while also building and maintaining their own connectors. They did so because either (a) the ETL solution didn’t support the connector they wanted, or (b) the solution supported it, but not in the way they needed.\\nWhen you look at Fivetran, for instance, you’ll see that after 8 years, they only support 150 connectors. The hard part about ETL/ELT is not about building the connectors but maintaining them. It is costly, and any cloud-based closed-source solution will be restricted by an ROI (return on investment) consideration. It isn’t profitable for them to support the long tail of connectors, so they only focus on the most popular integrations.\\nDuring those 45 interactions, we also identified a third issue. Some of the companies were about to stop using Fivetran for some connectors because it started to become too pricey. The value of an ELT solution is about replacing a paid data engineer that builds and maintains a connector in-house. The amount of work required from an engineer is almost the same whether a low volume or a high volume of data is being moved. So with volume-based pricing, at some point, it just stops making sense to use an external solution.\\nAnd the last inherent issue with a cloud-based approach: although cloud data warehouses are winning the enterprise market, it is because they are considered part of the data infrastructure. All other solutions must go through a rigorous privacy compliance process that will take several months.\\n\\xa0\\n3. Our Solution\\n\\xa0\\n\\nWhat we were saying:\\xa0\\nWe’re building an open-source ELT platform that syncs data from SaaS apps, APIs, and databases to data warehouses, data lakes, and other databases. Our solution can fully integrate with your data infrastructure and stack if you are using Kubernetes or Airflow for orchestration or DBT for transformation. Our goal is to become the open-source standard for anything ELT by the end of 2021.\\n\\xa0\\n4. How it addresses the problem\\n\\xa0\\n\\nWhat we were saying:\\xa0\\nBy making it trivial to build and maintain connectors using Airbyte rather than doing it in-house, we will become the new standard for building connectors. This will help us support the long tail of connectors. And since connectors run as Docker containers, you can build them in the language of your choice.\\nAs connectors are open-sourced, any team can edit a pre-built connector and tune it to their needs. If a connector breaks, anyone can jump on the code, submit a PR, and, once approved, the change can be propagated across all the existing users of that connector.\\nAirbyte focuses first on a self-hosted offer: pricing will be based on the feature and number of connectors used. The pricing will not be indexed on the volume of data. Being open sourced enables us to have a bottom-up approach with a frictionless adoption from data teams without going through privacy compliance, as we handle data security as a first-class citizen.\\n\\xa0\\n5. Who We’re Targeting / Our Go-To-Market\\n\\xa0\\n\\nWhat we were saying:\\xa0\\nWith Airbyte, we wanted to address 2 different audiences:\\n\\nData consumers, including data analysts and scientists.\\nData engineers who were building and maintaining the connectors themselves, or managing the data infrastructure.\\n\\nWhat made Fivetran successful is that they enabled data consumers to leverage the data without the help of data engineers—they made them autonomous (as much as Segment made product teams autonomous). Airbyte provides a UI that makes it very easy to start replicating data for non-technical users. It takes literally 2 minutes for a data analyst to replicate data from Salesforce to Snowflake, including the deployment through our Docker Compose.\\nIn addition, Airbyte will offer data integration connectors through an API that data engineers can leverage to build their own workflow and applications. This is also a way for Airbyte to address SaaS businesses that want to offer their own integrations to their customers through Airbyte.\\n\\xa0\\n6. Our Team\\n\\xa0\\n\\nIn this slide, we review the expertise of the team. In our case, we have 4 people coming from Liveramp, where they built and maintained more than 1,000 connectors and were moving more than 100TB of data every day. Our goal is to make investors understand that we’ve already done it in the past, but now the goal is to enable every company to do it.\\n\\xa0\\n7. Our Velocity\\n\\xa0\\n\\nThis is possibly the most important slide, as it validates everything you’ve said before with facts. The number of companies using you is most likely the most important data point for investors.\\nWhat we were saying:\\nWe actually started working on Airbyte at the end of July. We soft-launched an MVP 2 months later (at the end of September), with only 6 connectors. We wanted to have feedback as early as possible. It’s been 6-7 weeks since we soft-launched, and we are now used by X companies, we have Y contributors, and we’ve ramped up the number of connectors to 43.\\n\\xa0\\n8. Our Roadmap\\n\\xa0\\n\\nNow that we’ve shown our velocity up to this point, it’s time to show what we can accomplish in the next few quarters. Investors want to see where you will be when it is time to raise the next round. This slide’s goal is to accomplish this.\\n\\xa0\\n9. Our Ask\\n\\xa0\\n\\nThis is the last slide of the pitch. The goal here is that you shouldn’t need to say much! It shouldn’t come as a surprise and is a direct consequence of the roadmap slide.\\n\\xa0\\n10. Appendix!\\n\\xa0\\n\\nYou need such a slide to separate the main deck from the appendix. No surprise here! We only showed the appendix slides if asked relevant questions.\\n\\xa0\\n11. Some growth metrics [1]\\n\\xa0\\n\\nThe point for us here was to insist that we hadn’t done a hard launch yet, only a soft launch on our social networks on 09/24. The rest of the growth is organic.\\n\\xa0\\n12. Some growth metrics [2]\\n\\xa0\\n\\nInvestors will always ask for the names of companies that are using you. So you should always have such a slide ready. The more recognizable the names of the companies are, the better.\\n\\xa0\\n13. The competitive landscape\\n\\xa0\\n\\nWhat we were saying:\\nHere are a few choices we made that distinguish Airbyte from other open-source solutions:\\n\\nAirbyte’s connectors are usable out of the box through a UI and an API, with monitoring, scheduling, and orchestration.\\nAirbyte runs connectors as Docker containers, so they can be built in the language of your choice.\\nAirbyte’s components are modular, and you can decide to use subsets of the features to better fit in with your data infrastructure (e.g., orchestration with Airflow or K8s or Airbyte’s…).\\nWe intend to integrate with DBT for the transformation piece soon and let the community contribute normalization schemas for all connectors.\\nUnlike Singer, Airbyte uses one single open-source repo to standardize and consolidate all developments from the community, leading to higher-quality connectors. We built a compatibility layer with Singer so that Singer taps can run within Airbyte.\\n\\n\\xa0\\n14. Our investors\\n\\xa0\\n\\nThis is probably the slide we showed the least often, as serious VCs will already know who your main investors are. But you should still have it ready just in case.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nCrafting an Elevator Pitch for your Data Science Startup\\nHow To Get Funding For AI Startups\\nSix Ways For Data Scientists to Succeed at a Startup\",\n",
       " 'By Devin Partida, Editor-in-Chief of ReHack.com.\\ncomments\\nData professionals remain in high demand as data plays an increasingly crucial role in business. Despite the data science labor shortage, though, this field can be a competitive one. If workers want to boost their chances of securing a desirable position, they can add some variation to their resumes.\\nVariety on a resume demonstrates flexibility, which is an essential skill for data professionals today. It can be an excellent resume-booster, but jumping from position to position to improve a resume isn’t practical. Thankfully, applicants can add variety without needing to take on many new jobs.\\nHere are seven ways data professionals can add variation to their resumes.\\n\\xa0\\n\\n\\xa0\\n1. List Any Publications Under Your Name\\n\\xa0\\nJob titles aren’t the only things on a resume that showcase variety. If applicants have any publications like books, journal articles or whitepapers under their name, they should list them. These stand out to potential employers as an example that a worker is serious about their field.\\nRemember that everything should be relevant to the specific job at hand. Only data-related publications should appear on a resume for a data-related position. If workers don’t have any relevant published works to their name, they can look for opportunities to be part of one.\\n\\xa0\\n2. Mention Noteworthy Projects\\n\\xa0\\nThroughout their career, data professionals will work on various projects, either for work or as hobbies. Whether they’re an impressive personal pursuit or an event that saved their company money, these projects are examples of relevant skills. Mentioning specific projects instead of general work descriptions also adds variation to an otherwise repetitive resume.\\nAnything that showcases different approaches or skills, or is particularly impressive, is worth mentioning. Descriptions of these projects don’t need to be long and should focus on what makes them unique. It’s best to express these accomplishments in numbers and metrics, which stand out more than words.\\n\\xa0\\n3. Look for Unique Opportunities in Current Positions\\n\\xa0\\nAn applicant’s current company may have projects available that would add variation to a resume. As data professionals work, they should look out for any opportunities to learn or employ new skills. Volunteering to be part of these tasks will help build a more impressive resume.\\nData professionals can ask their managers about any such opportunities or keep an eye out for them. Whether it’s working on a special company project or pioneering a new process for the department, these appear more often than some may think.\\n\\xa0\\n4. Perform Freelance Work\\n\\xa0\\nData professionals don’t have to confine themselves to the work available in their current position. Data scientists and analysts are in-demand workers, so they can perform freelance work to boost their resumes. Freelance projects enable professionals to take on tasks they wouldn’t otherwise perform, adding variation without another full-time job.\\nSince this type of work enables professionals to choose their own hours, it can fit around their current schedule. Workers don’t need to take on too much extra work. Just a few projects can add some needed variety.\\n\\xa0\\n5. Work for an International Company\\n\\xa0\\nIf professionals want to find another full-time position that will add variety, they may consider working internationally. International companies are more likely to have different projects than those available in the U.S. Even if they don’t, working with different cultures showcases flexibility.\\nThere are several ways to work internationally, so workers can find something no matter their current situation. Data professionals may even be able to find contract work for international companies, so they can do it on top of their other work.\\n\\xa0\\n6. Consider Their Soft Skills\\n\\xa0\\nEven through various positions, data professionals may find that their day-to-day work looks similar. These workers can still find chances to add variation to their resume by mentioning the soft skills they’ve accumulated. Even if the data-centric work looks the same from position to position, different work environments may develop various soft skills.\\nCrunching numbers isn’t the only skill that’s important to the work of a data professional. They also need to communicate results to various audiences, work in teams and be adaptable. Highlighting these soft skills instead of tasks that would look more similar adds variety to a resume.\\n\\xa0\\n7. Get Specific\\n\\xa0\\nHiring managers typically only have around 30 seconds to look over an applicant’s resume. Data professionals need to showcase variety without taking up too much space as a result. Being specific about all descriptors also forces applicants to focus on what makes each entry unique.\\nA general, zoomed-out look at different data positions may make them all look the same. Getting specific about each one highlights what makes them different.\\n\\xa0\\nBecome the Most Appealing Candidate Possible\\n\\xa0\\nThere’s a lot of variety in data-related work, even if it may not be immediately apparent. Data professionals looking to improve their resumes can follow these steps to showcase this variation. They can then become an even more appealing candidate in an already in-demand field.\\n\\xa0\\nBio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com.\\nRelated:\\n\\nHow Data Scientists Can Avoid ‘Lost in Translation’ Syndrome When Communicating With Management\\nHow Automation Is Improving the Role of Data Scientists\\nIs Data Science for Me? 14 Self-examination Questions to Consider',\n",
       " 'comments\\nBy Arnab Bose, Chief Scientific Officer, and Aditya Aggarwal, Advanced Analytics Practice Lead, Abzooba\\n\\xa0\\nMLOps Motivation\\n\\xa0\\nMachine Learning (ML) models\\xa0built by data scientists represent a small fraction of the components that comprise an enterprise production deployment workflow, as illustrated in Fig 1 [1]\\xa0below.\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\xa0To operationalize\\xa0ML models,\\xa0data scientists are required to work\\xa0closely with multiple other teams such as business, engineering, and\\xa0operations.\\xa0This represents organizational challenges in terms of communication, collaboration, and coordination. The goal of MLOps is to streamline such challenges with well-established practices. Additionally,\\xa0MLOps brings about agility and speed that is a cornerstone in today\\'s digital world.\\n\\nFig 1: Only a small fraction of real-world ML systems are composed of the ML code, as shown by the small box in the middle.\\u200b The required surrounding infrastructure is vast and complex.\\n\\xa0\\nMLOps challenges similar to DevOps\\n\\xa0\\nThe challenges of ML model operationalization have a lot in common with software productionisation where\\xa0DevOps has proven itself.\\nTherefore, adopting the\\xa0best practices from DevOps\\xa0is a prudent approach to help data scientists overcome\\xa0challenges common to software productionisation. For example, the use of agile methodology promoted by DevOps in contrast to waterfall methodology is an efficiency boost. Additional DevOps practices used in MLOps are listed in Table 1.\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\nTable 1: MLOps leveraging DevOps.\\n\\n\\n\\nML model operationalization Challenges\\nSolution from DevOps\\n\\n\\n1) Continuous integration and continuous delivery (CI/CD):\\xa0To\\xa0set up a pipeline such that the updates are continuously built and ready for production accurately, securely, and seamlessly.\\nUse a\\xa0CI/CD framework\\xa0to build, test, and deploy software. It offers the benefits of\\xa0reproducibility, security, and code version control.\\n\\n\\n2) Longer development to deployment lifecycle:\\xa0Data Scientists develop models/algorithms\\xa0and hand them over\\xa0to operations to deploy into production. Lack of coordination and improper handoff between the two\\xa0parties lead to delays and errors.\\n3) Ineffective communication\\xa0between\\xa0teams leads to delays in the final solution:\\xa0The evaluation of an\\xa0ML solution usually comes towards the end of the project lifecycle. With\\xa0the development teams usually working in silos, the solution becomes a black-box to other stakeholders.\\xa0This is worsened by the lack of intermediate feedback. These\\xa0pose\\xa0significant challenges in terms of time, effort, and resources.\\u200b\\u200b\\u200b\\nAgile methodology\\xa0solves\\xa0this coordination problem by enforcing an end-to-end pipeline set up at the initial stage itself. Agile methodology divides the project into a sprint.\\xa0In each sprint, developers deliver incremental features that are ready for deployment.\\xa0The output from each sprint (made using pipelines) is visible to each and every member from a very early stage of the project. Therefore, the risk of\\xa0last-minute surprises reduces, and\\xa0early feedback\\xa0becomes\\xa0a common practice. In industry parlance, this does a \"shift left\" to coordination issues.\\n\\n\\n\\n\\xa0\\n\\xa0MLOps challenges different from DevOps\\n\\xa0\\nAccording to industry parlance, MLOps is DevOps for ML. While it is true to some extent, there are challenges typical to ML\\xa0that need\\xa0to be addressed by MLOps platforms.\\nAn example of such a challenge is the role of data. \\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200bIn\\xa0traditional software engineering (i.e., software 1.0), developers write logic and rules (as code) that are well defined in the program space, as demonstrated in Fig 2\\xa0[2].\\xa0However, in machine learning (i.e., software 2.0), data scientists write code that defines how to use parameters to solve a business problem. The parameter values are found\\xa0using\\xa0data (with techniques such as gradient descent). These values may change\\xa0with different versions of the data, thereby changing the code behavior.\\xa0In other words, data plays an equally important role as the written code in defining the output.\\xa0And both can change independently of each other. This adds a\\xa0layer of data complexity in addition to the model code as an intrinsic part of\\xa0the software that needs to be defined and tracked.\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\xa0\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\n\\nFig 2: Software 1.0 vs. Software 2.0.\\nThe various challenges that need to be taken care of by an MLOps platform are listed in Table 2.\\nTable 2: ML specific challenges.\\n\\n\\n\\nML Specific challenges\\nDescription\\n\\n\\n1) Data and hyper-parameters versioning\\xa0\\nIn traditional software application, code versioning tools are used to\\xa0track changes. Version control is a prerequisite for any continuous integration (CI) solution as it enables reproducibility in a fully automated fashion. Any change in source code\\xa0triggers the CI/CD pipeline to\\xa0build, test and deliver production-ready\\xa0code.\\nIn Machine Learning, output model can change if algorithm code or hyper-parameters or data change. While code and hyper-parameters are controlled by developers, change in data may not be. This warrants the concept of data and hyper-parameters versioning\\xa0in addition to algorithm code. Note that data versioning is a challenge for unstructured data such as images and audio and that\\xa0MLOps platforms adopt unique approaches to this challenge.\\n\\n\\n2)\\xa0Iterative development and experimentations\\nML algorithm and model development is\\xa0iterative and experimental. It requires a lot of parameter tuning and feature engineering. ML pipelines work with data versions, algorithm code versions and/or hyper-parameters. Any change in these artifacts (independently) trigger new deployable model versions that warrant experimentation and metrics calculations. MLOps platforms tracks the complete lineage for these artifacts.\\n\\n\\n3)\\xa0Testing\\nMachine Learning requires data and model testing\\xa0to detect problems as early in the ML pipeline as possible.\\na) Data validation -\\xa0check that the\\xa0data is clean with no anomalies and new data is conformant to prior distribution.\\nb) Data preprocessing -\\xa0check that data is preprocessed efficiently and in a scalable manner and avoid any training-serving skew [3].\\nc) Algorithm validation\\xa0-\\xa0track classification / regression metrics based on business problem as well as ensure algorithm fairness.\\n\\n\\n4) Security\\nML models in production are often part of a larger system where its output is consumed by applications that may or may not be known. This exposes multiple security risks. MLOps needs to provide\\xa0security and access control\\xa0to make sure outputs of ML models is used by known users only.\\n\\n\\n5) Production Monitoring\\xa0\\nModels in production requires continuous monitoring\\xa0to make sure models are performing per expectation as they process new data. There are multiple dimensions of monitoring such as covariate shift, prior shift, among others\\n\\n\\n6) Infrastructure requirement\\nML applications need\\xa0scale and compute power\\xa0that translates into complex infrastructure. For example, GPU may be necessary during experimentations and production scaling may be necessary dynamically.\\xa0\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\n\\n\\n\\n\\xa0\\nMLOps Components\\n\\xa0\\n\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200bWith the background in MLOps and its similarity\\xa0to as well difference from DevOps, the following describes the different components that comprise an MLOps framework, as shown in Fig 3. The workflow underlying them is through the agile methodology, as indicated in Section 2.\\n\\nFig 3: MLOps Framework.\\n\\nUse case discovery:\\xa0This stage involves collaboration between\\xa0business and data scientists\\xa0to define a business problem and translate that into a problem statement and objectives solvable by ML with associated relevant KPIs (Key Performance Indicator).\\n\\u200b\\u200b\\u200b\\u200bData Engineering:\\xa0This stage involves collaboration between a data engineer and a data scientist to acquire\\xa0data from various sources and prepare the data (processing/validation) for\\xa0modeling.\\n\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200bMachine Learning\\xa0pipeline:\\xa0This stage is designing and deploying a pipeline integrated with CI/CD. Data scientists use pipelines for multiple experimentation and testing. The platform keeps\\xa0track of data and model lineage\\xa0and associated KPIs across the experiments.\\nProduction deployment:\\xa0This stage accounts for secure and seamless deployment into\\xa0the production server of choice, be it public cloud, on-premise, or hybrid.\\nProduction monitoring:\\xa0This stage includes both model and infrastructure monitoring. Models are continuously monitored using configured KPIs like changes in input data distribution or changes in model performance.\\xa0Triggers are set for more experimentations with new algorithms, data, and hyper-parameters that generate a new version of\\xa0the ML pipeline. Infrastructure is\\xa0monitored per memory and compute requirements and\\xa0to scale as needed.\\n\\n\\u200b\\u200b\\u200b\\u200b\\u200b\\nReferences\\n\\nSculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, M. Young, J. Crespo, and D. Dennison, “Hidden technical debt in machine learning systems”, in Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, 2015, pp. 2503–2511. [Online]. Available:\\xa0http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems\\n\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b Karpathy, \"Software 2.0\", November 12, 2017 [Online]. Available:\\xa0https://medium.com/@karpathy/software-2-0-a64152b37c35\\nBreck, M. Zinkevich,\\xa0N. Polyzotis, S. Whang and S. Roy, \"Data validation for machine learning\", in\\xa0Proceedings of the 2nd\\xa0SysML Conference, Palo Alto, CA, USA, 2019. Available:\\xa0https://mlsys.org/Conferences/2019/doc/2019/167.pdf\\n\\n\\xa0\\nBios: Dr. Arnab Bose is Chief Scientific Officer at Abzooba, a data analytics company and an adjunct faculty at the University of Chicago where he teaches Machine Learning and Predictive Analytics, Machine Learning Operations, Time Series Analysis and Forecasting, and Health Analytics in the Master of Science in Analytics program. He is a 20-year predictive analytics industry veteran who enjoys using unstructured and structured data to forecast and influence behavioral outcomes in healthcare, retail, finance, and transportation. His current focus areas include health risk stratification and chronic disease management using machine learning, and production deployment and monitoring of machine learning models.\\nAditya Aggarwal serves as Data Science – Practice Lead at Abzooba Inc. With more than 12+ years of experience in driving business goals through data-driven solutions, Aditya specializes in predictive analytics, machine learning, business intelligence & business strategy across a range of industries.\\nRelated:\\n\\nData Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes\\nTaming Complexity in MLOps\\nWhat Does it Mean to Deploy a Machine Learning Model?',\n",
       " \"comments\\nBy Himanshu Sharma, Bioinformatics Data Analyst\\n\\n\\nTree (Source:\\xa0By Author)\\n\\n\\xa0\\nInterpreting a machine learning model is a difficult task because we need to understand how a model works in the backend, what all parameters the model uses, and how the model is generating the prediction. There are different python libraries that we can use to create machine learning model visualizations and analyze who the model is working.\\nMachine Learning Model Dashboard\\nCreating dashboards to interpret machine learning model\\n\\xa0\\nStaker is an open-source python library that enables machine learning model interpretations for different types of black-box models. It helps us create different types of visualization, making it easier to understand how a model is working.\\nIn this\\xa0article, we will explore Skater and what are its different functionalities. Let’s get started…\\n\\xa0\\nInstalling required libraries\\n\\xa0\\nWe will start by installing a skater using pip installation. The command given below will install the skater using pip.\\n\\n!pip install -U skater\\n\\n\\n\\xa0\\nImporting required libraries\\n\\xa0\\nThe next step will be importing the required libraries. To interpret the model using Skater we first need to create a model.\\n\\n%matplotlib inline \\r\\nimport matplotlib.pyplot \\r\\nimport matplotlib.pyplot as plt \\r\\nimport numpy as np \\r\\nfrom sklearn.model_selection \\r\\nimport train_test_split from sklearn.ensemble \\r\\nimport RandomForestClassifier \\r\\nfrom sklearn import datasets \\r\\nfrom sklearn import svm\\r\\nfrom skater.core.explanations import Interpretation\\r\\nfrom skater.model import InMemoryModel\\r\\nfrom skater.core.global_interpretation.tree_surrogate \\r\\nimport TreeSurrogate\\r\\nfrom skater.util.dataops import show_in_notebook\\n\\n\\n\\xa0\\nCreating Model\\n\\xa0\\nWe will create a Random Forest Classifier and use the IRIS dataset.\\n\\niris = datasets.load_iris()\\r\\ndigits = datasets.load_digits()\\r\\nX = iris.data\\r\\ny = iris.target\\r\\nclf = RandomForestClassifier(random_state=0, n_jobs=-1)\\r\\n\\r\\nxtrain, xtest, ytrain, ytest = train_test_split(X,y,test_size=0.2, random_state=0) \\r\\nclf = clf.fit(xtrain, ytrain)\\r\\n\\r\\ny_pred=clf.predict(xtest)\\r\\nprob=clf.predict_proba(xtest)\\r\\n\\r\\nfrom skater.core.explanations import Interpretation\\r\\nfrom skater.model import InMemoryModel\\r\\nfrom skater.core.global_interpretation.tree_surrogate import TreeSurrogate\\r\\nfrom skater.util.dataops import show_in_notebook\\r\\n\\r\\ninterpreter = Interpretation(\\r\\n        training_data=xtrain, training_labels=ytrain, feature_names=iris.feature_names\\r\\n    )\\r\\n\\r\\npyint_model = InMemoryModel(\\r\\n            clf.predict_proba,\\r\\n            examples=xtrain,\\r\\n            target_names=iris.target_names,\\r\\n            unique_values=np.unique(ytrain).tolist(),\\r\\n            feature_names=iris.feature_names,\\r\\n        )\\n\\n\\n\\xa0\\nCreating Visualizations\\n\\xa0\\nWe will start by creating different visualizations that will help us analyze how the model we have created is working.\\n1. Partial dependence plot\\nThis plot shows us how a particular feature affects the model's prediction.\\n\\ninterpreter.partial_dependence.plot_partial_dependence(\\r\\n   ['sepal length (cm)'] , pyint_model, n_jobs=-1, progressbar=False, grid_resolution=30, with_variance=True,figsize = (10, 5)\\r\\n)\\n\\n\\n\\n\\nPDP Plot (Source:\\xa0By Author)\\n\\n\\xa0\\n2.\\xa0Feature importance\\nIn this graph, we will analyze the importance of features in the model that we have created.\\n\\nplots = interpreter.feature_importance.plot_feature_importance(pyint_model, ascending=True, progressbar=True,\\r\\n                                n_jobs=-1)\\n\\n\\n\\n\\nFeature Importance (Source:\\xa0By Author)\\n\\n\\xa0\\n3.\\xa0Surrogate tree\\nIt is a pictorial representation of the random forest model that we have created. At each step, it is showing the Gini index value, class, etc.\\n\\nsurrogate_explainer = interpreter.tree_surrogate(oracle=pyint_model, seed=5)\\r\\nsurrogate_explainer.fit(xtrain, ytrain)\\r\\nsurrogate_explainer.plot_global_decisions(show_img=True)\\n\\n\\n\\n\\nSurrogate Tree(Source:\\xa0By Author)\\n\\n\\xa0\\nThis is how we can use Skater to create different graphs that help us analyze how a model is performing. Go ahead try this with different datasets and let me know your comments in the response section.\\nThis article is in collaboration with Piyush Ingale\\n\\xa0\\nBefore You Go\\n\\xa0\\nThanks\\xa0for reading! If you want to get in touch with me, feel free to reach me on hmix13@gmail.com or my\\xa0LinkedIn Profile. You can view my\\xa0Github\\xa0profile for different data science projects and packages tutorials. Also, feel free to explore\\xa0my profile\\xa0and read different articles I have written related to Data Science.\\n\\xa0\\nBio: Himanshu Sharma is a Bioinformatics Data Analyst at MEDGENOME. Himanshu is a Data Science Enthusiast with hands-on experience in analysing datasets, creating machine learning and deep learning models. He has worked on creating different data science projects and Poc's for different organisations. He has vast experience in creating CNN models for Image recognition and object detection along with RNN for time series prediction. Himanshu is an active blogger and have published around 100+ articles in the field of Data Science. \\nOriginal. Reposted with permission.\\nRelated:\\n\\nAutomating Machine Learning Model Optimization\\nThe Explainable Boosting Machine\\nInterpretability, Explainability, and Machine Learning – What Data Scientists Need to Know\",\n",
       " \"comments\\nBy Dylan Sherry, EvalML Team Lead\\n\\nAlteryx hosts two open-source projects for modeling.\\nFeaturetools\\xa0is a framework to perform automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning.\\nCompose\\xa0is a tool for automated prediction engineering. It allows you to structure prediction problems and generate labels for supervised learning.\\nWe’ve seen Featuretools and Compose enable users to easily combine multiple tables into transformed and aggregated features for machine learning, and to define time series supervised machine learning use-cases.\\nThe question we then asked was: what happens next? How can users of Featuretools and Compose build machine learning models in a simple and flexible way?\\nWe’re excited to announce that a new open-source project has joined the Alteryx open-source ecosystem.\\xa0EvalML\\xa0is a library for automated machine learning (AutoML) and model understanding, written in Python.\\n\\nimport evalml\\r\\n\\r\\n# obtain features, a target and a problem type for that target\\r\\nX, y = evalml.demos.load_breast_cancer()\\r\\nproblem_type = 'binary'\\r\\nX_train, X_test, y_train, y_test = evalml.preprocessing.split_data(\\r\\n    X, y, problem_type=problem_type, test_size=.2)\\r\\n\\r\\n# perform a search across multiple pipelines and hyperparameters\\r\\nautoml = AutoMLSearch(X=x, y=y, problem_type=problem_type)\\r\\nautoml.search()\\r\\n\\r\\n# the best pipeline is already refitted on the entire training data\\r\\nbest_pipeline = automl.best_pipeline\\r\\nbest_pipeline.predict(X_test)\\n\\n\\n\\n\\nEvalML's AutoML search in action\\n\\n\\xa0\\nEvalML provides a simple, unified interface for building machine learning models, using those models to generate insights and to make accurate predictions. EvalML provides access to multiple modeling libraries under the same API. EvalML supports a variety of supervised machine learning problem types including regression, binary classification and multiclass classification. Custom objective functions let users phrase their search for a model directly in terms of what they value. Above all we’ve aimed to make EvalML stable and performant, with ML performance testing on every release.\\n\\xa0\\nWhat’s Cool about EvalML\\n\\xa0\\n1. Simple Unified Modeling API\\nEvalML reduces the amount of effort it takes to get to an accurate model, saving time and complexity.\\nEvalML pipelines produced by AutoML include preprocessing and feature engineering steps out of the box. Once users have identified the target column of the data which they’d like to model, EvalML’s AutoML will run a search algorithm to train and score a collection of models, will enable users to select one or more models from that collection, and to then use those models for insight-driven analysis or to generate predictions.\\nEvalML was designed to work well with\\xa0Featuretools, which can integrate data from multiple tables and generate features to turbocharge ML models, and with\\xa0Compose, a tool for label engineering and time series aggregation. EvalML users can easily control how EvalML will treat each inputted feature, as a numeric feature, a categorical feature, text, date-time, etc.\\n\\n\\nYou can use Compose and Featuretools with EvalML to build machine learning models\\n\\n\\xa0\\nEvalML models are represented using a pipeline data structure, composed of a graph of components. Every operation applied to your data by AutoML is recorded in the pipeline. This makes it easy to turn from selecting a model to deploying a model. It's also easy to define custom components, pipelines and objectives in EvalML, whether for use in AutoML or as standalone elements.\\n\\xa0\\n2. Domain-Specific Objective Functions\\nEvalML supports defining custom objective functions which you can tailor to match your data and your domain. This allows you to articulate what makes a model valuable in your domain, and to then use AutoML to find models which deliver that value.\\nThe custom objectives are used to rank models on the AutoML leaderboard during and after the search process. Using a custom objective will help guide the AutoML search towards models which are the highest impact. Custom objectives will also be used by AutoML to tune the classification threshold of binary classification models.\\nThe EvalML documentation provides\\xa0examples of custom objectives\\xa0and how to use them effectively.\\n\\xa0\\n3. Model Understanding\\nEvalML grants access to a variety of models and tools for model understanding. Currently supported are feature importance and permutation importance, partial dependence, precision-recall, confusion matrices, ROC curves, prediction explanations, and binary classifier threshold optimization.\\n\\n\\nAn example of partial dependence from the\\xa0EvalML documentation\\n\\n\\xa0\\n\\xa0\\n4. Data Checks\\nEvalML's data checks can catch common problems with your data prior to modeling, before they cause model quality problems or mysterious bugs and stack traces. Current data checks include a simple approach to detecting\\xa0target leakage, where the model is given access to information during training which won’t be available at prediction-time, detection of invalid datatypes, high class imbalance, highly null columns, constant columns, and columns which are likely an ID and not useful for modeling.\\n\\n\\xa0\\nGetting Started Using EvalML\\n\\xa0\\nYou can get started with EvalML by visiting\\xa0our documentation page, where we have\\xa0installation instructions\\xa0as well as\\xa0tutorials\\xa0which show examples of how to use EvalML,\\xa0a user guide\\xa0which describes the components and core concepts of EvalML,\\xa0API reference\\xa0and more. The EvalML codebase lives at\\xa0https://github.com/alteryx/evalml. To get in touch with the team, check out our\\xa0open-source slack. We are actively contributing to the repository and will respond to any issues you post.\\n\\xa0\\nWhat’s Next?\\n\\xa0\\nEvalML has an active feature roadmap, including time series modeling, parallel evaluation of pipelines during AutoML, upgrades to the AutoML algorithm, new model types and preprocessing steps, tools for model debugging and model deployment, support for anomaly detection, and much more.\\nWant to hear more? If you’re interested in hearing about updates as the project continues, please take a moment to follow this blog, star\\xa0our repo in GitHub, and stay tuned for more features and content on the way!\\n\\xa0\\nBio: Dylan Sherry is a software engineer who leads the team building the EvalML AutoML package. Dylan has a decade of experience working on automated modeling technologies.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nUber Open Sources the Third Release of Ludwig, its Code-Free Machine Learning Platform\\nAlgorithms for Advanced Hyper-Parameter Optimization/Tuning\\nBuild Your Own AutoML Using PyCaret 2.0\",\n",
       " 'By Matthew Mayo, KDnuggets.\\ncomments\\nQuantum Stat first came through with The Big Bad NLP Database, a collection of freely-accessible NLP datasets, curated from around the internet. It then released The Super Duper NLP Repo, which, at the time of introduction, provided centralized access to 100 freely-accessible NLP notebooks, curated from around the internet, and ready to launch in Colab with a single click. Now Quantum Stat is back with arguably its most ambitious NLP clearinghouse product yet.\\nThe NLP Model Forge is here to help you create NLP models quickly and easily. As conveyed to me by Quantum Stat CEO Ricky Costa:\\n\\xa0\\n\\n[The NLP Model Forge] allows users to generate code snippets from 1,400 NLP models curated from top NLP research companies such as Hugging Face Facebook DeepPavlov and AI2.\\n\\n\\xa0\\n\\n\\xa0\\nThe general idea of The NLP Model Forge is that you are able to browse and search example model code by task type, which include sequence and token classification, question answering, text summarization, text generation, translation, and more. Each model has a number of attributes, which include:\\n\\nname (e.g. distilbert-base-uncased-finetuned-sst-2-english)\\nsource language\\ntype (e.g. DistilBERT)\\ntags(e.g. pytorch, tf, rust, distilbert, text-classification, sst-2 dataset)\\nlabels (e.g. 0: NEGATIVE, 1: POSITIVE)\\n\\nAll of these model attributes, when taken together, provide a rich description and overview of realistic model expectations and implementation details. For example, it can be easily gleaned from the above example model\\'s attributes that it is a DistilBERT text classification (sentiment analysis) model with implementations available in at least PyTorch and TensorFlow.\\nOnce a desired model has been located, one can toggle the \"load\" switch in order to queue up the model code, after which the \"get code\" button can be used to load in the selected model code. See the image below for more details.\\n\\n\\xa0\\nOnce the code screen has been opened, the particular model\\'s code can be viewed directly. Alternatively, a Colab notebook can be launched with a single click in order to have the model code snippet opened into an executable environment. See the image below for more details.\\n\\n\\xa0\\nThe NLP Model Forge seems best suited to individuals looking to bounce around and try out different models in their own projects, without needing to extensively research the ins and outs of each prior to giving them a test run. Once a suitable model has been settled upon, I could see the example code snippets being modified, extended, and personalized for maximum implementation effect. It would best be embraced as a fast iteration prototype assist than complete code for an end product, but, to be fair, nowhere is it stated that The NLP Model Forge is anything but this.\\nThe NLP Model Forge is a promising project, and a worthy addition to Quantum Stat\\'s NLP product repertoire. If you are looking for a centralized spot for browsing and searching model code snippets, or to get ideas for your next NLP project, it is definitely worth a visit.\\n\\xa0\\nRelated:\\n\\nThe Big Bad NLP Database: Access Nearly 300 Datasets\\nThe Super Duper NLP Repo: 100 Ready-to-Run Colab Notebooks\\nNatural Language Processing Recipes: Best Practices and Examples',\n",
       " \"comments\\nBy Sean O'Connor, a science and technology author and investigator.\\nThe fast Walsh Hadamard transform (WHT) is a simplified version of the Fast Fourier Transform (FFT.)\\nThe 2-point WHT of the sequence a, b is just the sum and difference of the 2 values:\\n\\nWHT(a, b) = a+b, a-b.\\r\\n\\r\\n\\n\\n\\xa0\\nIt is self-inverse allowing for a fixed constant:\\n\\nWHT(a+b, a-b) = 2a, 2b\\r\\n\\r\\n\\n\\n\\xa0\\nDue to (a+b) + (a-b) = 2a and (a+b) - (a-b) = 2b.\\nThe constant can be split between the two Walsh Hadamard transforms using a scaling factor of √2 to give a normalized WHTN:\\n\\nWHTN(a, b) = (a+b)/√2, (a-b)/√2\\r\\n\\r\\nWHTN((a+b)/√2, (a-b)/√2) = a, b\\r\\n\\r\\n\\n\\n\\xa0\\nThat particular constant results in the vector length of a, b being unchanged after transformation since a2+b2 =((a+b)/√2)2+ ((a-b)/√2)2 as you may easily calculate.\\nThe 2-point transform can be extended to longer sequences by sequentially adding and subtracting pairs of similar terms, alike in the pattern of + and – symbols they contain.\\nTo transform a 4-point sequence a, b, c, d first do two 2-point transforms:\\n\\nWHT(a, b) = a+b, a-b\\r\\n\\r\\nWHT(c, d) = c+d, c-d\\r\\n\\r\\n\\n\\n\\xa0\\nThen add and subtract the alike terms a+b and c+d:\\n\\nWHT(a+b, c+d) = a+b+c+d, a+b-c-d\\r\\n\\r\\n\\n\\n\\xa0\\nand the alike terms a-b and c-d:\\n\\nWHT(a-b, c-d) = a-b+c-d, a-b-c+d\\r\\n\\r\\n\\n\\n\\xa0\\nThe 4-point transform of a, b, c, d then is\\n\\nWHT(a, b, c, d) = a+b+c+d,\\xa0 a+b-c-d, a-b+c-d, a-b-c+d\\r\\n\\r\\n\\n\\n\\xa0\\nWhen there are no more similar terms to add and subtract, that signals completion (after log2(n) stages, where n is 4 in this case.)\\xa0 The computational cost of the algorithm is nlog2(n) add/subtract operations, where n, the size of the transform, is restricted to being a positive integer power of 2 in the general case.\\nIf the transform was done using matrix operations, the cost would be much higher (n2 fused multiply-add operations.)\\n\\nFigure 1.\\xa0 The 4-point Walsh Hadamard transform calculated in matrix form.\\nThe +1, -1 entries in Figure 1 are presented in a certain natural order which most of the actual algorithms for calculating the WHT result in, which is fortunate since then the matrix is symmetric, orthogonal and self-inverse.\\nYou can also view the +1, -1 patterns of the WHT as waveforms.\\n\\n\\nFigure 2.\\xa0 The waveforms of the 8-point WHT presented in natural order.\\nWhen you calculate the WHT of a sequence of numbers, you are really just determining how much of each waveform is embedded in the original sequence.\\xa0 And that is complete and total information with which you can fully reconstruct any sequence from its transform.\\nThe waveforms of the WHT typically correlate strongly with the patterns found in natural data like images, allowing the transform to be used for data compression.\\n\\nFigure 3.\\xa0 A 65536-pixel image compressed to 5000 points using a WHT.\\nIn Figure 3, a 65536-pixel image was transformed with a WHT, the 5000 maximum magnitude embeddings were preserved, and then the inverse transform was applied (simply another WHT.)\\nThe central limit theorem (CLT) tells you that adding a large quantity of random numbers results in the Normal distribution with its characteristic bell curve.\\xa0 The CLT applies equally to sums and differences of a large quantity of random numbers.\\xa0 As a result, C.M. Rader proposed (in 1969) using the WHT to quickly generate Normally distributed random numbers from conventional uniformly distributed random numbers.\\xa0 You simply generate a sequence of uniform random numbers, say between –1 and 1, and then transform them using the WHT.\\nSimilarly, you can disrupt the orderly waveform patterns of the WHT by choosing a fixed randomly chosen pattern of sign flips to apply to any input to the transform.\\xa0 That is equivalent to multiplying the WHT matrix H with a diagonal matrix D of randomly chosen +1, -1 entries giving HD.\\xa0 The disrupted waveform patterns in HD then fail to correlate with any of the patterns seen in natural data.\\xa0 As a result, the output of HD has the Normal distribution and is actually a fast Random Projection of the natural data.\\xa0 Random projections have a wide number of applications in machine learning, such as locality sensitive hashing, compressive sensing, random projection trees, neural network pre and post-processing etc.\\n\\xa0\\nReferences\\nWalsh (Hadamard) Transform:\\n\\nhttps://archive.org/details/bbc-rd-reports-1974-07\\nhttps://www.jjj.de/fxt/\\n\\nNormal Distribution:\\n\\nhttps://archive.org/details/DTIC_AD0695042\\n\\nRandom Projections:\\n\\nhttps://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41466.pdf\\n\\nOther Applications:\\n\\nhttps://ai462qqq.blogspot.com/\\n\\nRelated:\\n\\nEssential Machine Learning Algorithms: A Beginner’s Guide\\nAll Machine Learning Algorithms You Should Know in 2021\\nAn easy guide to choose the right Machine Learning algorithm\",\n",
       " 'comments\\nBy Walid Amamou, Founder of UBIAI\\n\\n\\nPhoto by\\xa0Alina Grubnyak\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nSince the seminal paper “Attention is all you need” of Vaswani et al, Transformer models have become by far the state of the art in NLP technology. With applications ranging from NER, Text Classification, Question Answering or text generation, the applications of this amazing technology are limitless.\\nMore specifically, BERT — which stands for Bidirectional Encoder Representations from Transformers— leverages the transformer architecture in a novel way. For example, BERT analyses both sides of the sentence with a randomly masked word to make a prediction. In addition to predicting the masked token, BERT predicts the sequence of the sentences by adding a classification token [CLS] at the beginning of the first sentence and tries to predict if the second sentence follows the first one by adding a separation token[SEP] between the two sentences.\\n\\n\\nBERT Architecture\\n\\n\\xa0\\nIn this tutorial, I will show you how to fine-tune a BERT model to predict entities such as skills, diploma, diploma major and experience in software job descriptions. If you are interested to go a step further and extract relations between entities, please read our\\xa0article\\xa0on how to perform joint entities and relation extraction using transformers.\\nFine tuning transformers requires a powerful GPU with parallel processing. For this we use Google Colab since it provides freely available servers with GPUs.\\nFor this tutorial, we will use the newly released\\xa0spaCy 3 library\\xa0to fine tune our transformer. Below is a step-by-step guide on how to fine-tune the BERT model on spaCy 3.\\n\\xa0\\nData Labeling:\\n\\xa0\\nTo fine-tune BERT using spaCy 3, we need to provide training and dev data in the spaCy 3 JSON format (see here) which will be then converted to a .spacy binary file. We will provide the data in IOB format contained in a TSV file then convert to spaCy JSON format.\\nI have only labeled 120 job descriptions with entities such as\\xa0skills,\\xa0diploma,\\xa0diploma major,\\xa0and\\xa0experience\\xa0for the training dataset and about 70 job descriptions for the dev dataset.\\nIn this tutorial, I used the\\xa0UBIAI\\xa0annotation tool because it comes with extensive features such as:\\n\\nML auto-annotation\\nDictionary, regex, and rule-based auto-annotation\\nTeam collaboration to share annotation tasks\\nDirect annotation export to IOB format\\n\\nUsing the regular expression feature in UBIAI, I have pre-annotated all the experience mentions that follows the pattern “\\\\d.*\\\\+.*” such as “5 + years of experience in C++”. I then uploaded a csv dictionary containing all the software languages and assigned the entity skills. The pre-annotation saves a lot of time and will help you minimize manual annotation.\\n\\n\\nUBIAI Annotation Interface\\n\\n\\xa0\\nFor more information about\\xa0UBIAI\\xa0annotation tool, please visit the\\xa0documentation\\xa0page and my previous post “Introducing UBIAI: Easy-to-Use Text Annotation for NLP Applications”.\\nThe exported annotation will look like this:\\n\\nMS B-DIPLOMA\\r\\nin O\\r\\nelectrical B-DIPLOMA_MAJOR\\r\\nengineering I-DIPLOMA_MAJOR\\r\\nor O\\r\\ncomputer B-DIPLOMA_MAJOR\\r\\nengineering I-DIPLOMA_MAJOR\\r\\n. O\\r\\n5+ B-EXPERIENCE\\r\\nyears I-EXPERIENCE\\r\\nof I-EXPERIENCE\\r\\nindustry I-EXPERIENCE\\r\\nexperience I-EXPERIENCE\\r\\n. I-EXPERIENCE\\r\\nFamiliar O\\r\\nwith O\\r\\nstorage B-SKILLS\\r\\nserver I-SKILLS\\r\\narchitectures I-SKILLS\\r\\nwith O\\r\\nHDD B-SKILLS\\n\\n\\nIn order to convert from IOB to JSON (see documentation\\xa0here), we use spaCy 3 command:\\n\\n!python -m spacy convert drive/MyDrive/train_set_bert.tsv ./ -t json -n 1 -c iob\\r\\n!python -m spacy convert drive/MyDrive/dev_set_bert.tsv ./ -t json -n 1 -c iob\\n\\n\\nAfter conversion to spaCy 3 JSON, we need to convert both the training and dev JSON files to .spacy binary file using this command (update the file path with your own):\\n\\n!python -m spacy convert drive/MyDrive/train_set_bert.json ./ -t spacy!python -m spacy convert drive/MyDrive/dev_set_bert.json ./ -t spacy\\n\\n\\n\\xa0\\nModel Training:\\n\\nOpen a new Google Colab project and make sure to select GPU as hardware accelerator in the notebook settings.\\nIn order to accelerate the training process, we need to run parallel processing on our GPU. To this end we install the NVIDIA 9.2 cuda library:\\n\\n\\n!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604–9–2-local_9.2.88–1_amd64.deb!dpkg -i cuda-repo-ubuntu1604–9–2-local_9.2.88–1_amd64.deb!apt-key add /var/cuda-repo-9–2-local/7fa2af80.pub!apt-get update!apt-get install cuda-9.2\\n\\n\\nTo check the correct cuda compiler is installed, run: !nvcc --version\\n\\nInstall the spacy library and spacy transformer pipeline:\\n\\n\\npip install -U spacy\\r\\n!python -m spacy download en_core_web_trf\\n\\n\\n\\nNext, we install the pytorch machine learning library that is configured for cuda 9.2:\\n\\n\\npip install torch==1.7.1+cu92 torchvision==0.8.2+cu92 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\\n\\n\\n\\nAfter pytorch install, we need to install spacy transformers tuned for cuda 9.2 and change the CUDA_PATH and LD_LIBRARY_PATH as below. Finally, install the cupy library which is the equivalent of numpy library but for GPU:\\n\\n\\n!pip install -U spacy[cuda92,transformers]\\r\\n!export CUDA_PATH=”/usr/local/cuda-9.2\"\\r\\n!export LD_LIBRARY_PATH=$CUDA_PATH/lib64:$LD_LIBRARY_PATH\\r\\n!pip install cupy\\n\\n\\n\\nSpaCy 3 uses a config file config.cfg that contains all the model training components to train the model. In\\xa0spaCy training page, you can select the language of the model (English in this tutorial), the component (NER) and hardware (GPU) to use and download the config file template.\\n\\n\\n\\nSpacy 3 config file for training.\\xa0Source\\n\\n\\xa0\\nThe only thing we need to do is to fill out the path for the train and dev .spacy files. Once done, we upload the file to Google Colab.\\n\\nNow we need to auto-fill the config file with the rest of the parameters that the BERT model will need; all you have to do is run this command:\\n\\n\\n!python -m spacy init fill-config drive/MyDrive/config.cfg drive/MyDrive/config_spacy.cfg\\n\\n\\nI suggest to debug your config file in case there is an error:\\n\\n!python -m spacy debug data drive/MyDrive/config.cfg\\n\\n\\n\\nWe are finally ready to train the BERT model! Just run this command and the training should start:\\n\\n\\n!python -m spacy train -g 0 drive/MyDrive/config.cfg — output ./\\n\\n\\nP.S: if you get the error cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_INVALID_PTX: a PTX JIT compilation failed, just uninstall cupy and install it again and it should fix the issue.\\nIf everything went correctly, you should start seeing the model scores and losses being updated:\\n\\n\\nBERT training on google colab\\n\\n\\xa0\\nAt the end of the training, the model will be saved under folder model-best. The model scores are located in meta.json file inside the model-best folder:\\n\\n“performance”:{\\r\\n“ents_per_type”:{\\r\\n“DIPLOMA”:{\\r\\n“p”:0.5584415584,\\r\\n“r”:0.6417910448,\\r\\n“f”:0.5972222222\\r\\n},\\r\\n“SKILLS”:{\\r\\n“p”:0.6796805679,\\r\\n“r”:0.6742957746,\\r\\n“f”:0.6769774635\\r\\n},\\r\\n“DIPLOMA_MAJOR”:{\\r\\n“p”:0.8666666667,\\r\\n“r”:0.7844827586,\\r\\n“f”:0.8235294118\\r\\n},\\r\\n“EXPERIENCE”:{\\r\\n“p”:0.4831460674,\\r\\n“r”:0.3233082707,\\r\\n“f”:0.3873873874\\r\\n}\\r\\n},\\r\\n“ents_f”:0.661754386,\\r\\n“ents_p”:0.6745350501,\\r\\n“ents_r”:0.6494490358,\\r\\n“transformer_loss”:1408.9692438675,\\r\\n“ner_loss”:1269.1254348834\\r\\n}\\n\\n\\nThe scores are certainly well below a production model level because of the limited training dataset, but it’s worth checking its performance on a sample job description.\\n\\xa0\\nEntity Extraction with Transformers\\n\\xa0\\nTo test the model on a sample text, we need to load the model and run it on our text:\\n\\nnlp = spacy.load(“./model-best”)\\r\\n\\r\\ntext = [\\r\\n\\'\\'\\'Qualifications\\r\\n- A thorough understanding of C# and .NET Core\\r\\n- Knowledge of good database design and usage\\r\\n- An understanding of NoSQL principles\\r\\n- Excellent problem solving and critical thinking skills\\r\\n- Curious about new technologies\\r\\n- Experience building cloud hosted, scalable web services\\r\\n- Azure experience is a plus\\r\\nRequirements\\r\\n- Bachelor\\'s degree in Computer Science or related field\\r\\n(Equivalent experience can substitute for earned educational qualifications)\\r\\n- Minimum 4 years experience with C# and .NET\\r\\n- Minimum 4 years overall experience in developing commercial software\\r\\n\\'\\'\\'\\r\\n]\\r\\n\\r\\nfor doc in nlp.pipe(text, disable=[\"tagger\", \"parser\"]):\\r\\n    print([(ent.text, ent.label_) for ent in doc.ents])\\n\\n\\nBelow are the entities extracted from our sample job description:\\n\\n[\\r\\n(\"C\", \"SKILLS\"),\\r\\n(\"#\", \"SKILLS\"),\\r\\n(\".NET Core\", \"SKILLS\"),\\r\\n(\"database design\", \"SKILLS\"),\\r\\n(\"usage\", \"SKILLS\"),\\r\\n(\"NoSQL\", \"SKILLS\"),\\r\\n(\"problem solving\", \"SKILLS\"),\\r\\n(\"critical thinking\", \"SKILLS\"),\\r\\n(\"Azure\", \"SKILLS\"),\\r\\n(\"Bachelor\", \"DIPLOMA\"),\\r\\n(\"\\'s\", \"DIPLOMA\"),\\r\\n(\"Computer Science\", \"DIPLOMA_MAJOR\"),\\r\\n(\"4 years experience with C# and .NET\\\\n-\", \"EXPERIENCE\"),\\r\\n(\"4 years overall experience in developing commercial software\\\\n\\\\n\", \"EXPERIENCE\")\\r\\n]\\n\\n\\nPretty impressive for only using 120 training documents! We were able to extract most of the skills, diploma, diploma major, and experience correctly.\\nWith more training data, the model would certainly improve further and yield higher scores.\\n\\xa0\\nConclusion:\\n\\xa0\\nWith only a few lines of code, we have successfully trained a functional NER transformer model thanks to the amazing spaCy 3 library. Go ahead and try it out on your use case and please share your results. Note, you can use\\xa0UBIAI\\xa0annotation tool to label your data, we offer free 14 days trial.\\nAs always, if you have any comment, please leave a note below or email at admin@ubiai.tools!\\n\\xa0\\nBio: Walid Amamou is the Founder of UBIAI, an annotation tool for NLP applications, and holds a PhD in Physics.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Create and Deploy a Simple Sentiment Analysis App via API\\nHow to Apply Transformers to Any Length of Text\\nGreat New Resource for Natural Language Processing Research and Applications',\n",
       " 'comments\\nBy James Taylor, CEO and leading authority on Digital Decisioning and delivering business impact from AI and machine learning.\\n\\nI often see articles or posts that identify data integration or preparation as the key issues facing data science projects. This always puzzles me as this is not our lived experience - not what we see when we work with Fortune 500 companies adopting predictive analytics, machine learning, or AI. But I think I have figured it out. The problem is as follows:\\nWhat data scientists think counts as a \"data science project\" is not, in fact, a data science project.\\nLet me illustrate this with some data from a great study. Back in 2016, the Economist Information Unit did a survey on \"Broken links: Why analytics investments have yet to pay off\" and below, you see how this data appears to support the argument that data problems are #1.\\n\\nWow - pretty clear that Data integration/preparation is the biggest problem, with nearly twice as many projects reporting it as a problem as the next one.\\nIn fact, though, this is a subset of the data from the survey. Here\\'s the full data set:\\n\\nData integration and preparation only ranks\\xa0#4. Problem definition/framing, Solution approach/design, and Action/change management all rank higher. This is our experience.\\nIn large, established \"grown-up\" companies, data science projects fail for one or both of two reasons:\\n\\nThey are solving the wrong problem. They are building an analytic that is not what the business need, that will not solve a true business problem, or that is poorly designed to fit into the business context.\\nBecause they cannot action the model they build. They can\\'t change the business decision making to take advantage of the analytic by changing the decisions made and actions taken.\\n\\nAnd this illustrates the problem.\\nThe problem is that data scientists THINK their project starts with data and ends with the communication of their analysis. If that\\'s your focus, then data is your #1 problem.\\nBut this is not where data science projects start nor where they end. They have to start and end with the\\xa0business. That means starting with a\\xa0business\\xa0problem - a business decision that the business wants to improve - and ending with that problem being solved - the\\xa0business\\xa0behaves differently (better). If that\\'s your focus, then your problem is not data but problem definition and operationalization - making the analytic work IRL.\\nHere\\'s the difference shown in those phases. On the left, what many data scientists think their projects involved, and on the right, what it really involves.\\n\\nBottom line: If your data science team is telling you that data is their #1 problem, then they\\'re doing it wrong.\\nI\\'ve written about this before - check out this\\xa0article on the study itself\\xa0and this one on\\xa0adopting decision modeling\\xa0as a better way to define the problems your data science team is trying to solve. You might also like our recent white paper and videos on\\xa0Building an Analytic Enterprise.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nTop 10 Statistics Mistakes Made by Data Scientists\\nCommon mistakes when carrying out machine learning and data\\xa0science\\nWhat is the most important question for Data Science (and Digital Transformation)',\n",
       " 'comments\\nBy Lissie Mei, Data Scientist at Visa.\\nWe always deem data science as the “sexiest job of the 21st century.” When it comes to the transformation from a traditional company to an analytical company, either the company or the data scientists would expect to dive into the fancy world of analytics as soon as possible. But, is it always the case?\\n\\xa0\\nA troublesome start\\n\\xa0\\nEver since we, a Practicum team from UC Davis,\\xa0started the collaboration with Hilti, a leading manufacturing company in power tools and related services, we had provisioned several splendid blueprints: automation of pricing, propensity model…Working with such a great company was such a precious opportunity for us that we could barely wait to exploit our analytical skills to create business value. But when we started to tap into the data, we found that it is hard for us to directly acquire clean and structured data from a traditional company, as compared to from a data-driven company such as e-commerce companies.\\nAs I was mainly responsible for the data cleaning and engineering of the project, I witnessed how we were hindered in the analytical progress due to the unready data.\\n\\nI witnessed how we were hindered in the analytical progress due to the unready data.\\nWe were directly working with the finance team, but the other team, pricing operations, was actually taking charge of the database. In the beginning, the process was heavily lagged because we could barely request and inquire about the data or people in time. Moreover, as the sales data of Hilti was sensitive and the company lacked a secure way to transfer data, a time-consuming masking process was needed upon every request of data. Thirdly, weak data engineering led to the inconsistency among several referring tables, and we could barely proceed with a solid model or conclusion. Finally, we have to deal with various data types: CSV, JSON, SQLite, etc…Indeed a good chance to learn, though.\\nAfter around two months, we got all the data ready, and every anomaly case was discussed and solved.\\n\\xa0\\nDiving time!\\n\\xa0\\nOur well-developed frames of visualizations and models couldn’t wait to have a taste of the fresh data. However, the most embarrassing thing happened when we were presenting the first proposal with actual figures.\\n\\nGuess what? The big numbers didn’t seem to match. After a quick discussion, we realized that we didn’t receive the complete data at all. We were only focusing on the detail of data, such as anomalies and relationships among data sources, but we forgot to do the basic checks such as sum and count. This is a lesson that I would remember for a lifetime. Truly!\\n\\xa0\\nWhy data engineering is so important\\n\\xa0\\nThe most important thing that I learned from the data engineering drama is that the kind of roles working behind the scene, such as data engineers, are actually holding the gateway of innovation.\\xa0When a traditional company considers exploiting their data, the most efficient and first-step action should be improving the data engineering process. With good data engineers, the company can build a healthy and scalable data pipeline making it much easier for data analysts to carry out data mining and finding business insights.\\n\\nI also learned that why a lot of companies require their data analysts to have knowledge in programming related tools, such as Python and Scala, apart from analytics tools such as SQL and Excel. Usually, we cannot expect a “full-stack” analyst, but it is necessary that we have someone who can communicate with both engineering people and management people. Although a clear allocation of work is important for high efficiency, a guru of every data tool is indeed attractive.\\n\\nFull-stack… makes sense!\\nWhat I am expecting myself to learn in the future is the knowledge of both the front side and the backside, such as Java, JavaScript, Kafka, Spark, and Hive, and I believe eventually, they would be the sparkling point in my experience.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Think Like a Data Scientist\\nHow to Build Disruptive Data Science Teams: 10 Best Practices\\nOn Building Effective Data Science Teams',\n",
       " 'comments\\nBy Venkat Raman, Co-Founder at Aryma Labs\\n\\n\\nImage by author (adapted from Francois Chollet\\'s Prgramming vs ML paradigm diagram)\\n\\n\\xa0\\nAbstraction - some succinct definitions.\\n\\n“Abstraction is the technique of hiding implementation by providing a layer over the functionality.”\\n\"Abstraction, as a process, denotes the extracting of the essential details about an item, or a group of items, while ignoring the inessential details\"\\n\"Abstraction - Its main goal is to handle complexity by hiding unnecessary details from the user\"\\n\\nAbstraction as a concept and implementation in software engineering is good. But when extended to Data Science and overdone, becomes dangerous.\\nRecently, the issue of sklearn’s\\xa0default L2 penalty\\xa0in its logistic regression algorithm came up again.\\nThis issue was first discovered in 2019 by Zachary Lipton.\\n\\n\\xa0\\nOn the same issue, an excellent blog titled ‘Scikit-learn’s Defaults are wrong’ was written by W.D.\\xa0Here is the\\xa0link to that article.\\nThis article IMHO is a must read for any serious Data Scientist.\\nWhile the author of the article has excellently captured the design pattern flaws, I would just like to build on it and add the problem of ‘Too much abstraction’.\\nIn one of my\\xa0recent posts, I had highlighted how abstracting away GLM in sklearn’s logistic regression makes large number of people believe that Regression in Logistic Regression is merely a misnomer and it has nothing to do with Regression!!\\nBelow is an image from that article highlighting the issue.\\n\\n\\xa0\\nSo, why is ‘Too much abstraction’ a problem in Data Science?\\n\\xa0\\nI took the liberty of modifying Francois chollets famous diagram on difference between traditional programming and ML to drive home some important points regarding ‘too much abstraction’.\\n\\n\\xa0\\nFirstly, in normal programming, if you do abstraction, you just abstract away the fixed rules. This works out fine in software development realm as you don’t want certain people to tinker around ‘fixed rules’ or they simply don’t care ‘how things work under the hood’.\\nBut in Data science, if you do too much abstraction, you are also abstracting away the intuition of how the algorithm works and most importantly you are hiding away the knobs and levers necessary to tweak the model.\\n\\xa0\\nIn Data science, if you do \\'too much abstraction\\', you are also abstracting away the intuition of how the algorithm works and most importantly you are hiding away the knobs and levers necessary to tweak the model.\\n\\nLet’s not forget that the role of data scientist is to develop intuition of how the algorithms works and then tweak the relevant knobs/ levers to make the model a right fit to solve business problem.\\nTaking this away from Data Scientists is just counter intuitive.\\nThese aside, there are other pertinent questions on ‘too much abstraction’.\\nLet’s revisit one of the Abstraction definitions from above: “Abstraction -\\xa0Its main goal is to handle complexity by hiding unnecessary details from the user.”\\nWhen it comes to data science libraries or low code solutions, the question arises who decides ‘what is unnecessary’? Who decides which knobs and levers a user can or can’t see and tweak?\\nAre the people making these decisions well trained in Statistics and machine learning concepts? or are they coming from a purely programming background?\\nIn this regard I can’t help but loan some apt excerpts from W.D ‘s article\\xa0\"One of the more common concerns you’ll hear–not only from formally trained statisticians, but also DS and ML practitioners–is that many people being churned through boot camps and other CS/DS programs respect neither statistics nor general good practices for data management\".\\nOn the user side in Data Science, here are the perils of using libraries or low code solutions with ‘too much abstraction’.\\n\\nNobody knows the statistical/ML knowledge level of the user or the training they may or may not have had.\\nAt the hands of a person with poor stats/ML knowledge these are just \\'execute the lines with closed eyes\\' and see the magic happen.\\n\\nThe dangers of doing Data Science wrongly just becomes that much exacerbated. Not to mention ‘You don’t need math for ML’ and ‘Try all models’ kind of articles encouraging people to do data science without much diligence. Any guesses for what could go wrong ?\\nData science is not some poem that it can be interpreted in any which way. There is a definitive right and wrong way to do data science and implement data science solutions.\\nAlso, Data Science is just not about predictions. How these predictions are made and what ingredients led to those predictions also matter a lot. ‘Too much abstraction’ abstracts out these important parts too.\\n\\xa0\\nRead the Documentation\\n\\xa0\\nComing to defense of these ‘too much abstracted’ libraries and solutions, some remark the user should ‘Read the documentation carefully and in detail’.\\nWell not many have the time and most importantly some low code solutions and libraries are sold on the idea of ‘Perform ML in 2-3 lines of code’ or ‘Do modelling faster’.\\nSo again, referencing W.D, ‘read the doc is a cop-out’. Especially if it comes from low code solution providers.\\n\\xa0\\nA Bigger Problem to Ponder Upon\\n\\xa0\\nHaving said all this, Sklearn is still by and large a good library for Machine Learning. The problem of L2 default might be one of the very few flaws.\\nHowever, I would urge the readers to ponder over this:\\nIf abstracting away some details in one ML algorithm could cause so much issues, imagine what abstracting away details from dozen or so ML algorithms in a single line of code could result in. Some low code libraries do exactly that.\\n\\nIf abstracting away some details in one ML algorithm could cause so much issues, imagine what abstracting away details from dozen or so ML algorithms in a single line of code could result in!\\n\\nI am not against abstraction or automation per say. My concern is only with ‘too much abstraction’ . And I don’t have a concrete answer for how to tackle ‘too much abstraction’ in data science libraries. One can only wonder if there is even a middle ground.\\nBut one thing is very clear. The issues of ‘too much abstraction’ in Data Science are real.\\nThe more one abstracts away, the more is the chance of doing data science wrongly.\\nPerhaps all we can do is, be wary of low code solutions and libraries.\\xa0Caveat emptor.\\xa0\\n\\xa0\\nBio: Venkat Raman is Co-Founder at Aryma Labs.\\nOriginal. Reposted with permission.\\nRelated:\\n\\n5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist\\nManaging Your Reusable Python Code as a Data Scientist\\nUnleashing the Power of MLOps and DataOps in Data Science',\n",
       " 'comments\\nBy Kevin Gray and Dr. Anna Farzindar\\n\\n\\nOriginal art by Dr. Anna Farzindar\\n\\n\\xa0\\nKevin Gray: AI has become part of our daily lives, hasn’t it!\\nDr. Anna Farzindar: I was working on my laptop when my college daughter said “Mom please don’t do anything wrong with AI!” Then two days later during our family dinner, my younger freshman high school daughter told a story about a video on social media showing a small home care robot that tricked the owner and lied. She asked me “Mom, aren’t you afraid of robots?”\\nThese short conversations made me think about how the new generation is a big consumer of technology but, at the same time, they are concerned and worried about the future AI.\\n\\xa0\\nKG: Getting back to basics, what is AI?\\nAF: From talking to your virtual assistance on smartphone (like SIRI), watching a recommended movie on Netflix, searching on Google, following the suggested Instagram posts, using the sophisticated methods of an auto trading stock market, applying the decision making systems for your loan approval, or (soon) sitting in a self-driving car, AI algorithms are so embedded in our daily life that is hard to imagine living a single day without them! AI is like our closest friend who serves us. But is AI our best friend who looking our interests, or could it turn to an enemy?\\nArtificial intelligence (AI) is the field of computer science which creates human-like intelligence, even with the capacity of predicting the future. AI algorithms give machines the ability of performing tasks by learning from experience and data then refining their learning from new input to adapt to new situations.\\nIntelligent robots and simulations of the human brain have been the topic of fiction for decades. Is it true that AI will take over the human race and become a danger to humanity? How many people will lose their jobs because of robots? Will smart phones keep children occupied for hours and even replace parenthood? Is AI an imminent threat to humankind? Who actually controls AI and what are their goals? \\nIt is time for everyone to be aware of the impact and ethics of artificial intelligence no matter if we are a user of technology or a programmer who creates the algorithms. The question is where we are heading as humans in this AI world.\\n\\xa0\\nKG: Can AI really become as intelligent as humans? \\nAF: Based on the Oxford dictionary the definition of intelligence is: the ability to learn, understand and think in a logical way about things; the ability to do this well. Considering this definition of intelligence if a machine can learn and perform the tasks in a logical way then it could be intelligent!\\nIn 1950 Alan Turing, an English mathematician and computer scientist, proposed a method to determine if a machine can think like a human. In this test one person as a questioner communicates in natural language with two separate rooms to get an answer, one with a computer and one with a human. After a number of questions, if the human evaluator can’t decide whether the answers came from a human or AI for half of the test runs, then the machine is said to have passed the test.\\xa0\\nSome researchers showed that AI can pass Turing test in many complex natural language processing (NLP) tasks like machine translation and automatic summarization, simulating a human in 60% of the cases.\\nTo assess\\xa0the quality of AI performance we need to define the evaluation metrics.\\nIntrinsic evaluations directly judge the quality of output of a machine by direct analyses in terms of some set of norms. For example, in stock market prediction we can train the model on historical financial datasets (e.g. for the past ten years) and make a prediction by reading a window of data from the last 50 days to determine next day’s opening price. The intrinsic evaluation will measure how the predicted price is different from actual price.\\nExtrinsic evaluations assess the performance of AI system components based on how they affect the completion of some other tasks. In our stock market prediction example, in addition to predicting the stock price, if we analyze several aspects such as the positive and negative sentiment from tweets, impact of the news and measure other financial technical indicators, then all of these components could be evaluated to determine the performance of the whole system.\\xa0\\nIn some applications AI performance is good enough for specific tasks that have been requested. But in other applications, AI execution is largely superior to humans because of access to the big data, power of computing machines, speed of processing, powerful algorithms, contribution of team of engineers and billions of dollars of investment to create AI systems.\\n\\xa0\\nKG: What is emotional intelligence? \\nAF: Emotional intelligence is the ability to perceive, understand emotions and integrate emotion to facilitate thought and responses to a task. Some AI applications are designed for interpretation of human emotion. For example, AI can help the company to understand their customers better by measuring the consumer’s emotion from the reviews or social media. Also, AI can analyze spoken expressions from calls to customer service and find voice patterns. Some systems have the ability for automatic recognition of the facial expressions from videos and capturing the emotional reactions in real time.\\nBut there are some biases in emotional AI technologies used to interpret the human emotion. For example, the system could assign more negative emotions to people of certain ethnicities\\xa0or cultures that could mislead an organization in understanding customer satisfaction and, consequently, make wrong decisions.\\n\\xa0\\nKG: How dangerous really is AI? What are some of the ways it now is and potentially could be seriously abused? \\nAF: Most of the AI algorithms are developed with poorly specified goals but are rapidly deployed in a large-scale environment. The impact of \"super intelligent\" machines is unknown and it could be difficult or impossible for humans to control it. From the past experiences such as the atomic scientists leading to a catastrophic event, there is an urgent need to take responsibility for AI before humans are surprised by their creation.\\nTherefore, there is a necessity to develop specific legal and ethical guidelines for everyone: for technology developers to be aware of the algorithms and biases, for industry on to how use AI and connect the data in their various applications with different objectives, for governments to assist them better in planning for AI, and for consumers to be aware of AI and its decision-making in transparent and explainable processes.\\nWe could create unconscious and implicit biases in the algorithms. Imagine a brilliant young computer scientist from the Middle East who applies for a job and sends her resume to companies but gets no answers from HR. This is one of the current inequalities in the field of AI, which is the use of machine learning in recruitment for candidate assessment and preselection. \\nIf there are no records of past Middle Eastern female engineers in the company, then the model will never be able to select a candidate with this profile! In this case, AI made an unfair decision by rejecting a qualified candidate. “Fairness” is the behavior of AI models without privileging or discriminating against an individual or group of users, for example, based on their gender or race.\\xa0\\n\\xa0\\nKG: What can we do to prevent abuse of AI?\\xa0Are there things ordinary citizens can do?\\nAF: The spreading of false information and fake news is an important case of abuse of AI. Recently “DeepFake” apps raised concerns regarding candidates for the 2020 US election.\\xa0DeepFake, a combination for “deep learning” and “fake,” refers to AI software that can merge a digital face onto an existing video and audio of a person. \\nThe manipulation of behavior of AI could affect the vulnerable groups like teenagers, specific races or ethnic minorities by unintentionally promoting hate by AI systems. For example, recently “Black Lives Matter” drew attention to the problem of racism, but some people against this movement posted the racist videos on social media. When a video gets a certain number of views, the AI model promotes it as interesting content for more exposure to viewers, resulting in more hate in society. Also, fake news and misinformation about COVID-19 are another example of abuse of AI. Social media allows people across the globe to spread false information and conspiracy theories. AI is not intelligent enough to distinguish what are good values and bad values for human beings, but people can.\\nAlso, it is important to study the long-term effect of AI, for example the impact on the next generation. Some concerns are the possibility of addiction to Instagram or social media, raising pampered children with physical and mental problems, and lost connections between the parents and children. \\nAI ethics must go beyond the news headlines and theoretical discussions. It is imperative to develop the capacity to learn about the consequences of our work as developers of AI systems, or as consumers of such technologies in influencing the wider world. \\n\\xa0\\nKG: Could you elaborate more on the ethics of AI? \\nAF: The ethics of AI concern the moral obligation of tasks performed by machines and how it impacts on humans. For example, many companies integrate AI to improve their business by collecting data on users’ behaviors and analyzing patterns. Do they use the data or sell them to third parties in a responsible manner? What are the guidelines for making ethical choices by industry?\\nThe ethics includes different aspects of AI such as fairness, bias in AI, explainable artificial intelligence, manipulation of behavior, human-robot interaction, AI safety, adversarial attack, and data privacy. \\n\\nFairness\\xa0is the behavior of AI without privileging one arbitrary group of users over others, e.g. based on their age, gender or race. For example, in the hiring process many AI systems fail to give the equal opportunity to some candidates.\\nBias in AI is the errors in a system with unfair outcomes. For example, some facial recognition algorithms falsely identified African-American and Asian\\xa0faces more than white\\xa0faces due to bias and lack of data in training the machine learning.\\nExplainable and Transparent AI\\xa0means how easily the results of a machine can be understood by humans. Especially with recent AI technologies and deep learning techniques, it is very hard to understand how the system predicts the future using the millions of patterns from the past experience and makes an automatic decision based on it. For example, applications for a credit card or loan approval by AI is hard to track in a transparent way.\\nManipulation of Behavior could happen in business or gaming for example. Most social media platforms and games are designed based on human psychology to leave users to “feel” in control. In gambling the system gives the “illusion of control” to users.\\nEthics of Human-Robot Interaction means AI can be used to manipulate humans into believing and doing things. For example, elderly care must clarify the purpose the robot will serve.\\nAI safety can be defined as the effort to ensure that\\xa0AI\\xa0is deployed in ways that do not harm and humans can control it. \\xa0For example, in designing and deploying self-driving\\xa0cars human safety\\xa0must be the primary objective.\\nEthical Adversaries - Adversaries are inputs to AI models that the attackers have intentionally designed to push AI decision-making towards making mistakes. Then AI will perform unexpected behaviors and make unfair decisions. For example, by adding some noise and wrong data into the input that would cause error, similar to optical illusions for machines.\\xa0\\nData Privacy is responsibly collecting, using and storing\\xa0data pertaining to an individual or group. Data ethics\\xa0is doing the right thing with\\xa0data for human and society. For example, when an organization processes personal\\xa0data\\xa0and sells these data to a\\xa0third party, they must be responsible for data protection when the third party is vulnerable to security or\\xa0violations of privacy.\\n\\n\\xa0\\nKG: I found Stuart Russell’s Human Compatible: Artificial Intelligence and the Problem of Control very informative. Are there other books, articles or podcasts you can recommend to those who’d like to learn more about this topic?\\nAF: I am giving a webinar on Ethics of AI on October 21st, 2020 and the video will be available on Youtube. \\nThere are some resources for better understanding of AI such as: Artificial intelligence and life in 2030 \\nThere are several interesting TED talks about Ethics of AI, such as: Stuart Russell: 3 principles for creating safer AI\\nMore details about the Ethics of Artificial Intelligence and Robotics could be found at: https://plato.stanford.edu/entries/ethics-ai/\\nSome information about Bias in AI could be eye-opener such as: The Truth About Algorithms by Cathy O\\'Neil\\nI think it is time to create a new scientific field to investigate Artificial Intelligence (AI) and its influence on people, their communities, society, and humanity. A discipline to study short-term and long-term goals and impacts of AI which could be called AIology!\\nKG: Thank you, Anna!\\n\\xa0\\nKevin Gray is President of Cannon Gray, a marketing science and analytics consultancy.\\nAnna Farzindar, Ph.D. is a faculty member of the Department of Computer Science, Viterbi School of Engineering, University of Southern California. Her Instagram art page is https://www.instagram.com/annafarzindar and her personal website is www.farzindar.com.\\nAbout the painting:\\xa0\\nTitle:\\xa0\\xa0Ethics of Technology\\nTechnique: watercolor on rice paper\\nSize: 37in x 25in\\nYear:2020\\nArtist: Anna Farzindar\\n\\xa0\\nRelated:\\n\\nFree From Stanford: Ethical and Social Issues in Natural Language Processing\\nAutomatic Text Summarization in a Nutshell\\nRecommender Systems in a Nutshell',\n",
       " 'comments\\nBy Matthew Brems, Growth Manager @ Roboflow\\nYou may have heard about\\xa0OpenAI\\'s CLIP model. If you looked it up, you read that CLIP stands for \"Contrastive Language-Image Pre-training.\" That doesn\\'t immediately make much sense to me, so\\xa0I read the paper\\xa0where they develop the CLIP model –\\xa0and the corresponding blog post.\\nI\\'m here to break CLIP down for you in a – hopefully – accessible and fun read! In this post, I\\'ll cover:\\n\\nwhat CLIP is,\\nhow CLIP works, and\\nwhy CLIP is cool.\\n\\n\\xa0\\nWhat is CLIP?\\n\\xa0\\nCLIP is the first multimodal (in this case, vision and text) model tackling computer vision and was recently released\\xa0by OpenAI\\xa0on January 5, 2021. From the\\xa0OpenAI CLIP repository, \"CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3.\"\\nDepending on your background, this\\xa0may\\xa0make sense -- but there\\'s a lot in here that may be unfamiliar to you. Let\\'s unpack it.\\n\\nCLIP is a neural network model.\\nIt is trained on 400,000,000 (image, text) pairs. An (image, text) pair might be a picture and its caption. So this means that there are 400,000,000 pictures and their captions that are matched up, and this is the data that is used in training the CLIP model.\\n\"It can predict the most relevant text snippet, given an image.\"\\xa0You can input an image into the CLIP model, and it will return for you the likeliest caption or summary of that image.\\n\"without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3.\"\\xa0Most machine learning models learn a specific task. For example, an image classifier trained on classifying dogs and cats is expected to do well on the task we\\'ve given it: classifying dogs and cats. We generally would not expect a machine learning model trained on dogs and cats to be very good at detecting raccoons. However, some models -- including CLIP, GPT-2, and GPT-3 -- tend to perform well on tasks they aren\\'t directly trained to do, which is called \"zero-shot learning.\"\\n\"Zero-shot learning\" is when a model attempts to predict a class it saw zero times in the training data. So, using a model trained on exclusively cats and dogs to then detect raccoons. A model like CLIP, because of how it uses the text information in the (image, text) pairs, tends to do really well with zero-shot learning -- even if the image you\\'re looking at is really different from the training images, your CLIP model will likely be able to give a good guess for the caption for that image.\\n\\nTo put this all together, the CLIP model is:\\n\\na neural network model built on hundreds of millions of images and captions,\\ncan return the best caption given an image, and\\nhas impressive \"zero-shot\" capabilities, making it able to accurately predict entire classes it\\'s never seen before!\\n\\nWhen I wrote my\\xa0Introduction to Computer Vision\\xa0post, I described computer vision as \"the ability for a computer to see and understand what it sees in manner similar to humans.\"\\nWhen I\\'ve taught natural language processing, I described NLP in a similar way: \"the ability for a computer to understand language in manner similar to humans.\"\\nCLIP is a bridge between computer vision and natural language processing.\\nIt\\'s not\\xa0just\\xa0a bridge between computer vision and natural language processing -- it\\'s a\\xa0very powerful bridge\\xa0between the two that has a lot of flexibility and a lot of applications.\\n\\xa0\\nHow does CLIP work?\\n\\xa0\\nIn order for images and text to be connected to one another, they must both be\\xa0embedded. You\\'ve worked with embeddings before, even if you haven\\'t thought of it that way. Let\\'s go through an example. Suppose you have one cat and two dogs. You could represent that as a dot on a graph, like below:\\n\\n\\nEmbedding of \"1 cat, 2 dogs.\" (Source.)\\n\\n\\xa0\\nIt may not seem very exciting, but we just embedded that information on the X-Y grid that you probably learned about in middle school (formally called\\xa0Euclidean space). You could also embed your friends\\' pet information on the same graph and/or you could have chosen plenty of different ways to represent that information (e.g. put dogs before cats, or add a third dimension for raccoons).\\nI like to think of embedding as a way to smash information into mathematical space. We just took information about dogs and cats and smashed it into mathematical space.\\xa0We can do the same thing with text and with images!\\nThe CLIP model consists of two sub-models called encoders:\\n\\na text encoder that will embed (smash) text into mathematical space.\\nan image encoder that will embed (smash) images into mathematical space.\\n\\nWhenever you fit a\\xa0supervised learning model, you have to find some way to measure the \"goodness\" or the \"badness\" of that model – the goal is to fit a model that is as \"most good\" and \"least bad\" as possible.\\nThe CLIP model is no different: the text encoder and image encoder are fit to maximize goodness and minimize badness.\\nSo, how do we measure \"goodness\" and \"badness?\"\\nIn the image below, you\\'ll see a set of purple text cards going into the text encoder. The output for each card would be a series of numbers. For example, the top card,\\xa0pepper the aussie pup\\xa0would enter the text encoder – the thing smashing it into mathematical space – and come out as a series of numbers like (0, 0.2, 0.8).\\nThe exact same thing will happen for the images: each image will go into the image encoder and the output for each image will also be a series of numbers. The picture of, presumably Pepper the Aussie pup, will come out like (0.05, 0.25, 0.7).\\n\\n\\nThe pre-training phase. (Source.)\\n\\n\\xa0\\n\"Goodness\" of our model\\nIn an ideal world, the series of numbers for the text \"pepper the aussie pup\" will be very close (identical) to the series of numbers for the corresponding image.\\xa0In fact, this should be the case everywhere: the series of numbers for the text should be very close to the series of numbers for the corresponding image. One way for us to measure \"goodness\" of our model is how close the embedded representation (series of numbers) for each text is to the embedded representation for each image.\\nThere is a convenient way to calculate the similarity between two series of numbers:\\xa0the cosine similarity. We won\\'t get into the inner workings of that formula here, but rest assured that it\\'s a tried and true method of seeing how similar two vectors, or series of numbers, are. (Though it isn\\'t the only way!)\\nIn the image above, the light blue squares represent where the text and image coincide. For example, T1 is the embedded representation of the first text; I1 is the embedded representation of the first image. We want the cosine similarity for I1 and T1 to be as high as possible. We want the same for I2 and T2, and so on for all of the light blue squares.\\xa0The higher these cosine similarities are, the more \"goodness\" our model has!\\n\"Badness\" of our model\\nAt the same time as wanting to maximize the cosine similarity for each of those blue squares, there are a lot of grey squares that indicate where the text and image don\\'t align. For example, T1 is the text \"pepper the aussie pup\" but perhaps I2 is\\xa0an image of a raccoon.\\n\\n\\nPicture of a raccoon with bounding box annotation. (Source.)\\n\\n\\xa0\\nCute though this raccoon is, we want the cosine similarity between this image (I2) and the text\\xa0pepper the aussie pup\\xa0to be pretty small, because this isn\\'t Pepper the Aussie pup!\\nWhile we wanted the blue squares to all have high cosine similarities (as that measured \"goodness\"), we want all of the grey squares to have low cosine similarities, because that measures \"badness.\"\\n\\n\\nMaximize cosine similarity of the blue squares; minimize cosine similarity of the grey squares. (Source.)\\n\\n\\xa0\\nHow do the text and image encoders get fit?\\nThe text encoder and image encoder get fit at the same time by simultaneously maximizing the cosine similarity of those blue squares and minimizing the cosine similarity of the grey squares, across all of our text+image pairs.\\n\\nNote: this can take a very long time depending on the size of your data. The CLIP model trained on 400,000,000 labeled images. The training process took 30 days across\\xa0592 V100 GPUs. This would have cost $1,000,000 to train on AWS on-demand instances!\\n\\nOnce the model is fit, you can pass an image into the image encoder to retrieve the text description that best fits the image – or, vice versa, you can pass a text description into the model to retrieve an image, as you\\'ll see in some of the applications below!\\nCLIP is a bridge between computer vision and natural language processing.\\n\\xa0\\nWhy is CLIP cool?\\n\\xa0\\nWith this bridge between computer vision and natural language processing, CLIP has a ton of advantages and cool applications. We\\'ll focus on the applications, but a few advantages to call out:\\n\\nGeneralizability: Models are usually super brittle, capable of knowing only the very specific thing you trained them to do. CLIP expands knowledge of classification models to a wider array of things by leveraging semantic information in text. Standard classification models completely discard the semantic meaning of the class labels and simply \\xa0enumerated numeric classes behind the scenes; CLIP works by understanding the meaning of the classes.\\nConnecting text / images better than ever before: CLIP may quite literally be the \"world\\'s best caption writer\" when considering speed and accuracy together.\\nAlready-labeled data: CLIP is built on images and captions that were already created; other state-of-the-art computer vision algorithms required significant additional human time spent labeling.\\n\\n\\n\\nWhy does @OpenAI\\'s CLIP model matter?https://t.co/X7bnSgZ0or\\n— Joseph Nelson (@josephofiowa) January 6, 2021\\n \\n\\xa0\\nSome of the uses of CLIP so far:\\n\\nCLIP has been used to index photos on sites like Unsplash.\\nOne Twitter user took celebrities including Elvis Presley, Beyoncé, and Billie Eilish,\\xa0and used CLIP and StyleGAN to generate portraits in the style of \"My Little Pony.\"\\nHave you played Pictionary? Now you can play online at\\xa0paint.wtf, where you\\'ll be judged by CLIP.\\nCLIP could be used to easily improve NSFW filters!\\nFind photos matching a mood – for example, via a poetry passage.\\nOpenAI has also created\\xa0DALL-E, which creates images from text.\\n\\nWe hope you\\'ll check out some of the above – or create your own! We\\'ve got a\\xa0CLIP tutorial\\xa0for you to follow. If you do something with it,\\xa0let us know so we can add it to the above list!\\nIt\\'s important to note that CLIP is\\xa0a\\xa0bridge between computer vision and natural language processing. CLIP is not the only bridge between them. You could build those text and image encoders very differently or find other ways of connecting the two.\\xa0However, CLIP has so far been an exceptionally innovative technique that has promoted significant additional innovation.\\nWe\\'re eager to see what you build with CLIP and to see the advancements that are built on top of it!\\n\\xa0\\nBio: Matthew Brems is Growth Manager @ Roboflow.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nOpenAI Releases Two Transformer Models that Magically Link Language and Computer Vision\\nEvaluating Object Detection Models Using Mean Average Precision\\nReducing the High Cost of Training NLP Models With SRU++',\n",
       " \"comments\\nBy Roman Orac, Data Scientist.\\n\\nPhoto by\\xa0NASA\\xa0on\\xa0Unsplash.\\nI recently wrote two introductory articles about processing Big Data with\\xa0Dask\\xa0and\\xa0Vaex\\xa0— libraries for processing bigger than memory datasets. While writing, a question popped up in my mind:\\nCan these libraries really process bigger than memory datasets, or is it all just a sales slogan?\\nThis intrigued me\\xa0to do a practical experiment with Dask and Vaex and try to process a bigger than memory dataset. The dataset was so big that you cannot even open it with pandas.\\n\\xa0\\nWhat do I mean by Big Data?\\n\\xa0\\n\\nPhoto by\\xa0ev\\xa0on\\xa0Unsplash.\\nBig Data is a loosely defined term,\\xa0which has as many definitions as there are hits on Google. In this article, I use the term to describe a dataset that is so big that we need specialized software to process it. With Big, I am referring to “bigger than the main memory on a single machine.”\\nDefinition from Wikipedia:\\n\\nBig data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software.\\n\\n\\xa0\\nWhat are Dask and Vaex?\\n\\xa0\\n\\nPhoto by\\xa0JESHOOTS.COM\\xa0on\\xa0Unsplash.\\nDask\\xa0provides advanced parallelism for analytics, enabling performance at scale for the tools you love. This includes numpy, pandas, and sklearn. It is open-source and freely available. It uses existing Python APIs and data structures to make it easy to switch between Dask-powered equivalents.\\nVaex\\xa0is a high-performance Python library for lazy Out-of-Core DataFrames (similar to Pandas) to visualize and explore big tabular datasets. It can calculate basic statistics for more than a billion rows per second. It supports multiple visualizations allowing interactive exploration of big data.\\nDask and Vaex Dataframes are not fully compatible with Pandas Dataframes, but some most common “data wrangling” operations are supported by both tools. Dask is more focused on scaling the code to compute clusters, while Vaex makes it easier to work with large datasets on a single machine.\\n\\xa0\\nThe Experiment\\n\\xa0\\n\\nPhoto by\\xa0Louis Reed\\xa0on\\xa0Unsplash.\\nI generated two CSV files with 1 million rows and 1000 columns. The size of a file was 18.18 GB, which is 36.36 GB combined. Files have random numbers from a Uniform distribution between 0 and 100.\\n\\nTwo CSV files with random data. Photo made by the author.\\n\\xa0\\n\\nimport pandas as pd\\r\\nimport numpy as np\\r\\nfrom os import path\\r\\nn_rows = 1_000_000\\r\\nn_cols = 1000\\r\\nfor i in range(1, 3):\\r\\n    filename = 'analysis_%d.csv' % i\\r\\n    file_path = path.join('csv_files', filename)\\r\\n    df = pd.DataFrame(np.random.uniform(0, 100, size=(n_rows, n_cols)), columns=['col%d' % i for i in range(n_cols)])\\r\\n    print('Saving', file_path)\\r\\n    df.to_csv(file_path, index=False)\\r\\ndf.head()\\r\\n\\r\\n\\n\\n\\xa0\\n\\nHead of a file. Photo made by the author.\\nThe experiment was run on a MacBook Pro with 32 GB of main memory — quite a beast. When testing the limits of a pandas Dataframe, I surprisingly found that reaching a Memory Error on such a machine is quite a challenge!\\nmacOS starts dumping data from the main memory to SSD when the memory is running near its capacity. The upper limit for pandas Dataframe was 100 GB of free disk space on the machine.\\n\\nWhen your Mac needs memory, it will push something that isn’t currently being used into a swapfile for temporary storage. When it needs access again, it will read the data from the swap file and back into memory.\\n\\nI’ve spent some time thinking about how I should address this issue so that the experiment would be fair. The first idea that came to my mind was to disable swapping so that each library would have only the main memory available — good luck with that on macOS. After spending a few hours, I wasn’t able to disable swapping.\\nThe second idea was to use a brute force approach. I’ve filled the SSD to its full capacity so that the operating system couldn’t use swap as there was no free space left on the device.\\n\\nYour disk is almost full notification during the experiment. Photo made by the author.\\nThis worked! pandas couldn’t read two 18 GB files, and Jupyter Kernel crashed.\\nIf I performed this experiment again, I would create a virtual machine with less memory. That way, it would be easier to show the limits of these tools.\\nCan Dask or Vaex help us and process these large files? Which one is faster? Let’s find out.\\n\\xa0\\nVaex vs. Dask\\n\\xa0\\n\\nPhoto by\\xa0Frida Bredesen\\xa0on\\xa0Unsplash.\\nWhen designing the experiment, I thought about basic operations when performing Data Analysis, like grouping, filtering, and visualizing data. I came up with the following operations:\\n\\ncalculating 10th quantile of a column,\\nadding a new column,\\nfiltering by column,\\ngrouping by column and aggregating,\\nvisualizing a column.\\n\\nAll of the above operations perform a calculation using a single column, e.g.:\\n\\n# filtering with a single column\\r\\ndf[df.col2 > 10]\\r\\n\\r\\n\\n\\n\\xa0\\nSo I was intrigued to try an operation, which requires all data to be processed:\\n\\ncalculate the sum of all of the columns.\\n\\nThis can be achieved by breaking down the calculation into smaller chunks. E.g., reading each column separately and calculating the sum, and in the last step calculating the overall sum. These types of computational problems are known as\\xa0Embarrassingly parallel\\xa0— no effort is required to separate the problem into separate tasks.\\n\\xa0\\nVaex\\n\\xa0\\n\\nPhoto by\\xa0Photos by Lanty\\xa0on\\xa0Unsplash.\\nLet’s start with Vaex. The experiment was designed in a way that follows best practices for each tool — this is using binary format HDF5 for Vaex. So we need to convert CSV files to HDF5 format (The Hierarchical Data Format version 5).\\n\\nimport glob\\r\\nimport vaex\\r\\ncsv_files = glob.glob('csv_files/*.csv')\\r\\nfor i, csv_file in enumerate(csv_files, 1):\\r\\n    for j, dv in enumerate(vaex.from_csv(csv_file, chunk_size=5_000_000), 1):\\r\\n        print('Exporting %d %s to hdf5 part %d' % (i, csv_file, j))\\r\\n        dv.export_hdf5(f'hdf5_files/analysis_{i:02}_{j:02}.hdf5')\\r\\n\\r\\n\\n\\n\\xa0\\nVaex needed 405 seconds to covert two CSV files (36.36 GB) to two HDF5 files, which have 16 GB combined. Conversion from text to binary format reduced the file size.\\nOpen HDF5 dataset with Vaex:\\n\\ndv = vaex.open('hdf5_files/*.hdf5')\\r\\n\\r\\n\\n\\n\\xa0\\nVaex needed 1218 seconds to read the HDF5 files. I expected it to be faster as Vaex claims near-instant opening of files in binary format.\\nFrom Vaex documentation:\\n\\nOpening such data is instantenous regardless of the file size on disk: Vaex will just memory-map the data instead of reading it in memory. This is the optimal way of working with large datasets that are larger than available RAM.\\n\\nDisplay head with Vaex:\\n\\ndv.head()\\r\\n\\r\\n\\n\\n\\xa0\\nVaex needed 1189 seconds to display head. I am not sure why displaying the first 5 rows of each column took so long.\\nCalculate 10th quantile with Vaex:\\nNote, Vaex has percentile_approx function, which calculates an approximation of quantile.\\n\\nquantile = dv.percentile_approx('col1', 10)\\r\\n\\r\\n\\n\\n\\xa0\\nVaex needed 0 seconds to calculate the approximation of the 10th quantile for the col1 column.\\nAdd a new column with Vaex:\\n\\ndv[‘col1_binary’] = dv.col1 > dv.percentile_approx(‘col1’, 10)\\r\\n\\r\\n\\n\\n\\xa0\\nVaex has a concept of virtual columns, which stores an expression as a column. It does not take up any memory and is computed on the fly when needed. A virtual column is treated just like a normal column. As expected, Vaex needed 0 seconds to execute the command above.\\nFilter data with Vaex:\\nVaex has a concept of\\xa0selections, which I didn’t use as Dask doesn’t support selections, which would make the experiment unfair. The filter below is similar to filtering with pandas, except that Vaex does not copy the data.\\n\\ndv = dv[dv.col2 > 10]\\r\\n\\r\\n\\n\\n\\xa0\\nVaex needed 0 seconds to execute the filter above.\\nGrouping and aggregating data with Vaex:\\nThe command below is slightly different from pandas as it combines grouping and aggregation. The command groups the data by col1_binary and calculate the mean for col3:\\n\\ngroup_res = dv.groupby(by=dv.col1_binary, agg={'col3_mean': vaex.agg.mean('col3')})\\r\\n\\r\\n\\n\\n\\xa0\\n\\nCalculating mean with Vaex. Photo made by the author.\\nVaex needed 0 seconds to execute the command above.\\nVisualize the histogram:\\nVisualization with bigger datasets is problematic as traditional tools for data analysis are not optimized to handle them. Let’s try if we can make a histogram of col3 with Vaex.\\n\\nplot = dv.plot1d(dv.col3, what='count(*)', limits=[0, 100])\\r\\n\\r\\n\\n\\n\\xa0\\n\\nVisualizing data with Vaex. Photo made by the author.\\nVaex needed 0 seconds to display the plot, which was surprisingly fast.\\nCalculate the sum of all columns\\nMemory is not an issue when processing a single column at a time. Let’s try to calculate the sum of all the numbers in the dataset with Vaex.\\n\\nsuma = np.sum(dv.sum(dv.column_names))\\r\\n\\r\\n\\n\\n\\xa0\\nVaex needed 40 seconds to calculate the sum of all columns.\\n\\xa0\\nDask\\n\\xa0\\n\\nPhoto by\\xa0Kelly Sikkema\\xa0on\\xa0Unsplash.\\nNow, let’s repeat the operations above but with Dask. The Jupyter Kernel was restarted before running Dask commands.\\nInstead of reading CSV files directly with Dask’s read_csv function, we convert the CSV files to HDF5 to make the experiment fair.\\n\\nimport dask.dataframe as dd\\r\\nds = dd.read_csv('csv_files/*.csv')\\r\\nds.to_hdf('hdf5_files_dask/analysis_01_01.hdf5', key='table')\\r\\n\\r\\n\\n\\n\\xa0\\nDask needed 763 seconds for conversion. Let me know in the comments if there is a faster way to convert the data with Dask. I tried to read the HDF5 files that were converted with Vaex with no luck.\\nBest practices with Dask:\\n\\nHDF5 is a popular choice for Pandas users with high performance needs. We encourage Dask DataFrame users to store and load data using Parquet instead.\\n\\nOpen HDF5 dataset with Dask:\\n\\nimport dask.dataframe as dd\\r\\n\\r\\nds = dd.read_csv('csv_files/*.csv')\\r\\n\\r\\n\\n\\n\\xa0\\nDask needed 0 seconds to open the HDF5 file. This is because I didn’t explicitly run the compute command, which would actually read the file.\\nDisplay head with Dask:\\n\\nds.head()\\r\\n\\r\\n\\n\\n\\xa0\\nDask needed 9 seconds to output the first 5 rows of the file.\\nCalculate the 10th quantile with Dask:\\nDask has a quantile function, which calculates actual quantile, not an approximation.\\n\\nquantile = ds.col1.quantile(0.1).compute()\\r\\n\\r\\n\\n\\n\\xa0\\nDask wasn’t able to calculate quantile as Juptyter Kernel crashed.\\nDefine a new column with Dask:\\nThe function below uses the quantile function to define a new binary column. Dask wasn’t able to calculate it because it uses quantile.\\n\\nds['col1_binary'] = ds.col1 > ds.col1.quantile(0.1)\\r\\n\\r\\n\\n\\n\\xa0\\nFilter data with Dask:\\n\\nds = ds[(ds.col2 > 10)]\\r\\n\\r\\n\\n\\n\\xa0\\nThe command above needed 0 seconds to execute as Dask uses the delayed execution paradigm.\\nGrouping and aggregating data with Dask:\\n\\ngroup_res = ds.groupby('col1_binary').col3.mean().compute()\\r\\n\\r\\n\\n\\n\\xa0\\nDask wasn’t able to group and aggregate the data.\\nVisualize the histogram of col3:\\n\\nplot = ds.col3.compute().plot.hist(bins=64, ylim=(13900, 14400))\\r\\n\\r\\n\\n\\n\\xa0\\nDask wasn’t able to visualize the data.\\nCalculate the sum of all columns:\\n\\nsuma = ds.sum().sum().compute()\\r\\n\\r\\n\\n\\n\\xa0\\nDask wasn’t able to sum all the data.\\n\\xa0\\nResults\\n\\xa0\\nThe table below shows the execution times of the Vaex vs. Dask experiment. NA means that the tool couldn’t process the data, and Jupyter Kernel crashed.\\n\\nSummary of execution times in the experiment. Photo made by the author.\\n\\xa0\\nConclusion\\n\\xa0\\n\\nPhoto by\\xa0Joshua Golde\\xa0on\\xa0Unsplash.\\nVaex requires conversion of CSV to HDF5 format, which doesn’t bother me as you can go to lunch, come back, and the data will be converted. I also understand that in harsh conditions (like in the experiment) with little or no main memory reading data will take longer.\\nWhat I don’t understand is the time that Vaex needed to display the head of the file (1189 seconds for the first 5 rows!). Other operations in Vaex are heavily optimized, which enables us to do interactive data analysis on bigger than main memory datasets.\\nI kinda expected the problems with Dask as it is more optimized for compute clusters instead of a single machine. Dask is built on top of pandas, which means that operations that are slow in pandas, stay slow in Dask.\\nThe winner of the experiment is clear. Vaex was able to process bigger than the main memory file on a laptop while Dask couldn’t. This experiment is specific as I am testing performance on a single machine, not a compute cluster.\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nPandas on Steroids: End to End Data Science in Python with Dask\\nWhy and How to Use Dask with Big Data\\nGood-bye Big Data. Hello, Massive Data!\",\n",
       " 'comments\\nBy Devin Petersohn, PhD Student at UC Berkeley.\\n\\nIn most companies, Data Engineers support Data Scientists in various ways. Often this means translating or productionizing the notebooks and scripts that a Data Scientist has written.\\xa0A large portion of the Data Engineer’s role could be replaced with better tooling for Data Scientists, freeing Data Engineers to do more impactful (and scalable) work.\\n\\xa0\\nWhy does this matter?\\n\\xa0\\nThere’s a sentiment making its way around the internet (again): We don’t need Data Scientists, we need Data Engineers.\\n\\n(source)\\n\\n(source)\\nThese articles focus on the\\xa0number of available job positions for the title of “Data Engineer” vs. “Data Scientist.” Let’s put aside the fact that the hiring managers who post these positions often don’t know the difference between the two jobs and use them interchangeably (or use whatever is in style at the moment). For the sake of this article, we can take the existence of the positions at face value. The question then becomes:\\xa0Is the surplus of available Data Engineer positions solely a personnel problem?\\n\\xa0\\nData Science is messy because it reflects the real world\\n\\xa0\\nData Scientists are domain experts (on top of knowing statistics), and they don’t often have a strong background in programming. I’ve seen this expertise discounted in multiple Twitter and forum threads, with software engineers and other “technical people” asking questions like “Why don’t they just learn Spark?” This type of mentality completely misses the fact that Data Scientists can already do what they want to do at smaller scales with their existing tools. Data Scientists want to gain insights, not worry about building elegant pipelines. Companies want something actionable, not beautiful.\\n\\nInsights are more important than elegant pipelines.\\n\\nPopular Data Science tools are also criticized by more technical people and academics: “Why would anyone use pandas?”\\xa0pandas must be the most popular tool to hate by people who have no use for it. It is loved (or at least appreciated) by the Data Scientists who use it daily, however.\\npandas, among other tools, was built to handle the\\xa0messiness\\xa0of the real world. Just look at how many parameters\\xa0read_csv\\xa0has:\\n\\n(read_csv reference)\\nIf pandas is so bad, why has nothing unseated it as the standard dataframe for Python Data Science? Why does it continue to grow in adoption year after year? It’s not the fastest, it’s not the most robust, so why?\\n\\xa0\\nData Engineers have to handle the messiness that scalable tools can’t\\n\\xa0\\nThe scalable systems (e.g., Apache Spark) that are robust enough for production use can’t handle the messiness of the real world as-is. It’s difficult to scale without clean and simple assumptions, and the messier the problem, the harder it is to scale. Data Engineers handle the messiness because scalable tools can’t.\\nMessiness, in this case, can mean:\\n\\nGroup/Join Key Skew\\nPartitioning\\nDebugging Distributed Systems\\nCluster configuration and resource provisioning\\n\\nNone of these are things that you have to worry about with smaller-scale systems. Outside of the Bay Area, most Data Engineers spend time debugging and translating to a distributed system, usually Spark.\\n\\nMultiple rewrites are necessary to turn one-time insights into production jobs.\\nWe can’t really fault anyone here. The people who built the scalable tools in use today were building for highly technical users like themselves. Highly technical people don’t need their tooling to handle messiness for them, and often they want knobs to tune.\\xa0Dogfooding\\xa0is a popular concept in system engineering: “those that built it also use it.” I think worrying so heavily about dogfooding can in part cause the landscape we are seeing in data science today: “only people as technical as those that built the system\\xa0can\\xa0use it.”\\n\\xa0\\nWhat, then, should Data Engineers do?\\n\\xa0\\nThe Data Science ecosystem needs systems that don’t\\xa0only\\xa0focus on the problems of those building it. Data Scientists have been mostly stuck using the same or similar tools for the last 10+ years. The explanation for this is twofold: (1) Data Scientists love using their existing tools because they understand them, and (2) those who are capable of building large-scale systems have largely (unintentionally) overlooked the problems of those less technical than they.\\nWe need Data Engineers to help build scalable tools that empower Data Scientists, not translate pandas to Spark. Who better to help build the next generation of Data Science tools than today’s Data Engineers?\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nHow to Build an Impressive Data Science Resume\\nWhy You Should Consider Being a Data Engineer Instead of a Data Scientist\\nData careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space',\n",
       " \"Here are the most popular KDnuggets posts in February:\\r\\n\\nMost Viewed - Platinum Badge (>30,000 UPV)\\n\\n We Don't Need Data Scientists, We Need Data Engineers, by Mihail Eric (*)\\r\\n How to create stunning visualizations using python from scratch, by Sharan Kumar R (*)\\r\\n How to Get Your First Job in Data Science without Any Work Experience, by Madison Hunter (*)\\r\\n Telling a Great Data Story: A Visualization Decision Tree, by Stan Pugsley (*)\\r\\n Data Science vs Business Intelligence, Explained, by Stan Pugsley (*)\\r\\n\\n\\nMost Viewed - Gold Badge (>18,000 UPV)\\n\\n Powerful Exploratory Data Analysis in just two lines of code, by Francois Bertrand (*)\\r\\n Data Science Learning Roadmap for 2021, by Harshit Tyagi\\r\\n The Best Data Science Project to Have in Your Portfolio, by Soner Yildirim (*)\\r\\n Machine Learning Systems Design: A Free Stanford Course, by Matthew Mayo (*)\\r\\n Cartoon: Data Scientist vs Data Engineer, by Gregory Piatetsky (*)\\r\\n\\n\\nMost Viewed - Silver Badge (> 7,500 UPV)\\n\\n How Reading Papers Helps You Be a More Effective Data Scientist, by Eugene Yan\\r\\n Build Your First Data Science Application, by Naser Tamimi\\r\\n Deep learning doesn't need to be a black box, by Ben Dickson\\r\\n Approaching (Almost) Any Machine Learning Problem, by Matthew Mayo\\r\\n How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals, by Emma Ding\\r\\n 3 Ways Understanding Bayes Theorem Will Improve Your Data Science, by Nicole Janeway Bills\\r\\n\\n\\n\\n\\nMost Shared - Platinum Badge (>1,000 shares)\\n\\n We Don't Need Data Scientists, We Need Data Engineers, by Mihail Eric\\r\\n\\n\\nMost Shared - Gold Badge (>500 shares)\\n\\n How to Get Your First Job in Data Science without Any Work Experience, by Madison Hunter\\r\\n How to create stunning visualizations using python from scratch, by Sharan Kumar R\\r\\n Telling a Great Data Story: A Visualization Decision Tree, by Stan Pugsley\\r\\n Data Science vs Business Intelligence, Explained, by Stan Pugsley\\r\\n Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall, by Ahmed Gad (*)\\r\\n 6 NLP Techniques Every Data Scientist Should Know, by Sara Metwalli (*)\\r\\n\\n\\nMost Shared - Silver Badge (>300 shares)\\n\\n Data Science Learning Roadmap for 2021, by Harshit Tyagi\\r\\n Essential Math for Data Science: Introduction to Matrices and the Matrix Product, by Hadrien Jean (*)\\r\\n Deep learning doesn't need to be a black box, by Ben Dickson\\r\\n Machine Learning Systems Design: A Free Stanford Course, by Matthew Mayo\\r\\n Getting Started with 5 Essential Natural Language Processing Libraries, by Matthew Mayo (*)\\r\\n Build Your First Data Science Application, by Naser Tamimi\\r\\n\\n\\r\\n(*) indicates that badge added or upgraded based on these monthly results.\\r\\n\\nMost Shareable (Viral) Blogs\\r\\nAmong the top blogs, here are the blogs with the highest ratio of shares/unique views, which suggests that people who read it really liked it. \\r\\n\\n 6 NLP Techniques Every Data Scientist Should Know, by Sara Metwalli\\r\\n Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall, by Ahmed Gad\\r\\n Getting Started with 5 Essential Natural Language Processing Libraries, by Matthew Mayo\\r\\n Essential Math for Data Science: Introduction to Matrices and the Matrix Product, by Hadrien Jean\\r\\n 10 resources for data science self-study, by Benjamin Obi Tayo\",\n",
       " 'comments\\nBy Liudmyla Taranenko, Data Scientist at MobiDev\\n\\xa0\\nCan artificial intelligence positively change the world?\\n\\xa0\\nWired reports a gender bias exists in AI and, in 2018, found that only 12% of AI researchers are women. When I started my career as a Data Analyst, a Data Science engineer position was not widely available in Ukraine. And mostly, it wasn’t available for female Maths graduates without special skills and experience. Self-education and getting acquainted with ML algorithms took me some time and a lot of effort. Nowadays, I work as an AI engineer at MobiDev, and the more experience I get, the more willing I am to share my experiences with people in my articles and webinars.\\xa0\\nToday, I want to talk about some exceptional women working in AI who inspire me in my everyday work.\\xa0\\nTwo of them, Joy Buolamwini and Fei-Fei Li, along with others, were honored for International Women’s Day in 2019 by KDnuggets.\\n\\xa0\\nJoy Buolamwini - Founder, Algorithmic Justice League\\n\\xa0\\nAlgorithmic Justice League’s mission is to raise public awareness about AI and its impact on the reduction of AI-bias and harm. Her pioneering research, published in 2018/2019, showed how facial recognition software used by Amazon, Microsoft, IBM, and other companies was not “machine neutral.” It had a distinctly-lower performance when trying to recognize darker female faces as accurately as those of white men. Fast Company reported that the software was taken off the market for re-configuration based on Buolamwini’s research.\\n\\xa0\\nFei-Fei Li - Professor of Computer Science, Stanford University\\n\\xa0\\nFei-Fei Li is a Stanford professor and the founder of Stanford’s Human-Centered AI Institute. She developed ImageNet, first published in 2009, as a training tool to teach AI how to identify objects. ImageNet was considered the birth of AI-dataset training, with the first tests achieving 71.8% accuracy. Since then, the annual ImageNet challenge is a contest to see which algorithm identifies objects with the lowest error rate. The last competition was held in 2017 when the winning algorithm hit 97.3% accuracy, which is better than human abilities.\\n\\xa0\\nBut here are eight other women, not yet honored by KDNuggets, who use AI in positive ways and whose contributions to make a better world are significant. The list is in alphabetical order by last name.\\n\\n\\xa0\\n1. Monica Abarca, Co-Founder & CEO, qAIRa\\xa0 – Making the Planet Cleaner\\n\\xa0\\nAs the co-founder and CEO of qAIRa, Monica Abarca develops technological solutions to deal with air contamination. Dangerous levels of air contamination are responsible for over four million deaths each year. The qAIRa company combines drone technology with air-quality monitoring, using data analytics to identify air contamination, and produces real-time maps showing critical areas.\\nGovernment officials use this information to protect the public from the dangers of air-borne contaminants in an emergency, such as an industrial gas leak, natural disaster, or chemical spill. Another use for these AI-driven systems is to improve overall air quality by effectively monitoring targeted areas subject to air-pollution abatement initiatives.\\n\\xa0\\n2. Regina Barzilay, Faculty Lead & Professor, MIT\\xa0 – Helping to Prevent Breast Cancer\\n\\xa0\\nWorking as the faculty lead and a professor at MIT, Regina Barzilay has long been a thought leader in this AI space. She was the first person to receive the $1 million prize for the AAAI Squirrel AI Award for Artificial Intelligence for the Benefit of Humanity.\\nProfessor Barzilay is a strong advocate for having standards that ensure equity and fairness in applying AI technology, especially in medicine. As an example, she points to the Tyrer-Cuzick model used to determine a patient’s risk of getting breast cancer by analyzing imaging data. This system has a modest accuracy with white women but fails miserably with women of African or Asian descent.\\nThis failure happens because the software has not been adequately machine-trained on enough sets of images from racially-diverse groups. This problem is due to undesirable researcher-bias. It is readily proven and unconscionable. Professor Barzilay insists that software developers eliminate bias by validating AI software on different groups of people or by allowing it to be open source to compare system accuracy using diverse models.\\n\\xa0\\n3. Hulya Emir-Farinas, Director of Data Science, FitBit\\xa0 – Researching People’s Motivation to Lead a Healthier Life\\n\\xa0\\nAs the Director of Data Science at Fitbit R&D, Dr. Emir-Farinas works with AI applications using machine learning combined with behavioral science and healthcare. She is working to answer questions that include:\\n\\nWhat motivates an individual to pursue a healthier lifestyle?\\nWhat are real and imagined barriers that prevent making a positive change?\\nHow does Fitbit enhance its users’ ability to make lifestyle changes?\\n\\nAnswering such questions necessitates a multi-disciplinary approach involving behavioral economics, behavior science, health science, and machine learning to deliver the appropriate interventions, and encouragement, and to make the solution “smart” with more personalization.\\n\\xa0\\n4. Dina Machuve, Lecturer and Researcher at the Nelson Mandela African Institution of Science and Technology – Improving Agriculture\\n\\xa0\\nAt the Nelson Mandela African Institution of Science and Technology, Dina Machuve is a lecturer and researcher. She focuses on creating data-driven solutions to improve agriculture. One application is a diagnostics system that helps identify diseases in poultry using bioinformatics and computer vision technology.\\nThere are over 380 million household farms that create food for 70% of the people living in developing countries. Manchuve’s solution, now deployed in Tanzania, uses systematic data collection and analysis in small to medium sized farms. The project demonstrates the value of using AI methods of deep learning for disease diagnostics to improve livestock health. It works by collecting data for analysis in low-resourced settings from the 3.7 million households that raise chickens.\\n\\xa0\\n5. Deborah Raji, Fellow, Mozilla – Preventing Demographic Bias\\n\\xa0\\nDeborah Raji was given the 2020 Barlow award at the Electronic Frontier Foundation Pioneer Award Ceremony for her work on racial-bias in AI. Her concentration is the negative impact AI has on minorities when dealing with the American justice system. She advocates eradicating and replacing the seriously-flawed facial recognition and surveillance systems used by law enforcement in many American cities.\\n\\xa0\\n6. Tempest van Schaik, Senior Machine Learning Engineer, Microsoft – Helping Children with Cystic Fibrosis\\n\\xa0\\nAs a Senior Machine Learning Engineer at Microsoft, Tempest van Schaik is on the Commercial Software Engineering team in the Data Science division for Microsoft’s Azure cloud-based services. This team is responsible for coding high-level AI projects for the cloud.\\nOne exciting project is Fizzyo, a device that improves physiotherapy for patients with Cystic Fibrosis. It turns breathing exercises into controls for video games. This device makes the experience more fun for children undergoing therapy and provides data collection of every breath while the child plays, which leads to better treatment outcomes.\\n\\xa0\\n7. Lucy Vasserman, Staff Software Engineer, Google – Helping Animal Conservation\\n\\xa0\\nAt Google, Lucy Vasserman works as a staff software engineer on innovative AI projects. Google partnered with conservation groups to use AI to study wildlife. Since the 1990s, the groups have collected 4.5 million photos of animals using camera traps. That database of images formed the initial image library for Wildlife Insights. Others can add camera trap images to help map wildlife globally and to build the growing image database. Anyone can access the database online to explore the photo and the location maps of the camera traps.\\nVasserman worked on the program to train it to identify around 100 species. It can process and categorize 3,600 photos per hour. The machine learning software analyses the images to discover trends such as the population size of individual species, predator/prey interactions, and how wild animals respond to human hunting and encroachment on their habitat.\\n\\xa0\\n8. Fernanda Viégas, Principal Scientist, Google – Making Attractive Visualizations of Complex Data\\n\\xa0\\nAt Google, Fernanda Viégas works as a data visualization expert. Examples of the visualization systems created by her with other Big Picture team members, can be experienced at fernandaviegas.com. She is the co-leader of Google’s PAIR (People + AI Research).\\nSome of her projects show wind currents, collaboration patterns of Wikipedia editorial activities, and dynamic maps of world-news events. Dr. Viégas produces highly-visual, data-driven artwork that is part of a permanent collection at the Museum of Modern Art in New York.\\nAs a Latina who came from a non-technical background, she obtained her doctorate at the MIT Media Lab. She makes an effort to increase diversity initiatives in technological fields to reduce the underrepresentation of women and minorities in data science.\\n\\xa0\\nConclusion\\n\\xa0\\nWomen are encouraged to get involved in AI due to its world-changing qualities and to reduce the gender-bias that exists. Take inspiration from the late Justice Ruth Bader Ginsburg of the U.S. Supreme Court. When asked, “How many justices would she like to be women on the Supreme Court?” She said, “All of them.”\\n\\xa0\\nBio: Liudmyla Taranenko is a Data Scientist at MobiDev.\\nRelated:\\n\\n19 Inspiring Women in AI, Big Data, Data Science, Machine Learning\\nResources for Women in AI, Data Science, and Machine Learning\\nGender Diversity in AI Research',\n",
       " \"By Benjamin Obi Tayo, Ph.D., DataScienceHub.\\ncomments\\n\\nIt's always good idea to maintain two versions of your project, one locally and the other on Github.\\nThis article will discuss some useful tips that will enable you to better organize your data science projects. Before delving into some tips for data science project management, let’s first discuss why it is important to organize your project.\\n\\xa0\\n4 Reasons why it is important to organize your project\\n\\xa0\\n\\nOrganization increases productivity. If a project is well organized, with everything placed in one directory, it makes it easier to avoid wasting time searching for project files such as datasets, codes, output files, and so on.\\nA well-organized project helps you to keep and maintain a record of your ongoing and completed data science projects.\\nCompleted data science projects could be used for building future models. If you have to solve a similar problem in the future, you can use the same code with slight modifications.\\nA well-organized project can easily be understood by other data science professionals when shared on platforms such as Github.\\n\\nFor illustrative purposes, we will use the cruise ship data set. We assume that we would like to build a machine learning model for recommending cruise ship crew size based on predictor variables such as age, tonnage, passengers, length, cabins, etc. In section I, we describe how the project should be organized locally. Then in section I, we describe how to create a Github repository for the project. It is always recommended that you maintain two versions of your project, one locally and the other on Github. The advantage of this is that you can access the Github version of your project from anywhere in the world and at any time, as long as you have an internet connection. Another advantage is that if something were to happen with your local computer that could impact your computer adversely, such as viruses in the computer, then you can always be confident that you still have your project files on Github that can serve as a backup.\\n\\xa0\\nI. Local Project Directory\\n\\xa0\\nIt is good to have a project directory for each project that you are working on.\\nDirectory Name\\nWhen creating a project directory for your project, it’s good to select a directory name that reflects your project, for example, for the machine learning model for recommending crew size, one may choose a directory name such as\\xa0ML_Model_for_Predicting_Ships_Crew_Size.\\nDirectory content\\nYour project directory should contain the following:\\n(1)\\xa0Project plan:\\xa0This could be a world document where you describe what your project is all about. You may start by providing a brief synopsis followed by step by step plan of what you would like to accomplish. For example, before building a model, you may ask yourself:\\n\\nWhat are the predictor variables?\\nWhat is the target variable? Is my target variable discrete or continuous?\\nShould I use classification or regression analysis?\\nHow do I handle missing values in my dataset?\\nShould I use normalization or standardization when bringing variables to the same scale?\\nShould I use Principal Component Analysis or not?\\nHow do I tune hyperparameters in my model?\\nHow do I evaluate my model to detect biases in the dataset?\\nShould I use ensemble methods where I train using different models then perform an ensemble average, e.g. using classifiers such as SVM, KNN, Logistic Regression, then average over 3 models?\\nHow do I select the final model?\\n\\n(2) Project datasets: You should include the comma separated value (CSV) files for all the datasets to be used for the project. In this example, there is just one CSV file:\\xa0cruise_ship_info.csv.\\n(3) Project Codes:\\xa0Once you’ve figured out what your project plans and objectives are, it is time to start coding. Depending on the type of problem you are solving, you may decide to use a jupyter notebook or an R script for writing your code. Let’s just assume we are going to be using a jupyter notebook.\\nOn your jupyter notebook, start by adding a project heading or title, for example:\\n\\nMachine Learning Model for Predicting a Ship’s Crew Size\\r\\n\\r\\n\\n\\n\\xa0\\nThen you may provide a brief synopsis of your project, followed by the author’s name, and date, for example:\\n\\nWe build a simple model using the cruise_ship_info.csv data set for predicting a ship’s crew size. \\r\\nThis project is organized as follows: \\r\\n(a) data preprocessing and variable selection; \\r\\n(b) basic regression model; \\r\\n(c) hyper-parameters tuning; and \\r\\n(d) techniques for dimensionality reduction.\\r\\n\\r\\nAuthor: Benjamin O. Tayo\\r\\nDate: 4/8/2019\\r\\n\\r\\n\\n\\n\\xa0\\nAs you develop the code, you want to make sure that the jupyter notebook is well organized into sections that highlight the machine learning model building workflow, such as:\\n\\nImportation of necessary python libraries\\r\\n\\r\\nImportation of dataset\\r\\n\\r\\nExploratory data analysis\\r\\n\\r\\nFeature selection and dimensionality reduction\\r\\n\\r\\nFeature scaling and data partitioning into train and test sets\\r\\n\\r\\nModel building, testing, and evaluation\\r\\n\\r\\n\\n\\n\\xa0\\nFor sample project jupyter notebook and R script files, please see the following Github reps,\\xa0bot13956/ML_Model_for_Predicting_Ships_Crew_Size and bot13956/weather_pattern.\\n(4) Project Outputs:\\xa0You may also store key project outputs in your local directory. Some key project outputs could be data visualizations, graphs illustrating model error as a function of different parameters, or tables containing key outputs such as R2 values, mean square errors, or regression coefficients. Project outputs are very handy because they can be used to prepare project reports or PowerPoint presentation slides to be presented to your data science team or to the business administrators in your company.\\n(5) Project report: In some cases, you may have to put together a project report to describe project accomplishments and provide prescribed actions to be taken based on the findings and insights from your model. In this case, you need to put together a project report using MS word. When writing a project report, you can make good use of some visualizations produced from your main code. You want to add these to the report. Your main code may be added as an appendix to the project report.\\nAn example of a project report file can be found at\\xa0bot13956/Monte_Carlo_Simulation_Loan_Status.\\n\\xa0\\nII. Github Project Directory\\n\\xa0\\nOnce you’ve solved the problem of interest, you then have to create a project repository on GitHub and upload project files such as datasets, jupyter notebooks, R program scripts, and sample outputs. Creating a GitHub repository for any data science project is extremely important. It enables you to have access to your code at all times. You get to share your code with a community of programmers and other data scientists. Also, it is a means for you to showcase your data science skills.\\nTips for creating a Github repository:\\xa0Make sure you choose a suitable title for your repository. For example:\\n\\nRepository Name: bot13956/ML_Model_for_Predicting_Ships_Crew_Size\\r\\n\\r\\n\\n\\n\\xa0\\nThen include a README file to provide a synopsis of what your project is all about.\\n\\nAuthor: Benjamin O. Tayo\\r\\nDate: 4/8/2019\\r\\n\\r\\nWe build a simple model using the cruise_ship_info.csv data set for predicting a ship's crew size. \\r\\nThis project is organized as follows: \\r\\n(a) data preprocessing and variable selection; \\r\\n(b) basic regression model; \\r\\n(c) hyper-parameters tuning; and \\r\\n(d) techniques for dimensionality reduction.\\r\\n\\r\\ncruise_ship_info.csv: dataset used for model building.\\r\\n\\r\\nShip_Crew_Size_ML_Model.ipynb: the jupyter notebook containing code.\\r\\n\\r\\n\\n\\n\\xa0\\nThen you may upload your project files, including the dataset, jupyter notebook, and sample outputs.\\nHere is an example of a Github repository for a machine learning project:\\nRepository URL:\\xa0https://github.com/bot13956/ML_Model_for_Predicting_Ships_Crew_Size.\\nIn summary, we’ve discussed how a data science project has to be organized. Good organization leads to better productivity and efficiency. When next you have to work on a new project, please take the time to organize your project. This will not only help with increasing efficiency and productivity, but it will also help to minimize errors. Also, keeping good records of all your current and completed projects enables you to create a repository where you can save all your projects for future use.\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nLearn to build an end to end data science project\\nAutomatic Version Control for Data Scientists\\nSoftware engineering fundamentals for Data Scientists\",\n",
       " \"comments\\nBy Michael Grogan, Data Science Consultant\\n\\n\\nSource: Photo by\\xa0Tumisu\\xa0from\\xa0Pixabay\\n\\n\\xa0\\nTools such as Python or R are most often used to conduct deep time series analysis.\\nHowever, knowledge of how to work with time series data using SQL is essential, particularly when working with very large datasets or data that is constantly being updated.\\nHere are some useful commands that can be invoked in SQL to better work with time series data within the data table itself.\\n\\xa0\\nBackground\\n\\xa0\\nIn this example, we are going to work with weather data collected across a range of different times and locations.\\nThe data types\\xa0in the table of the PostgreSQL database are as below:\\n\\nweather=# SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'weatherdata';\\r\\n column_name |          data_type          \\r\\n-------------+-----------------------------\\r\\n date        | timestamp without time zone\\r\\n mbhpa       | integer\\r\\n temp        | numeric\\r\\n humidity    | integer\\r\\n place       | character varying\\r\\n country     | character varying\\r\\n realfeel    | integer\\r\\n timezone    | integer\\r\\n(8 rows)\\n\\n\\nAs you can see,\\xa0date\\xa0is defined as a timestamp data type without time zone (which we will also look at in this article).\\nThe variable of interest is temp (temperature) — we are going to look at ways to analyse this variable more intuitively using SQL.\\n\\xa0\\nCalculating Moving Averages\\n\\xa0\\nHere is a snippet of some of the columns in the data table:\\n\\n        date         | mbhpa | temp  | humidity \\r\\n---------------------+-------+-------+----------\\r\\n 2020-10-12 18:33:24 |  1010 | 13.30 |       74\\r\\n 2020-10-15 02:12:54 |  1017 |  7.70 |       75\\r\\n 2020-10-14 23:53:42 |  1016 |  8.80 |       75    \\r\\n 2020-10-15 11:03:25 |  1016 |  9.70 |       75      \\r\\n 2020-10-15 13:05:23 |  1017 | 12.30 |       74    \\r\\n 2020-10-15 18:47:25 |  1015 | 12.10 |       74     \\r\\n 2020-10-16 23:23:23 |  1011 |  9.10 |       75   \\r\\n 2020-10-20 10:25:15 |   967 | 13.80 |       83   \\r\\n 2020-10-27 16:30:30 |   980 | 12.00 |       75   \\r\\n 2020-10-29 15:12:07 |   988 | 11.70 |       75   \\r\\n 2020-10-28 18:42:52 |   990 |  8.80 |       77\\n\\n\\nSuppose we wish to calculate a moving average of temperature across different time periods.\\nTo do this, we firstly need to make sure that the data is ordered by date, and decide on how many periods should be included in the averaging window.\\nTo start with, a 7-period moving average is used, with all temperature values ordered by date.\\n\\n>>> select date, avg(temp) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) FROM weatherdata where place='Place Name';        date         |         avg         \\r\\n---------------------+---------------------\\r\\n 2020-11-12 16:36:40 |  8.8285714285714286\\r\\n 2020-11-14 15:45:08 |  9.8142857142857143\\r\\n 2020-11-15 08:53:26 | 10.3857142857142857\\r\\n 2020-11-17 10:50:32 | 11.2285714285714286\\r\\n 2020-11-18 14:18:58 | 11.8000000000000000\\r\\n 2020-11-25 14:54:11 | 11.6285714285714286\\r\\n 2020-11-25 19:00:21 | 10.9142857142857143\\r\\n 2020-11-25 19:05:31 | 10.2000000000000000\\r\\n 2020-11-25 23:41:34 |  9.2857142857142857\\r\\n 2020-11-26 15:03:10 |  9.4857142857142857\\r\\n 2020-11-26 17:18:33 |  8.3428571428571429\\r\\n 2020-11-26 21:30:39 |  7.9142857142857143\\r\\n 2020-11-26 22:29:17 |  7.6142857142857143\\n\\n\\nNow, let’s add a 30 and 60-period moving average. We will store these averages along with the 7-period moving average in the one table.\\n\\n>>> select date, temp, avg(temp) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW), avg(temp) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW), avg(temp) OVER (ORDER BY date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW), avg(temp) OVER (ORDER BY date ROWS BETWEEN 59 PRECEDING AND CURRENT ROW) FROM weatherdata where place='Place Name';\\n\\n\\n\\n\\nSource: Output Created by Author\\n\\n\\xa0\\nMore information on how to calculate moving averages within SQL can be found at the following resource by\\xa0sqltrainingonline.com.\\n\\xa0\\nWorking with time zones\\n\\xa0\\nYou will notice that the timestamp contains a date and time. While this is fine when storing just one location in the table, things can get quite tricky when working with locations across multiple time zones.\\nNote that an integer variable named\\xa0timezone\\xa0was created within the table.\\nSuppose that we are analysing weather patterns across different places with a range of time zones ahead of the inputted time — in this case, all data points were inputted at GMT time.\\n\\n       date          | timezone \\r\\n---------------------+----------\\r\\n 2020-05-09 15:29:00 |       11\\r\\n 2020-05-09 17:05:00 |       11\\r\\n 2020-05-09 17:24:00 |       11\\r\\n 2020-05-10 13:02:00 |       11\\r\\n 2020-05-13 19:13:00 |       11\\r\\n 2020-05-10 13:04:00 |       11\\r\\n 2020-05-10 15:47:00 |       11\\r\\n 2020-05-13 19:10:00 |       11\\r\\n 2020-05-14 17:17:00 |       11\\r\\n 2020-05-09 15:20:00 |        5\\r\\n 2020-05-09 17:04:00 |        5\\r\\n 2020-05-09 17:25:00 |        5\\r\\n 2020-05-09 18:12:00 |        5\\r\\n 2020-05-10 13:02:00 |        5\\r\\n 2020-05-10 15:50:00 |        5\\r\\n 2020-05-10 20:32:00 |        5\\r\\n 2020-05-11 17:31:00 |        5\\r\\n 2020-05-13 19:11:00 |        5\\r\\n 2020-05-17 21:41:00 |       11\\r\\n 2020-05-15 14:08:00 |       11\\r\\n 2020-05-14 16:55:00 |        5\\r\\n 2020-05-15 14:10:00 |        5\\r\\n(22 rows)\\n\\n\\nThe new times can be calculated as follows:\\n\\nweather=# select date + interval '1h' * timezone from weatherdata;\\r\\n      ?column?       \\r\\n---------------------\\r\\n 2020-05-10 02:29:00\\r\\n 2020-05-10 04:05:00\\r\\n 2020-05-10 04:24:00\\r\\n 2020-05-11 00:02:00\\r\\n 2020-05-14 06:13:00\\r\\n 2020-05-11 00:04:00\\r\\n 2020-05-11 02:47:00\\r\\n 2020-05-14 06:10:00\\r\\n 2020-05-15 04:17:00\\r\\n 2020-05-09 20:20:00\\r\\n 2020-05-09 22:04:00\\r\\n 2020-05-09 22:25:00\\r\\n 2020-05-09 23:12:00\\r\\n 2020-05-10 18:02:00\\r\\n 2020-05-10 20:50:00\\r\\n 2020-05-11 01:32:00\\r\\n 2020-05-11 22:31:00\\r\\n 2020-05-14 00:11:00\\r\\n 2020-05-18 08:41:00\\r\\n 2020-05-16 01:08:00\\r\\n 2020-05-14 21:55:00\\r\\n 2020-05-15 19:10:00\\r\\n(22 rows)\\n\\n\\nWe can now store the new times as an updated variable, which we will name as\\xa0newdate.\\n\\n>>> select date + interval '1h' * (timezone+1) as newdate, temp, mbhpa from weatherdata;      newdate       | temp | mbhpa\\r\\n--------------------+------+-------\\r\\n2020-05-10 03:29:00 |  4.2 |  1010\\r\\n2020-05-10 05:05:00 |  4.1 |  1009\\r\\n2020-05-10 05:24:00 |  3.8 |  1009\\n\\n\\nThis clause allows us to generate the updated times (which would reflect the actual time in each specific place when variables such as temperature, barometric pressure, etc, were recorded.\\n\\xa0\\nInner Join and Having Clauses\\n\\xa0\\nYou will notice in the above table that temperature values are included across a range of places.\\nSuppose that wind speed is also calculated for each place in a separate table.\\nIn this case, we wish to calculate the average temperature across each listed place where the wind speed is higher than 20.\\nThis can be accomplished using the\\xa0inner join\\xa0and\\xa0having\\xa0clauses as follows:\\n\\n>>> select t1.place, avg(t1.temp), avg(t2.gust) from weatherdata as t1 inner join wind as t2 on t1.place=t2.place group by t1.place having avg(t2.gust)>'20';      place      |         avg          |         avg          \\r\\n-----------------+----------------------+----------------------\\r\\n Place 1         |        17.3          |        22.4\\r\\n Place 2         |        14.3          |        26.8\\r\\n Place 3         |        7.1           |        27.1\\n\\n\\n\\xa0\\nConclusion\\n\\xa0\\nIn this article, you have been introduced to some introductory examples of using SQL to work with time series data.\\nIn particular, you saw how to:\\n\\nCalculate moving averages\\nWork with different time zones\\nCalculate averages across different subsets of data\\n\\nMany thanks for your time and any questions or feedback are greatly appreciated.\\nDisclaimer: This article is written on an “as is” basis and without warranty. It was written with the intention of providing an overview of data science concepts, and should not be interpreted as professional advice. The findings and interpretations in this article are those of the author and are not endorsed by or affiliated with any third-party mentioned in this article.\\n\\xa0\\nBio: Michael Grogan is a Data Science Consultant. He posesses expertise in time series analysis, statistics, Bayesian modeling, and machine learning with TensorFlow.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nMultidimensional multi-sensor time-series data analysis framework\\nRejection Sampling with Python\\nDeep Learning Is Becoming Overused\",\n",
       " 'By Kevin Vu, Exxact Corp.\\ncomments\\n\\nWhat is a Deep Learning Framework?\\nA deep learning framework is a software package used by researchers and data scientists to design and train deep learning models. The idea with these frameworks is to allow people to train their models without digging into the algorithms underlying deep learning, neural networks, and machine learning.\\nThese frameworks offer building blocks for designing, training, and validating models through a high-level programming interface. Widely used deep learning frameworks such as PyTorch, TensorFlow, MXNet, and others can also use GPU-accelerated libraries such as cuDNN and NCCL to deliver high-performance multi-GPU accelerated training.\\nWhy Use a Deep Learning Framework?\\n\\nThey supply readily available libraries for defining layers, network types (CNNs, RNNs), and common model architectures\\nThey can support computer vision applications; image, speech, and natural language processing\\nThey have familiar interfaces via popular programming languages such as Python, C, C++, and Scala\\nMany deep learning frameworks are accelerated by NVIDIA deep learning libraries such as cuDNN, NCCl, and cuBLAS for GPU accelerated deep learning training\\n\\n\\xa0\\nExample Frameworks\\n\\xa0\\n\\n\\n\\nFramework\\nQualities\\nDifferentiators\\n\\n\\n\\n\\nTensorFlow\\n\\n\\nEasy to use - well defined APIs, documentation\\nFlexible - ideal for researching and prototyping new ideas\\nMultiple tools for building on top of TensorFlow such as TensorFlow Slim, Scikit Flow, PrettyTensor, Keras, and TFLearn\\nTensorFlow Lite allows for deployment on mobile and embedded devices\\nJavaScript library can deploy models via the web browser and Node.js\\n\\n\\n\\n\\nGreat community engagement and support\\nLarge body of existing TensorFlow samples and code, accelerates research\\nComputational graph visualizations via TensorBoard\\nPython interface\\n\\n\\n\\n\\nAesara\\xa0(successor to\\xa0Theano)\\n\\n\\nAutomatic differentiation as a symbolic expression\\nComputation graph optimizations and on-the-fly code generation for speed, numerical stability, and memory usage\\n\\n\\n\\n\\nLow-level and flexible for research of new ideas\\nPython-based, with NumPy integration\\nNo multi GPU\\n\\n\\n\\n\\nCaffe\\n\\n\\nDesigned for computer vision framework problems\\nToo rigid for researching new algorithms\\nCaffe is in maintenance mode\\n\\n\\n\\n\\nNVIDIA fork of Caffe is maintained and updated by NVIDIA\\nDelivers optimized performance on the latest GPU hardware\\n\\n\\n\\n\\nCaffe2\\n\\n\\nCaffe2 is now a part of PyTorch, and the APIs are being deprecated\\n\\n\\n\\n\\n\\nPyTorch\\n\\n\\nPyTorch is based on Python. It is the successor of Torch, which was based on the Lua programming language\\nPrimary audience is researchers\\nSupports dynamic computational graphs\\nPyTorch 1.0 is a new iteration that includes PyTorch merged with Caffe2 (current stable version is 1.9.0\\xa0from June 2021)\\n\\n\\n\\n\\nNow the primary framework used at Facebook/Used by Facebook FAIR for research\\nExtremely flexible for research\\nShares the same backend as the popular Torch framework it was based on\\n\\n\\n\\n\\nChainer\\n\\n\\nModels that are fast to prototype and easier to debug\\nCuPy: NumPy-equivalent multi-dimensional CUDA® array-library\\nExtensions & Tools: ChainerRL, ChainerMN, for computer vision\\nIs now in maintenance mode since the company behind it is changing their primary framework to PyTorch\\n\\n\\n\\n\\nDynamic computation graphs with a Python API are strengths of Chainer and PyTorch\\n\\n\\n\\n\\nApache MXNet\\n\\n\\nPrimary focus is training and deploying deep neural networks\\nMatlab support\\nPortable and scalable, including multi-GPU and multi-node support\\n\\n\\n\\n\\nSupports 8 language bindings, including C++, Python, Julia, Java, Clojure, R, Scala, and Perl\\nAllows for a mix of symbolic and imperative programming\\n\\n\\n\\n\\nMatlab\\n\\n\\nBeyond just DL classifiers: combine image/vision processing with DL\\nAutomates distributed training and deployment to a data center and embedded without manual coding\\n\\n\\n\\n\\nHigh productivity desktop IDE (integrated development environment) that makes research, prototyping, and debugging easy\\nMATLAB expert technical support\\nEasily integrates into existing MATLAB and simulink workflows\\n\\n\\n\\n\\n\\n\\xa0\\nMore Details on TensorFlow\\n\\xa0\\nAn open-source software library created by Google, TensorFlow is a popular tool for machine learning, especially for training deep neural networks. TensorFlow’s API primarily supports Python and C, but there are also APIs for C++, JavaScript, and Java; however, only the Python API is guaranteed to be stable.\\nTensorFlow’s community has also developed support for a number of other languages, including C#, Haskell, Julia, R, Ruby, Rust, and Scala.\\nThe advantage of TensorFlow is that it has so many entry points. Beyond languages, there is a wide range of tools that integrate with or are built on top of TensorFlow.\\nTensorFlow also has a very large community of users where you can get help, and it’s well documented.\\n\\xa0\\nMore Details on Keras\\n\\xa0\\nKeras is an open-source library that’s focused on providing a simple Python API for neural networks with features such as scalability for GPU clusters. It’s built on top of TensorFlow 2.0 and can also run on Theano.\\nKeras\\xa0has the same portability as TensorFlow, meaning you can run it in a browser, as well as mobile and embedded devices. Keras is used by a number of major organizations, including CERN and NASA.\\n\\xa0\\nMore Details on PyTorch\\n\\xa0\\nPyTorch is another product of the FAANG ecosystem coming from Facebook’s AI Research lab (FAIR). PyTorch is largely focused on computer vision and natural language processing (NLP) tasks. Like TensorFlow, PyTorch’s primary interface language is Python, but there is also C++ support.\\nPyTorch’s community has a number of tools that integrate with the library, such as Skorch for scikit-learn compatibility, extBrewer for NLP, NeMo toolkit for conversational AI, and\\xa0PyTorch Lightning\\xa0which is similar in idea to TensorFlow and Keras in that it’s focused on simplifying the coding required to get a model working.\\nPyTorch is also a good stand-in for NumPy (a popular tool in machine learning and data science) with tensors, which are like NumPy arrays but optimized to run on CPUs or GPUs. PyTorch has an experimental deployment method for mobile devices but is optimized to run on cloud computing platforms, including Amazon Web Services, Google Cloud, and Microsoft Azure.\\nThere are a wide number of deep learning frameworks to choose from. If one of the options listed here doesn’t suit your needs, there are others, including Amazon’s Gluon (based on MXNet), DL4J, and Sonnet.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nGeometric foundations of Deep Learning\\nHigh Performance Deep Learning, Part 1\\nEvaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall',\n",
       " 'comments\\nBy Gowrisankar JG, Software Developer at Hexaware\\nMost Professional Machine Learning practitioners follow the ML Pipeline as a standard, to keep their work efficient and to keep the flow of work. A pipeline is created to allow data flow from its raw format to some useful information. All sub-fields in this pipeline’s modules are equally important for us to produce quality results, and one of them is\\xa0Hyper-Parameter Tuning.\\n\\n\\nA Generalized Machine Learning Pipeline\\n\\n\\xa0\\nMost of us know the best way to proceed with Hyper-Parameter Tuning is to use the GridSearchCV or RandomSearchCV from the sklearn module. But apart from these algorithms, there are many other Advanced methods for Hyper-Parameter Tuning. This is what the article is all about, Introduction to Advanced Hyper-Parameter Optimization, Transfer Learning and when & how to use these algorithms to make out the best of them.\\nBoth of the algorithms, Grid-Search and Random-Search are instances of Uninformed Search. Now, let’s dive deep !!\\n\\xa0\\nUninformed search\\n\\xa0\\nHere in these algorithms, each iteration of the Hyper-parameter tuning does not learn from the previous iterations. This is what allows us to parallelize our work. But, this isn’t very efficient and costs a lot of computational power.\\nRandom search tries out a bunch of hyperparameters from a uniform distribution randomly over the preset list/hyperparameter search space (the number iterations is defined). It is good in testing a wide range of values and normally reaches to a very good combination very fastly, but the problem is that, it doesn’t guarantee to give the best parameter’s combination.\\nOn the other hand, Grid search will give the best combination, but it can takes a lot of time and the computational cost is high.\\n\\n\\nSearching Pattern of Grid and Random Search\\n\\n\\xa0\\nIt may look like grid search is the better option, compared to the random one, but bare in mind that when the dimensionality is high, the number of combinations we have to search is enormous. For example, to grid-search ten boolean (yes/no) parameters you will have to test 1024 (2¹⁰) different combinations. This is the reason, why random search is sometimes combined with clever heuristics, is often used.\\n\\xa0\\nWhy bring Randomness in Grid Search? [Mathematical Explanation]\\n\\xa0\\nRandom search is more of a stochastic search/optimization perspective — the reason we introduce noise (or some form of stochasticity) into the process is to potentially\\xa0bounce out\\xa0of poor local minima. While this is more typically used to explain the intuition in general optimization (like stochastic gradient descent for updating parameters, or learning temperature-based models), we can think of humans looking through the meta-parameter space as simply a higher-level optimization problem. Since most would agree that these dimensional spaces (reasonably high) lead to a non-convex form of optimization, we humans, armed even with some clever heuristics from the previous research, can get stuck in the local optima.\\nTherefore, Randomly exploring the search space might give us better coverage, and more importantly, it might help us find better local optima.\\nSofar in Grid and Random Search Algorithms, we have been creating all the models at once and combining their scores before deciding the best model at the end.\\nAn alternative approach would be to build models sequentially, learning from each iteration. This approach is termed as\\xa0Informed Search.\\n\\xa0\\nInformed Method: Coarse to Fine Tuning\\n\\xa0\\nA basic informed search methodology.\\nThe process follows:\\n\\nRandom search\\nFind promising areas in the search space\\nGrid search in the smaller area\\nContinue until optimal score is obtained\\n\\nYou could substitute (3) with random searches before the grid search.\\n\\xa0\\nWhy Coarse to Fine?\\n\\xa0\\nCoarse to Fine\\xa0tuning\\xa0optimizes and uses the advantages of both grid and random search.\\n\\nWide searching capabilities of random search\\nDeeper search once you know where a good spot is likely to be\\n\\nNo need to waste time on search spaces that are not giving good results !! Therefore, this better utilizes the spending of time and computational efforts, i.e we can iterate quickly, also there is boost in the performance.\\n\\xa0\\nInformed Method: Bayesian Statistics\\n\\xa0\\nThe most popular informed search method is Bayesian Optimization. Bayesian Optimization was originally designed to optimize black-box functions.\\nThis is a basic theorem or rule from\\xa0Probability Theory and Statistics, in case if you want to brush up and get refreshed with the terms used here, refer\\xa0this.\\n\\nBayes Rule | Theorem\\nA statistical method of using\\xa0new evidence\\xa0to iteratively update our beliefs about some\\xa0outcome. In simpler words, it is used to calculate the probability of an event based on its association with another event.\\n\\n\\n\\nSource:\\xa0Bayes Theorem in Data Science\\n\\n\\xa0\\n\\nLHS is the probability of A, given B has occurred. B is some new evidence. This is known as the ‘posterior’.\\nRHS is how we calculate this.\\nP(A) is the ‘prior’. The initial hypothesis about the event. It is different to P(A|B), the P(A|B) is the probability given new evidence.\\nP(B) is the ‘marginal likelihood’ and it is the probability of observing this new evidence.\\nP(B|A) is the ‘likelihood’ which is the probability of observing the evidence, given the event we care about.\\n\\nApplying the logic of Bayes rule to hyperparameter tuning:\\n\\nPick a hyperparameter combination\\nBuild a model\\nGet new evidence (i.e the score of the model)\\nUpdate our beliefs and chose better hyperparameters next round\\n\\n\\nBayesian hyperparameter tuning is quite new but is very popular for larger and more complex hyperparameter tuning tasks as they work well to find optimal\\xa0hyperparameter combinations in these kinds of situations.\\n\\n\\xa0\\nNote\\n\\xa0\\nFor more complex cases you might want to dig a bit deeper and explore all the details about Bayesian optimization. Bayesian optimization can only work on continuous hyper-parameters, and not categorical ones.\\n\\xa0\\nBayesian Hyper-parameter Tuning with HyperOpt\\n\\xa0\\nHyperOpt package, uses a form of Bayesian optimization for parameter tuning that allows us to get the best parameters for a given model. It can optimize a model with hundreds of parameters on a very large scale.\\n\\n\\nHyperOpt:\\xa0Distributed Hyper-parameter Optimization\\n\\n\\xa0\\nTo know more about this library and the parameters of HyperOpt library feel free to visit\\xa0here. And visit\\xa0here\\xa0for a quick tutorial with adequate explanation on how to use HyperOpt for Regression and Classification.\\n\\xa0\\nIntroducing the HyperOpt package.\\n\\xa0\\nTo undertake Bayesian hyperparameter tuning we need to:\\n\\nSet the Domain: Our Grid i.e. search space (with a bit of a twist)\\nSet the Optimization algorithm (default: TPE)\\nObjective function to minimize: we use “1-Accuracy”\\n\\n\\nKnow more about the Optimization Algorithm used, Original Paper of TPE (Tree of Parzen Estimators)\\n\\n\\xa0\\nSample Code for using HyperOpt [ Random Forest ]\\n\\xa0\\nHyperOpt does not use point values on the grid but instead, each point represents probabilities for each hyperparameter value. Here, simple uniform distribution is used, but there are many more if you check the\\xa0documentation.\\n\\nHyperOpt implemented on Random Forest\\n\\nTo really see this in action !!\\xa0try on a larger search space, with more trials, more CVs and a larger dataset size.\\n\\n\\n\\xa0\\nFor practical implementation of HyperOpt refer:\\n\\xa0\\n[1]\\xa0Hyperopt Bayesian Optimization for Xgboost and Neural network\\n[2]\\xa0Tuning using HyperOpt in python\\n\\nCurious to know why XGBoost has high potential in winning competitions ?? Read the below article to expand your knowledge !!\\n\\nXGBoost — Queen of Boosting Algorithms?\\nKaggler’s Favo Algorithm | Understanding How & Why XGBoost is used to win Kaggle competitions\\n\\xa0\\n\\xa0\\nInformed Method: Genetic Algorithms\\n\\xa0\\nWhy does this work well?\\n\\nIt allows us to learn from previous iterations, just like Bayesian hyperparameter tuning.\\nIt has the additional advantage of some\\xa0randomness\\nTPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.\\n\\n\\xa0\\nA useful library for genetic hyperparameter tuning: TPOT\\n\\xa0\\n\\nTPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.\\nConsider TPOT your\\xa0Data Science Assistant\\xa0for advanced optimization.\\n\\nPipelines not only include the model (or multiple models) but also work on features and other aspects of the process. Plus it returns the Python code of the pipeline for you! TPOT is designed to run for many hours to find the best model. You should have a much larger population and offspring size as well as hundreds of more generations to find a good model.\\n\\xa0\\nTPOT Components ( Key Arguments )\\n\\xa0\\n\\ngenerations\\xa0— Iterations to run training for\\npopulation_size\\xa0— The number of models to keep after each iteration\\noffspring_size\\xa0— Number of models to produce in each iteration\\nmutation_rate\\xa0— The proportion of pipelines to apply randomness to\\ncrossover_rate\\xa0— The proportion of pipelines to breed each iteration\\nscoring\\xa0— The function to determine the best models\\ncv\\xa0— Cross-validation strategy to use\\n\\n\\n\\nTPOT Classifier\\n\\n\\xa0\\nWe will keep default values for\\xa0mutation_rate\\xa0and\\xa0crossover_rate\\xa0as they are best left to the default without deeper knowledge on genetic programming.\\n\\xa0\\nNotice: No algorithm-specific hyperparameters?\\n\\xa0\\nSince TPOT is an open-source library for performing\\xa0AutoML\\xa0in Python.\\nAutoML ??\\nAutomated Machine Learning\\xa0(AutoML) refers to techniques for automatically discovering well-performing models for predictive modeling tasks with very little user involvement.\\n\\n\\nOutput for the above code snippet\\n\\n\\xa0\\nTPOT is quite unstable when not run for a reasonable amount of time. The below code snippets shows the instability of TPOT. Here, only the random state has been changed in the below three codes, but the Output shows major differences in choosing the pipeline, i.e. model and it’s hyperparameters.\\n\\n\\n\\nYou can see in the output the score produced by the chosen model (in this case a version of Naive Bayes) over each generation, and then the final accuracy score with the hyperparameters chosen for the final model. This is a great first example of using TPOT for automated hyperparameter tuning. You can now extend this on your own and build great machine learning models!\\nTo understand more about TPOT:\\n[1]\\xa0TPOT for Automated Machine Learning in Python\\n[2] For more information in using TPOT, visit the\\xa0documentation.\\n\\xa0\\nSummary\\n\\xa0\\nIn informed search, each iteration learns from the last, whereas in Grid and Random, modelling is all done at once and then the best is picked. In case for small datasets, GridSearch or RandomSearch would be fast and sufficient.\\nAutoML approaches provide a neat solution to properly select the required hyperparameters that improve the model’s performance.\\nInformed methods explored were:\\n\\n‘Coarse to Fine’ (Iterative random then grid search).\\nBayesian hyperparameter tuning, updating beliefs using evidence on model performance (HyperOpt).\\nGenetic algorithms, evolving your models over generations (TPOT).\\n\\nI hope you’ve learned some useful methodologies for your future work undertaking hyperparameter tuning in Python!\\nCreate REST API in Minutes With Go / Golang\\nIn this article, there is a short comparison between different routers and then the walk-through to create a REST API…\\n\\xa0\\nIf you are curious to know about Golang’s Routers and want to try out a simple web development project using Go, I suggest to read the above article.\\nFor more informative articles from me, follow me on\\xa0medium.\\nAnd if you’re passionate about Data Science/Machine Learning, feel free to add me on\\xa0LinkedIn.\\n\\n\\xa0\\nReferences\\n[1]\\xa0Bayesian Hyperparameter Optimization — A Primer\\n[2]\\xa0How To Make Deep Learning Models That Don’t Suck\\n[3]\\xa0Algorithms for Hyper-Parameter Optimization\\n[4]\\xa0Grid Search and Bayesian Hyperparameter Optimization\\n[5]\\xa0Tree-structured Parzen Estimator\\n[6]\\xa0Informed Search - Hyperparameter Tuning\\n\\xa0\\nBio: Gowrisankar JG (@jg_gowrisankar) is passionate about Data Science, Data Analytics, Machine Learning and #NLP, and is a Software Developer at Hexaware. \\nOriginal. Reposted with permission.\\nRelated:\\n\\nAutomated Machine Learning: The Free eBook\\nTop Python Libraries for Data Science, Data Visualization & Machine Learning\\nBuild Your Own AutoML Using PyCaret 2.0',\n",
       " 'comments\\nBy Moez Ali, Founder & Author of PyCaret\\n\\n\\nPyCaret — An open-source, low-code machine learning library in Python\\n\\n\\xa0\\nPyCaret\\n\\xa0\\nPyCaret\\xa0is an open-source, low-code machine learning library and end-to-end model management tool built-in Python for automating machine learning workflows. Its ease of use, simplicity, and ability to quickly and efficiently build and deploy end-to-end machine learning pipelines will amaze you.\\nPyCaret is an alternate low-code library that can replace hundreds of lines of code with few lines only. This makes the experiment cycle exponentially fast and efficient.\\nPyCaret is\\xa0simple and easy to use.\\xa0All the operations performed in PyCaret are sequentially stored in a\\xa0Pipeline\\xa0that is fully automated for\\xa0deployment.\\xa0Whether it’s imputing missing values, one-hot-encoding, transforming categorical data, feature engineering, or even hyperparameter tuning, PyCaret automates all of it. To learn more about PyCaret, watch this 1-minute video.\\n\\n\\nPyCaret — An open-source, low-code machine learning library in Python\\n\\n\\xa0\\n\\xa0\\nFeatures of PyCaret\\n\\xa0\\n\\n\\nImage by Author\\n\\n\\xa0\\n\\xa0\\nModules in PyCaret\\n\\xa0\\nPyCaret is a modular library arranged into modules and each module representing a machine learning use-case. As of the writing of this story, the following modules are supported:\\n\\n\\nImage by Author — Machine Learning use-case supported in PyCaret\\n\\n\\xa0\\n* Time Series module is in making and will be available in the next major release.\\n\\xa0\\nInstalling PyCaret\\n\\xa0\\nInstalling PyCaret is very easy and takes only a few minutes. We strongly recommend using a virtual environment to avoid potential conflicts with other libraries.\\nPyCaret’s default installation is a slim version of pycaret that only installs hard dependencies\\xa0listed here.\\n\\n# install slim version (default)\\r\\npip install pycaret# install the full version\\r\\npip install pycaret[full]\\n\\n\\nWhen you install the full version of pycaret, all the optional dependencies as\\xa0listed here\\xa0are also installed.\\n\\n\\nPyCaret by numbers — Image by author\\n\\n\\xa0\\n\\xa0\\n👉 Let’s get started\\n\\xa0\\nBefore I show you how easy it is to do machine learning with PyCaret, let’s talk a little bit about the machine learning lifecycle at a high level:\\n\\n\\nMachine Learning Life Cycle — Image by Author (Read from left-to-right)\\n\\n\\xa0\\n\\nBusiness Problem —\\xa0This is the first step of the machine learning workflow. It may take from few days to a few weeks to complete, depending on the use case and complexity of the problem. It is at this stage, data scientists meet with subject matter experts (SME’s) to gain an understanding of the problem, interview key stakeholders, collect information, and set the overall expectations of the project.\\nData Sourcing & ETL —\\xa0Once the problem understanding is achieved, it then comes to using the information gained during interviews to source the data from the enterprise database.\\nExploratory Data Analysis (EDA) —\\xa0Modeling hasn’t started yet. EDA is where you analyze the raw data. Your goal is to explore the data and assess the quality of the data, missing values, feature distribution, correlation, etc.\\nData Preparation —\\xa0Now it’s time to prepare the data model training. This includes things like dividing data into a train and test set, imputing missing values, one-hot-encoding, target encoding, feature engineering, feature selection, etc.\\nModel Training & Selection —\\xa0This is the step everyone is excited about. This involves training a bunch of models, tuning hyperparameters, model ensembling, evaluating performance metrics, model analysis such as AUC, Confusion Matrix, Residuals, etc, and finally selecting one best model to be deployed in production for business use.\\nDeployment & Monitoring —\\xa0This is the final step which is mostly about MLOps. This includes things like packaging your final model, creating a docker image, writing the scoring script, and then making it all work together, and finally publish it as an API that can be used to obtain predictions on the new data coming through the pipeline.\\n\\nThe old way of doing all this is pretty cumbersome, long, and requires a lot of technical know-how and I possibly cannot cover it in one tutorial. However, in this tutorial, I will use PyCaret to demonstrate how easy it has become for a data scientist to do all this very efficiently.\\n\\xa0\\n👉 Business Problem\\n\\xa0\\nFor this tutorial, I will be using a very popular case study by Darden School of Business, published in\\xa0Harvard Business. The case is regarding the story of two people who are going to be married in the future. The guy named\\xa0Greg\\xa0wanted to buy a ring to propose to a girl named\\xa0Sarah. The problem is to find the ring Sarah will like, but after a suggestion from his close friend, Greg decides to buy a diamond stone instead so that Sarah can decide her choice. Greg then collects data of 6000 diamonds with their price and attributes like cut, color, shape, etc.\\n\\xa0\\n👉 Data\\n\\xa0\\nIn this tutorial, I will be using a dataset from a very popular case study by the Darden School of Business, published in\\xa0Harvard Business. The goal of this tutorial is to predict the diamond price based on its attributes like carat weight, cut, color, etc. You can download the dataset from\\xa0PyCaret’s repository.\\n\\n# load the dataset from pycaret\\r\\nfrom pycaret.datasets import get_data\\r\\ndata = get_data(\\'diamond\\')\\n\\n\\n\\n\\nSample rows from data\\n\\n\\xa0\\n\\xa0\\n👉 Exploratory Data Analysis\\n\\xa0\\nLet’s do some quick visualization to assess the relationship of independent features (weight, cut, color, clarity, etc.) with the target variable i.e.\\xa0Price\\n\\n# plot scatter carat_weight and Price\\r\\nimport plotly.express as px\\r\\nfig = px.scatter(x=data[\\'Carat Weight\\'], y=data[\\'Price\\'], \\r\\n                 facet_col = data[\\'Cut\\'], opacity = 0.25, template = \\'plotly_dark\\', trendline=\\'ols\\',\\r\\n                 trendline_color_override = \\'red\\', title = \\'SARAH GETS A DIAMOND - A CASE STUDY\\')\\r\\nfig.show()\\n\\n\\n\\n\\n\\nLet’s check the distribution of the target variable.\\n\\n# plot histogram\\r\\nfig = px.histogram(data, x=[\"Price\"], template = \\'plotly_dark\\', title = \\'Histogram of Price\\')\\r\\nfig.show()\\n\\n\\n\\n\\n\\nNotice that distribution of\\xa0Price\\xa0is right-skewed, we can quickly check to see if log transformation can make\\xa0Price\\xa0approximately normal to give fighting chance to algorithms that assume normality.\\n\\nimport numpy as np# create a copy of data\\r\\ndata_copy = data.copy()# create a new feature Log_Price\\r\\ndata_copy[\\'Log_Price\\'] = np.log(data[\\'Price\\'])# plot histogram\\r\\nfig = px.histogram(data_copy, x=[\"Log_Price\"], title = \\'Histgram of Log Price\\', template = \\'plotly_dark\\')\\r\\nfig.show()\\n\\n\\n\\n\\n\\nThis confirms our hypothesis. The transformation will help us to get away with skewness and make the target variable approximately normal. Based on this, we will transform the\\xa0Price\\xa0variable before training our models.\\n\\xa0\\n👉 Data Preparation\\n\\xa0\\nCommon to all modules in PyCaret, the\\xa0setup\\xa0is the first and the only mandatory step in any machine learning experiment using PyCaret. This function takes care of all the data preparation required before training models. Besides performing some basic default processing tasks, PyCaret also offers a wide array of pre-processing features. To learn more about all the preprocessing functionalities in PyCaret, you can see this\\xa0link.\\n\\n# initialize setup\\r\\nfrom pycaret.regression import *\\r\\ns = setup(data, target = \\'Price\\', transform_target = True, log_experiment = True, experiment_name = \\'diamond\\')\\n\\n\\n\\n\\nsetup function in pycaret.regression module\\n\\n\\xa0\\nWhen you initialize the\\xa0setup\\xa0function in PyCaret, it profiles the dataset and infers the data types for all input features. If all data types are correctly inferred, you can press enter to continue.\\nNotice that:\\n\\nI have passed\\xa0log_experiment = True\\xa0and\\xa0experiment_name = \\'diamond\\'\\xa0, this will tell PyCaret to automatically log all the metrics, hyperparameters, and model artifacts behind the scene as you progress through the modeling phase. This is possible due to integration with\\xa0MLflow.\\nAlso, I have used\\xa0transform_target = True\\xa0inside the\\xa0setup. PyCaret will transform the\\xa0Price\\xa0variable behind the scene using box-cox transformation. It affects the distribution of data in a similar way as log transformation\\xa0(technically different). If you would like to learn more about box-cox transformations, you can refer to this\\xa0link.\\n\\n\\n\\nOutput from setup — truncated for display\\n\\n\\xa0\\n\\xa0\\n👉 Model Training & Selection\\n\\xa0\\nNow that data is ready for modeling, let’s start the training process by using\\xa0compare_models\\xa0function. It will train all the algorithms available in the model library and evaluates multiple performance metrics using k-fold cross-validation.\\n\\n# compare all models\\r\\nbest = compare_models()\\n\\n\\n\\n\\nOutput from compare_models\\n\\n\\xa0\\n\\n# check the residuals of trained model\\r\\nplot_model(best, plot = \\'residuals_interactive\\')\\n\\n\\n\\n\\nResiduals and QQ-Plot of the best model\\n\\n\\xa0\\n\\n# check feature importance\\r\\nplot_model(best, plot = \\'feature\\')\\n\\n\\n\\n\\n\\n\\xa0\\nFinalize and Save Pipeline\\n\\xa0\\nLet’s now finalize the best model i.e. train the best model on the entire dataset including the test set and then save the pipeline as a pickle file.\\n\\n# finalize the model\\r\\nfinal_best = finalize_model(best)# save model to disk\\r\\nsave_model(final_best, \\'diamond-pipeline\\')\\n\\n\\nsave_model\\xa0function will save the entire pipeline (including the model) as a pickle file on your local disk. By default, it will save the file in the same folder as your Notebook or script is in but you can pass the complete path as well if you would like:\\n\\nsave_model(final_best, \\'c:/users/moez/models/diamond-pipeline\\'\\n\\n\\n\\xa0\\n👉 Deployment\\n\\xa0\\nRemember we passed\\xa0log_experiment = True\\xa0in the setup function along with\\xa0experiment_name = \\'diamond\\'\\xa0. Let’s see the magic PyCaret has done with the help of MLflow behind the scene. To see the magic let’s initiate the MLflow server:\\n\\n# within notebook (notice ! sign infront)\\r\\n!mlflow ui# on command line in the same folder\\r\\nmlflow ui\\n\\n\\nNow open your browser and type “https://localhost:5000”. It will open a UI like this:\\n\\n\\nhttps://localhost:5000\\n\\n\\xa0\\nEach entry in the table above represents a training run resulting in a trained Pipeline and a bunch of metadata such as DateTime of a run, performance metrics, model hyperparameters, tags, etc. Let’s click on one of the models:\\n\\n\\nPart I — CatBoost Regressor\\n\\n\\xa0\\n\\n\\nPart II — CatBoost Regressor (continued)\\n\\n\\xa0\\n\\n\\nPart III — CatBoost Regressor\\n\\n\\xa0\\nNotice that you have an address path for the\\xa0logged_model. This is the trained Pipeline with Catboost Regressor. You can read this Pipeline using the\\xa0load_model\\xa0function.\\n\\n# load model\\r\\nfrom pycaret.regression import load_model\\r\\npipeline = load_model(\\'C:/Users/moezs/mlruns/1/b8c10d259b294b28a3e233a9d2c209c0/artifacts/model/model\\')# print pipeline\\r\\nprint(pipeline)\\n\\n\\n\\n\\nOutput from print(pipeline)\\n\\n\\xa0\\nLet’s now use this Pipeline to generate predictions on the new data\\n\\n# create a copy of data and drop Price\\r\\ndata2 = data.copy()\\r\\ndata2.drop(\\'Price\\', axis=1, inplace=True)# generate predictions\\r\\nfrom pycaret.regression import predict_model\\r\\npredictions = predict_model(pipeline, data=data2)\\r\\npredictions.head()\\n\\n\\n\\n\\nPredictions generated from Pipeline\\n\\n\\xa0\\nWoohoo! We now have inference from our trained Pipeline. Congrats, if this is your first one. Notice that all the transformations such as target transformation, one-hot-encoding, missing value imputation, etc. happened behind the scene automatically. You get a data frame with prediction in actual scale, and this is what you care about.\\n\\n\\nImage by Author\\n\\n\\xa0\\n\\n\\nImage by Author\\n\\n\\xa0\\nThere is no limit to what you can achieve using this lightweight workflow automation library in Python. If you find this useful, please do not forget to give us ⭐️ on our GitHub repository.\\nTo hear more about PyCaret follow us on\\xa0LinkedIn\\xa0and\\xa0Youtube.\\nJoin us on our slack channel. Invite link\\xa0here.\\n\\xa0\\nYou may also be interested in:\\n\\xa0\\nBuild your own AutoML in Power BI using PyCaret 2.0\\nDeploy Machine Learning Pipeline on Azure using Docker\\nDeploy Machine Learning Pipeline on Google Kubernetes Engine\\nDeploy Machine Learning Pipeline on AWS Fargate\\nBuild and deploy your first machine learning web app\\nDeploy PyCaret and Streamlit app using AWS Fargate serverless\\nBuild and deploy machine learning web app using PyCaret and Streamlit\\nDeploy Machine Learning App built using Streamlit and PyCaret on GKE\\n\\xa0\\nImportant Links\\n\\xa0\\nDocumentation\\nBlog\\nGitHub\\nStackOverflow\\nInstall PyCaret\\nNotebook Tutorials\\nContribute in PyCaret\\n\\xa0\\nWant to learn about a specific module?\\n\\xa0\\nClick on the links below to see the documentation and working examples.\\nClassification\\nRegression\\nClustering\\nAnomaly Detection\\nNatural Language Processing\\nAssociation Rule Mining\\n\\xa0\\nTHE END\\n\\xa0\\nBio: Moez Ali is a Data Scientist, and is Founder & Author of PyCaret.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nEasy MLOps with PyCaret + MLflow\\nWrite and train your own custom machine learning models using PyCaret\\n5 Things You Don’t Know About PyCaret',\n",
       " \"By Rahul Agarwal, MLE @ FB | Ex-Walmart DS | MLWhiz.\\ncomments\\n\\n\\xa0\\nPyTorch\\xa0has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.\\nI remember picking PyTorch up only after some extensive experimentation a couple of years back. To tell you the truth, it took me a lot of time to pick it up but am I glad that I moved from\\xa0Keras to PyTorch.\\xa0With its high customizability and pythonic syntax,\\xa0PyTorch is just a joy to work with, and I would recommend it to anyone who wants to do some heavy lifting with Deep Learning.\\nSo, in this PyTorch guide,\\xa0I will try to ease some of the pain with PyTorch for starters\\xa0and go through some of the most important classes and modules that you will require while creating any Neural Network with Pytorch.\\nBut, that is not to say that this is aimed at beginners only as\\xa0I will also talk about the\\xa0high customizability PyTorch provides and will talk about custom Layers, Datasets, Dataloaders, and Loss functions.\\nSo let’s get some coffee ☕ ️and start it up.\\n\\xa0\\nTensors\\nTensors are the basic building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU. In this part, I will list down some of the most used operations we can use while working with Tensors. This is by no means an exhaustive list of operations you can do with Tensors, but it is helpful to understand what tensors are before going towards the more exciting parts.\\n\\xa0\\n1. Create a Tensor\\nWe can create a PyTorch tensor in multiple ways. This includes converting to tensor from a NumPy array. Below is just a small gist with some examples to start with, but you can do a whole lot of\\xa0more things\\xa0with tensors just like you can do with NumPy arrays.\\n\\n\\n\\xa0\\n2. Tensor Operations\\nAgain, there are a lot of operations you can do on these tensors. The full list of functions can be found\\xa0here.\\n\\n\\nNote:\\xa0What are PyTorch Variables? In the previous versions of Pytorch, Tensor and Variables used to be different and provided different functionality, but now the Variable API is\\xa0deprecated, and all methods for variables work with Tensors. So, if you don’t know about them, it’s fine as they re not needed, and if you know them, you can forget about them.\\n\\xa0\\nThe nn.Module\\n\\xa0\\n\\nPhoto by\\xa0Fernand De Canne\\xa0on\\xa0Unsplash\\n\\xa0\\n\\n\\xa0\\nHere comes the fun part as we are now going to talk about some of the most used constructs in Pytorch while creating deep learning projects. nn.Module lets you create your Deep Learning models as a class. You can inherit from nn.Module to define any model as a class. Every model class necessarily contains an\\xa0__init__\\xa0procedure block and a block for the\\xa0forward\\xa0pass.\\n\\nIn the\\xa0__init__\\xa0part, the user can define all the layers the network is going to have but doesn't yet define how those layers would be connected to each other.\\nIn the\\xa0forward\\xa0pass block, the user defines how data flows from one layer to another inside the network.\\n\\nSo, put simply, any network we define will look like:\\n\\nHere we have defined a very simple Network that takes an input of size 784 and passes it through two linear layers in a sequential manner. But the thing to note is that we can define any sort of calculation while defining the forward pass, and that makes PyTorch highly customizable for research purposes. For example, in our crazy experimentation mode, we might have used the below network where we arbitrarily attach our layers. Here we send back the output from the second linear layer back again to the first one after adding the input to it(skip connection) back again(I honestly don’t know what that will do).\\n\\nWe can also check if the neural network forward pass works. I usually do that by first creating some random input and just passing that through the network I have created.\\n\\nx = torch.randn((100,784))\\r\\nmodel = myCrazyNeuralNet()\\r\\nmodel(x).size()\\r\\n--------------------------\\r\\ntorch.Size([100, 10])\\n\\n\\xa0\\n\\xa0\\nA word about Layers\\nPytorch is pretty powerful, and you can actually create any new experimental layer by yourself using\\xa0nn.Module. For example, rather than using the predefined Linear Layer\\xa0nn.Linear\\xa0from Pytorch above, we could have created our\\xa0custom linear layer.\\n\\nYou can see how we wrap our weights tensor in\\xa0nn.Parameter.\\xa0This is done to make the tensor to be considered as a model parameter. From PyTorch\\xa0docs:\\nParameters are\\xa0Tensor\\xa0subclasses, that have a very special property when used with\\xa0Module\\xa0- when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear in\\xa0parameters()\\xa0iterator\\nAs you will later see, the\\xa0model.parameters()\\xa0iterator will be an input to the optimizer. But more on that later.\\nRight now, we can now use this custom layer in any PyTorch network, just like any other layer.\\n\\nBut then again, Pytorch would not be so widely used if it didn’t provide a lot of ready to made layers used very frequently in wide varieties of Neural Network architectures. Some examples are: nn.Linear, nn.Conv2d, nn.MaxPool2d, nn.ReLU,\\xa0nn.BatchNorm2d, nn.Dropout, nn.Embedding,\\xa0nn.GRU/nn.LSTM, nn.Softmax,\\xa0nn.LogSoftmax, nn.MultiheadAttention, nn.TransformerEncoder, nn.TransformerDecoder\\nI have linked all the layers to their source where you could read all about them, but to show how I usually try to understand a layer and read the docs, I would try to look at a very simple convolutional layer here.\\n\\nSo, a Conv2d Layer needs as input an Image of height H and width W, with\\xa0Cin\\xa0channels. Now, for the first layer in a convnet, the number of in_channels would be 3 (RGB), and the number of out_channels can be defined by the user. The kernel_size mostly used is 3x3, and the stride normally used is 1.\\nTo check a new layer which I don’t know much about, I usually try to see the input as well as output for the layer like below where I would first initialize the layer:\\n\\nconv_layer = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = (3,3), stride = 1, padding=1)\\n\\n\\xa0\\nAnd then pass some random input through it. Here 100 is the batch size.\\n\\nx = torch.randn((100,3,24,24))\\r\\nconv_layer(x).size()\\r\\n--------------------------------\\r\\ntorch.Size([100, 64, 24, 24])\\n\\n\\xa0\\nSo, we get the output from the convolution operation as required, and I have sufficient information on how to use this layer in any Neural Network I design.\\n\\xa0\\nDatasets and DataLoaders\\nHow would we pass data to our Neural nets while training or while testing? We can definitely pass tensors as we have done above, but Pytorch also provides us with pre-built Datasets to make it easier for us to pass data to our neural nets. You can check out the complete list of datasets provided at\\xa0torchvision.datasets\\xa0and\\xa0torchtext.datasets. But, to give a concrete example for datasets, let’s say we had to pass images to an Image Neural net using a folder which has images in this structure:\\n\\ndata\\r\\n    train\\r\\n        sailboat\\r\\n        kayak\\r\\n        .\\r\\n        .\\n\\n\\xa0\\nWe can use\\xa0torchvision.datasets.ImageFolder\\xa0dataset to get an example image like below:\\n\\n\\nThis dataset has 847 images, and we can get an image and its label using an index. Now we can pass images one by one to any image neural network using a for loop:\\n\\nfor i in range(0,len(train_dataset)):\\r\\n    image ,label = train_dataset[i]\\r\\n    pred = model(image)\\n\\n\\xa0\\nBut that is not optimal. We want to do batching.\\xa0We can actually write some more code to append images and labels in a batch and then pass it to the Neural network. But Pytorch provides us with a utility iterator\\xa0torch.utils.data.DataLoader\\xa0to do precisely that. Now we can simply wrap our\\xa0train_dataset\\xa0in the Dataloader, and we will get batches instead of individual examples.\\n\\ntrain_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10)\\n\\n\\xa0\\nWe can simply iterate with batches using:\\n\\nfor image_batch, label_batch in train_dataloader:\\r\\n    print(image_batch.size(),label_batch.size())\\r\\n    break\\r\\n------------------------------------------------------------------\\r\\ntorch.Size([64, 3, 224, 224]) torch.Size([64])\\n\\n\\xa0\\nSo actually, the whole process of using datasets and Dataloaders becomes:\\n\\nYou can look at this particular example in action in my previous blogpost on Image classification using Deep Learning\\xa0here.\\nThis is great, and Pytorch does provide a lot of functionality out of the box. But the main power of Pytorch comes with its immense customization. We can also create our own custom datasets if the datasets provided by PyTorch don’t fit our use case.\\n\\xa0\\nUnderstanding Custom Datasets\\nTo write our custom datasets, we can make use of the abstract class\\xa0torch.utils.data.Dataset\\xa0provided by Pytorch. We need to inherit this\\xa0Dataset\\xa0class and need to define two methods to create a custom Dataset.\\n\\n__len__\\xa0: a function that returns the size of the dataset. This one is pretty simple to write in most cases.\\n__getitem__: a function that takes as input an index\\xa0i\\xa0and returns the sample at index\\xa0i.\\n\\nFor example, we can create a simple custom dataset that returns an image and a label from a folder. See that most of the tasks are happening in\\xa0__init__\\xa0part where we use\\xa0glob.glob\\xa0to get image names and do some general preprocessing.\\n\\nAlso, note that we open our images one at a time in the\\xa0__getitem__\\xa0method and not while initializing. This is not done in __init__ because we don't want to load all our images in the memory and just need to load the required ones.\\nWe can now use this dataset with the utility\\xa0Dataloader\\xa0just like before. It works just like the previous dataset provided by PyTorch but without some utility functions.\\n\\n\\xa0\\nUnderstanding Custom DataLoaders\\nThis particular section is a little advanced and can be skipped going through this post as it will not be needed in a lot of situations. But I am adding it for completeness here.\\nSo let’s say you are looking to provide batches to a network that processes text input, and the network could take sequences with any sequence size as long as the size remains constant in the batch. For example, we can have a BiLSTM network that can process sequences of any length. It’s alright if you don’t understand the layers used in it right now; just know that it can process sequences with variable sizes.\\n\\nThis network expects its input to be of shape (batch_size,\\xa0seq_length) and works with any\\xa0seq_length. We can check this by passing our model two random batches with different sequence lengths(10 and 25).\\n\\nmodel = BiLSTM()\\r\\ninput_batch_1 = torch.randint(low = 0,high = 10000, size = (100,10))\\r\\ninput_batch_2 = torch.randint(low = 0,high = 10000, size = (100,25))\\r\\nprint(model(input_batch_1).size())\\r\\nprint(model(input_batch_2).size())\\r\\n------------------------------------------------------------------\\r\\ntorch.Size([100, 1])\\r\\ntorch.Size([100, 1])\\n\\n\\xa0\\nNow, we want to provide tight batches to this model, such that each batch has the same sequence length based on the max sequence length in the batch to minimize padding. This has an added benefit of making the neural net run faster. It was, in fact, one of the methods used in the winning submission of the Quora Insincere challenge in Kaggle, where running time was of utmost importance.\\nSo, how do we do this? Let’s write a very simple custom dataset class first.\\n\\nAlso, let’s generate some random data which we will use with this custom Dataset.\\n\\n\\nExample of one random sequence and label. Each integer in the sequence corresponds to a word in the sentence.\\n\\xa0\\n\\n\\xa0\\nWe can use the custom dataset now using:\\n\\ntrain_dataset = CustomTextDataset(X,y)\\n\\n\\xa0\\nIf we now try to use the Dataloader on this dataset with\\xa0batch_size>1, we will get an error. Why is that?\\n\\ntrain_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10)\\r\\nfor xb,yb in train_dataloader:\\r\\n    print(xb.size(),yb.size())\\n\\n\\xa0\\n\\nThis happens because the sequences have different lengths, and our data loader expects our sequences of the same length. Remember that in the previous image example, we resized all images to size 224 using the transforms, so we didn’t face this error.\\nSo, how do we iterate through this dataset so that each batch has sequences with the same length, but different batches may have different sequence lengths?\\nWe can use\\xa0collate_fn\\xa0parameter in the DataLoader that lets us define how to stack sequences in a particular batch. To use this, we need to define a function that takes as input a batch and returns (x_batch,\\xa0y_batch\\xa0) with padded sequence lengths based on\\xa0max_sequence_length\\xa0in the batch. The functions I have used in the below function are simple NumPy operations. Also, the function is properly commented so you can understand what is happening.\\n\\nWe can now use this\\xa0collate_fn\\xa0with our Dataloader as:\\n\\ntrain_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10,collate_fn = collate_text)for xb,yb in train_dataloader:\\r\\n    print(xb.size(),yb.size())\\n\\n\\xa0\\n\\nSee that the batches have different sequence lengths now\\n\\xa0\\n\\n\\xa0\\nIt will work this time as we have provided a custom\\xa0collate_fn.\\xa0And see that the batches have different sequence lengths now. Thus we would be able to train our BiLSTM using variable input sizes just like we wanted.\\n\\xa0\\nTraining a Neural Network\\nWe know how to create a neural network using\\xa0nn.Module.\\xa0But how to train it? Any neural network that has to be trained will have a training loop that will look something similar to below:\\n\\nIn the above code, we are running five epochs and in each epoch:\\n\\nWe iterate through the dataset using a data loader.\\nIn each iteration, we do a forward pass using\\xa0model(x_batch)\\nWe calculate the Loss using a\\xa0loss_criterion\\nWe back-propagate that loss using\\xa0loss.backward()\\xa0call. We don't have to worry about the calculation of the gradients at all, as this simple call does it all for us.\\nTake an optimizer step to change the weights in the whole network using\\xa0optimizer.step(). This is where weights of the network get modified using the gradients calculated in\\xa0loss.backward()\\xa0call.\\nWe go through the validation data loader to check the validation score/metrics. Before doing validation, we set the model to eval mode using\\xa0model.eval().Please note we don't back-propagate losses in eval mode.\\n\\nTill now, we have talked about how to use\\xa0nn.Module\\xa0to create networks and how to use Custom Datasets and Dataloaders with Pytorch. So let's talk about the various options available for Loss Functions and Optimizers.\\n\\xa0\\nLoss functions\\nPytorch provides us with a variety of\\xa0loss functions\\xa0for our most common tasks, like Classification and Regression. Some most used examples are nn.CrossEntropyLoss, nn.NLLLoss, nn.KLDivLoss and nn.MSELoss. You can read the documentation of each loss function, but to explain how to use these loss functions, I will go through the example of\\xa0nn.NLLLoss\\n\\nThe documentation for NLLLoss is pretty succinct. As in, this loss function is used for Multiclass classification, and based on the documentation:\\n\\nthe input expected needs to be of size (batch_size\\xa0x\\xa0Num_Classes\\xa0) — These are the predictions from the Neural Network we have created.\\nWe need to have the log-probabilities of each class in the input — To get log-probabilities from a Neural Network, we can add a\\xa0LogSoftmax\\xa0Layer as the last layer of our network.\\nThe target needs to be a tensor of classes with class numbers in the range(0, C-1) where C is the number of classes.\\n\\nSo, we can try to use this Loss function for a simple classification network. Please note the\\xa0LogSoftmax\\xa0layer after the final linear layer. If you don't want to use this\\xa0LogSoftmax\\xa0layer, you could have just used\\xa0nn.CrossEntropyLoss\\n\\nLet’s define a random input to pass to our network to test it:\\n\\n# some random input:X = torch.randn(100,784)\\r\\ny = torch.randint(low = 0,high = 10,size = (100,))\\n\\n\\xa0\\nAnd pass it through the model to get predictions:\\n\\nmodel = myClassificationNet()\\r\\npreds = model(X)\\n\\n\\xa0\\nWe can now get the loss as:\\n\\ncriterion = nn.NLLLoss()\\r\\nloss = criterion(preds,y)\\r\\nloss\\r\\n------------------------------------------\\r\\ntensor(2.4852, grad_fn=<NllLossBackward>)\\n\\n\\xa0\\n\\xa0\\nCustom Loss Function\\nDefining your custom loss functions is again a piece of cake, and you should be okay as long as you use tensor operations in your loss function. For example, here is the\\xa0customMseLoss\\n\\ndef customMseLoss(output,target):\\r\\n    loss = torch.mean((output - target)**2)     \\r\\n    return loss\\n\\n\\xa0\\nYou can use this custom loss just like before. But note that we don’t instantiate the loss using criterion this time as we have defined it as a function.\\n\\noutput = model(x)\\r\\nloss = customMseLoss(output, target)\\r\\nloss.backward()\\n\\n\\xa0\\nIf we wanted, we could have also written it as a class using\\xa0nn.Module\\xa0, and then we would have been able to use it as an object. Here is an NLLLoss custom example:\\n\\n\\xa0\\nOptimizers\\nOnce we get gradients using the\\xa0loss.backward()\\xa0call, we need to take an optimizer step to change the weights in the whole network. Pytorch provides a variety of different ready to use optimizers using the torch.optim module. For example: torch.optim.Adadelta, torch.optim.Adagrad, torch.optim.RMSprop and the most widely used torch.optim.Adam. To use the most used Adam optimizer from PyTorch, we can simply instantiate it with:\\n\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))\\n\\n\\xa0\\nAnd then use\\xa0optimizer.zero_grad()\\xa0and\\xa0optimizer.step()\\xa0while training the model.\\nI am not discussing how to write custom optimizers as it is an infrequent use case, but if you want to have more optimizers, do check out the\\xa0pytorch-optimizer\\xa0library, which provides a lot of other optimizers used in research papers. Also, if you anyhow want to create your own optimizers, you can take inspiration using the source code of implemented optimizers in\\xa0PyTorch\\xa0or\\xa0pytorch-optimizers.\\n\\nOther optimizers from\\xa0pytorch-optimizer\\xa0library\\n\\xa0\\n\\n\\xa0\\n\\xa0\\nUsing GPU/Multiple GPUs\\nTill now, whatever we have done is on the CPU. If you want to use a GPU, you can put your model to GPU using\\xa0model.to('cuda'). Or if you want to use multiple GPUs, you can use\\xa0nn.DataParallel. Here is a utility function that checks the number of GPUs in the machine and sets up parallel training automatically using\\xa0DataParallel\\xa0if needed.\\n\\nThe only thing that we will need to change is that we will load our data to GPU while training if we have GPUs. It’s as simple as adding a few lines of code to our training loop.\\n\\n\\xa0\\nConclusion\\nPytorch provides a lot of customizability with minimal code. While at first, it might be hard to understand how the whole ecosystem is structured with classes, in the end, it is simple Python. In this post, I have tried to break down most of the parts you might need while using Pytorch, and I hope it makes a little more sense for you after reading this.\\nYou can find the code for this post here on my\\xa0GitHub\\xa0repo, where I keep codes for all my blogs.\\nIf you want to learn more about Pytorch using a course based structure, take a look at the\\xa0Deep Neural Networks with PyTorch\\xa0course by IBM on Coursera. Also, if you want to know more about Deep Learning, I would like to recommend this excellent course on\\xa0Deep Learning in Computer Vision\\xa0in the\\xa0Advanced machine learning specialization.\\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at\\xa0Medium\\xa0or Subscribe to my\\xa0blog\\xa0to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter\\xa0@mlwhiz.\\nAlso, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\\nBio: Rahul Agarwal is Senior Statistical Analyst at WalmartLabs. Follow him on Twitter\\xa0@mlwhiz.\\nOriginal. Reposted with permission.\\nRelated:\\n\\n6 bits of advice for Data Scientists\\nThe Hitchhiker’s Guide to Feature Extraction\\nThe 5 Classification Evaluation Metrics Every Data Scientist Must Know\",\n",
       " 'comments\\nBy Francesco Casalegno, Project Manager and Machine Learning Engineer\\n\\n\\nParadoxes contradict our expectations. Photo by\\xa0Greg & Lois Nunes\\xa0on\\xa0Unsplash.\\n\\n\\xa0\\nObservation bias and sub-group differences can easily produce\\xa0statistical paradoxes\\xa0in any data science application. Ignoring these elements can therefore completely undermine the conclusions of our analysis.\\nIt is indeed not unusual to observe surprising phenomena such as\\xa0sub-groups\\xa0trends that are completely reverted in the aggregated data. In this article we look at the 3 most common kinds of statistical paradoxes encountered in Data Science.\\n\\xa0\\n1.\\xa0Berkson’s Paradox\\n\\xa0\\nA first striking example is the observed\\xa0negative association between COVID-19 severity and smoking cigarettes\\xa0(see e.g. the European Commission review by\\xa0Wenzel 2020). Smoking cigarettes is a well-known risk factor for respiratory diseases, so how do we explain this contradiction?\\nThe work of\\xa0Griffith 2020\\xa0recently published on Nature suggests that this can be a case of\\xa0Collider Bias, also called\\xa0Berkson’s Paradox. To understand this paradox, let us consider the following graphical model, where we include a third random variable: “being hospitalized”.\\n\\n\\nBerkson’s Paradox:\\xa0“hospitalization” is a collider variable for both “smoking cigarettes” and “COVID-19 severity”. (Image by author)\\n\\n\\xa0\\nThis third variable “being hospitalized” is a\\xa0collider\\xa0of the first two. This means that both smoking cigarettes and having severe COVID-19 increase chances of being ill in a hospital. Berkson’s Paradox precisely arises when we\\xa0condition on a collider, i.e. when we only observe data from hospitalized people rather than considering the whole population.\\nLet’s consider the following example dataset. In the left figure we have observations from the whole population, while on the right figure we only consider a subset of hospitalized people (i.e. we condition on the collider variable).\\n\\n\\nBerkson’s Paradox:\\xa0If we condition on the collider “hospitalization”, we observe a reversal in the relation between smoking and COVID-19! (Image by author)\\n\\n\\xa0\\nIn the left figure we can observe the\\xa0positive correlation\\xa0between COVID-19 severity and smoking cigarettes that we expected as we know that smoking is a risk factor for respiratory diseases.\\nBut in the right figure—where we only consider hospital patients—we see the opposite trend! To understand this, consider the following points.\\n\\nHaving high severity of COVID-19 increases chances of being hospitalized. In particular, if severity > 1 hospitalization is required.\\nSmoking several cigarettes a day is a major risk factor for a variety of diseases (heart attacks, cancer, diabetes), which increase the chances of being hospitalized for some reason.\\nHence, if a\\xa0hospital patient\\xa0has\\xa0lower COVID-19\\xa0severity, they have\\xa0higher chances of smoking cigarettes! Indeed, they must have some disease different from COVID-19 (e.g. heart attacks, cancer, diabetes) to justify their hospitalization, and this disease may very well be caused by their smoking cigarettes.\\n\\nThis example is very similar to the original work of\\xa0Berkson 1946, where the author noticed a negative correlation between\\xa0cholecystitis\\xa0and\\xa0diabetes\\xa0in hospital patients, despite diabetes being a risk factor for cholecystitis.\\n\\xa0\\n2. Latent Variables\\n\\xa0\\nThe presence of a\\xa0latent variable\\xa0may also produce an apparently inverted correlation between two variables. While Berkson’s Paradox arises because of the conditioning on a collider variable (which should therefore be avoided), this other kind of paradox can be fixed by\\xa0conditioning on the latent variable.\\nLet’s consider, for instance, the relation between number of firefighter deployed to extinguish a fire and the number people that are injured in the fire. We would expect that having more firefighters would improve the outcome (to some extent—see\\xa0Brooks’s Law), yet a positive correlation is observed in aggregated data:\\xa0the more firefighters are deployed, the higher the number of injured!\\nTo understand this paradox, let us consider the following graphical model. The key is to consider again a third random variable: “fire severity”.\\n\\n\\nLatent Variable Paradox:\\xa0“fire severity” is a latent variable for both “n of firefighters deployed” and “n of injured”. (Image by author)\\n\\n\\xa0\\nThis third latent variable positively correlates with the other two. Indeed, more\\xa0severe fires tend to cause more injuries, and at the same time they\\xa0require more firefighters\\xa0to be extinguished.\\nLet’s consider the following example dataset. In the left figure we have aggregated observations from all kinds of fires, while on the right figure we only consider observations corresponding to three fixed degrees of fire severity (i.e. we condition our observations on the latent variable).\\n\\n\\nLatent Variables:\\xa0If we condition on the latent variable “fire severity”, we observe a reversal in the relation between number of firefighters deployed and number of injured people! (Image by author)\\n\\n\\xa0\\nIn the right figure, where we\\xa0condition observations on the degrees of fire severity, we can see the negative correlation we would have expected.\\n\\nFor a fire of given severity we can indeed observe that\\xa0the more the firefighters deployed, the fewer the injured people.\\nIf the we look at\\xa0fires with higher severity, we observe the\\xa0same trend\\xa0even though both number of firefighters deployed and number of injured people are higher.\\n\\n\\xa0\\n3. Simpson’s Paradox\\n\\xa0\\nSimpson’s Paradox\\xa0is a surprising phenomenon arising when a trend that is consistently observed in sub-groups, but the trend is inverted if sub-groups are merged. It is often related to the\\xa0class imbalance in data sub-groups.\\nA notorious occurrence of this paradox is from\\xa0Bickel 1975, where acceptance rates to the University of California weer analysed to find\\xa0evidence of sex discrimination, and two apparently contradicting facts were revealed.\\n\\nOn the one hand,\\xa0in\\xa0every department\\xa0he observed that\\xa0female applicants had higher acceptance rates than male applicants.\\nOn the other hands,\\xa0aggregate\\xa0numbers\\xa0showed that\\xa0female applicants had lower acceptance rates than male applicants.\\n\\nTo see how this is possible, let’s consider the following dataset with the two departments Dept. A and Dept. B.\\n\\nOut of 100 male applicants: 80 applied to Dept. A and 68 were accepted (85%), while 20 applied to Dept. B and 12 were accepted (60%).\\nOut of 100 female applicants: 30 applied to Dept. A and 28 were accepted (93%), while 70 applied to Dept. B and 46 were accepted (66%).\\n\\n\\n\\nSimpson’s Paradox:\\xa0female applicants are more likely to be accepted in each department, but the overall female acceptance rate is inferior to the male one! (Image by author)\\n\\n\\xa0\\nThe paradox is expressed by the following inequalities.\\n\\n\\nSimpson’s Paradox:\\xa0The inequalities behind the apparent contradiction. (Image by author)\\n\\n\\xa0\\nWe can now understand the origin of our seemingly contradictory observations. The point is that there is a significant\\xa0class imbalance\\xa0in the sex of applicants in each of the two departments (Dept. A: 80–30, Dept. B: 20–70). Indeed,\\xa0most\\xa0female students applied to the more competitive Dept. B\\xa0(which has low rates of admission), while\\xa0most\\xa0male students applied to the less competitive Dept. A\\xa0(which has higher rates of admission). This causes the contradictory observations we had.\\n\\xa0\\nConclusions\\n\\xa0\\nLatent variables,\\xa0collider variables, and\\xa0class imbalance\\xa0can easily produce\\xa0statistical paradoxes\\xa0in many data science applications. A particular attention to these key points is therefore essential to correctly derive trends and analyse the results.\\n\\xa0\\nBio: Francesco Casalegno is a Project Manager and Machine Learning Engineer, with a passion for solving any sort of problems related in general to Data Science. His background is in Software Engineering and Applied Mathematics. Francesco is always looking forward to new challenges, and he strongly believes in continuously improving himself and the team working with him.\\nOriginal. Reposted with permission.\\nRelated:\\n\\n10 Statistical Concepts You Should Know For Data Science Interviews\\nRejection Sampling with Python\\nThe Inferential Statistics Data Scientists Should Know',\n",
       " 'By Matthew Mayo, KDnuggets.\\ncomments\\nRecently, GitHub publicly unveiled Copilot, the preview of its \"AI pair programmer,\" a code completion style tool designed to provide line or function suggestions in your IDE. It has certainly made waves in the world of programming and beyond, and you have likely heard at least something about it.\\n\\nBut Copilot is more than simple autocomplete and is more context aware than other code assistants. Powered by OpenAI\\'s Codex AI system, Copilot contextualizes a situation using docstrings, function names, comments, and preceding code to best generate and suggest what it determines to be the most appropriate code. Copilot is designed to improve over time, \"learning\" from how developers use it.\\n\\nTrained on billions of lines of public code, GitHub Copilot puts the knowledge you need at your fingertips, saving you time and helping you stay focused.\\n\\n\\xa0\\nCurrently available for Visual Studio Code and platforms powered by a VS Code backend — such as GitHub\\'s Codespaces — Copilot \"understands\" dozens of languages, with the technical preview being noted as doing \"especially well for Python, JavaScript, TypeScript, Ruby, and Go.\" You can accept default code suggestions, cycle through additional proposals, modify the code you accept, or ignore Copilot suggestions at a particular point in your code altogether.\\nPresently, Copilot is only available via approved request. But fret not; open source alternatives of varying specification exist, and are available for you to try out right now.\\nLet\\'s have a look at 4 code-generation and -suggestion alternatives to GitHub Copilot that you can use in your programming today. Though my investigation into these was prompted by the discovery of Second Mate (below), I have listed the options in descending order by number of GitHub stars, as it seemed as good a way to do so as any.\\n\\xa0\\nCaptain Stack\\n\\xa0\\n\\xa0\\nWe will start this off with a code suggestion tool, as opposed to code generation, but do so as with 514 stars at publication time, this seems to be the most popular of our alternatives.\\n\\nThis feature is somewhat similar to Github Copilot\\'s code suggestion. But instead of using AI, it sends your search query to Google, then retrieves StackOverflow answers and autocompletes them for you.\\n\\n\\xa0\\n\\n\\xa0\\nCaptain Stack only works with VSCode, making it an especially Copilot analog, and is installed as a VSCode extension.\\nUse Captain Stack to automate your Stack Overflow code copying! :)\\n\\xa0\\nGPT-Code-Clippy (GPT-CC)\\n\\xa0\\n\\xa0\\nGPT-CC is a code-generation tool which employs a GPT-3 model for generation.\\n\\nGPT-Code-Clippy (GPT-CC) is an open source version of GitHub Copilot, a language model -- based on GPT-3, called GPT-Codex -- that is fine-tuned on publicly available code from GitHub.\\n\\n\\xa0\\n\\n\\xa0\\nThe VSCode extension of GPT-CC is available here. Somewhat curiously, from this extension repo, is the following reference to sitting atop the aforementioned Captain Stack:\\n\\nThis extension also sits completely atop this other clone of Github Copilot aptly named Captain Stack, since instead of synthesizing the answers using deep learning, it extracts them from StackOverflow posts.\\n\\n\\xa0\\nThe main model repo has 74 stars as of publication time.\\n\\xa0\\nSecond Mate\\n\\xa0\\n\\xa0\\nSecond Mate is a code-generation tool for Emacs, leveraging a GPT model.\\n\\nAn open-source, mini imitation of GitHub Copilot using EleutherAI GPT-Neo-2.7B (via Huggingface Model Hub) for Emacs.\\nThis is a much smaller model so will likely not be as effective as Copilot, but can still be interesting to play around with!\\n\\n\\xa0\\n\\n\\xa0\\nSetup for Second Mate includes running a Flask app as a backend, and configuring the Emacs plugin to point at said backend server URL for submitting requests. Second Mate has 46 stars as of publication time.\\n\\xa0\\nClara-Copilot VSCode\\n\\xa0\\n\\xa0\\nLastly, Clara-Copilot is a sparsely-documented VSCode Copilot alternative that doesn\\'t explain upfront what mechanism it\\'s using to accomplish its goals.\\n\\nA alternative to Github Copilot for vscode until you get the access to github copilot.\\n\\n\\xa0\\n(Edit: eagle-eyed reader FIREHAWK has noted, in the comments below, that Clara-Copilot makes use of Code Grepper, making it a code search and recommendation solution as opposed to code generation).\\n\\nIt does, however, provide an example of how to use the extension, and boasts that it \"[s]upports around ~ 50 Programming aproximately LOL !,\" and \"[g]ives you snippets at an instant.\" The repo does have 28 stars at publication. Try at your own caution :)\\n\\xa0\\nHopefully this small collection of alternatives provides you with something to hold you over until Copilot is released to the masses. You might even find that one of these are useful enough to work for you long term. Thanks to the respective authors of these tools.\\n\\xa0\\nRelated:\\n\\nGitHub Copilot: Your AI pair programmer – what is all the fuss about?\\nManaging Your Reusable Python Code as a Data Scientist\\nData Scientists, You Need to Know How to Code',\n",
       " 'By Jesus Rodriguez, Intotheblock.\\ncomments\\n\\n\\nSource:\\xa0https://neurohive.io/en/news/google-introduced-gpipe-new-library-for-efficiently-training-large-scale-neural-networks/\\n\\n\\xa0\\n\\nI recently started a new newsletter focus on AI education. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:\\n\\n\\n\\xa0\\nMicrosoft and Google have been actively working on new models for training deep neural networks. The result of that work has been the release of two new frameworks:\\xa0Microsoft’s PipeDream\\xa0and\\xa0Google’s GPipe\\xa0that follow similar principles to scale the training of deep learning models. Both projects have been detailed in respective research papers(PipeDream,\\xa0GPipe) which I would try to summarize today.\\nTraining is one of those areas of the lifecycle of deep learning programs that we don’t think of as challenging until the model’s hit certain scale. While training basic models during experimentation is relatively trivial, the complexity scales linearly with the quality and size of the model. For example, the winner of the 2014\\xa0ImageNet visual recognition challenge\\xa0was\\xa0GoogleNet, which achieved 74.8% top-1 accuracy with 4 million parameters, while just three years later, the winner of the 2017 ImageNet challenge went to\\xa0Squeeze-and-Excitation Networks, which achieved 82.7% top-1 accuracy with 145.8 million (36x more) parameters. However, in the same period, GPU memory has only increased by a factor of ~3.\\n\\nAs models scale in order to achieve higher levels of accuracy, the training of those models becomes increasingly challenging. The previous example demonstrates that is unsustainable to rely on GPU infrastructure improvements to achieve better training. Instead, we need distributed computing methods that parallelize training workloads across different nodes in order to scale training. That concept of parallelizable training might sound trivial but it results extremely complicated in practice. If you think about it, we are talking about partitioning the knowledge acquisition aspects of a model across different nodes and recombining the pieces into a cohesive model after. However, training parallelism is a must in order to scale deep learning models. To address those challenges, Microsoft and Google have devoted months of research and engineering that resulted in the release of GPipe and PipeDream respectively.\\n\\xa0\\nGoogle’s GPipe\\n\\xa0\\nGPipe focuses on scaling training workloads for deep learning programs. The complexity of training processes from an infrastructure standpoint is an often-overlooked aspect of deep learning models. Training datasets are getting larger and more complex. For instance, in the health care space, is not uncommon to encounter models that need to be trained using millions of high resolution images. As a result, training processes often take a long time to complete and result incredibly expensive from the memory and CPU consumption.\\nAn effective way to think about the parallelism of deep learning models is to divide it between data and model parallelism. The data parallelism approach employs large clusters of machines to split the input data across them. Model parallelism attempts to move the model to accelerators, such as GPUs or TPUs, which have special hardware to accelerate model training. At a high level, almost all training datasets can be parallelized following certain logic but the same can’t be said about models. For instance, some deep learning models are composed of parallel branches which can be trained independently. In that case, a classic strategy is to divide the computation into partitions and assign different partitions to different branches. However, that strategy falls short in deep learning models that stack layers sequentially, presenting a challenge to parallelize computation efficiently.\\nGPipe combines both data and model parallelism by leveraging aa technique called pipelining. Conceptually, GPipe is a distributed machine learning library that uses synchronous stochastic gradient descent and pipeline parallelism for training, applicable to any DNN that consists of multiple sequential layers. GPipe partitions a model across different accelerators and automatically splits a mini-batch of training examples into smaller micro-batches. This model allows GPipe’s accelerators to operate in parallel maximizing the scalability of the training process.\\nThe following figure illustrates the GPipe model with a neural network with sequential layers is partitioned across four accelerators. Fk is the composite forward computation function of kth partition. Bk is the corresponding backpropagation function. Bk depends on both Bk+1 from upper layer and the intermediate activations of Fk. In the top model, we can see how the sequential nature of the network leads to the underutilization of resources. The bottom figure shows the GPipe approach in which the input mini-batch is divided into smaller macro-batches which can be processed by the accelerators at the same time.\\n\\n\\nSource:\\xa0https://arxiv.org/pdf/1811.06965.pdf\\n\\n\\xa0\\nMicrosoft’s PipeDream\\n\\xa0\\nA few months ago, Microsoft Research announced the creation of\\xa0Project Fiddle, a series of research projects to streamline distributed deep learning. PipeDreams is one of the first released from Project Fiddle focusing on the parallelization of the training of deep learning models.\\nPipeDream takes a different approach from other methods to scale the training of deep learning models leveraging a technique known as pipeline parallelism. This approach tries to address some of the challenges of data and model parallelism techniques such as the ones used in GPipe. Typically, data parallelism methods suffer from high communication costs at scale when training on cloud infrastructure and can increase GPU compute speed over time. Similarly, model parallelism techniques often leverage hardware resources inefficiently and places an undue burden on programmers to determine how to split their specific model given a hardware deployment.\\n\\n\\nSource:\\xa0https://www.microsoft.com/en-us/research/uploads/prod/2019/08/fiddle_pipedream_sosp19.pdf\\n\\n\\xa0\\nPipeDream tries to overcome some of the challenges of data-model parallelism methods by using a technique called pipeline parallelism. Conceptually, Pipeline-parallel computation involves partitioning the layers of a DNN model into multiple stages, where each stage consists of a consecutive set of layers in the model. Each stage is mapped to a separate GPU that performs the forward pass (and backward pass) for all layers in that stage.\\nGiven a specific deep neural network, PipeDream automatically determines how to partition the operators of the DNN based on a short profiling run performed on a single GPU, balancing computational load among the different stages while minimizing communication for the target platform. PipeDream effectively load balances even in the presence of model diversity (computation and communication) and platform diversity (interconnect topologies and hierarchical bandwidths). PipeDream’s approach to training parallelism its principles offer several advantages over data-model parallelism methods. For starters, PipeDream requires less communications between the worker nodes as each worker in a pipeline execution has to communicate only subsets of the gradients and output activations, to only a single other worker. Also, PipeDream separates computation and communication in a way that leads to easier parallelism.\\n\\n\\nSource:\\xa0https://www.microsoft.com/en-us/research/uploads/prod/2019/08/fiddle_pipedream_sosp19.pdf\\n\\n\\xa0\\nTraining parallelism is one of the key challenges for building larger and more accurate deep learning models. An active area of research within the deep learning community, training parallelism methods needs to combine effective concurrent programming techniques with the nature of deep learning models. While still in early stages, Google’s GPipe and Microsoft’s PipeDream stand on its own merits as two of the most creative approaches to training parallelism available to deep learning developers.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nUber Open Sources the Third Release of Ludwig, its Code-Free Machine Learning Platform\\nWhat I learned from looking at 200 machine learning tools\\nNetflix’s Polynote is a New Open Source Framework to Build Better Data Science Notebooks',\n",
       " 'By Jesus Rodriguez, Intotheblock.\\ncomments\\nThis year I am going to make an attempt to summarize important papers in the AI space, in very short and simple terms that can be easily understood by anyone.\\n\\nI recently started a new newsletter focus on AI education and\\xa0already has over 50,000 subscribers. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:\\n\\n\\n\\xa0\\n“I’ve seen the future of artificial intelligence(AI) and it’s called MuZero”. Those were the words used by one of my mentors when he read the\\xa0first preliminary research paper about MuZero published by DeepMind in 2019. A single deep learning model that can master games like Atari, Go, Chess or Shogi without even knowing the rules. That seems like something out of a sci-fi book. Well, that’s the essence of MuZero as described by DeepMind in a\\xa0new research paper published in Nature a few weeks ago.\\nConceptually, MuZero presents a solution to one of the toughest challenges in the deep learning space: planning. Since the early days of machine learning, researchers have looked at techniques that can both effectively learn a model given an environment and also plan the best course of action. Think about a self-driving car or a stock market scenario in which the rules of the environment are constantly changing. Typically, those environment has resulted incredibly challenging for planning in deep learning models. At a high level, most efforts related to planning in deep neural network fit into the following categories:\\n1) Lookahead Search Systems:\\xa0This type of systems rely on knowledge of the environment for its planning. AlphaZero is a prominent example of models in this group. However, lookahead search techniques struggled when applied to messy environments.\\n2) Model-Based Systems:\\xa0This type of systems try to learn a representation of the environment in order to plan. Systems such as Agent57 have been successful in this area but they can be incredibly expensive to implement.\\nMuZero combines ideas from both approaches but using an incredibly simple principle. Instead of trying to model the entire environment, MuZero solely focuses on its most important aspects that can drive the most useful planning decisions. Specifically, MuZero decomposes the problem in three elements critical to planning:\\n1)\\xa0The value:\\xa0how good is the current position?\\n2)\\xa0The policy:\\xa0which action is the best to take?\\n3)\\xa0The reward:\\xa0how good was the last action?\\nFor instance, using the given position in a game, MuZero uses a representation function\\xa0H\\xa0to map the observations to an input embedding used by the model. Planned actions are described by a dynamic function\\xa0G\\xa0and a prediction function\\xa0F.\\n\\n\\nSource:\\xa0https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules\\n\\n\\xa0\\nThe experience collected is used to train a neural network. It is important to notice that the experience includes both observations and rewards as well as the results of searches.\\n\\n\\nSource:\\xa0https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules\\n\\n\\xa0\\nUsing this simple idea DeepMind was able to evolve MuZero into a model able to achieve super-human performance in complex planning problems ranging from Chess to Atari. In all benchmarks, MuZero outperformed state-of-the-art reinforcement learning algorithms.\\nThe impact of methods such as MuZero in deep learning planning is likely to be relevant for years to come. Certainly, we should keep an eye into what DeepMind is going to do next in this area.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nFacebook Open Sourced New Frameworks to Advance Deep Learning Research\\nHow LinkedIn Uses Machine Learning in its Recruiter Recommendation Systems\\nLearning by Forgetting: Deep Neural Networks and the Jennifer Aniston Neuron',\n",
       " 'By Terence Shin, Data Scientist | MSc Analytics & MBA student.\\ncomments\\n\\nImage Created by Author.\\nProbability distributions were so daunting when I first saw them, partly because there are so many of them, and they all have such unfamiliar names.\\nFast forward to today, and I realized that they’re actually very simple concepts to understand when you strip away all of the math behind them, and that’s exactly what we’re going to do today.\\nRather than getting into the mathematical side of things, I’m going to conceptually go over what I believe are the most fundamental and essential probability distributions.\\nBy the end of this article, you’ll not only learn about several probability distributions, but you’ll also realize how closely related many of these are to each other!\\nFirst, you need to know a couple of terms:\\n\\nA\\xa0probability distribution\\xa0simply shows the probabilities of getting different outcomes. For example, the distribution of flipping heads or tails is 0.5 and 0.5, respectively.\\nA\\xa0discrete distribution\\xa0is a distribution in which the values that the data can take on are\\xa0countable.\\nA\\xa0continuous distribution, on the other hand, is a distribution in which the values that the data can take on are\\xa0not countable.\\n\\n\\xa0\\n1. Normal Distribution\\n\\xa0\\n\\nImage created by Author.\\nThe normal distribution is arguably the most important distribution to know because many phenomena fit this distribution. IQs, heights of people, shoe size, birth weight are all examples that have a normal distribution.\\nThe\\xa0normal distribution\\xa0has a bell-shaped curve and has the following properties:\\n\\nIt has a symmetric bell shape.\\nThe mean and median are equal and are both located at the center of the distribution.\\n≈68% of the data falls within 1 standard deviation of the mean, ≈95% of the data falls within 2 standard deviations of the mean, and ≈99.7% of the data falls within 3 standard deviations of the mean.\\n\\nThe normal distribution is also an integral part of statistics, as it is the basis of several statistical inference techniques, including linear regression, confidence intervals, and hypothesis testing.\\n\\xa0\\n2. T-distribution\\n\\xa0\\nThe\\xa0t-distribution\\xa0is similar to the normal distribution but is generally shorter and has fatter tails. It is used instead of the normal distribution when the sample sizes are small.\\nOne thing to note is that as the sample size increases, the t-distribution converges to the normal distribution.\\n\\xa0\\n3. Gamma Distribution\\n\\xa0\\n\\nImage created by Author.\\nThe Gamma distribution is used to predict the wait time until a future event occurs. It is useful when something has a natural minimum of 0.\\nIt’s also generalized distribution of the chi-squared distribution and the exponential distribution (which we’ll talk about later).\\n\\xa0\\n4. Chi-Squared Distribution\\n\\xa0\\n\\nImage created by Author.\\nAs said above, the chi-squared distribution is a particular case of the gamma distribution. As there’s a lot to the chi-squared distribution, I won’t go into too much detail, but there are several uses for it:\\n\\nIt allows you to estimate confidence intervals for a population standard deviation.\\nIt is the distribution of sample variances when the underlying distribution is normal.\\nYou can test deviances of differences between expected and observed values.\\nYou can conduct a chi-squared test.\\n\\nNote: Don’t worry so much if this one confused you because the following distributions are much simpler to understand and get a grasp of!\\n\\xa0\\n5. Uniform Distribution\\n\\xa0\\nThe\\xa0uniform distribution\\xa0is really simple — each outcome has an equal probability. An example of this is rolling a dye.\\nThe image above shows a distribution that is approximately uniformly distributed.\\n\\xa0\\n6. Bernoulli Distribution\\n\\xa0\\n\\nImage created by Author.\\nIn order to understand the Bernoulli Distribution, you first need to know what a Bernoulli trial is. A\\xa0Bernoulli trial\\xa0is a random experiment with only two possible outcomes, success or failure, where the probability of success is the same every time.\\nTherefore, the\\xa0Bernoulli distribution\\xa0is a discrete distribution for one Bernoulli trial.\\nFor example, flipping a coin can be represented by a Bernoulli distribution, as well as rolling an odd number on a dye.\\n\\xa0\\n7. Binomial Distribution\\n\\xa0\\n\\nImage created by Author.\\nNow that you understand the Bernoulli distribution, the binomial distribution simply represents multiple Bernoulli trials. Specifically, the\\xa0binomial distribution\\xa0is a discrete distribution that represents the probability of getting x successes out of n independent Bernoulli trials.\\nHere are some examples that use the binomial distribution:\\n\\nWhat is the probability of getting 5 heads out of 10 coin flips?\\nWhat is the probability of getting 10 conversions out of 100 emails (assuming the probability of converting is the same)?\\nWhat is the probability of getting 20 responses from 500 customer feedback surveys (assuming the probability of getting a response is the same)?\\n\\nOne interesting thing about the binomial distribution is that it converges to a normal distribution as n (# of Bernoulli trials) gets large.\\n\\xa0\\n8. Geometric Distribution\\n\\xa0\\nThe geometric distribution is also related to the Bernoulli distribution, like the binomial distribution, except that it answers a slightly different question. The\\xa0geometric distribution\\xa0represents the probability of having x Bernoulli (p) failures until first success? In other words, it answers, “how many trials are needed until your first success?”\\nAn example of this is, “how many lottery tickets do I need to buy until I buy a winning ticket?”\\nYou can also use the geometric distribution to find the probability of the number of\\xa0Bernoulli (1-p) successes until failure. The geometric can also be used to check if an event is i.i.d if it fits the distribution.\\n\\xa0\\n9. Weibull Distribution\\n\\xa0\\n\\nImage created by Author.\\nThe Weibull distribution is like the geometric distribution, except it is a continuous distribution. Therefore, the\\xa0Weibull distribution\\xa0models the amount of time it takes for something to fail or the time between failures.\\nThe Weibull distribution can answer questions like:\\n\\nHow long until a particular lightbulb dies?\\nHow long until a customer churns?\\n\\n\\xa0\\n10. Poisson Distribution\\n\\xa0\\n\\nImage created by Author.\\nThe\\xa0Poisson distribution\\xa0is a discrete distribution that represents how many times an event is likely to occur within a specific time period.\\nThe Poisson distribution is most commonly used in queuing theory, which answers questions along the lines of “how many customers are likely to come (queue) within a given period of time?”.\\n\\xa0\\n11. Exponential Distribution\\n\\xa0\\n\\nImage created by Author.\\nThe\\xa0exponential distribution\\xa0is closely related to the Poisson distribution. If arrivals are distributed Poisson, then the time between arrivals (aka inter-arrival times) has the exponential distribution.\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\n10 Must-Know Statistical Concepts for Data Scientists\\nData Science 101: Normalization, Standardization, and Regularization\\nEssential Math for Data Science: Probability Density and Probability Mass Functions',\n",
       " 'comments\\nBy Aqsa Zafar, Ph.D. Scholar in Machine Learning | Founder at MLTUT | Solopreneur | Blogger.\\n\\n\\xa0\\n1. Dog’s Breed Identification\\n\\xa0\\nThere are various dog breeds, and most of them are similar to each other. As a beginner, you can build a\\xa0Dog’s breed identification model\\xa0to identify the dog’s breed.\\nFor this project, you can use the dog breeds dataset to classify various dog breeds from an image. You can download the dog breeds dataset from\\xa0Kaggle.\\nI also found this complete tutorial for\\xa0Dog Breed Classification using Deep Learning by Kirill Panarin.\\n\\xa0\\n2. Face Detection\\n\\xa0\\nThis is also a good deep learning project for beginners. In this project, you have to build a deep learning model that detects the\\xa0human faces from the image.\\nFace recognition is\\xa0computer vision technology. In face detection, you have to\\xa0locate and visualize\\xa0the human faces in any digital image.\\nYou can build this project in Python using OpenCV. For the complete tutorial, check this article, Real-time Face Recognition with Python & OpenCV.\\n\\xa0\\n3. Crop Disease Detection\\n\\xa0\\nIn this project, you have to build a model that\\xa0predicts diseases in crops\\xa0using RGB images. For building a Crop disease detection model,\\xa0Convolutional Neural Networks\\xa0(CNN) are used.\\nCNN takes an image to identify the disease and detect it. There are various steps in Convolutional Neural Network. These steps are:\\n\\nConvolution Operation.\\nReLU Layer.\\nPooling.\\nFlattening.\\nFull Connection.\\n\\nYou can download the\\xa0Agriculture crop images dataset\\xa0from\\xa0Kaggle.\\n\\xa0\\n4. Image Classification with CIFAR-10 Dataset\\n\\xa0\\nImage classification is the best project for beginners. In an image classification project, you have to classify the images into various classes.\\nFor this project, you can use\\xa0CIFAR-10 Dataset, which contains\\xa060,000\\xa0color images. These images are categorized into\\xa010 classes, such as cars, birds, dogs, horses, ships, trucks, etc.\\n\\nSource:\\xa0CIFAR-10 dataset.\\nFor\\xa0training data, there are\\xa050,000 images, and for\\xa0test data, 10,000 images\\xa0are used. Image classification is one of the most used applications of deep learning. You can download the\\xa0CIFAR-10 dataset\\xa0here.\\n\\xa0\\n5.\\xa0Handwritten Digit Recognition\\n\\xa0\\nTo explore and test your deep learning skills, I think this is the best project to consider. In this project, you will build a recognition system that recognizes human handwritten digits.\\nYou can check this tutorial for\\xa0Handwritten Digit Recognition using Python.\\nThis tutorial uses the\\xa0MNIST dataset\\xa0and a special type of deep neural network that is\\xa0Convolutional Neural Networks.\\n\\xa0\\n6.\\xa0Color Detection\\n\\xa0\\nThis is a beginner-level project where you have to build an\\xa0interactive app. This app will identify the selected color from any image. There are\\xa016 million colors\\xa0based on the different RGB color values, but we only know a few colors.\\nTo implement this project, you need to have\\xa0a labeled dataset\\xa0of all the colors that we know, and then you need to calculate which color resembles the most with the selected color value.\\nIn order to implement this project, you should be familiar with\\xa0Computer Vision Python libraries OpenCV and Pandas.\\nYou can check all the details regarding this project\\xa0here.\\n\\xa0\\n7.\\xa0Real-time Image Animation\\n\\xa0\\nThis is an\\xa0open-source project\\xa0on computer vision. In this project, you have to perform image animation in real-time using OpenCV. I have taken this image from the project’s GitHub repository.\\n\\nSource: GitHub.\\nAs you can see in the image, the model mimics the expression of the person in front of the camera and changes the image expression accordingly.\\nThis project is useful, especially if you are planning to enter into\\xa0the fashion, retail, or advertising industry.\\xa0You can check the code of this project at\\xa0GitHub\\xa0and\\xa0Colab notebook too.\\n\\xa0\\n8.\\xa0Driver Drowsiness Detection\\n\\xa0\\nRoad Accident\\xa0is a serious problem, and the major reason is the\\xa0sleepy drivers.\\xa0But you can prevent this problem by creating a\\xa0driver drowsiness detection\\xa0system.\\nDriver Drowsiness Detection system detects the drowsiness of the driver by\\xa0constantly assessing the driver’s eyes and alerting him with alarms.\\nFor this project, a\\xa0webcam\\xa0is necessary to monitor the driver’s eyes.\\xa0Python, OpenCV, and Keras\\xa0are used to alert the driver when he feels sleepy.\\nYou can check this complete project tutorial here, Driver Drowsiness Detection System with OpenCV & Keras.\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Aqsa Zafar, Ph.D. scholar in Data Mining researches \"Depression Detection from Social Media via Data Mining,\" and writes about Data Science and machine learning at MLTUT to\\xa0share knowledge and experience in the field.\\nRelated:\\n\\nTop 10 Data Science Projects for Beginners\\n4 Tips for Dataset Curation for NLP Projects\\n21 Machine Learning Projects – Datasets Included',\n",
       " \"By Matthew Mayo, KDnuggets.\\ncomments\\n\\nPhoto by Hitesh Choudhary on Unsplash\\n\\xa0\\nThis article contains 5 useful Python code snippets that a beginner might find helpful for data processing.\\nPython is a flexible, general purpose programming language, providing for many ways to approach and achieve the same task. These snippets shed light on one such approach for a given situation; you might find them useful, or find that you have come across another approach that makes more sense to you.\\n\\xa0\\n1. Concatenate Multiple Text Files\\n\\xa0\\nLet's start with concatenating multiple text files. Should you have a number of text files in a single directory you need concatenated into a single file, this Python code will do so.\\nFirst we get a list of all the txt files in the path; then we read in each file and write out its contents to the new output file; finally, we read the new file back in and print its contents to screen to verify.\\n\\nimport glob\\r\\n\\r\\n# Load all txt files in path\\r\\nfiles = glob.glob('/path/to/files/*.txt')\\r\\n\\r\\n# Concatenate files to new file\\r\\nwith open('2020_output.txt', 'w') as out_file:\\r\\n    for file_name in files:\\r\\n        with open(file_name) as in_file:\\r\\n            out_file.write(in_file.read())\\r\\n\\r\\n# Read file and print\\r\\nwith open('2020_output.txt', 'r') as new_file:\\r\\n    lines = [line.strip() for line in new_file]\\r\\nfor line in lines: print(line)\\n\\n\\n\\nfile 1 line 1\\r\\nfile 1 line 2\\r\\nfile 1 line 3\\r\\nfile 2 line 1\\r\\nfile 2 line 2\\r\\nfile 2 line 3\\r\\nfile 3 line 1\\r\\nfile 3 line 2\\r\\nfile 3 line 3\\n\\n\\n\\xa0\\n2. Concatenate Multiple CSV Files Into a DataFrame\\n\\xa0\\nStaying with the theme of file concatenation, this time let's tackle concatenating a number of comma separated value files into a single Pandas dataframe.\\nWe first get a list of the CSV files in our path; then, for each file in the path, we read the contents into its own dataframe; afterwards, we combine all dataframes into a single frame; finally, we print out the results to inspect.\\n\\nimport pandas as pd\\r\\nimport glob\\r\\n\\r\\n# Load all csv files in path\\r\\nfiles = glob.glob('/path/to/files/*.csv')\\r\\n\\r\\n# Create a list of dataframe, one series per CSV\\r\\nfruit_list = []\\r\\nfor file_name in files:\\r\\n    df = pd.read_csv(file_name, index_col=None, header=None)\\r\\n    fruit_list.append(df)\\r\\n\\r\\n# Create combined frame out of list of individual frames\\r\\nfruit_frame = pd.concat(fruit_list, axis=0, ignore_index=True)\\r\\n\\r\\nprint(fruit_frame)\\n\\n\\n\\n            0   1    2\\r\\n0      grapes   3  5.5\\r\\n1      banana   7  6.8\\r\\n2       apple   2  2.3\\r\\n3      orange   9  7.2\\r\\n4  blackberry  12  4.3\\r\\n5   starfruit  13  8.9\\r\\n6  strawberry   9  8.3\\r\\n7        kiwi   7  2.7\\r\\n8   blueberry   2  7.6\\n\\n\\n\\xa0\\n3. Zip & Unzip Files to Pandas\\n\\xa0\\nLet's say you are working with a Pandas dataframe, such as the resulting frame in the above snippet, and want to compress the frame directly to file for storage. This snippet will do so.\\nFirst we will create a dataframe to use with our example; then we will compress and save the dataframe directly to file; finally, we will read the frame back into a new frame directly from compressed file and print out for verificaiton.\\n\\nimport pandas as pd\\r\\n\\r\\n# Create a dataframe to use\\r\\ndf = pd.DataFrame({'col_A': ['kiwi', 'banana', 'apple'],\\r\\n\\t           'col_B': ['pineapple', 'grapes', 'grapefruit'],\\r\\n\\t\\t   'col_C': ['blueberry', 'grapefruit', 'orange']})\\r\\n\\r\\n# Compress and save dataframe to file\\r\\ndf.to_csv('sample_dataframe.csv.zip', index=False, compression='zip')\\r\\nprint('Dataframe compressed and saved to file')\\r\\n\\r\\n# Read compressed zip file into dataframe\\r\\ndf = pd.read_csv('sample_dataframe.csv.zip',)\\r\\nprint(df)\\n\\n\\n\\nDataframe compressed and saved to file\\r\\n\\r\\n    col_A       col_B       col_C\\r\\n0    kiwi   pineapple   blueberry\\r\\n1  banana      grapes  grapefruit\\r\\n2   apple  grapefruit      orange\\n\\n\\n\\xa0\\n4. Flatten Lists\\n\\xa0\\nPerhaps you have a situation where you are working with a list of lists, that is, a list in which all of its elements are also lists. This snippet will take this list of embedded lists and flatten it out to one linear list. \\nFirst we will create a list of lists to use in our example; then we will use list comprehensions to flatten the list in a Pythonic manner; finally, we print the resulting list to screen for verification.\\n\\n# Create of list of lists (a list where all of its elements are lists)\\r\\nlist_of_lists = [['apple', 'pear', 'banana', 'grapes'], \\r\\n                 ['zebra', 'donkey', 'elephant', 'cow'],\\r\\n\\t         ['vanilla', 'chocolate'], \\r\\n                 ['princess', 'prince']]\\r\\n\\r\\n# Flatten the list of lists into a single list\\r\\nflat_list = [element for sub_list in list_of_lists for element in sub_list]\\r\\n\\r\\n# Print both to compare\\r\\nprint(f'List of lists:\\\\n{list_of_lists}')\\r\\nprint(f'Flattened list:\\\\n{flat_list}')\\n\\n\\n\\nList of lists:\\r\\n[['apple', 'pear', 'banana', 'grapes'], ['zebra', 'donkey', 'elephant', 'cow'], ['vanilla', 'chocolate'], ['princess', 'prince']]\\r\\n\\r\\nFlattened list:\\r\\n['apple', 'pear', 'banana', 'grapes', 'zebra', 'donkey', 'elephant', 'cow', 'vanilla', 'chocolate', 'princess', 'prince']\\n\\n\\n\\xa0\\n5. Sort List of Tuples\\n\\xa0\\nThis snippet will entertain the idea of sorting tuples based on specified element. Tuples are an often overlooked Python data structure, and are a great way to store related pieces of data without using a more complex structure type.\\nIn this example, we will first create a list of tuples of size 2, and fill them with numeric data; next we will sort the pairs, separately by both first and second elements, printing the results of both sorting processes to inspect the results; finally, we will extend this sorting to mixed alphanumeric data elements.\\n\\n# Some paired data\\r\\npairs = [(1, 10.5), (5, 7.), (2, 12.7), (3, 9.2), (7, 11.6)]\\r\\n\\r\\n# Sort pairs by first entry\\r\\nsorted_pairs  = sorted(pairs, key=lambda x: x[0])\\r\\nprint(f'Sorted by element 0 (first element):\\\\n{sorted_pairs}')\\r\\n\\r\\n# Sort pairs by second entry\\r\\nsorted_pairs  = sorted(pairs, key=lambda x: x[1])\\r\\nprint(f'Sorted by element 1 (second element):\\\\n{sorted_pairs}')\\r\\n\\r\\n# Extend this to tuples of size n and non-numeric entries\\r\\npairs = [('banana', 3), ('apple', 11), ('pear', 1), ('watermelon', 4), ('strawberry', 2), ('kiwi', 12)]\\r\\nsorted_pairs  = sorted(pairs, key=lambda x: x[0])\\r\\nprint(f'Alphanumeric pairs sorted by element 0 (first element):\\\\n{sorted_pairs}')\\n\\n\\n\\nSorted by element 0 (first element):\\r\\n[(1, 10.5), (2, 12.7), (3, 9.2), (5, 7.0), (7, 11.6)]\\r\\n\\r\\nSorted by element 1 (second element):\\r\\n[(5, 7.0), (3, 9.2), (1, 10.5), (7, 11.6), (2, 12.7)]\\r\\n\\r\\nAlphanumeric pairs sorted by element 0 (first element):\\r\\n[('apple', 11), ('banana', 3), ('kiwi', 12), ('pear', 1), ('strawberry', 2), ('watermelon', 4)]\\r\\n\\n\\n\\nAnd there you have 5 Python snippets which may be helpful to beginners for a few different data processing tasks.\\n\\xa0\\nRelated:\\n\\nData Preparation in SQL, with Cheat Sheet!\\nHow to Clean Text Data at the Command Line\\nTop Python Libraries for Data Science, Data Visualization & Machine Learning\",\n",
       " 'By Matthew Mayo, KDnuggets.\\ncomments\\nMaybe you have dipped your toe in the waters of natural language processing by auditing Stanford\\'s From Languages to Information course. Perhaps you have used the course material from Stanford\\'s Natural Language Processing with Deep Learning to hone this additional particular set of skills. You may have learned from one of these many other freely-available top-notch natural language processing courses.\\nIf you have previously toured some of these other courses, perhaps it\\'s time to take a look at this relatively new offering from Stanford, Ethical and Social Issues in Natural Language Processing (CS384), an advanced seminar course covering \"issues in natural language processing related to ethical and social issues and the overall impact of these algorithms on people and society.\"\\n\\n\\xa0\\nLike a number of other Stanford CS courses (including those mentioned above), the learning materials for this course — including a few class slides, but mostly an impressive collection of relevant academic papers (sadly, videos for this course are not publicly available) — are freely-available to anyone interested in using them. Taught by professor Dan Jurafsky, along with teaching assistants Peter Henderson and Hang Jiang, one can expect the following from the course (taken directly from the course\\'s website):\\n\\nTopics include: bias in NLP data and models, privacy and computational profiling, measuring civility and toxicity online, computational propaganda, manipulation and framing, fairness/equity, power, and various applications to social good. We\\'ve drawn heavily on related classes like\\xa0Yulia Tsvetkov\\xa0and\\xa0Alan Black\\'s\\xa0Computational Ethics for NLP\\xa0and\\xa0Emily Bender\\'s\\xa0Ethics in NLP.\\n\\nRather cleverly, the class schedule\\'s individual weeks are classified as either covering \"ways to avoid ethical or social problems in doing NLP research,\" which are coded as \"red weeks\" signifying that \"(NLP Should) Do No Harm,\" or covering \"ways to apply NLP to help solve social or ethical problems,\" or \"blue weeks\" expressing that \"(NLP Should) Do Good.\" \\nBroad weekly topics, supported by the aforementioned comprehensive collection of academic papers on the subjects, include:\\n\\nGender Bias in NLP Models and Data\\nRacial Bias or Disparity in NLP Models\\nNLP as a tool for detecting stereotypes or bias\\nNLP for identifying toxicity/hate/abuse\\nNLP for Studying Propaganda and Political Misinformation\\nNLP for Fact-Checking/Fake News Detection\\nNLP for Studying Framing and its Biases\\n\\n\\n\\nThe duality of NLP. From Stanford\\'s Ethical and Social Issues in Natural Language Processing (CS384) course slides.\\n\\n\\xa0\\nThis does not cover all of the subjects either, and lessons and reading materials are incredibly up to date. For example, there is a section on \"Issues in NLP related to COVID,\" which is obviously a timely and bleeding edge theme. \\nGiven the importance of ethics and social issues to so much of what data science and machine learning touches, and relatedly (and by extension) natural language processing, a course like this one should be perceived by all as being on equal footing as courses which transmit the technical know-how needed to practice and perform research in these areas. Numerous resources, including books and other courses, are now raising ethics and social issues to equal status, but devoted courses such as Stanford\\'s Ethical and Social Issues in Natural Language Processing (CS384) are worthy of independent study. \\nDo yourself a favor and have a look at the material freely-available in this course\\'s materials. You will either have your current understanding of such an important array of topics reinforced, or you will be startled at the vast concerns you have overlooked to this point. Either way, the result will be your takeaway of some additional knowledge on genuinely pressing and far-reaching issues.\\n\\xa0\\nRelated:\\n\\nThe Best NLP with Deep Learning Course is Free\\nFrom Languages to Information: Another Great NLP Course from Stanford\\n10 Free Top Notch Natural Language Processing Courses',\n",
       " 'By Devin Partida, Editor-in-Chief of ReHack.com.\\ncomments\\n\\nThe job market for data scientists is more active than ever and on track for rapid growth over the next few years. The U.S. Bureau of Labor Statistics predicts that the number of available positions will rise about 28% through 2026.\\nCompanies are investing significant amounts of money into market research and business analysis, creating new opportunities for long-time data scientists and those new to the field. At the same time, the job market is also becoming more competitive. The average compensation for data science positions is rising as these jobs become more important to businesses, encouraging hiring managers to more carefully vet new hires.\\nData scientists wanting to stay competitive or break into the field will need the right approach. These techniques will help them search for and secure a new position.\\n\\xa0\\n The State of the Global Data Science Job Market\\n\\xa0\\n\\xa0\\nPeople are generating more information than ever — experts believe worldwide data is on track to be in excess of 175 zettabytes by 2025. At the same time, innovations in AI and big data analysis have made large data sets more valuable than ever for businesses — but only if they work with trained scientists who can uncover the necessary insights.\\nHalf of all surveyed businesses have used AI in one way or another, and more say they plan to invest further in data-driven solutions in the near future.\\nRight now, it’s not unusual for a single data science job posting to receive hundreds of applications. Higher demand also means rising compensation, and businesses are being more careful in who they hire for these positions.\\nIn response, many hiring managers are inflating the job requirements of new data science openings — demanding stronger credentials, more experience and additional keywords. Even data scientists with good qualifications or strong academic track records aren’t guaranteed a position right now.\\n\\xa0\\n Best Practices for Becoming Competitive in the Global Job Market\\n\\xa0\\nData scientists who want to break into the field or secure a new position will need the right strategy to succeed. These six tips will help established professionals and those new to the industry secure work.\\n\\xa0\\n1. Know the Right Words to Use\\n\\xa0\\nFamiliarity with popular industry keywords — like Python, SQL, AI and data analytics — can help you write a CV and resume that will more effectively communicate your skill set and get past the resume filters often used by hiring managers.\\nKeeping up with changing industry demands can also help keep you competitive. While Python remains an essential skill, more businesses expect familiarity with deep learning, gradient boosting machines and big data analytics. Many companies also expect applicants to have used a wide variety of approaches for data mining and analysis in the past.\\n\\xa0\\n2. Communicate Familiarity With Industry-Standard Tools\\n\\xa0\\nWhen applying for positions that expect knowledge of artificial intelligence, emphasizing knowledge in data science and machine learning may help you secure an interview.\\nAt the same time, keyword stuffing, the act of unnaturally filling a resume with keywords to beat resume scanners or attract the attention of hiring managers, should be avoided. Try to only employ them in a resume or CV when they are relevant and help you explain your unique background and data science skill set.\\n\\xa0\\n3. Learn How Big Businesses Look for Data Science Professionals\\n\\xa0\\nExamining how major companies hire data scientists can also help you improve your resume and CV. AI and ML company Daitaku was recently featured in a case study about how it finds data scientists internationally. The report emphasizes how skill sets matter more than geography.\\n\\xa0\\n4. Take Advantage of General Job-Seeking Best Practices\\n\\xa0\\nJob application best practices typically also help data scientists looking for a new position. Tailoring your CV and cover letter to each job you apply for will take some extra effort. Still, it can help you communicate your particular skill set before an interview and illustrate how you’re a good fit for a certain position.\\n\\xa0\\n5. Network With Other Data Scientists\\n\\xa0\\nActively networking with other data scientists and recruiters looking for professionals can help you expand your network and more easily find openings that match your skills and experience level.\\nWhile waiting to hear back from hiring managers, you may also look for short-term work that can help you further develop your skills and add a bullet point or two to your resume.\\n\\xa0\\n6. Consider Freelance Work\\n\\xa0\\nBusinesses needing data scientists but struggling to fill new positions might offer temporary and freelance work to qualified applicants. Platforms like UpWork and freelance job search boards can provide you with leads on these positions.\\n\\xa0\\n Looking Forward: How Data Scientists Can Stay Competitive\\n\\xa0\\n\\xa0\\nThere are more openings for data scientists than ever, but that doesn’t mean the market is becoming less competitive. The growing value of data science and the lack of skilled candidates has companies hiring very carefully.\\nData scientists wanting to find a new position or break into the market should keep on top of industry trends and become familiar with various mining and analysis techniques. Best practices for job searching — like customized CVs and careful use of keywords — can also help them secure an interview.\\nYou can stand out amid a sea of competitors and land your ideal data science job by employing these techniques.\\n\\xa0\\nBio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com\\nRelated:\\n\\nHow to Succeed in Becoming a Freelance Data Scientist\\nHow Data Professionals Can Add More Variation to Their Resumes\\nHow Automation Is Improving the Role of Data Scientists',\n",
       " 'By Terence Shin, Data Scientist | MSc Analytics & MBA student.\\ncomments\\n\\nPhoto by\\xa0Austin Distel\\xa0on\\xa0Unsplash.\\nFirst of all, I wanted to give a huge shoutout to all of the nurses, doctors, grocery clerks, public administrators, and anyone else that’s putting their lives at risk to serve their communities.\\nLet’s not take this for granted.\\xa0Take this time in isolation to learn new skills, read books, and improve yourself.\\xa0For those interested in data, data analytics, or data science, I’m providing a list of fourteen data science projects that you can do during your spare time!\\nThere are three types of projects:\\n\\nVisualization projects\\nExploratory data analysis (EDA) projects\\nPrediction modeling\\n\\n\\xa0\\nVisualization Projects\\n\\xa0\\nPerhaps the quickest projects to complete are data visualizations! Below are three interesting datasets that you can use to create some intriguing visualizations to add to your portfolio.\\n\\nCoronavirus visualizations\\n\\nDifficulty: Easy\\nLink to dataset\\xa0here.\\n\\nLearn how to build dynamic visualizations using Plotly to show how the coronavirus has spread globally over time like the one above! Plotly is an amazing library that makes data visualizations dynamic, appealing, and simple.\\nIf you want to learn how to build a visualization like the one above, check out my tutorial\\xa0here.\\nMy friend, Jack, also wrote an article on predicting the coronavirus recovery\\xa0here!\\n\\nAustralian Wildfire Visualizations\\n\\nDifficulty: Easy\\nLink to dataset\\xa0here.\\n\\nTaken from Vox.\\nThe 2019–2020 bushfire season, also known as the black summer, consisted of several extreme wildfires starting in June 2019. The fires burnt an estimated 18.6 million hectares and over 5,900 buildings, according to Wikipedia.\\nThis makes for an interesting project! Leverage your data visualization skills using Plotly or Matplotlib to show the magnitude and geographical impact of the wildfires.\\nSee how my friend, Jack, predicted Brazil’s wildfire patterns\\xa0here!\\n\\nEarth Surface Temperature Visualization\\n\\nDifficulty: Easy-Medium\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0William Bossen\\xa0on\\xa0Unsplash.\\nHave any climate change deniers? Create some data visualizations to show how the Earth’s surface temperatures have changed over time. You can do this by creating a line graph or another animated Choropleth map!\\nBonus: create a prediction model that shows what Earth’s temperatures are expected to be in fifty years.\\n\\xa0\\nExploratory Data Analysis Projects\\n\\xa0\\nExploratory Data Analysis (EDA), also known as Data Exploration, is a step in the Data Analysis Process, where a number of techniques are used to better understand the dataset being used.\\nIf you want to learn more about EDA, check out my guide\\xa0here!\\n\\nNew York Airbnb Data Exploration\\n\\nDifficulty: Medium\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0Oliver Niblett\\xa0on\\xa0Unsplash.\\nSince 2008, guests and hosts have used Airbnb to expand on traveling possibilities and present more personalized ways of experiencing the world. This dataset contains information on 2019 listings in New York and its geographical information, prices, number of reviews, and more.\\nSome questions that you can try to answer are as follows:\\n\\nWhich hosts are the busiest and why?\\nWhat areas have more traffic than others, and why is that the case?\\nAre there any relationships between prices, number of reviews, and the number of days that a given listing is booked?\\n\\n\\xa0\\n\\nMost Important Factors related to Employee Attrition and Performance\\n\\nDifficulty: Easy\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0Campaign Creators\\xa0on\\xa0Unsplash.\\nIBM created a synthetic dataset that you can use to understand how various factors affect employee attrition and satisfaction. Some of the variables include education, job involvement, performance rating, and work-life balance.\\nExplore this dataset and see if there are any significant variables that indeed affect employee satisfaction. Take it a step further and see if you can rank the variables from most important to the least.\\n\\nWorld University Rankings\\n\\nDifficulty: Easy\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0Vasily Koloda\\xa0on\\xa0Unsplash.\\nDo you think your country has the best university in the world? What does it mean to be the ‘best’ university to start with? This dataset contains three global university rankings. Using this data, see if you can answer the following questions:\\n\\nWhat countries are the top universities in?\\nWhat are the main factors that determine one’s world ranking?\\n\\n\\xa0\\n\\nAlcohol and school success\\n\\nDifficulty: Easy\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0Kevin Kelly\\xa0on\\xa0Unsplash.\\nDoes alcohol affect students’ grades? If not, what does? This data was obtained in a survey from students in math and Portuguese language courses in secondary school. It contains several variables like alcohol consumption, family size, involvement in extracurriculars.\\nUsing this, explore the relationship between school performance and various factors. As a bonus, see if you can predict a student’s final grade based on other variables!\\n\\nPokemon Data Exploration\\n\\nDifficulty: Easy\\nLink to dataset\\xa0here.\\n\\nTaken from Pokemon.com.\\nFor all of you gamers out there, here’s a dataset that contains information on all 802 Pokemon from all seven generations. Here are several questions that you can try to answer!\\n\\nWhich generation has the strongest Pokemon? Which has the weakest?\\nWhat Pokemon type is the strongest? The weakest?\\nIs it possible to build a classifier to identify a legendary Pokemon?\\nAre there any correlations between physical traits and strength stats (attack, defense, speed, etc.)?\\n\\n\\xa0\\n\\nExploring Factors of Life Expectancy\\n\\nDifficulty: Easy\\nLink to dataset\\xa0here.\\nWHO created a dataset of the health status of all countries over time and includes statistics on life expectancy, adult mortality, and more. Using this dataset, explore the relationships between various variables. What has the biggest impact on life expectancy?\\nThis dataset was created to answer the following questions:\\n\\nDo various predicting factors that have been chosen initially really affect Life expectancy? What are the predicting variables actually affecting life expectancy?\\nShould a country having a lower life expectancy value(<65) increase its healthcare expenditure in order to improve its average lifespan?\\nHow do Infant and Adult mortality rates affect life expectancy?\\nDoes Life Expectancy have a positive or negative correlation with eating habits, lifestyle, exercise, smoking, drinking alcohol, etc.\\nWhat is the impact of schooling on the lifespan of humans?\\nDoes Life Expectancy have a positive or negative relationship with drinking alcohol?\\nDo densely populated countries tend to have a lower life expectancy?\\nWhat is the impact of Immunization coverage on life Expectancy?\\n\\nCheck out my article on\\xa0Predicting Life Expectancy with Regression\\xa0for inspiration!\\n\\xa0\\nPrediction Modeling\\n\\xa0\\n\\nTime Series Forecast on Energy Consumption\\n\\nDifficulty: Medium-Advanced\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0Matthew Henry\\xa0on\\xa0Unsplash.\\nThis dataset is composed of power consumption data from PJM’s website. PJM is a regional transmission organization in the United States. Using this dataset, see if you can build a time series model to predict energy consumption. In addition to that, see if you can find trends around hours of the day, holiday energy usage, and long term trends!\\n\\xa0\\n\\nLoan Prediction Forecast\\n\\nDifficulty: Easy\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0Dmitry Demidko\\xa0on\\xa0Unsplash.\\nTaken from Analytics Vidhya, this dataset has 615 rows and 13 columns on past loans that have and haven’t been approved. See if you can create a model that predicts whether a loan will get approved or not.\\n\\xa0\\n\\nUsed Car Price Estimator\\n\\nDifficulty: Medium\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0Parker Gibbs\\xa0on\\xa0Unsplash.\\nCraigslist is the world’s largest collection of used vehicles for sale. This dataset is composed of scraped data from Craigslist and is updated every few months. Using this data set, see if you can create a dataset that predicts whether a car listing is over or underpriced.\\nCheck out my model that predicts used car prices\\xa0here!\\n\\nDetecting Credit Card Fraud\\n\\nDifficulty: Medium-Advanced\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0rupixen.com\\xa0on\\xa0Unsplash.\\nThis dataset presents transactions that occurred in two days, with 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, with the positive class (frauds) account for 0.172% of all transactions. Learn how to work with unbalanced datasets and build a credit card fraud detection model.\\n\\xa0\\n\\nSkin Cancer Image Detection\\n\\nDifficulty: Advanced\\nLink to dataset\\xa0here.\\n\\nPhoto by\\xa0Allie Smith\\xa0on\\xa0Unsplash.\\nWith over 10,000 images, see if you can build a neural network to detect skin cancer. This definitely the hardest project and requires extensive knowledge of neural networks and image recognition.\\xa0Tip: refer to kernels created by other users if you’re stuck!\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Acquire the Most Wanted Data Science Skills\\nHow I Levelled Up My Data Science Skills In 8 Months\\nData Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science',\n",
       " 'comments\\nBy Saurabh Raj, IIT Jammu.\\n\\xa0\\nWhy evaluation is necessary?\\n\\xa0\\nLet me start with a very simple example.\\nRobin and Sam both started preparing for an entrance exam for engineering college. They both shared a room and put an equal amount of hard work while solving numerical problems. They both studied almost the same hours for the entire year and appeared in the final exam. Surprisingly, Robin cleared, but Sam did not. When asked, we got to know that there was one difference in their strategy of preparation, “test series.” Robin had joined a test series, and he used to test his knowledge and understanding by giving those exams and then further evaluating where is he lagging. But Sam was confident, and he just kept training himself.\\nIn the same fashion, as discussed above, a machine learning model can be trained extensively with many parameters and new techniques, but as long as you are skipping its evaluation, you cannot trust it.\\nHow to read the Confusion Matrix?\\nA\\xa0confusion matrix\\xa0is a correlation between the predictions of a model and the actual class labels of the data points.\\n\\nConfusion Matrix for a Binary Classification.\\nLet’s say you are building a model that detects whether a person has diabetes or not. After the train-test split, you got a test set of length 100, out of which 70 data points are labeled positive (1), and 30 data points are labelled negative (0). Now let me draw the matrix for your test prediction:\\n\\nOut of 70 actual positive data points, your model predicted 64 points as positive and 6 as negative. Out of 30 actual negative points, it predicted 3 as positive and 27 as negative.\\nNote:\\xa0In the notations,\\xa0True Positive, True Negative, False Positive, & False Negative, notice that the second term (Positive or Negative) is denoting your prediction, and the first term denotes whether you predicted right or wrong.\\nBased on the above matrix, we can define some very important ratios:\\n\\nTPR (True Positive Rate) = ( True Positive / Actual Positive )\\nTNR (True Negative Rate) = ( True Negative/ Actual Negative)\\nFPR (False Positive Rate) = ( False Positive / Actual Negative )\\nFNR (False Negative Rate) = ( False Negative / Actual Positive )\\n\\nFor our case of diabetes detection model, we can calculate these ratios:\\nTPR = 91.4%\\nTNR = 90%\\nFPR = 10%\\nFNR = 8.6%\\nIf you want your model to be smart, then your model has to predict correctly. This means your\\xa0True Positives\\xa0and\\xa0True Negatives should be as high as possible, and at the same time, you need to minimize your mistakes for which your\\xa0False Positives\\xa0and\\xa0False Negatives should be as low as possible.\\xa0Also in terms of ratios, your\\xa0TPR & TNR should be very high\\xa0whereas\\xa0FPR & FNR should be very low,\\nA smart model: TPR ↑ , TNR ↑, FPR ↓, FNR ↓\\nA dumb model: Any other combination of TPR, TNR, FPR, FNR\\nOne may argue that it is not possible to take care of all four ratios equally because, at the end of the day, no model is perfect. Then what should we do?\\nYes, it is true. So that is why we build a model keeping the domain in our mind. There are certain domains that demand us to keep a specific ratio as the main priority, even at the cost of other ratios being poor. For example, in cancer diagnosis, we cannot miss any positive patient at any cost. So we are supposed to keep TPR at the maximum and FNR close to 0. Even if we predict any healthy patient as diagnosed, it is still okay as he can go for further check-ups.\\n\\xa0\\nAccuracy\\n\\xa0\\nAccuracy is what its literal meaning says, a measure of how accurate your model is.\\nAccuracy = Correct Predictions / Total Predictions\\nBy using confusion matrix, Accuracy = (TP + TN)/(TP+TN+FP+FN)\\nAccuracy is one of the simplest performance metrics we can use. But let me warn you, accuracy can sometimes lead you to false illusions about your model, and hence you should first know your data set and algorithm used then only decide whether to use accuracy or not.\\nBefore going to the failure cases of accuracy, let me introduce you with two types of data sets:\\n\\nBalanced:A data set that contains almost equal entries for all labels/classes. E.g., out of 1000 data points, 600 are positive, and 400 are negative.\\nImbalanced:A data set that contains a biased distribution of entries towards a particular label/class. E.g., out of 1000 entries, 990 are positive class, 10 are negative class.\\n\\nVery Important: Never use accuracy as a measure when dealing with imbalanced test set.\\nWhy?\\nSuppose you have an imbalanced test set of 1000 entries with\\xa0990 (+ve)\\xa0and\\xa010 (-ve). And somehow, you ended up creating a poor model which always predicts “+ve” due to the imbalanced train set. Now when you predict your test set labels, it will always predict “+ve.” So out of 1000 test set points, you get 1000 “+ve” predictions. Then your accuracy would come,\\n990/1000 = 99%\\nWhoa! Amazing! You are happy to see such an awesome accuracy score.\\nBut, you should know that your model is really poor because it always predicts “+ve” label.\\nVery Important: Also, we cannot compare two models that return probability scores and have the same accuracy.\\nThere are certain models that give the probability of each data point for belonging to a particular class like that in Logistic Regression. Let us take this case:\\n\\nTable 1.\\nAs you can see,\\xa0If P(Y=1) > 0.5, it predicts class 1.\\xa0When we calculate accuracy for both M1 and M2, it comes out the same, but it is quite evident that\\xa0M1 is a much better model than M2 by taking a look at the probability scores.\\nThis issue is beautifully dealt with by\\xa0Log Loss, which I explain later in the blog.\\n\\xa0\\nPrecision & Recall\\n\\xa0\\nPrecision:\\xa0It is the ratio of True Positives (TP) and the total positive predictions. Basically, it tells us how many times your positive prediction was actually positive.\\n\\nRecall :\\xa0It is nothing but\\xa0TPR\\xa0(True Positive Rate explained above). It tells us about out of all the positive points how many were predicted positive.\\n\\nF-Measure:\\xa0Harmonic mean of precision and recall.\\n\\nTo understand this, let’s see this example: When you ask a query in google, it returns 40 pages, but only 30 were relevant. But your friend, who is an employee at Google, told you that there were 100 total relevant pages for that query. So it’s precision is 30/40 = 3/4 = 75% while it’s recall is 30/100 = 30%. So, in this case, precision is “how useful the search results are,” and recall is “how complete the results are.”\\n\\xa0\\nROC & AUC\\n\\xa0\\nReceiver Operating Characteristic Curve (ROC):\\nIt is a\\xa0plot between TPR (True Positive Rate) and FPR (False Positive Rate)\\xa0calculated by taking multiple threshold values from the reverse sorted list of probability scores given by a model.\\n\\nA typical ROC curve.\\nNow, how do we plot ROC?\\nTo answer this, let me take you back to Table 1 above. Just consider the M1 model. You see, for all x values, we have a probability score. In that table, we have assigned the data points that have a score of more than 0.5 as class 1. Now sort all the values in descending order of probability scores and one by one take threshold values equal to all the probability scores. Then we will have threshold values = [0.96,0.94,0.92,0.14,0.11,0.08]. Corresponding to each threshold value, predict the classes, and calculate TPR and FPR. You will get 6 pairs of TPR & FPR. Just plot them, and you will get the ROC curve.\\nNote: Since the maximum TPR and FPR value is 1, the area under the curve (AUC) of ROC lies between 0 and 1.\\nThe area under the blue dashed line is 0.5. AUC = 0 means very poor model, AUC = 1 means perfect model. As long as your model’s AUC score is more than 0.5. your model is making sense because even a random model can score 0.5 AUC.\\nVery Important:\\xa0You can get very high AUC even in a case of a dumb model generated from an imbalanced data set. So always be careful while dealing with imbalanced data set.\\nNote:\\xa0AUC had nothing to do with the numerical values probability scores as long as the order is maintained. AUC for all the models will be the same as long as all the models give the same order of data points after sorting based on probability scores.\\n\\xa0\\nLog Loss\\n\\xa0\\nThis performance metric checks the deviation of probability scores of the data points from the cut-off score and assigns a penalty proportional to the deviation.\\nFor each data point in a binary classification, we calculate it’s log loss using the formula below,\\n\\nLog Loss formula for a Binary Classification.\\nwhere p = probability of the data point to belong to class 1 and y is the class label (0 or 1).\\nSuppose if p_1 for some x_1 is 0.95 and p_2 for some x_2 is 0.55 and cut off probability for qualifying for class 1 is 0.5. Then both qualify for class 1, but the log loss of p_2 will be much more than the log loss of p_1.\\n\\nAs you can see from the curve, the range of log loss is [0, infinity).\\nFor each data point in multi-class classification, we calculate it’s log loss using the formula below,\\n\\nLog Loss formula for multi-class classification.\\nwhere y(o,c) = 1 if x(o,c) belongs to class 1. The rest of the concept is the same.\\n\\xa0\\nCoefficient of Determination\\n\\xa0\\nIt is denoted by\\xa0R². While predicting target values of the test set, we encounter a few errors (e_i), which is the difference between the predicted value and actual value.\\nLet’s say we have a test set with n entries. As we know, all the data points will have a target value, say [y1,y2,y3…….yn]. Let us take the predicted values of the test data be [f1,f2,f3,……fn].\\nCalculate the\\xa0Residual Sum of Squares, which is the sum of all the errors (e_i) squared,\\xa0by using this formula where fi is the predicted target value by a model for i’th data point.\\n\\nTotal Sum of Squares.\\nTake the mean of all the actual target values:\\n\\nThen calculate the\\xa0Total Sum of Squares, which is proportional to the variance of the test set target values:\\n\\nIf you observe both the formulas of the sum of squares, you can see that the only difference is the 2nd term, i.e., y_bar and fi. The total sum of squares somewhat gives us an intuition that it is the same as the residual sum of squares only but with predicted values as [ȳ, ȳ, ȳ,…….ȳ ,n times]. Yes, your intuition is right. Let’s say there is a very simple mean model that gives the prediction of the average of the target values every time irrespective of the input data.\\nNow we formulate R² as:\\n\\nAs you can see now, R² is a metric to compare your model with a very simple mean model that returns the average of the target values every time irrespective of input data. The comparison has 4 cases:\\ncase 1: SS_R = 0\\n(R² = 1) Perfect model with no errors at all.\\ncase 2: SS_R > SS_T\\n(R² < 0) Model is even worse than the simple mean model.\\ncase 3: SS_R = SS_T\\n(R² = 0) Model is same as the simple mean model.\\ncase 4: SS_R < SS_T\\n(0< R² <1) Model is okay.\\n\\xa0\\nSummary\\n\\xa0\\nSo, in a nutshell, you should know your data set and problem very well, and then you can always create a confusion matrix and check for its accuracy, precision, recall, and plot the ROC curve and find out AUC as per your needs. But if your data set is imbalanced, never use accuracy as a measure. If you want to evaluate your model even more deeply so that your probability scores are also given weight, then go for Log Loss.\\nRemember, always evaluate your training!\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nIdiot’s Guide to Precision, Recall, and Confusion Matrix\\nUsing Confusion Matrices to Quantify the Cost of Being Wrong\\nAchieving Accuracy with your Training Dataset',\n",
       " 'comments\\nBy Rik Kraan, Vantage AI\\nKubernetes is a powerful container orchestration tool that automates deployment and management of\\xa0containers. If you have a simple lightweight application that exists of one service, don’t bother using Kubernetes. Kubernetes’ benefits emerge if your application has a\\xa0micro-service\\xa0architecture with several components working together. It is a ‘open-source system for automating deployment, scaling and management of containerized applications and comes with several benefits including:\\n\\nEasy (auto-)scaling based on demand\\nWays to make your application fault tolerant by distributing workloads in a way that it will remain functional in case of partial failure\\nAutomated health-checks and self-healing processes\\nTaking care of the communication between your micro-services and balancing incoming traffic evenly over all your resources\\n\\nStarting with Kubernetes may seem daunting at first, but if you grasp the main concepts of it and play around with the excellent tutorials on the\\xa0official website, you can get started fairly easily.\\nIn\\xa0this blog I will:\\n\\nProvide a quick overview of the main concepts of Kubernetes\\nDemonstrate how to start your own local cluster\\nDeploy a MySQL database on your cluster\\nSet up an Flask app that functions as REST API to communicate with the database\\n\\n\\n\\nNetwork. Photo by\\xa0Alina Grubnyak\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nKubernetes basics\\n\\xa0\\nIn this section I will cover the basics of Kubernetes without too many details; feel free to dive deeper by reading the official documentation.\\nA Kubernetes cluster consists of a\\xa0master\\xa0and one or more\\xa0worker nodes.\\xa0This architecture is one of the main features of Kubernetes. As you will see, your micro-services are distributed over different nodes so they will remain healthy if one of the worker nodes fails. The\\xa0master\\xa0is responsible for managing the cluster and exposes the API via which you can communicate with your cluster. By default,\\xa0worker nodes\\xa0come with a few components including some pre-installed software that enables running containers of popular container services as\\xa0Docker\\xa0and\\xa0containerd.\\nThree main concepts are essential to deploy your own applications on a Kubernetes cluster:\\xa0Deployments, Pods and Services.\\n\\nA\\xa0Deployment\\xa0is a set of instructions provided to the\\xa0master\\xa0on how to create and update your application. With these instructions the\\xa0master\\xa0will schedule and run your application on individual\\xa0worker nodes. The deployment is continuously monitored by the\\xa0master. If one of the instances of your applications goes down (e.g. if a\\xa0worker node\\xa0goes down), it will be automatically replaced by a new instance.\\n\\n\\n\\nKubernetes cluster with a deployment (source:\\xa0https://kubernetes.io/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/)\\n\\n\\xa0\\n\\nA\\xa0Pod\\xa0is the atomic unit within the Kubernetes platform. It represents a group of one or more containers and some shared resources for those containers (shared storage, a unique cluster IP address etc.). If you create a\\xa0deployment, this deployment will create\\xa0pods\\xa0with containers inside them. Each\\xa0pod\\xa0is bound to a\\xa0worker node. It is essential to understand that a\\xa0worker node\\xa0can have multiple\\xa0pods,\\xa0and all these pods will be rebuild on a different available\\xa0worker node\\xa0if the current\\xa0worker node\\xa0fails.\\n\\n\\n\\nOverview of a\\xa0worker node\\xa0with several pods (source:\\xa0https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/)\\n\\n\\xa0\\n\\nA\\xa0service\\xa0basically defines a logical set of\\xa0pods\\xa0and defines a policy on how to access them. This is necessary as\\xa0pods\\xa0can go down and be restarted (e.g. if a\\xa0worker node\\xa0is deleted or crashes). A\\xa0service\\xa0routes traffic across a set of pods and allow pods to die and replicate without impacting your application. When defining a service, you can specify the type of the service. By default Kubernetes creates a ClusterIP service, which makes your\\xa0service\\xa0only accessible from inside the cluster. You may want to expose some of your\\xa0services\\xa0(e.g. frontends) to the outside world. In this case you can create a\\xa0LoadBalancer\\xa0service, which creates an external load balancer and assigns a fixed external IP to it, so it can be accessed from outside the cluster (for example in your browser).\\n\\n\\n\\nA cluster with 3\\xa0worker nodes, several pods and two services (A & B) tying pods together (source:\\xa0https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)\\n\\n\\xa0\\nGetting started with your own cluster\\n\\xa0\\nIf you want to get your cluster to work quickly: all the code in this blog (and an explanatory Readme) can be found\\xa0here. The application we will build consists of two micro-services:\\n\\na MySQL database\\na Flask app that implements an API to access and perform CRUD (create, read, update delete) operations on the database.\\n\\n\\nPrerequisites:\\xa0Have\\xa0kubectland\\xa0minikube\\xa0installed (https://kubernetes.io/docs/tasks/tools/). And make sure your\\xa0Docker CLI\\xa0uses the\\xa0Docker deamon\\xa0in your cluster via the command\\xa0eval $(minikube docker-env). No worries: if you restart your terminal you will automatically use your own\\xa0Docker daemon\\xa0again. Finally start your local cluster via the command\\xa0minikube start.\\n\\n\\xa0\\nFirst things first: when setting up a MySQL database we need to take into account two things. 1) To access the database we need some credentials configured and 2) we will need a persistent volume for the database so we will not lose all our data if the nodes will accidentally be taken down.\\n\\xa0\\nCreating secrets\\n\\xa0\\nKubernetes has it’s own method of dealing with your sensitive information by configuring Kubernetes Secrets. This can be done with a simple YAML file. These\\xa0secrets\\xa0can be accessed by any\\xa0pod\\xa0in your cluster by specifying environment variables (which we will see later on).\\xa0Secrets\\xa0should be specified as\\xa0base64-encoded\\xa0strings. So first we have to get the encoded version of your password via your terminal:\\xa0echo -n <super-secret-passwod> | base64. Copy the output and embed it in the following secrets.yml file at the db_root_password field. The\\xa0metadata.name\\xa0field is important as we have to specify this in a later stage, so be sure to remember it\\n\\nYou can now add the secrets to your cluster via your terminal:\\xa0kubectl apply -f secrets.yml\\xa0. And see if it worked by checking the secrets via\\xa0kubectl get secrets.\\n\\xa0\\nPersistent volume\\n\\xa0\\nA persistent volume is a storage resource with a lifecycle independent of a\\xa0Pod. This means that the storage will persist if a\\xa0pod\\xa0goes down. As Kubernetes has the permission to restart\\xa0pods\\xa0at any time, it is a good practice to set your database storage to a\\xa0persistent volume. A\\xa0persistent volume\\xa0can be a directory on your local filesystem, but also a storage service of a cloud provider (for example AWS Elastic Block Storage or Azure Disk). The type of the\\xa0persistent volume\\xa0can be specified when creating the\\xa0persistent\\xa0volume.\\xa0For this tutorial you will use a\\xa0hostPath\\xa0type, which will create a volume on your\\xa0minikube\\xa0node. However, make sure to use another type (see the documentation) in a production environment as your data will be lost if you delete your minikube node when using a\\xa0hostPath\\xa0type.\\nMaking your application use a\\xa0persistent volume\\xa0exists of two parts:\\n\\nSpecifying the actual storage type, location, size and properties of the\\xa0volume.\\nSpecify a\\xa0persistent volume claim\\xa0that requests a specific size and access modes of the persistent volume for your deployments.\\n\\nCreate a\\xa0persistent-volume.yml\\xa0file and specify the size (in this example 2GB), access modes and the path the files will be stored. The\\xa0spec.persistentVolumeReclaimPolicy\\xa0specifies what should be done if the\\xa0persistent volume claim\\xa0is deleted. In the case of a stateful application like the MySQL database, you want to retain the data if the claim is deleted, so you can manually retrieve or backup the data. The default reclaim policy is inherited from the type of\\xa0persistent volume,\\xa0so it is good practice to always specify it in the yml file.\\n\\nAgain you can add the storage via\\xa0kubectl apply -f persistent-volume.yml\\xa0. And see if the details of your created resources via\\xa0kubectl describe pv mysql-pv-volume\\xa0and\\xa0kubectl describe pvc mysql-pv-claim. As you made a hostPath type\\xa0persistent volume, you can find the data by logging into the minikube node\\xa0minikube ssh\\xa0and navigate to the spcified path (/mnt/data).\\n\\xa0\\nDeploy the MySQL server\\n\\xa0\\nWith our secrets and persistent volume (claim) in place, we can start building our application. First we will deploy a\\xa0MySQL\\xa0server. Pull the latest\\xa0mysql\\xa0imagedocker pull mysql\\xa0and create the\\xa0mysql-deployment.yml\\xa0file. There are several things worth mentioning about this file. We specify that we only spin-up one pod (spec.replicas: 1). The deployment will manage all pods with a label\\xa0db\\xa0specified by\\xa0spec.selector.matchLabels.app: db\\xa0. The\\xa0templatefield and all it’s subfields specify the characteristics of the\\xa0pod.\\xa0It will run the image\\xa0mysql,\\xa0will be named\\xa0mysql\\xa0as well and looks for the db_root_password field in the\\xa0flaskapi-secrets\\xa0secret\\xa0and will set the value to the MYSQL_ROOT_PASSWORD environment variable. Furthermore we specify a port that the container exposes and which path should be mounted to the persistent volume spec.selector.template.spec.containers.volumeMounts.mountPath: /var/lib/mysql. At the bottom we also specify a service also called mysql of the\\xa0LoadBalancertype so we can access our database via this service.\\n\\nYou can now deploy the MySQL server with\\xa0kubectl apply -f mysql-deployment.yml. And see if a pod is running via\\xa0kubectl get pods.\\n\\xa0\\nCreate database and table\\n\\xa0\\nThe last thing we have to do before implementing the API is initializing a database and schema on our MySQL server. We can do this using multiple methods, but for the sake of simplicity let’s access the MySQL server via the newly created\\xa0service.\\xa0As the pod running the MySQL\\xa0service\\xa0is only accessible from inside the cluster, you will start up a temporary pod that serves as\\xa0mysql-client:\\n\\nSet up the\\xa0mysql-client\\xa0via the terminal:\\xa0kubectl run -it --rm --image=mysql --restart=Never mysql-client -- mysql --host mysql --password=<your_password>. Fill in the (decoded) password that you specified in the\\xa0secrets.yml\\xa0file.\\nCreate the database, table and schema. You can do whatever you like, but to make sure the sample Flask app will work do as follows:\\n\\n\\nCREATE DATABASE flaskapi;\\r\\nUSE flaskapi;\\r\\n\\r\\nCREATE TABLE users(user_id INT PRIMARY KEY AUTO_INCREMENT, user_name VARCHAR(255), user_email VARCHAR(255), user_password VARCHAR(255));\\n\\n\\n\\xa0\\nDeploying the API\\n\\xa0\\nFinally it is time to deploy your REST API. The following gist demonstrates an example of a Flask app that implements the API with only two endpoints. One for checking if the API functions and one for creating users in our database. In the GitHub\\xa0repo\\xa0you can find the python file that has endpoints for reading, updating and deleting entries in the database as well. The password for connecting to the database API is retrieved from the environment variables that were set by creating\\xa0secrets.\\xa0The rest of the environment variables (e.g\\xa0MYSQL_DATABASE_HOST) is retrieved from the MySQL\\xa0service\\xa0that was implemented before (further on I will explain how to make sure the Flask app has access to this information).\\n\\nTo deploy this app in your Kubernetes cluster you have to make an image of this Flask app by creating a simple Dockerfile. Nothing special, preparing your container, installing requirements, copying the folder content and running the Flask app. Go to the\\xa0GitHub repo\\xa0to find the Dockerfile and the requirements.txt file that is required for building the image. Before you can deploy the Flask app in the Kubernetes cluster, you first have to build the image and name it\\xa0flask-api\\xa0via\\xa0docker build . -t flask-api.\\n\\nNow it is time to define the\\xa0deployment\\xa0and\\xa0service\\xa0for the Flask app that implements a RESTful API. The deployment will start up 3 pods (specified in the flaskapp-deployment.yml at the spec.replicas: 3 field) Within each of these\\xa0pods\\xa0a container is created from the\\xa0flask-api\\xa0image you just build. To make sure Kubernetes uses the locally built image (instead of downloading an image from an external repo like Dockerhub) make sure to set the imagePullPolicy to never. To make sure the Flask app can communicate with the database a few environment variables should be set. The\\xa0db_root_password\\xa0is retrieved from your created secrets. Each container that starts up inherits environmental variables with information of all running\\xa0services, including IP and port addresses. So you don’t have to worry about having to specify the host and port of the MySQL database to the Flask app. Finally, you will define a\\xa0service\\xa0of the\\xa0LoadBalancer\\xa0type to divide the incoming traffic between the three pods.\\n\\n\\xa0\\nMaking requests to the API\\n\\xa0\\nYou are now ready to use our API and interact with your database. The last step is to expose the API\\xa0service\\xa0to the outside world via your terminal:\\xa0minikube service flask-service. You will now see something like\\n\\nGo to the provided URL and you will see the\\xa0Hello World\\xa0message, to make sure your API is running correctly. You can now interact with the API using your favorite request service like\\xa0Postman\\xa0or\\xa0curl\\xa0in your terminal. To create a user provide a\\xa0json\\xa0file with a name, email and pwd field. for example:curl -H \"Content-Type: application/json\" -d \\'{\"name\": \"<user_name>\", \"email\": \"<user_email>\", \"pwd\": \"<user_password>\"}\\' <flask-service_URL>/create. If you implemented the other methods of the API (as defined in the\\xa0GitHub repo) as well, you may now be able to query all users in the database via:\\xa0curl <flask-service_URL>/users.\\n\\xa0\\nConclusion\\n\\xa0\\ncurl\\xa0in your terminal. To create a user provide a\\xa0json\\xa0file with a name, email and pwd field. for example:curl -H \"Content-Type: application/json\" -d \\'{\"name\": \"<user_name>\", \"email\": \"<user_email>\", \"pwd\": \"<user_password>\"}\\' <flask-service_URL>/create. If you implemented the other methods of the API (as defined in the\\xa0GitHub repo) as well, you may now be able to query all users in the database via:\\xa0curl <flask-service_URL>/users.\\n\\xa0\\nIn this hands-on tutorial you set up\\xa0deployments,\\xa0services\\xa0and\\xa0pods,\\xa0implemented a RESTful API by deploying a Flask app and connected it with other micro-services (a MySQL database in this case). You can keep running this locally, or implement it on a remote server for example in the cloud and get it to production. Feel free to clone the\\xa0repo\\xa0and adjust the API as you like, or add additional micro-services.\\nFeel free to reach out to me if you have any additional questions, remarks or suggestions!\\n\\xa0\\nBio: Rik Kraan\\xa0is a medical doctor with a PhD in radiology, working as a data scientist at\\xa0Vantage AI, a data science consultancy company in the Netherlands. Get in touch via\\xa0rik.kraan@vantage-ai.com\\nOriginal. Reposted with permission.\\nRelated:\\n\\nKubernetes vs. Amazon ECS for Data Scientists\\nCreate and Deploy your First Flask App using Python and Heroku\\nData Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes',\n",
       " \"Sponsored Post.\\n\\n\\xa0\\n1. You must be interested in the business side.\\nThere’s a reason “business analytics” is called just that—it’s centered on how data insights can be used to solve business problems. Degrees like the online Master of Science in Business Analytics from Carnegie Mellon University build business knowledge and communication skills at the same time as they teach advanced analytical skills.\\n“You’ll learn multivariate regression models in one class, and in another learn to use those skills to model customer preferences, create sales-forecasting models, and use the information as the underpinning for strategy,” explained David Lamont, associate teaching professor in the Tepper School of Business.\\n2. Problem-solving is as important as programming skills.\\xa0\\nMSBA applicants usually need to know basic mathematics, statistics, and have some programming capabilities. But those who excel in analytics have a relentless curiosity and are unafraid of tackling difficult problems.\\nIf you are someone who wants to learn how to apply your analytical skills to make a real impact within an organization, then you are well-suited for a business analytics degree.\\n3. It really is possible to complete a master’s while working full time.\\xa0\\nOnline programs like Tepper’s are built with professionals in mind, offering asynchronous coursework and live sessions that meet in evenings.\\n“Life doesn't stop just because you're in school,” said Andrew Kwiatkowski, an MSBA student at Tepper. “CMU is famously rigorous but the professors and staff have provided terrific support and guidance.”\\n4. Make sure the degree will give you a good return on your investment.\\nGoing back to school is an investment of your money and your time. That’s why it’s important to choose a program that will\\n\\ngive you long-term skills for your entire career arc,\\nprovide a professional network that you’ll actually want to access, and\\nadd cachet to your resume.\\n\\n“The Tepper MSBA degree carries a network and it carries an exceptional brand. But it’s not just these benefits: we don’t let people out the door unless they have real skills. That’s part of the DNA of CMU as well as Tepper,” said Lamont.\\nThe online Master of Science in Business Analytics from Carnegie Mellon University’s Tepper School of Business features a comprehensive curriculum that encompasses data visualization, machine learning, large-scale data management and more.\\nBegin your application or talk to an admissions counselor at 888-876-8959.\",\n",
       " \"Sponsored Post.\\n\\nIt’s no secret that deep learning is amazing. For the past eight years, the deep learning revolution has taken hold of computing and made technologies we thought might never be possible a daily reality.\\nBut isn’t actualising deep learning beyond the capabilities of anyone without big company backing, a research team, or a statistics PhD? How can regular machine learning engineers and developers take those cutting edge algorithms from the R&D labs and put them into production?\\nDeep Learning Design Patterns is here to help. It presents deep learning models in a unique-but-familiar new way: as extendable design patterns you can easily plug-and-play into your software projects.\\nThis accessible, jargon-lite book is written by Andrew Ferlitsch, an expert on computer vision and deep learning at Google Cloud AI Developer Relations. Andrew’s whole career is based around making deep learning accessible to everyday developers, and that’s just what his book does.\\nUsing diagrams, code samples, and easy-to-understand language, Andre shares insights from Google’s state-of-the-art neural networks and AI research papers. You'll quickly learn to incorporate the very latest models and techniques into your apps as idiomatic, composable, and reusable design patterns.\\nDeep Learning Design Patterns is published by Manning Publications—so you know its quality is assured. You can save 50% off Deep Learning Design Patterns, as well as all other Manning books and videos!\\nJust enter the code kdmath50 at checkout when you buy from manning.com.\\n\\nFind out more.\",\n",
       " 'comments\\nBy Susan Sivek, Data Science Journalist for Alteryx.\\n\"Normal,\" \"standard,\" \"regular\": These are all fairly similar. Let\\'s just put -ization on the end of each one, too. That won\\'t ever be confusing, right?\\nIf we could go back to the beginnings of statistics and data science, maybe we could advocate for choosing more distinctive words for these concepts. Alas, we\\'re stuck with these terms for now.\\nEach of these three -izations plays a unique role in your data preparation and analysis process. Let\\'s get some clarity on each, so you know when and how to use them.\\nImage via\\xa0GIPHY.\\n\\xa0\\nFeature Scaling: Normalization and Standardization\\n\\xa0\\nOne use of \"normalization\" is\\xa0text\\xa0normalization, the process by which text is prepared for analysis with natural language processing tools. The term is also used in describing database structure and organization.\\nHowever, there\\'s yet another commonly used (but still somewhat variable) meaning of normalization: methods for scaling your data.\\nLet\\'s talk first about what \"scaling your data\" means with the fictional library dataset below. Say you have a variable (aka feature) that has a wide range of values (and hence\\xa0variance), like the \"Library Checkouts\" field below — especially as compared to the variance of \"Average Rating\":\\n\\n\\n\\nTitle\\nAverage Rating (1 to 5)\\nLibrary Checkouts\\n\\n\\nUncanny Valley\\n3.0\\n45\\n\\n\\nQuantum\\n3.4\\n1,301\\n\\n\\nThe Lady Tasting Tea\\n3.8\\n2,122\\n\\n\\nThe Midnight Library\\n4.1\\n12,310\\n\\n\\n\\n\\xa0\\nThis variation in variance (oof) can cause issues for machine learning. To address it, feature scaling in some form, such as the methods described below, is generally recommended.\\xa0Neural networks\\xa0and support vector machines are sensitive to scaling, along with algorithms that use the distances between points in their calculations, like clustering and\\xa0PCA.\\nImage via\\xa0GIPHY.\\nA feature with wide-ranging values can have a disproportionate influence on these models\\' predictions when compared to other features. Therefore, it\\'s typically better to constrain all the features\\' values to a narrower range, so they are all integrated equally into the model. \"Scaling\" encompasses a variety of procedures that make the variables more comparable.\\nMin-Max Normalization\\nLet\\'s dive into one form of normalization, which is one variety of feature scaling. \"Min-max normalization\" or \"min-max scaling\" recalculates all the values of your variables so that they fall within the range [0, 1] or [-1, 1]. (Check out\\xa0an equation\\xa0for this process.) The [0, 1] range is typically required for neural networks.\\nOur dataset above, if scaled so that values fall within [0, 1], would look like this:\\n\\n\\n\\nTitle\\nAverage Rating (1 to 5)\\nLibrary Checkouts\\n\\n\\nUncanny Valley\\n0\\n0\\n\\n\\nQuantum\\n0.364\\n0.102\\n\\n\\nThe Lady Tasting Tea\\n0.727\\n0.169\\n\\n\\nThe Midnight Library\\n1.000\\n1.000\\n\\n\\n\\n\\xa0\\nAs you can see, the minimum values and maximum values for each variable end up at the top and bottom of the [0, 1] range; the other values lie in between. Most importantly, all the values across the features are more comparable and may contribute to a better-performing model. However, as you can imagine, this method is not as effective with outliers, which can pull the minimum and/or maximum values strongly in one direction.\\nIf you want to use this approach in Python and are using scikit-learn (one of the libraries included in Designer\\'s Python Tool), you can use\\xa0MinMaxScaler, for which the [0, 1] range is the default.\\xa0MaxAbsScaler\\xa0is another option and may be better for sparse datasets, as it preserves the data\\'s inherent structure. The scikit-learn User Guide has\\xa0an excellent section\\xa0on these techniques. In Alteryx Designer, you can try out the user-created\\xa0FeatureScaler\\xa0macro. This macro can also convert your data (for example, a model\\'s predictions on your normalized data) from their normalized form back to their original units.\\nStandardization\\nJust to be extra confusing, standardization is sometimes used to cover all these forms of scaling. However, one popular use of the term is a scaling method that can be more specifically called z-score standardization. This approach takes your features\\' values and scales them so that they end up being\\xa0normally distributed\\xa0(fitting that familiar old bell curve). The values are transformed, so their mean is 0, and their standard deviation is 1. This method is also sensitive to outliers\\' influence.\\nStandardization is especially important for machine learning algorithms that use distance measures (e.g., k-nearest neighbors, k-means clustering, principal component analysis) and for those that are built on the assumption that your data are normally distributed. These will likely perform better if you provide data that fit that assumption.\\nAs above, one option is to use Python and scikit-learn, where StandardScaler will tackle this job. If you want to standardize your data in Designer, you can locate and use\\xa0this macro\\xa0that\\'s installed to support the predictive analytics tools.\\nWhich Method and When?\\nAs in the\\xa0recent\\xa0posts\\xa0on model evaluation metrics, there\\'s no one right answer for all situations. You can try multiple methods of normalization and see which one helps your model perform better.\\nIf your data has outliers that could be problematic for the approaches described above, you may want to try\\xa0RobustScaler\\xa0in scikit-learn, which uses the median and interquartile range to scale the data and retains the outliers. Here\\'s\\xa0a helpful tutorial\\xa0for RobustScaler, and you can also check out\\xa0this great visual comparison\\xa0of what data with outliers look like when handled with each of these approaches.\\nFinally, remember that you usually will want to apply these methods to your training dataset only, not to your entire dataset. Scaling your entire dataset and\\xa0then\\xa0splitting it for training/testing allows some information about the distribution of the entire dataset to be available during training.\\xa0If you split after scaling, your test dataset\\'s scaled values would be determined by \"knowledge\" of the entire dataset.\\xa0However, that information will not be available when the model is actually used in production. This problem is one form of what\\'s called\\xa0data leakage.\\xa0Instead, split your dataset, train your model, preprocess your test data according to the same parameters used for the training data, and then\\xa0assess your model\\'s performance.\\n\\xa0\\nRegularization: Addressing a Different Issue\\n\\xa0\\nThis term seems like it should be sorted into the same category with normalization and standardization. Just looking at the word itself — it sounds like a similar concept, right?\\nRegularization is actually a strategy used to build better-performing models by reducing the odds of overfitting, or when your model does such a good job of matching your training data that it performs badly on new data. In other words, regularization is a way to help your model generalize better by preventing it from becoming too complex.\\nHowever, regularization is not part of data preprocessing, unlike normalization and standardization. Instead, it is an optional component in the model-building process. Regularization is often discussed in the context of regression models. In Designer, you can optionally\\xa0use ridge regression, LASSO, or elastic net regularization\\xa0when building linear and logistic regression models. However, regularization is definitely also relevant for other algorithms, including\\xa0neural networks\\xa0and support vector machines.\\nIn the simplest terms, depending on the method used, regularization for regression models may reduce the number of variables included in a model and/or may try to bring their coefficients closer to zero or a combination of both. For neural networks, regularization could also include\\xa0weight decay;\\xa0dropout, where some layers\\' output is ignored; and\\xa0early stopping\\xa0when a model\\'s training ends early because it is generalizing less well as training proceeds (among\\xa0other approaches).\\nAs you can tell, regularization is in a whole different zone of the machine learning process from normalization and standardization, so don\\'t let its deceptively similar sound trip you up!\\nAdditional Resources\\n\\nAbout Feature Scaling and Normalization\\xa0and the effect of standardization for machine learning algorithms\\nScikit-learn documentation for\\xa0scaling data during preprocessing\\nStandardization in Cluster Analysis\\nWhat is Regularization?\\nSimple is Best: Occam\\'s Razor in Data Science\\nHow to Avoid Overfitting in Deep Learning Neural Networks\\n\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Susan Currie Sivek, Ph.D. is the data science journalist for the Alteryx Community where she explores data science concepts with a global audience. She is also the host of the\\xa0Data Science Mixer\\xa0podcast. Her background in academia and social science informs her approach to investigating data and communicating complex ideas — with a dash of creativity from her training in journalism.\\nRelated:\\n\\nEasy Guide To Data Preprocessing In Python\\nData Transformation: Standardization vs Normalization\\n4 Tips for Advanced Feature Engineering and Preprocessing',\n",
       " \"comments\\nBy Orhan G. Yalçın, AI Researcher\\nIf you are reading this article, I am sure that we share similar interests and are/will be in similar industries. So let’s connect via\\xa0Linkedin! Please do not hesitate to send a contact request!\\xa0Orhan G. Yalçın — Linkedin\\n\\nPhoto by\\xa0Esther Jiao\\xa0on\\xa0Unsplash\\n\\xa0\\n\\n\\xa0\\nIn this post, we will dive into the details of TensorFlow Tensors. We will cover all the topics related to Tensors in Tensorflow in these five simple steps:\\n\\nStep I: Definition of Tensors →\\xa0What is a Tensor?\\nStep II: Creation of Tensors →\\xa0Functions to Create Tensor Objects\\nStep III: Qualifications of Tensors →\\xa0Characteristics and Features of Tensor Objects\\nStep IV: Operations with Tensors →\\xa0Indexing, Basic Tensor Operations, Shape Manipulation, and Broadcasting\\nStep V: Special Types of Tensors →\\xa0Special Tensor Types Other than Regular Tensors\\n\\nLet’s start!\\n\\xa0\\nDefinition of Tensors: What is a Tensor?\\n\\xa0\\n\\nFigure 1. A Visualization of Rank-3 Tensors (Figure by Author)\\n\\xa0\\n\\n\\xa0\\nTensors are TensorFlow’s multi-dimensional arrays with uniform type. They are very similar to NumPy arrays, and they are immutable, which means that they cannot be altered once created. You can only create a new copy with the edits.\\nLet’s see how Tensors work with code example. But first, to work with TensorFlow objects, we need to import the\\xa0TensorFlow\\xa0library. We often use\\xa0NumPy\\xa0with TensorFlow, so let’s also import NumPy with the following lines:\\n\\n\\xa0\\nCreation of Tensors: Creating Tensor Objects\\nThere are several ways to create a\\xa0tf.Tensor\\xa0object. Let’s start with a few examples. You can create Tensor objects with several TensorFlow functions, as shown in the below examples:\\n\\ntf.constant, tf.ones, tf.zeros, and tf.range are some of the functions you can use to create Tensor objects\\n\\nOutput:\\r\\ntf.Tensor([[1 2 3 4 5]], shape=(1, 5), dtype=int32)\\r\\ntf.Tensor([[1. 1. 1. 1. 1.]], shape=(1, 5), dtype=float32) \\r\\ntf.Tensor([[0. 0. 0. 0. 0.]], shape=(1, 5), dtype=float32) \\r\\ntf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\\n\\n\\xa0\\nAs you can see, we created Tensor objects with the shape\\xa0(1, 5)\\xa0with three different functions and a fourth Tensor object with the shape\\xa0(5, )using\\xa0tf.range()\\xa0function. Note that tf.ones and tf.zeros accepts the shape as the required argument since their element values are pre-determined.\\n\\xa0\\nQualifications of Tensors: Characteristics and Features of Tensor Objects\\nTensorFlow Tensors are created as\\xa0tf.Tensor\\xa0objects, and they have several characteristic features. First of all, they have a rank based on the number of dimensions they have. Secondly, they have a shape, a list that consists of the lengths of all their dimensions. All tensors have a size, which is the total number of elements within a Tensor. Finally, their elements are all recorded in a uniform Dtype (data type). Let’s take a closer look at each of these features.\\n\\xa0\\nRank System and Dimension\\nTensors are categorized based on the number of dimensions they have:\\n\\nRank-0 (Scalar) Tensor:\\xa0A tensor containing a single value and no axes (0-dimension);\\nRank-1 Tensor:\\xa0A tensor containing a list of values in a single axis (1-dimension);\\nRank-2 Tensor:\\xa0A tensor containing 2-axes (2-dimensions); and\\nRank-N Tensor:\\xa0A tensor containing N-axis (N-dimensions).\\n\\n\\nFigure 2. Rank-1 Tensor | Rank-2 Tensor| Rank-3 Tensor (Figure by Author)\\n\\xa0\\n\\n\\xa0\\nFor example, we can create a Rank-3 tensor by passing a three-level nested list object to the\\xa0tf.constant\\xa0function. For this example, we can split the numbers into a 3-level nested list with three-element at each level:\\n\\nThe code to create a Rank-3 Tensor object\\n\\nOutput:\\r\\ntf.Tensor( [[[ 0  1  2]   \\r\\n             [ 3  4  5]]   \\r\\n             \\r\\n            [[ 6  7  8]   \\r\\n             [ 9 10 11]]],\\r\\n  shape=(2, 2, 3), dtype=int32)\\n\\n\\xa0\\nWe can view the number of dimensions that our `rank_3_tensor` object currently has with the `.ndim` attribute.\\n\\n\\nOutput:\\r\\nThe number of dimensions in our Tensor object is 3\\n\\n\\xa0\\n\\xa0\\nShape\\nThe shape feature is another attribute that every Tensor has. It shows the size of each dimension in the form of a list. We can view the shape of the\\xa0rank_3_tensor\\xa0object we created with the\\xa0.shape\\xa0attribute, as shown below:\\n\\n\\nOutput:\\r\\nThe shape of our Tensor object is (2, 2, 3)\\n\\n\\xa0\\nAs you can see, our tensor has 2 elements at the first level, 2 elements in the second level, and 3 elements in the third level.\\n\\xa0\\nSize\\nSize is another feature that Tensors have, and it means the total number of elements a Tensor has. We cannot measure the size with an attribute of the Tensor object. Instead, we need to use\\xa0tf.size()\\xa0function. Finally, we will convert the output to NumPy with the instance function\\xa0.numpy()\\xa0to get a more readable result:\\n\\n\\nOutput:\\r\\nThe size of our Tensor object is 12\\n\\n\\xa0\\n\\xa0\\nDtypes\\nTensors often contain numerical data types such as floats and ints, but may contain many other data types such as complex numbers and strings.\\nEach Tensor object, however, must store all its elements in a single uniform data type. Therefore, we can also view the type of data selected for a particular Tensor object with the\\xa0.dtype\\xa0attribute, as shown below:\\n\\n\\nOutput:\\r\\nThe data type selected for this Tensor object is <dtype: 'int32'>\\n\\n\\xa0\\n\\xa0\\nOperations with Tensors\\n\\xa0\\nIndexing\\nAn index is a numerical representation of an item’s position in a sequence. This sequence can refer to many things: a list, a string of characters, or any arbitrary sequence of values.\\nTensorFlow also follows standard Python indexing rules, which is similar to list indexing or NumPy array indexing.\\nA few rules about indexing:\\n\\nIndices start at zero (0).\\nNegative index (“-n”) value means backward counting from the end.\\nColons (“:”) are used for slicing:\\xa0start:stop:step.\\nCommas (“,”) are used to reach deeper levels.\\n\\nLet’s create a\\xa0rank_1_tensor\\xa0with the following lines:\\n\\n\\nOutput: \\r\\ntf.Tensor([ 0  1  2  3  4  5  6  7  8  9 10 11], \\r\\n  shape=(12,), dtype=int32)\\n\\n\\xa0\\nand test out our rules no.1, no.2, and no.3:\\n\\n\\nOutput: \\r\\nFirst element is: 0 \\r\\nLast element is: 11 \\r\\nElements in between the 1st and the last are: [ 1  2  3  4  5  6  7  8  9 10]\\n\\n\\xa0\\nNow, let’s create our\\xa0rank_2_tensor\\xa0object with the following code:\\n\\n\\nOutput:\\r\\ntf.Tensor( [[ 0  1  2  3  4  5]  \\r\\n            [ 6  7  8  9 10 11]], shape=(2, 6), dtype=int32)\\n\\n\\xa0\\nand test the 4th rule with several examples:\\n\\n\\nOutput: \\r\\nThe first element of the first level is: [0 1 2 3 4 5] \\r\\nThe second element of the first level is: [ 6  7  8  9 10 11] \\r\\nThe first element of the second level is: 0 \\r\\nThe third element of the second level is: 2\\n\\n\\xa0\\nNow, we covered the basics of indexing, so let’s take a look at the basic operations we can conduct on Tensors.\\n\\xa0\\nBasic Operations with Tensors\\nYou can easily do basic math operations on tensors such as:\\n\\nAddition\\nElement-wise Multiplication\\nMatrix Multiplication\\nFinding the Maximum or Minimum\\nFinding the Index of the Max Element\\nComputing Softmax Value\\n\\nLet’s see these operations in action. We will create two Tensor objects and apply these operations.\\n\\nWe can start with addition.\\n\\n\\nOutput:\\r\\ntf.Tensor( [[ 3.  7.]  \\r\\n            [11. 15.]], shape=(2, 2), dtype=float32)\\n\\n\\xa0\\nLet’s continue with the element-wise multiplication.\\n\\n\\nOutput:\\r\\ntf.Tensor( [[ 2. 12.]  \\r\\n            [30. 56.]], shape=(2, 2), dtype=float32)\\n\\n\\xa0\\nWe can also do matrix multiplication:\\n\\n\\nOutput:\\r\\ntf.Tensor( [[22. 34.]  \\r\\n            [46. 74.]], shape=(2, 2), dtype=float32)\\n\\n\\xa0\\nNOTE:\\xa0Matmul operations lays in the heart of deep learning algorithms. Therefore, although you will not use matmul directly, it is crucial to be aware of these operations.\\nExamples of other operations we listed above:\\n\\n\\nOutput:\\r\\nThe Max value of the tensor object b is: 7.0 \\r\\nThe index position of the Max of the tensor object b is: [1 1] \\r\\nThe softmax computation result of the tensor object b is: [[0.11920291 0.880797  ]  [0.11920291 0.880797  ]]\\n\\n\\xa0\\n\\xa0\\nManipulating Shapes\\nJust as in NumPy arrays and pandas DataFrames, you can reshape Tensor objects as well.\\nThe tf.reshape operations are very fast since the underlying data does not need to be duplicated. For the reshape operation, we can use thetf.reshape()\\xa0function. Let's use the\\xa0tf.reshape\\xa0function in code:\\n\\n\\nOutput:\\r\\nThe shape of our initial Tensor object is: (1, 6) \\r\\nThe shape of our initial Tensor object is: (6, 1) \\r\\nThe shape of our initial Tensor object is: (3, 2) \\r\\nThe shape of our flattened Tensor object is: tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32)\\n\\n\\xa0\\nAs you can see, we can easily reshape our Tensor objects. But beware that when doing reshape operations, a developer must be reasonable. Otherwise, the Tensor might get mixed up or can even raise an error. So, look out for that 😀.\\n\\xa0\\nBroadcasting\\nWhen we try to do combined operations using multiple Tensor objects, the smaller Tensors can stretch out automatically to fit larger tensors, just as NumPy arrays can. For example, when you attempt to multiply a scalar Tensor with a Rank-2 Tensor, the scalar is stretched to multiply every Rank-2 Tensor element. See the example below:\\n\\n\\nOutput:\\r\\ntf.Tensor( [[ 5 10]  \\r\\n            [15 20]], shape=(2, 2), dtype=int32)\\n\\n\\xa0\\nThanks to broadcasting, you don’t have to worry about matching sizes when doing math operations on Tensors.\\n\\xa0\\nSpecial Types of Tensors\\nWe tend to generate Tensors in a rectangular shape and store numerical values as elements. However, TensorFlow also supports irregular, or specialized, Tensor types, which are:\\n\\nRagged Tensors\\nString Tensors\\nSparse Tensors\\n\\n\\nFigure 3. Ragged Tensor | String Tensor| Sparse Tensor (Figure by Author)\\n\\xa0\\n\\n\\xa0\\nLet's take a closer look at what each of them is.\\n\\xa0\\nRagged Tensors\\nRagged tensors are tensors with different numbers of elements along the size axis, as shown in Figure X.\\nYou can build a Ragged Tensor, as shown below:\\n\\n\\nOutput:\\r\\n<tf.RaggedTensor [[1, 2, 3], \\r\\n                  [4, 5], \\r\\n                  [6]]>\\n\\n\\xa0\\n\\xa0\\nString Tensors\\nString Tensors are tensors, which stores string objects. We can build a String Tensor just as you create a regular Tensor object. But, we pass string objects as elements instead of numerical objects, as shown below:\\n\\n\\nOutput:\\r\\ntf.Tensor([b'With this' \\r\\n           b'code, I am' \\r\\n           b'creating a String Tensor'],\\r\\n  shape=(3,), dtype=string)\\n\\n\\xa0\\n\\xa0\\nSparse tensors\\nFinally, Sparse Tensors are rectangular Tensors for sparse data. When you have holes (i.e., Null values) in your data, Sparse Tensors are to-go objects. Creating a sparse Tensor is a bit time consuming and should be more mainstream. But, here is an example:\\n\\n\\nOutput:\\r\\ntf.Tensor( [[ 25   0   0   0   0]\\r\\n            [  0   0   0   0   0]\\r\\n            [  0   0  50   0   0]\\r\\n            [  0   0   0   0   0]\\r\\n            [  0   0   0   0 100]], shape=(5, 5), dtype=int32)\\n\\n\\xa0\\n\\xa0\\nCongratulations\\nWe have successfully covered the basics of TensorFlow’s Tensor objects.\\nGive yourself a pat on the back!\\nThis should give you a lot of confidence since you are now much more informed about the building blocks of the TensorFlow framework.\\nCheck\\xa0Part 1 of this tutorial series:\\nBeginner's Guide to TensorFlow 2.x for Deep Learning Applications\\nUnderstanding the TensorFlow Platform and What it has to Offer to a Machine Learning Expert\\nContinue with\\xa0Part 3 of the series:\\nMastering TensorFlow “Variables” in 5 Easy Step\\nLearn how to use TensorFlow Variables, their differences from plain Tensor objects, and when they are preferred over…\\n\\xa0\\nSubscribe to the Mailing List for the Full Code\\nIf you would like to have access to full code on Google Colab and the rest of my latest content, consider subscribing to the mailing list:\\n\\nSlide to Subscribe to My Newsletter\\nFinally, if you are interested in applied deep learning tutorials, check out some of my articles:\\nImage Classification in 10 Minutes with MNIST Dataset\\nUsing Convolutional Neural Networks to Classify Handwritten Digits with TensorFlow and Keras | Supervised Deep Learning\\nImage Generation in 10 Minutes with Generative Adversarial Networks\\nUsing Unsupervised Deep Learning to Generate Handwritten Digits with Deep Convolutional GANs using TensorFlow and the…\\nImage Noise Reduction in 10 Minutes with Convolutional Autoencoders\\nUsing Deep Convolutional Autoencoders to Clean (or Denoise) Noisy Images with the help of Fashion MNIST | Unsupervised…\\nUsing Recurrent Neural Networks to Predict Bitcoin (BTC) Prices\\nWouldn’t it be awesome if you were, somehow, able to predict tomorrow’s Bitcoin (BTC) price? Cryptocurrency market has…\\nBio: Orhan G. Yalçın is an AI Researcher in the legal domain. He is a qualified lawyer with business development and data science skills, and has previously worked as a legal trainee for Allen & Overy on capital markets, competition, and corporate law matters.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nWTF is a Tensor?!?\\nGetting Started with TensorFlow 2\\nThe Most Important Fundamentals of PyTorch you Should Know\",\n",
       " \"comments\\nBy Ajay Arunachalam, Orebro University\\nHello, friends. In this blog post, I will take you through my package “msda” useful for time-series sensor data analysis. A quick introduction about time-series data is also provided. The demo notebook can be found on\\xa0here\\nOne of the specific use case applications focused on\\xa0“Unsupervised Feature Selection”\\xa0using the package can be found in the blog post\\xa0here.\\n\\xa0\\nWhat is Time Series Data?\\n\\xa0\\nTime series data is information taken at\\xa0a particular duration. For instance, having a set of sensor data observed at particular equal paces, each sensor can be classified as time series. If the data is collected without any order in time, or at once, it is not time series data.\\nThere are two types of time series data:\\n1- Stock Series\\xa0(Measure of attribute, in particular point of time)\\n2- Flow Series\\xa0(Measure of activity, in a time interval)\\n\\xa0\\nComponents of Time Series Data\\n\\xa0\\nTo analyze time series data, we need to know the different pattern types. These patterns will together create the set of observations on time series.\\n1) Trend: A long pattern present in the time series. It represents the variations of low, medium and high frequency filtered out from the time series.\\nIf there is no increasing or decreasing pattern in the time series data, it is taken as\\xa0stationary\\xa0in the mean.\\nThere are two types of trend pattern:\\n\\nDeterministic:\\xa0In this case, the effects of shocks present in the time series are eliminated.\\nStochastic:\\xa0It is the process in which the effects of shocks are never eliminated as they have permanently changed the level of the time series.\\n\\n2) Cyclic:\\xa0The pattern exhibit up and down movements around a specified trend. The period of time is not fixed and usually composed of at least 2 months in duration.\\n3) Seasonal:\\xa0Pattern that reflects regular fluctuations. These short-term movements occur due to the seasonal and custom factors of people. The data faces regular and predictable changes which occurs on regular intervals of calendar. It always consist of fixed and known period.\\nThe main sources of seasonality:\\n\\nClimate\\nInstitutions\\nSocial habits and practices\\nCalendar etc.\\n\\nModels\\xa0to create a seasonal component in time series:\\n\\nAdditive Model\\xa0— It is the model in which the seasonal component is added with the trend component.\\nMultiplicative\\xa0Model\\xa0— In this model seasonal component is multiplied with the intercept if trend component is not present in the time series.\\n\\n4) Irregular:\\xa0It is an unpredictable component of time series.\\n\\xa0\\nTime Series Data vs Cross-Section Data\\n\\xa0\\nTime Series Data is composed of collection of data of one specific variable at particular interval of time. On the other hand, Cross-Section Data is consist of collection of data on multiple variables from different sources at a particular interval of time. Collection of company’s stock market data at regular interval of year is an example of time series data. But when the collection of company’s sales revenue, sales volume is collected for the past 3 months then it is taken as an example of cross-section data. Time series data is mainly used for obtaining results over an extended period of time, but cross-section data focuses on the information received from surveys at a particular time.\\n\\xa0\\nWhat is Time Series Analysis?\\n\\xa0\\nAnalysis is performed in order to understand the structure and functions produced by the time series.\\nTwo approaches are used for analyzing time series data are -\\n\\nIn the time domain\\nIn the frequency domain\\n\\nTime series analysis is mainly used for -\\n\\nDecomposing the time series\\nIdentifying and modeling the time-based dependencies\\nForecasting\\nIdentifying and model the system variation\\n\\n\\xa0\\nNeed of Time Series Analysis\\n\\xa0\\nIn order to model successfully, the time series is important in machine learning and deep learning. Time series analysis is used to understand the internal structure and functions that are used for producing the observations. Time Series analysis is used for -\\n\\nDescriptive —\\xa0Patterns are identified in correlated data. In other words, the variations in trends and seasonality in the time series are identified.\\nExplanation —\\xa0Understanding and modeling of data is performed.\\nForecasting —\\xa0The prediction from previous observations are performed for short term trends.\\nInvention Analysis —\\xa0Effect performed by any event in time series data, is analyzed.\\nQuality Control —\\xa0When the specific size deviates, it provides an alert.\\n\\n\\xa0\\nApplications of Time Series Analysis\\n\\xa0\\n\\n\\nFew Time-Series Application Area Examples\\n\\n\\xa0\\nNow, that we have seen through the basics of time-series, let’s dwell into the MSDA package & its details.\\n\\xa0\\nWhat is MDSA?\\n\\xa0\\nMSDA is an open source low-code Multi-Sensor Data Analysis library in Python that aims to reduce the hypothesis to insights cycle time in a time-series multi-sensor data analysis & experiments. It enables users to perform end-to-end proof-of-concept experiments quickly and efficiently. The module identifies events in the multidimensional time series by capturing the variation and trend to establish relationships aimed towards identifying the correlated features helping in feature selection from raw sensor signals.\\nThe package includes:-\\n\\nTime series analysis.\\nThe variation of each sensor column wrt time (increasing, decreasing, equal).\\nHow each column values varies wrt other column, and the maximum variation ratio between each column wrt other column.\\nRelationship establishment with trend array to identify most appropriate sensor.\\nUser can select window length and then check average value and standard deviation across each window for each sensor column.\\nIt provides count of growth/decay value for each sensor column values above or below a threshold value.\\nFeature Engineering\\n\\na) Features involving trend of values across various aggregation windows: change and rate of change in average, std. deviation across window.\\nb) Ratio of changes, growth rate with std. deviation.\\nc) Change over time.\\nd) Rate of change over time.\\ne) Growth or decay.\\nf) Rate of growth or decay.\\ng) Count of values above or below a threshold value.\\n\\xa0\\nOverview:-\\n\\xa0\\nPrototype for feature/sensor selection from multi-dimensional heterogeneous/homogeneous time series multi-sensor data. The intuitive representation of the framework is as shown below.\\n\\n\\nPictorial representation of multi-dimensional time series data feature selection\\n\\n\\xa0\\nFeatures Include:-\\n\\xa0\\n\\n\\nCore Functionalities in MSDA\\n\\n\\xa0\\nMSDA Workflow:-\\n\\xa0\\n\\n\\nMSDA algorithm workflow\\n\\n\\xa0\\nTerminal Installation:-\\n\\xa0\\nThe easiest way to install msda is using pip.\\n\\npip install msda\\n\\n\\nor\\n\\n$ git clone\\xa0https://github.com/ajayarunachalam/msda\\r\\n$ cd msda\\r\\n$ python setup.py install\\n\\n\\n\\xa0\\nInstall in Jupyter Notebook:-\\n\\xa0\\n\\n!pip install msda\\n\\n\\nFollow the rest as demonstrated in the demo example [here] —\\xa0https://github.com/ajayarunachalam/msda/tree/master/demo.ipynb\\n\\xa0\\nWho should use MSDA?\\n\\xa0\\nMSDA is an open source library that anybody can use. In my view, the ideal target audience of MSDA is:\\n\\nStudents.\\nResearchers for quick poc testing.\\nExperienced Data Scientists who want to increase productivity.\\nCitizen Data Scientists who prefer a low code solution.\\nData Science Professionals and Consultants involved in building Proof of Concept projects.\\n\\n\\xa0\\nContact\\n\\xa0\\nYou can reach me at ajay.arunachalam08@gmail.com\\nThank you for reading. Happy Learning :)\\n\\xa0\\nReferences\\n\\xa0\\nTime series\\nA time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is…\\n\\xa0\\nIntroduction to Time Series Analysis\\nHi folks,Thanks a lot for reading my blog posts and motivating me for writing.You can read all my blogs Here.\\n\\xa0\\nBio: Ajay Arunachalam (personal website) is a Postdoctoral Researcher (Artificial Intelligence) at Centre for Applied Autonomous Sensor Systems, Orebro University, Sweden. Prior to this, he was working as a Data Scientist at True Corporation, a Communications Conglomerate, working with Petabytes of data, building & deploying deep models in production. He truly believes that Opacity in AI systems is need of the hour, before we fully accept the power of AI. With this in mind, he has always strived to democratize AI, and be more inclined towards building Interpretable Models. His interest is in Applied Artificial Intelligence, Machine Learning, Deep Learning, Deep RL, and Natural Language Processing, specifically learning good representations. From his experience working on real-world problems, he fully acknowledges that finding good representations is the key in designing the system that can solve interesting challenging real-world problems, that go beyond human-level intelligence, and ultimately explain complicated data for us that we don't understand. In order to achieve this, he envisions learning algorithms that can learn feature representations from both unlabelled and labelled data, be guided with and/or without human interaction, and that are on different levels of abstractions in order to bridge the gap between low-level data and high-level abstract concepts.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nBuilding AI Models for High-Frequency Streaming Data\\nSimple & Intuitive Ensemble Learning in R\\nSimple Python Package for Comparing, Plotting & Evaluating Regression Models\",\n",
       " 'Sponsored Post.\\n\\nThere’s no doubt that IT certifications provide value to individuals and employers in today’s competitive job market. SAS recently surveyed SAS Coursera learners and 84% of certified individuals said a SAS certification helped improve their performance and advance their careers. This is similar to findings from the 2021 Pearson Vue Value of IT Certification Report, which highlights the benefits of increased confidence, greater determination to professionally succeed, increased job satisfaction, and greater work autonomy and independence. \\nCheck out the most popular certifications from SAS to see what certification you want to pursue next. Now through the end of 2021, you can save 55% on your exam!\\n\\xa0\\n1. SAS Certified Specialist: Base Programming\\nThe Base Programming Specialist certification is a performance-based exam that tests your skills and validates your knowledge in accessing and creating data structures, managing data, error handling and generating reports and output. Frequently on Certification Magazine’s Annual IT Salary survey, the average base salary for this certification is $99, 920 – a 15% increase in only two years. \\n\\xa0\\n2. SAS Certified Professional: Advanced Programming\\nThe Advanced Programming Professional certification is another performance-based exam that tests your skills and validates your knowledge in accessing data using SQL, macro processing, advanced SAS programming techniques such as hash objects and dimensional arrays. Performance-based exams mean that you will access SAS to write and execute SAS code during the exam. If you’re not sure which programming exam is right for you, look at the comparison chart to help guide you down the right path. \\n\\xa0\\n3. SAS Certified Professional: Artificial Intelligence & Machine Learning\\nThis certification is designed for those who want to demonstrate their AI and analytics expertise using open source and SAS tools to garner insight from data. This certification validates your knowledge in Machine Learning, Natural Language Processing, Computer Vision and Model Forecasting and Optimization. As the demand for AI and ML skills continues to peak, so will the need to acquire certifications like this. \\n\\xa0\\n4. SAS Certified Statistical Business Analyst\\nIf you use SAS Statistical Procedures or SAS/STAT® software to conduct and interpret complex statistical data analysis, then consider taking this certification. You’ll be tested on ANOVA, Linear and Logistic Regression, Measure Model Performance and preparing Inputs for Predictive Model Performance. \\n\\xa0\\n5. SAS® Viya® Programming Specialist\\nWith the continued acceleration of cloud adoption, comes the continued need to evolve IT skills and acquire certifications. The SAS Viya Programming Specialist certification is new, so although this is not our most popular certification (yet!) it deserves a worthy mention. Especially for those who want to demonstrate a cloud-ready mentality and expertise in working with CAS tables and data sources, using CAS-enabled procedures and user-defined formats, modifying DATA step and SQL programs to run in CAS and using the CAS programming language, CASL. Check out this SAS Users YouTube video to hear more about our current SAS Viya certifications.\\n\\xa0\\nEach certification has aligned training, exam prep resources and practice exams so that you feel confident and prepared heading into the certification exam. The global SAS® certification program offers more than just the certifications discussed here to ensure you can diversify your professional toolkit and exemplify the skills you have.',\n",
       " 'By Devin Partida, Editor-in-Chief of ReHack.com.\\ncomments\\nPeople in various industries are increasingly interested in using data science to learn more about notable trends that improve their decision-making. It’s also instrumental in predicting events and preventing unwanted consequences. Here are some thought-provoking examples.\\n\\nImage by Gerd Altmann from Pixabay\\n\\xa0\\n1. Thwarting Cyberattacks\\n\\xa0\\nMany industries now have an extensive online presence. However, targeted cyberattacks can quickly hinder entire companies, forcing employees to temporarily switch to pen-and-paper recordkeeping or deal with the loss of access to files or portals.\\nHowever, data science can help company IT teams stay on top of and stop threats. For example, associate rule learning (ARL) gauges risk based on certain characteristics. It allows managers to predict possible issues based on whether they share commonalities with past problems.\\nPrevention also becomes possible when data science helps spot outliers. Suppose a system sees that an unknown person with an IP address in a foreign country tried to access a workplace network in the middle of the night. It could automatically deny that individual until a human takes a closer look.\\n\\xa0\\n2. Improving Law Enforcement Policies \\n\\xa0\\nOngoing police violence issues against people of color have led to a call for drastic change in the law enforcement sector. However, most people don’t know the best ways to go about making progress.\\nAn organization called Campaign Zero uses a data-driven approach to suggest new policy frameworks for the future of policing. Researchers there reviewed 600 police union contracts. The results showed that 84% of the documents contained at least one stipulation that made it difficult to hold police officers accountable.\\nData also shows which locations have police departments associated with the deaths of Black men that are higher than the national homicide rate. Another way to apply data analytics is to assess which officers have above-average incidents of misconduct or complaints lodged against them. Police chiefs could then make departmental decisions about interventions that could predict and prevent unnecessary violence.\\n\\xa0\\n3. Keeping Output Levels High\\n\\xa0\\nPeople are particularly interested in how data science could assist industries when success almost entirely depends on achieving consistent output. In such cases, unexpected events or misguided judgments could lead to plunging profits and dissatisfied clients.\\nHowever, data science could support preventive maintenance programs used in facilities such as manufacturing plants. Research suggests that 80% of maintenance technicians would rather prevent issues than react to them. When plant leaders use connected sensors that feed into a data analysis platform, they can predict when parts will fail or prevent problems by alerting people to concerning trends.\\nIn Australia, government officials believe data science could be the key to improving results in the mining industry. Difficulties in finding new, high-quality mineral deposits mean companies often engage in costly exploration efforts that fall short of expectations. Data science could help with that, as well as monitor ventilation systems at established mines. Workers would stay safer while companies avoid excessive costs.\\n\\xa0\\n4. Helping Fitness Facilities Engage Customers \\n\\xa0\\nMany people can relate to the scenario of joining a gym and making good on promises to get fit but becoming less enthusiastic over time. Data science can predict what causes a loss of motivation and prevent it from happening. It can also assist in making personalized recommendations for members.\\nA venture-backed technology company called CIPIO helps gym owners convert data into actionable strategies. Records may indicate that a particular member has only attended yoga sessions, and their attendance has gradually become less consistent. The system could recommend that gym staff inform that person about a class that combines yoga with brief periods of intensive cardio. That suggestion could raise interest by presenting a different opportunity.\\nSuch a tool might also show broader trends, such as which months most members end their subscriptions. If the data indicates it usually happens in March, a club could offer extra incentives to reduce the chances of people leaving. It could also predict the likely interest in a new, specific type of class, such as chair workouts for older or injured people.\\n\\xa0\\n5. Avoiding Out-of-Stock Instances \\n\\xa0\\nConsumers understandably get upset when they intend to buy something and find empty shelves. That’s why many retailers now use big data to predict demand and prevent stock shortages. Data analytics systems can look at societal trends, weather patterns, regional preferences and other specifics to assess people’s likelihood of buying.\\nThat approach can also send the appropriate quantities of merchandise to the right locations, such as chain stores with national outlets. The data may show a rapid increase in people wanting workout clothes in Colorado, while such sales decline or remain flat in Arkansas. Retailers could use that information to keep stores adequately stocked.\\nSome large-scale festivals, such as the United Kingdom’s Glastonbury, have also relied on data science to predict outcomes and prevent disappointment. Researchers examined data to see whether a specific year was likely to have more or less rainfall than usual. That information helped retailers know what kind of merchandise to bring with them. The same approach enabled vendors to determine the most popular food and beverages so they could be better prepared.\\n\\xa0\\nData Science Has Abundant Potential \\n\\xa0\\nMost business leaders know the importance of understanding the overall likelihood of undesirable events and doing what they can to stop them from happening. Data science can make those aims more manageable, within and outside of the examples here.\\nIt’s impossible to predict the future with certainty, but data science allows tracking trends and extracting meaningful insights from them. Regardless of the industry or audience served, it can help business decision-makers operate their industries smoothly while minimizing adverse outcomes.\\n\\xa0\\nBio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com.\\nRelated:\\n\\nThe Potential of Predictive Analytics in Labor Industries\\nData Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions\\nHow Automation Is Improving the Role of Data Scientists',\n",
       " 'comments\\nBy Maarit Widmann, Data Scientist at KNIME\\nIntroduction\\n\\xa0\\nCar parking ticket machines used to only accept coins. A self-service vegetable stand used to only accept cash. And not such a long time ago I could buy a bus ticket from the bus driver! These days, however, you can (and you’re often encouraged to) pay for these and many other products and services by credit card. This leads to more and more transactions and also to types of transactions that didn’t exist before. Some time back, a credit card transaction for a vegetable stand would have looked suspicious!\\nWith the increasing variety and volume of credit card usage, fraud is evolving too [1]. This is a huge challenge! For automatic fraud detection and prevention, a number of supervised and unsupervised fraud detection models have been suggested. The unsupervised methods, such as a neural autoencoder, are anomaly detection models and don’t require labeled data. The supervised methods, such as a decision tree or a logistic regression model, require labeled data, which are often not available. Imagine someone manually recognizing and labeling the transactions as “fraudulent” or “legitimate”! Another problem is that the fraudulent transactions are very few compared to the large amounts of legitimate transactions. This imbalance of the target classes decreases the performance of the decision tree algorithm and of other classification algorithms [2].\\xa0\\nIn this article, we will work with labeled, highly imbalanced transactions data: For each fraudulent transaction we have 579 legitimate transactions. We’ll check if we can improve the performance of a decision tree model by resampling; that is, by artificially creating more data about fraudulent transactions. Along the way, we’ll explain three different resampling methods and evaluate their effects on the fraud prevention application. At the end, we’ll provide a link to a KNIME workflow - an example implementation of the different resampling methods.\\xa0\\n\\xa0\\nBuilding a Classification Model for Fraud Detection\\xa0\\n\\xa0\\nIn our demonstration we use the creditcard.csv data available on Kaggle. The data consist of 284807 credit card transactions, performed by EU cardholders in September 2013. 492 (0.2%) of the credit card transactions are fraudulent, and the remaining 284315 (99.8%) transactions are legitimate. The data contain a target class column with possible values fraudulent / legitimate, the time and amount of each transaction, and 28 principal components generated from the confidential features of the transactions.\\xa0\\nThe workflow in Figure 1 shows the steps for accessing, preprocessing, resampling, and modeling the transactions data. Inside the yellow box, we access the transactions data, encode the target column from 0/1 to legitimate/fraudulent, and partition the data into training and test sets using 80/20 split and stratified sampling on the target column. Inside the orange boxes, we build four different versions of a decision tree model for fraud detection: a baseline model trained on the original training data plus three models trained on\\xa0\\n\\nSMOTE oversampled data,\\xa0\\nBootstrap oversampled data, and\\xa0\\nBootstrap undersampled data.\\n\\n\\n\\nFigure 1. A workflow that accesses and preprocesses transactions data and implements four different decision tree models for fraud detection: one trained on the original data and three models trained on resampled data.\\n\\n\\xa0\\n\\xa0\\nResampling Techniques\\n\\xa0\\nTable 1 summarizes the resampling methods that we include in our demonstration:\\xa0\\n\\noversampling (SMOTE)\\xa0\\noversampling (Bootstrap)\\nundersampling (Bootstrap)\\n\\n\\xa0\\n\\n\\nTable 1. Overview of three resampling methods: SMOTE, oversampling (Bootstrap), and undersampling (Bootstrap)\\n\\n\\xa0\\nEffects of Resampling on Fraud Detection Performance\\n\\xa0\\nResampling has two drawbacks, especially when the target class is as highly imbalanced as in our case. Firstly, oversampling the minority class might lead to overfitting, i.e. the model learns patterns that only exist in the particular sample that has been oversampled. Secondly, undersampling the majority class might lead to underfitting, i.e. the model fails to capture the general pattern in the data [3].\\nWe compare the performances of the baseline model and the models trained on resampled data in terms of two scoring metrics: recall and precision (Figure 2). The metrics are explained in detail in the From Modeling to Scoring: Confusion Matrix and Class Statistics blog post.\\xa0\\n\\nRecall is the proportion of correctly predicted fraudulent transactions. The higher the recall, the more fraudulent transactions are prevented by the model.\\xa0\\nPrecision is the proportion of actual fraudulent transactions among those predicted as fraudulent. The higher the precision, the fewer false alarms are raised by the model.\\n\\n\\n\\nFigure 2. Recall and precision statistics obtained by four decision tree models for fraud detection, each one trained on a different training set\\n\\n\\xa0\\xa0\\nThe very low precision value 2% for the undersampled model in the bottom right corner in Figure 2 indicates underfitting: The undersampled model failed to learn the patterns underlying the\\xa0 legitimate transactions. This seems reasonable considering that we discarded 99.8% of the legitimate transactions in the undersampling phase! Indeed, with so few examples in the fraudulent class, the only effect of undersampling has been to damage the representation of the legitimate class.\\nIf you take a look at the performances obtained via oversampling in the two middle rows, you can see from their precision value that these models are raising more false alarms than the model trained on the full original data, while at the same time not improving the recognition of\\xa0 the pattern underlying the fraudulent transactions. All of this indicates that the model has overfitted the data.\\xa0\\nAs you can see, our fraud detection model is over-/underfitting when trained on resampled data. What is actually happening under the hood?\\xa0\\n\\xa0\\nDemonstrating Over- and Underfitting in Fraud Detection\\n\\xa0\\nTransactions data are confidential, and we can therefore only work with principal components as predictors of the target class fraudulent/legitimate. In order to better understand how a resampled model leads to over- or underfitting, let’s imagine we had some of the following columns in our data, since they often characterize fraudulent transactions [4]:\\n\\nShipping address equals billing address: yes/no\\nUrgent delivery: yes/no\\nNumber of different items in the order\\nNumber of orders of the same item\\nNumber of credit cards associated with the shipping address\\n\\nSince our training data only contain 394 fraudulent transactions, it could be, for example, that the majority of them are characterized by exceptionally many orders of the same item: one for 20 toasters, another for 50 smartphones, yet another for 25 winter coats, and so on. In reality, the fraudulent transactions are much more varying and continuously evolving. On the contrary, the 227451 legitimate transactions in the training data represent a huge variety of ways of using the credit card: for a banana, hotel booking, toaster, car parking, and more!\\xa0\\nIn the following, we explain how the different resampling methods skew the transactions data, and how this leads to a deterioration in model performance, as we saw before.\\n\\xa0\\nOversampling (SMOTE)\\n\\xa0\\nThe corresponding model has a low precision value (44%) and it therefore raises many false alarms. The training set contains the original fraudulent transactions plus the synthetically generated fraudulent transactions within the feature space of the fraudulent transactions. For example, if we had one fraudulent transaction that purchases 20 toasters, SMOTE resampling might produce thousands of slightly different fraudulent transactions that all purchase a toaster. Eventually, all legitimate transactions that include a toaster would be predicted as fraudulent. This kind of model is overfitting the training data (Figure 3).\\n\\n\\nFigure 3. Example class predictions generated by an overfitting fraud detection model trained on SMOTE oversampled data: An arbitrary feature, for example, product=toaster incorrectly determines fraudulent transactions in new data.\\n\\n\\xa0\\nOversampling (Bootstrap)\\n\\xa0\\nThis model performs worse than the baseline model in terms of both recall and precision. The training set contains thousands of exact copies of the original fraudulent transactions. For example, if we had a fraudulent transaction ordering 20 toasters, all legitimate transactions that ordered 20 items of the same product or a toaster would be suspicious, because these two features would have characterized so many fraudulent transactions in the oversampled data. At the same time, the model would have failed to generalize on large amounts of the same item as suspicious, emphasizing instead precisely 20 toasters as suspicious. Also this model is overfitting the training data (Figure 4).\\xa0\\n\\n\\nFigure 4. Example class predictions generated by an overfitting fraud detection model trained on Bootstrap oversampled data: an arbitrary set of features, for example, product=toaster and # of products=20 incorrectly determines fraudulent transactions in new data.\\n\\n\\xa0\\n\\xa0\\nUndersampling (Bootstrap)\\n\\xa0\\nThe model performs almost perfectly in terms of recall (92%), yet the worst in terms of precision (2%). Since more than 99% of the original transactions are discarded in the undersampling phase, our training data might only consist of credit card transactions for food and leave out hotel bookings, car parking, and many others. This kind of training data is not representative of the real data. Therefore, almost all transactions are predicted as fraudulent; the model is underfitting (Figure 5).\\xa0\\n\\n\\nFigure 5. Example class predictions generated by an underfitting fraud detection model trained on Bootstrap undersampled data: the model predicts all transactions as fraudulent in new data.\\n\\n\\xa0\\nDiagnosing Problems of Resampling in Fraud Detection\\n\\xa0\\nAs our example shows, in this case resampling can’t solve the problem of having too few fraudulent transactions in the dataset. However, resampling is shown to lead to performance gain when the a priori distribution is less skewed, for example, in disease detection [5]. Why is resampling failing then, for this credit card transaction dataset with too few fraudulent transactions in it?\\nFrauds are perpetrated in a large variety of patterns, and we have only a few fraudulent transactions in our training data. So fraud patterns are definitely under-represented in our training set. Resampling doesn’t solve the problem, because it does not increase the variety of the representation of fraudulent transactions, it just replicates in some form the fraud patterns represented in the dataset. Thus, the models trained on resampled data can only perform well in detecting some types of fraud, the types represented in the training data.\\xa0\\nIn summary, the class of fraudulent transactions is too under-represented (just 0.2% of the whole dataset!) to represent a meaningful description of all the fraud patterns that are out there. Even introducing new similar synthetic fraudulent transactions cannot significantly change the range of the represented fraudulent transactions.\\n\\xa0\\nConclusions\\n\\xa0\\nTransactions data are produced in huge volumes every day. In order to build a supervised model for fraud detection, they would need to be labeled. The labeling process, however, in this case, is tricky.\\nFirstly, even if we had the knowledge to appropriately label fraudulent transactions, the process would be very resource-intensive. Skillful experts at detecting frauds are rare and expensive and usually do not spend their time labeling datasets. Even with reliable and sufficient resources, manual labeling would take a prohibitively long time before a sufficiently large amount of data would be available.\\xa0\\nSecondly, expertise in fraud detection is so scarce, because criminals creatively devise ever newer fraud schemes and it is hard to keep up the pace with the newly introduced patterns. An expert might recognize all types of frauds known till then and still fail at recognizing the new fraud schemes, most recently created.\\nFinally, and luckily, there are generally fewer fraudulent transactions than legitimate transactions. Even after all this manual effort by extremely skillful people, we might still end up with an insufficient number of data for the fraudulent class.\\nThose are all reasons why fraud detection is often treated as a rare class problem, rather than an imbalanced class problem.\\nYet, we can try. With this dataset, can we artificially increase the sample size of fraudulent transactions by resampling the training data? Not really. Resampling can improve the model performance if the target classes are imbalanced and yet sufficiently represented. In this case, the problem is really the lack of data. Resampling is subsequently leading to over- or underfitting rather than to a better model performance.\\xa0\\nThis article just aims at giving you an idea of why, in some cases, resampling cannot work. Of course, better results could be obtained with more sophisticated resampling methods than those we have introduced in this article, like for example, a combination of under- and oversampling [6]. Better results could also be obtained with supervised algorithms other than the decision tree. Some machine learning supervised algorithms, such as logistic regression, are less sensitive to class imbalance than the decision tree, while other algorithms, such as ensemble models, are more robust to overfitting. Even better results could be obtained with the decision tree, for example by applying pruning techniques to avoid the overfitting effect or controlling the tree growth.\\nHowever, sometimes we must accept that the data is just not sufficient to describe the minority class. In this case, we must proceed with unlabeled data and try to isolate the events of the rare class via unsupervised algorithms, such as neural autoencoders, isolation forest, and clustering algorithms.\\nThe Resampling in Supervised Fraud Detection Models workflow, used in this article to show the limitations of resampling, can be downloaded for free from the KNIME Hub.\\xa0\\n\\xa0\\nReferences\\n\\xa0\\n[1]\"The Nilson Report.\" HSN Consultants, Inc., November 2019, \\xa0Issue 1164, https://nilsonreport.com/publication_newsletter_archive_issue.php?issue=1164. Accessed 14 Oct. 2020.\\n[2]\"Random Oversampling and Undersampling for Imbalanced Classification\" Machine Learning Mastery Pty. Ltd., January 2020, https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/. Accessed 14 Oct. 2020.\\n[3]\"Oversampling and Undersampling\" Medium, September 2010, https://towardsdatascience.com/oversampling-and-undersampling-5e2bbaf56dcf. Accessed 14 Oct. 2020.\\n[4]https://www.bluefin.com/support/identifying-fraudulent-transactions/\\n[5]\"Machine Learning Resampling Techniques for Class Imbalances\" Medium, January 2011, https://towardsdatascience.com/machine-learning-resampling-techniques-for-class-imbalances-30cbe2415867. Accessed 14 Oct. 2020.\\n[6]\"How to Combine Oversampling and Undersampling for Imbalanced Classification\" Machine Learning Mastery Pty. Ltd., January 2020, https://machinelearningmastery.com/combine-oversampling-and-undersampling-for-imbalanced-classification/. Accessed 14 Oct. 2020.\\n\\xa0\\nBio: Maarit Widmann is a Data Scientist at KNIME.\\nRelated:\\n\\nUndersampling Will Change the Base Rates of Your Model’s Predictions\\nThe 5 Most Useful Techniques to Handle Imbalanced Datasets\\nPro Tips: How to deal with Class Imbalance and Missing Labels',\n",
       " 'By Hadrien Jean, Machine Learning Scientist.\\ncomments\\n\\n\\xa0\\nCalculus is a branch of mathematics that gives tools to study the rate of change of functions through two main areas: derivatives and integrals. In the context of machine learning and data science, you might use integrals to calculate the area under the curve (for instance, to evaluate the performance of a model with the ROC curve, or to calculate probability from densities.\\nIn this article, you’ll learn about integrals and the area under the curve using the practical data science example of the area under the ROC curve used to compare the performances of two machine learning models. Building from this example, you’ll see the notion of the area under the curve and integrals from a mathematical point of view (from my book Essential Math for Data Science).\\n\\xa0\\nPractical Project\\n\\xa0\\nLet’s say that you would like to predict the quality of wines from various of their chemical properties. You want to do a binary classification of the quality (distinguishing very good wines from not very good ones). You’ll develop methods allowing you to evaluate your models considering imbalanced data with the area under the Receiver Operating Characteristics (ROC) curve.\\nDataset\\nTo do this, we’ll use a dataset showing various chemical properties of red wines and ratings of their quality. The dataset comes from here: https://archive.ics.uci.edu/ml/datasets/wine+quality. The related paper is Cortez, Paulo, et al.\\xa0”Modeling wine preferences by data mining from physicochemical properties.” Decision Support Systems 47.4 (2009): 547-553.\\n\\n\\nFigure 1: Illustration of wine quality modeling.\\n\\n\\xa0\\nAs illustrated in Figure 1, the dataset represents chemical analyses of wines (the features) and ratings of their quality. This rating is the target: this is what you’ll try to estimate.\\nFirst, let’s load the data and have a look at the features:\\n\\nwine_quality = pd.read_csv(\"https://raw.githubusercontent.com/hadrienj/essential_math_for_data_science/master/data/winequality-red.csv\",\\r\\n                           sep=\";\")\\r\\nwine_quality.columns\\r\\n\\n\\n\\n\\nIndex([\\'fixed acidity\\', \\'volatile acidity\\', \\'citric acid\\', \\'residual sugar\\',\\r\\n       \\'chlorides\\', \\'free sulfur dioxide\\', \\'total sulfur dioxide\\', \\'density\\',\\r\\n       \\'pH\\', \\'sulphates\\', \\'alcohol\\', \\'quality\\'],\\r\\n      dtype=\\'object\\')\\r\\n\\n\\n\\nThe last column\\xa0quality\\xa0is important as you’ll use it as the target of your classification. The quality is described by ratings from 3 to 8:\\n\\nwine_quality[\"quality\"].unique()\\r\\n\\n\\n\\n\\narray([5, 6, 7, 4, 8, 3])\\r\\n\\n\\n\\nSince the goal is to classify red wines of\\xa0very good\\xa0quality, let’s decide that the wines are very good when ratings are 7 or 8 and not very good otherwise.\\nLet’s create the dataset with\\xa0y\\xa0being the quality (the dependent variable, 0 for ratings less than 7 and 1 for ratings greater than or equal 7) and\\xa0X\\xa0containing all the other features.\\n\\nX = wine_quality.drop(\"quality\", axis=1).values\\r\\ny = wine_quality[\"quality\"] >= 7\\r\\n\\n\\n\\nThe first thing to do, before looking at the data, is to split it in a part for training your algorithms (the training set) and a part for testing them (the test set). This will allow you to evaluate the performance of your model on data unseen during the training.\\n\\nfrom sklearn.model_selection import train_test_split\\r\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)\\r\\n\\n\\n\\nPreprocessing\\nAs a first step, let’s standardize the data to help the convergence of the algorithm. You can use the class\\xa0StandardScaler\\xa0from Sklearn.\\nNote that you don’t want to consider the data from the test set to do the standardization. The method\\xa0fit_transform()\\xa0calculates the parameters needed for the standardization and apply it at the same time. Then, you can apply the same standardization to the test set without fitting again.\\n\\nfrom sklearn.preprocessing import StandardScaler\\r\\n\\r\\nscaler = StandardScaler()\\r\\nX_train_stand = scaler.fit_transform(X_train)\\r\\nX_test_stand = scaler.transform(X_test)\\r\\n\\n\\n\\nFirst Model\\nAs a first model, let’s train a logistic regression on the training set and calculate the classification accuracy (the percentage of correct classifications) on the test set:\\n\\nfrom sklearn.linear_model import LogisticRegression\\r\\nfrom sklearn.metrics import accuracy_score\\r\\n\\r\\nlog_reg = LogisticRegression(random_state=123, penalty=\"none\")\\r\\nlog_reg.fit(X_train_stand, y_train)\\r\\ny_pred = log_reg.predict(X_test_stand)\\r\\naccuracy_score(y_test, y_pred)\\r\\n\\n\\n\\n\\n0.8729166666666667\\r\\n\\n\\n\\nThe accuracy is about 0.87, meaning that 87% of the test examples have been correctly classified. Should you be happy with this result?\\n\\xa0\\nMetrics for Imbalanced Datasets\\n\\xa0\\nImbalanced Dataset\\nSince we separated the data into very good wines and not very good wines, the dataset is\\xa0imbalanced: there are different quantities of data corresponding to each target class.\\nLet’s check how many observations you have in the negative (not very good wines) and positive classes (very good wines):\\n\\n(y_train == 0).sum() / y_train.shape[0]\\r\\n\\n\\n\\n\\n0.8650580875781948\\r\\n\\n\\n\\n\\n(y_train == 1).sum() / y_train.shape[0]\\r\\n\\n\\n\\n\\n0.13494191242180517\\r\\n\\n\\n\\nIt shows that there are around 86.5% of the examples corresponding to class 0 and 13.5% to class 1.\\nSimple Model\\nTo illustrate this point about accuracy and imbalanced datasets, let’s creates a model as a baseline and look at its performance. It will help you to see the advantages to use other metrics than accuracy.\\nA very simple model using the fact that the dataset is imbalanced would always estimate the class with the largest number of observations. In your case, such a model would always estimate that all wines are bad and get a decent accuracy doing that.\\nLet’s simulate this model by creating random probabilities below 0.5 (for instance, a probability of 0.15 means that there is a 15% chance that the class is positive). We need these probabilities to calculate both the accuracy and other metrics.\\n\\nnp.random.seed(1)\\r\\ny_pred_random_proba = np.random.uniform(0, 0.5, y_test.shape[0])\\r\\ny_pred_random_proba\\r\\n\\n\\n\\n\\narray([2.08511002e-01, 3.60162247e-01, 5.71874087e-05, ...,\\r\\n       4.45509477e-01, 1.36436118e-02, 2.61025624e-01])\\r\\n\\n\\n\\nLet’s say that if the probability is above 0.5, the class is estimated as positive:\\n\\ndef binarize(y_hat, threshold):\\r\\n    return (y_hat > threshold).astype(int)\\r\\n\\r\\ny_pred_random = binarize(y_pred_random_proba, threshold=0.5)\\r\\ny_pred_random\\r\\n\\n\\n\\n\\narray([0, 0, 0, ..., 0, 0, 0])\\r\\n\\n\\n\\nThe variable\\xa0y_pred_random\\xa0contains only zeros. Let’s evaluate the accuracy of this random model:\\n\\naccuracy_score(y_test, y_pred_random)\\r\\n\\n\\n\\n\\n0.8625\\r\\n\\n\\n\\nThis shows that, even with a random model, the accuracy is not bad at all: it doesn’t mean that the model is good.\\nTo summarize, having a different number of observations corresponding to each class, you can’t rely on the accuracy to evaluate your model’s performance. In our example, the model could output only zeros and you would get around 86% accuracy.\\nYou need other metrics to assess the performance of models with imbalanced datasets.\\nROC Curves\\nA good alternative to the accuracy is the Receiver Operating Characteristics (ROC) curve. You can check the very good explanations of Aurélien Géron about ROC curves in Géron, Aurélien. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. O’Reilly Media, 2019.\\nThe main idea is to separate the estimations from the model into four categories:\\n\\nThe true positives (TP): the prediction is 1 and the true class is 1.\\nThe false positives (FP): the prediction is 1 but the true class is 0.\\nThe true negatives (TN): the prediction is 0 and the true class is 0.\\nThe false negatives (FN): the prediction is 0 but the true class is 1.\\n\\nLet’s calculate these values for your first logistic regression model. You can use the function\\xa0confusion_matrix\\xa0from Sklearn. It presents a table organized as follows:\\n\\n\\nFigure 2: Illustration of a confusion matrix.\\n\\n\\xa0\\n\\nfrom sklearn.metrics import confusion_matrix\\r\\nconfusion_matrix(y_test, y_pred_random)\\r\\n\\n\\n\\n\\narray([[414,   0],\\r\\n       [ 66,   0]])\\r\\n\\n\\n\\nYou can see that there is no positive observation that has been correctly classified (TP) with the random model.\\nDecision Threshold\\nIn classification tasks, you want to estimate the class of data samples. For models like logistic regression which outputs probabilities between 0 and 1, you need to convert this score to the class 0 or 1 using a\\xa0decision threshold, or just\\xa0threshold. A probability above the threshold is considered as a positive class. For instance, using the default choice of the decision threshold at 0.5, you consider that the estimated class is 1 when the model outputs a score above 0.5.\\nHowever, you can choose other thresholds, and the metrics you use to evaluate the performance of your model will depend on this threshold.\\nWith the ROC curve, you consider multiple thresholds between 0 and 1 and calculate the true positive rate as a function of the false positive rate for each of them.\\nYou can use the function\\xa0roc_curve\\xa0from Sklearn to calculate the false positive rate (fpr) and the true positive rate (tpr). The function outputs also the corresponding thresholds. Let’s try it with our simulated random model where outputs are only values bellow 0.5 (y_pred_random_proba).\\n\\nfrom sklearn.metrics import roc_curve\\r\\nfpr_random, tpr_random, thresholds_random = roc_curve(y_test, y_pred_random_proba)\\r\\n\\n\\n\\nLet’s have a look at the outputs:\\n\\nfpr_random\\r\\n\\n\\n\\n\\narray([0.        , 0.        , 0.07246377, ..., 0.96859903, 0.96859903,\\r\\n       1.        ])\\r\\n\\n\\n\\n\\ntpr_random\\r\\n\\n\\n\\n\\narray([0.        , 0.01515152, 0.01515152, ..., 0.98484848, 1.        ,\\r\\n       1.        ])\\r\\n\\n\\n\\n\\nthresholds_random\\r\\n\\n\\n\\n\\narray([1.49866143e+00, 4.98661425e-01, 4.69443239e-01, ...,\\r\\n       9.68347894e-03, 9.32364469e-03, 5.71874087e-05])\\r\\n\\n\\n\\nYou can now plot the ROC curve from these values:\\n\\nplt.plot(fpr_random, tpr_random)\\r\\n# [...] Add axes, labels etc.\\r\\n\\n\\n\\n\\n\\nFigure 3: ROC curve corresponding to the random model.\\n\\n\\xa0\\nFigure 3 shows the ROC curve corresponding to the random model. It gives you the true positive rate as a function of the false positive rate for each threshold.\\nHowever, be careful, the thresholds are from 1 to 0. For instance, the point at the bottom left corresponds to a threshold of 1: there is 0 true positive and 0 false positive because it is not possible to have a probability above 1, so with a threshold of 1, no observation can be categorized as positive. At the top right, the threshold is 0, so all observations are categorized as positive, leading to 100% of true positive but also 100% of false positive.\\nA ROC curve around the diagonal means that the model is not better than random which is the case here. A perfect model would be associated with a ROC curve with a true positive rate of 1 for all values of false positive rate.\\nLet’s now look at the ROC curve corresponding to the logistic regression model you trained earlier. You’ll need probabilities from the model, that you can get using\\xa0predict_proba()\\xa0instead of\\xa0predict:\\n\\ny_pred_proba = log_reg.predict_proba(X_test_stand)\\r\\ny_pred_proba\\r\\n\\n\\n\\n\\narray([[0.50649705, 0.49350295],\\r\\n       [0.94461852, 0.05538148],\\r\\n       [0.97427601, 0.02572399],\\r\\n       ...,\\r\\n       [0.82742897, 0.17257103],\\r\\n       [0.48688505, 0.51311495],\\r\\n       [0.8809794 , 0.1190206 ]])\\r\\n\\n\\n\\nThe first column is the score for the class 0 and the second column for the score 1 (thus, the total of each row is 1), so you can keep the second column only.\\n\\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\\r\\nplt.plot(fpr, tpr)\\r\\n# [...] Add axes, labels etc.\\r\\n\\n\\n\\n\\n\\nFigure 4: ROC curve corresponding to the logistic model.\\n\\n\\xa0\\nYou can see in Figure 4 that your model is actually better than a random model, which is not something you were able to know from the model accuracies (they were equivalent: around 0.86 for the random model and 0.87 for your model).\\nVisual inspection is good, but it would also be crucial to have a single numerical metric to compare your models. This is usually provided by the area under the ROC curve. You’ll see what is the area under the curve and how you can calculate in the next sections.\\n\\xa0\\nIntegrals\\n\\xa0\\nIntegration\\xa0is the inverse operation of differentiation. Take a function\\xa0f(x)\\xa0and calculate its derivative\\xa0f′(x), the\\xa0indefinite integral\\xa0(also called\\xa0antiderivative) of\\xa0f′(x)\\xa0gives you back\\xa0f(x)\\xa0(up to a constant, as you’ll soon see).\\nYou can use integration to calculate the\\xa0area under the curve, which is the area of the shape delimited by the function, as shown in Figure 5.\\n\\n\\nFigure 5: Area under the curve.\\n\\n\\xa0\\nA\\xa0definite integral\\xa0is the integral over a specific interval. It corresponds to the area under the curve in this interval.\\n\\xa0\\nExample\\n\\xa0\\nYou’ll see through this example how to understand the relationship between the integral of a function and the area under the curve. To illustrate the process, you’ll approximate the integral of the function\\xa0g(x)=2x\\xa0using a discretization of the area under the curve.\\nExample Description\\nLet’s take again the example of the moving train. You saw that speed as a function of time was the derivative of distance as a function of time. These functions are represented in Figure 6.\\n\\n\\nFigure 6: The left panel shows\\xa0f(x)\\xa0which is the distance as a function of time, and the right panel its derivative\\xa0g(x), which is the speed as a function of time.\\n\\n\\xa0\\nThe function shown in the left panel of Figure 6 is defined as\\xa0f(x)=x2. Its derivative is defined as\\xa0g(x)=2x.\\nIn this example, you’ll learn how to find an approximation of the area under the curve of\\xa0g(x).\\nSlicing the Function\\nTo approximate the area of a shape, you can use the slicing method: you cut the shape into small slices with an easy shape like rectangles, calculate the area of each of these slices and sum them.\\nYou’ll do exactly that to find an approximation of the area under the curve of\\xa0g(x).\\n\\n\\nFigure 7: Approximation of the area under the curve by discretizing the area under the curve of speed as a function of time.\\n\\n\\xa0\\nFigure 7 shows the area under the curve of\\xa0f′(x)\\xa0sliced as one-second rectangles (let’s call this difference\\xa0Δx). Note that we underestimate the area (look at the missing triangles), but we’ll fix that later.\\nLet’s try to understand the meaning of the slices. Take the first one: its area is defined as\\xa02⋅12⋅1. The height of the slice is the speed at one second (the value is 2). So there are two units of speed by one unit of time for this first slice. The area corresponds to a multiplication between speed and time: this is a distance.\\nFor instance, if you drive at 50 miles per hour (speed) for two hours (time), you traveled 50⋅2=100\\xa0miles (distance). This is because the unit of speed corresponds to a ratio between distance and time (like miles\\xa0per\\xa0hour). You get:\\n\\n\\xa0\\nTo summarize, the derivative of the distance by time function is the speed by time function, and the area under the curve of the speed by time function (its integral) gives you a distance. This is how derivatives and integrals are related.\\nImplementation\\nLet’s use slicing to approximate the integral of the function\\xa0g(x)=2x. First, let’s define the function\\xa0g(x):\\n\\ndef g_2x(x):\\r\\n    return 2 * x\\r\\n\\n\\n\\nAs illustrated in Figure 7, you’ll consider that the function is discrete and take a step of\\xa0Δx=1. You can create an\\xa0x-axis with values from zero to six, and apply the function\\xa0g_2x()\\xa0for each of these values. You can use the Numpy method\\xa0arange(start, stop, step)\\xa0to create an array filled with values from\\xa0start\\xa0to\\xa0stop\\xa0(not included):\\n\\ndelta_x = 1\\r\\nx = np.arange(0, 7, delta_x)\\r\\nx\\r\\n\\n\\n\\n\\narray([0, 1, 2, 3, 4, 5, 6])\\r\\n\\n\\n\\n\\ny = g_2x(x)\\r\\ny\\r\\n\\n\\n\\n\\narray([ 0,  2,  4,  6,  8, 10, 12])\\r\\n\\n\\n\\nYou can then calculate the slice’s areas by iterating and multiplying the width (Δx) by the height (the value of\\xa0y\\xa0at this point). of the slice. As you saw, this area (delta_x * y[i-1]\\xa0in the code below) corresponds to a distance (the distance of the moving train traveled during the\\xa0ith slice). You can finally append the results to an array (slice_area_all\\xa0in the code below).\\nNote that the index of\\xa0y\\xa0is\\xa0i-1\\xa0because the rectangle is on the left of the\\xa0x\\xa0value we estimate. For instance, the area is zero for\\xa0x=0\\xa0and\\xa0x=1.\\n\\nslice_area_all = np.zeros(y.shape[0])\\r\\nfor i in range(1, len(x)):\\r\\n    slice_area_all[i] = delta_x * y[i-1]\\r\\nslice_area_all\\r\\n\\n\\n\\n\\narray([ 0.,  0.,  2.,  4.,  6.,  8., 10.])\\r\\n\\n\\n\\nThese values are the slice’s areas.\\nTo calculate the distance traveled from the beginning to the corresponding time point (and not corresponding to each slice), you can calculate the cumulative sum of\\xa0slice_area_all\\xa0with the Numpy function\\xa0cumsum():\\n\\nslice_area_all = slice_area_all.cumsum()\\r\\nslice_area_all\\r\\n\\n\\n\\n\\narray([ 0.,  0.,  2.,  6., 12., 20., 30.])\\r\\n\\n\\n\\nThis is the estimated values of the area under the curve of\\xa0g(x)\\xa0as a function of\\xa0x. We know that the function\\xa0g(x)\\xa0is the derivative of\\xa0f(x)=x2, so we should get back\\xa0f(x)\\xa0by the integration of\\xa0g(x).\\nLet’s plot our estimation and\\xa0f(x)f(x), which we’ll call the “true function”, to compare them:\\n\\nplt.plot(x, x ** 2, label=\\'True\\')\\r\\nplt.plot(x, slice_area_all, label=\\'Estimated\\')\\r\\n\\n\\n\\n\\n\\nFigure 8: Comparison of estimated and original function.\\n\\n\\xa0\\nThe estimation represented in Figure 8 shows that the estimation is not bad, but could be improved. This is because we missed all these triangles represented in red in Figure\\n\\nOne way to reduce the error is to take a smaller value for\\xa0ΔxΔx, as illustrated in the right panel in Figure 9.\\n\\n\\n\\nFigure 9: Missing parts in slices of the speed function (in red). The error is smaller with a smaller\\xa0Δx.\\n\\n\\xa0\\nLet’s estimate the integral function with\\xa0Δx=0.1:\\n\\ndelta_x = 0.1\\r\\nx = np.arange(0, 7, delta_x)\\r\\ny = g_2x(x)\\r\\n#  [...] Calculate and plot slice_area_all\\r\\n\\n\\n\\n\\n\\nFigure 10: Smaller slice widths lead to a better estimation of the original function.\\n\\n\\xa0\\nAs shown in Figure 10, we recovered (at least, up to an additive constant) the original function whose derivative we integrated.\\nExtension\\nIn our previous example, you integrated the function\\xa02x, which is a linear function, but the principle is the same for any continuous function (see Figure 11 for instance).\\n\\n\\nFigure 11: The slicing method can be used with many linear or nonlinear function, including all continuous functions.\\n\\n\\xa0\\n\\xa0\\nRiemann Sum\\n\\xa0\\nApproximating an integral using this slicing method is called a\\xa0Riemann sum. Riemann sums can be calculated in different ways, as you can see in Figure 12.\\n\\n\\nFigure 12: Four kinds of Riemann sums for integral approximation.\\n\\n\\xa0\\nAs pictured in Figure 12, with the left Riemann sum, the curve is aligned with the left corner of the rectangle. With the right Riemann sum, the curve is aligned with the right corner of the rectangle. With the midpoint rule, the curve is aligned with the center of the rectangle. With the trapezoidal rule, a trapezoidal shape is used instead of a rectangle. The curve crosses both top corners of the trapezoid.\\n\\xa0\\nMathematical Definition\\n\\xa0\\nIn the last section, you saw the relationship between the area under the curve and integration (you got back the original function from the derivative). Let’s see now the mathematical definition of integrals.\\nThe integrals of the function\\xa0f(x)\\xa0with respect to\\xa0x\\xa0is denoted as following:\\n∫f(x)dx\\nThe symbol\\xa0dx\\xa0is called the\\xa0differential\\xa0of\\xa0x\\xa0and refers to the idea of an infinitesimal change of\\xa0x. It is a difference in\\xa0x\\xa0that approaches 0. The main idea of integrals is to sum an infinite number of slices which have an infinitely small width.\\nThe symbol\\xa0∫\\xa0is the integral sign and refers to the sum of an infinite number of slices.\\nThe height of each slice is the value\\xa0f(x). The multiplication of\\xa0f(x)\\xa0and\\xa0dx\\xa0is thus the area of each slice. Finally,\\xa0∫f(x):dx\\xa0is the sum of the slice areas over an infinite number of slices (the width of the slices tending to zero). This is the\\xa0area under the curve.\\nYou saw in the last section how to approximate function integrals. But if you know the derivative of a function, you can retrieve the integral knowing that it is the inverse operation. For example, if you know that:\\nd(x2)dx=2x\\nYou can conclude that the integral of\\xa02x\\xa0is\\xa0x2. However, there is a problem. If you add a constant to our function the derivative is the same because the derivative of a constant is zero. For instance,\\nd(x2+3)dx=2x\\nIt is impossible to know the value of the constant. For this reason, you need to add an unknown constant to the expression, as follows:\\n∫2xdx=x2+c\\nwith\\xa0cc\\xa0being a constant.\\nDefinite Integrals\\nIn the case of\\xa0definite integrals, you denote the interval of integration with numbers below and above the integral symbol, as following:\\n∫baf(x)dx\\nIt corresponds to the area under the curve of the function\\xa0f(x)\\xa0between\\xa0x=a\\xa0and\\xa0x=b, as illustrated in Figure 13.\\nFigure 13: Area under the curve between\\xa0x=a\\xa0and\\xa0x=b.\\n\\xa0\\nArea Under the ROC Curve\\n\\xa0\\nNow that you know how the area under the curve relates to integration, let’s see how to calculate it to compare numerically your models.\\nRemember that you had the ROC curves represented in Figure 14:\\n\\nplt.plot(fpr_random, tpr_random, label=\"Random\")\\r\\nplt.plot(fpr, tpr, label=\"Logistic regression\")\\r\\n# [...] Add axes, labels etc.\\r\\n\\n\\n\\n\\n\\nFigure 14: ROC curves of the random model (blue) and the logistic regression model (green).\\n\\n\\xa0\\nLet’s start with the random model. You want to sum each value of true positive rate multiplied by the width on the\\xa0x-axis that is the difference between the corresponding value of false positive rate and the one before. You can obtain these differences with:\\n\\nfpr_random[1:] - fpr_random[:-1]\\r\\n\\n\\n\\n\\narray([0.00241546, 0.01207729, 0.        , ..., 0.01207729, 0.        ,\\r\\n       0.06038647])\\r\\n\\n\\n\\nSo the area under the ROC curve of the random model is:\\n\\n(tpr_random[1:] * (fpr_random[1:] - fpr_random[:-1])).sum()\\r\\n\\n\\n\\n\\n0.5743302591128678\\r\\n\\n\\n\\nOr you can simply use the function\\xa0roc_auc_score()\\xa0from Sklearn using the true target values and the probabilities as input:\\n\\nfrom sklearn.metrics import roc_auc_score\\r\\n\\r\\nroc_auc_score(y_test, y_pred_random_proba)\\r\\n\\n\\n\\n\\n0.5743302591128678\\r\\n\\n\\n\\nAn area under the ROC curve of 0.5 corresponds to a model that is not better than random and an area of 1 corresponds to perfect predictions.\\nNow, let’s compare this value to the area under the ROC curve of your model:\\n\\nroc_auc_score(y_test, y_pred_proba[:, 1])\\r\\n\\n\\n\\n\\n0.8752378861074513\\r\\n\\n\\n\\nThis shows that your model is actually not bad and your predictions of the quality of the wine are not random.\\nIn machine learning, you can use a few lines of code to train complex algorithms. However, as you saw here, a bit of math can help you to make the most of it and speed up your work. It will give you more ease in various aspects of your discipline, even, for instance, understanding the documentation of machine learning libraries like Sklearn.\\n\\xa0\\nBio: Hadrien Jean is a machine learning scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog (hadrienj.github.io).\\nOriginal. Reposted with permission.\\nRelated:\\n\\nBoost your data science skills. Learn linear algebra.\\nPreprocessing for Deep Learning: From covariance matrix to image whitening\\nEssential Math for Data Science:\\u200a ‘Why’ and ‘How’',\n",
       " 'By Kevin Vu, Exxact Corp.\\ncomments\\n\\n\\xa0\\nIntroducing Deep Learning on Graphs\\n\\xa0\\nIf you’re a deep learning enthusiast you’re probably already familiar with some of the basic mathematical primitives that have been driving the impressive capabilities of what we call deep neural networks. Although we like to think of a basic artificial neural network as some nodes with some weighted connections, it’s more efficient computationally to think of neural networks as matrix multiplication all the way down. We might draw a cartoon of an artificial neural network like the figure below, with information traveling in from left to right from inputs to outputs (ignoring recurrent networks for now).\\n\\nMultilayer perceptron cartoon in the public domain,\\xa0Source\\n\\xa0\\nThis type of neural network is a feed-forward multilayer perceptron (MLP). If we want a computer to compute the forward pass for this model, it’s going to use a string of matrix multiplies and some sort of non-linearity (here represented by the Greek letter sigma) in the hidden layer:\\n\\nMLP matrix multiplication cartoon in the public domain,\\xa0Source\\n\\xa0\\nMLPs are well-suited for data that can be naturally shaped as 1D vectors. While neat and all, MLPs use an awful lot of parameters when data samples are large, and this isn’t a very efficient way to treat higher dimensional data like 2D images or 3D volumes. 2D data like images instead naturally lend themselves to the operation of convolution, wherein weights are applied in local neighborhoods across the entire image, instead of granting each point to point connection between layers it’s own weight. This type of weight sharing has a number of advantages, including translation equivariance, regularization, and parameter efficiency.\\nConvolution can be visualized like so:\\n\\nConvolution cartoon in the public domain,\\xa0Source\\n\\xa0\\nOf course we’re not going to sit down with pen and paper and perform these operations by hand, we want an algorithm that can quickly perform convolution across each image channel in a computer-friendly way.\\nIn principle, computers perform convolutions something like the following:\\n\\nConvolution as multiplication of matrices in the Fourier domain cartoon in the public domain,\\xa0Source\\n\\xa0\\nThat’s right, convolution operations are again implemented as the multiplication of matrices, although this time it is element-wise. This is thanks to the convolution theorem of the\\xa0Fourier transform, which states that multiplication in the Fourier domain relates to convolution in the spatial domain. But what happens when our data of interest isn’t particularly well-suited to representation as a 1D vector or a 2D/3D image, and is instead naturally represented as a graph?\\n\\nGraph cartoon in the public domain,\\xa0Source\\n\\xa0\\nFor our purposes, a graph is a collection of nodes connected by edges, as shown in the cartoon. The edges can have their own properties such as weights and/or directionality, and the nodes typically have some sort of states or features, just like the node activations in a feed-forward MLP.\\nIn a graph neural network, each “layer” is just a snapshot of the node states of the graph, and these are connected by operational updates related to each node and its neighbors, such as neural networks operating as the edges between nodes.\\nIf we want to use graph neural networks to achieve impressive results on graph-structured data, like what convolutional neural networks did for deep learning on images, we need an efficient way to implement these models on computers. That almost always means we need a way to convert the conceptual graph neural network framework to something that works on a modern deep learning GPU.\\n\\xa0\\nHow can we possibly convert the complicated idea of graph neural networks to another form of matrix multiplication?\\n\\xa0\\n\\nSource\\n\\xa0\\nA convenient way to represent the connections in a graph is with something called an adjacency matrix. As the name suggests, an adjacency matrix describes which nodes are next to each other (i.e.\\xa0connected to each other by edges) in the graph.\\nBut a graph neural network needs to operate on graphs with arbitrary structure (much like the convolutional kernels of a conv-net can work on images of different height and width), so we can’t expect the input data to have the same adjacency matrix each time or even for each adjacency matrix to have the same dimensions. We can deal with this by combining the adjacency matrices for several samples diagonally into a larger matrix describing all the connections in a batch.\\n\\nSource\\n\\xa0\\nThis allows us to deal with multiple graphs with different structures in a single batch, and you’ll notice that this formulation also results in weight sharing between nodes. There are a few more details to this: the adjacency matrix should be normalized so that feature scales don’t completely change, and there are other approaches to convolution on graphs than the graph convolution network approach (GCN) we are talking about here, but it’s a good starting point in understanding the GNN forward pass.\\nIt’s enough to give us an appreciation for the data preparation and mathematical operations needed to implement deep learning on graphs. Luckily, the interest in deep learning for graph-structured data has motivated the development of a number of open source libraries for graph deep learning, leaving more cognitive room for researchers and engineers to concentrate on architectures, experiments, and applications.\\nIn this article we’ll go through 7 up-and-coming open source libraries for graph deep learning, ranked in order of increasing popularity.\\n\\xa0\\n7 Open Source Libraries for Deep Learning on Graphs\\n\\xa0\\n7. GeometricFlux.jl\\n\\xa0\\n\\nSource\\n\\xa0\\nReflecting the dominance of the language for graph deep learning, and for deep learning in general, most of the entries on this list use Python and are built on top of TensorFlow, PyTorch, or JAX. This first entry, however, is an open source library for graph neural networks built on the Flux deep learning framework in the Julia programming language.\\nOne may be tempted to write off GeometricFlux.jl, and even the whole idea of using the Julia language for deep learning due to the relatively small number of practitioners, but it is a language with a growing community and offers a number of technical advantages over Python. One would have hardly predicted DeepMind would start ditching TensorFlow in favor of JAX just a few years ago (see entry number 5 on this list), and likewise in just a few short years we may see the Julia language start to supplant Python as the standard language for machine learning.\\nThe Julia programming language was designed from the start to be both highly productive (like Python), and fast like compiled languages including C. Julia language uses just-in-time compilation to achieve fast execution speed, while it’s read-execute-print loop (REPL) makes interactive and iterative programming reasonably productive. You will notice a slight delay when you run code for the first time, especially if you’re used to using Python in a particularly interactive way (like in Jupyter notebooks), but over time the speed-ups for a given workflow can be significant.\\nJulia is designed as a scientific programming language, and there has been significant development of automatic differentiation packages over the last five years or so. The end result is functionality that can combine research-centered libraries like the\\xa0DifferentialEquations.jl\\xa0package with machine learning capabilities as we see in the neural differential equations package\\xa0DiffEqFlux.jl. The same goes for GeometricFlux.jl, which is built to be compatible with the graph theory research\\xa0JuliaGraphs\\xa0ecosystem as well as other parts of Flux.\\nIf you’re using graph deep learning for work, it may be most efficient to stick with a library that’s built on PyTorch or the standard working framework for deep learning used for other projects. If you’re starting from scratch or doing research, however, GeometricFlux.jl makes a compelling entry point for graph deep learning and differentiable programming with Julia. The library’s friendly MIT License also makes it easy to build and contribute the tools you need, or to tackle some of the\\xa0open issues\\xa0from the project’s GitHub repository.\\n\\xa0\\n6. PyTorch GNN\\n\\xa0\\nThe\\xa0PyTorch Graph Neural Network library\\xa0is a graph deep learning library from Microsoft, still under active development at version ~0.9.x after being made public in May of 2020. PTGNN is made to be readily familiar for users familiar with building models based on the torch.nn.Module class, and handles the workflow tasks of dataloaders and turning graphs into PyTorch-ready tensors.\\nPTGNN is based on an interesting architecture called the\\xa0AbstractNeuralModel. This class encapsulates the entire process of training a graph neural network including tensorizing and pre-proccessing raw data, and includes the TNeuralModule that is the actual neural model sub-classed from PyTorch’s nn.Module class. The neural modules can be used independently of the AbstractNeuralModel object, and in fact can be combined with other types of PyTorch modules/layers if desired.\\nPTGNN is slightly younger than GeometricFlux.jl and has a less active commit history, but ekes out slightly more GitHub stars and forks. It has the same permissive and open source MIT License, but if you’re looking for a project to contribute to, you’ll need to be fairly self-directed. The “Issues” tab on GitHub provides little to no direction of what needs to be fixed or implemented. PTGNN has a few interesting design elements in its construction that may be of interest to work with or on, but if you’re a graph neural network enthusiast looking for a PyTorch-based graph deep learning library you may be better served by using PyTorch Geometric (number 1 on our list). PyTorch Geometric is more mature, having been in development for about 4 years now, and has an established and growing community of users and developers.\\n\\xa0\\n5. Jraph\\n\\xa0\\n\\nSource\\n\\xa0\\nLate last year you may have noticed a blog post from DeepMind with a little less pomp and circumstance than their usual headline-grabbing landmarks. In December 2020\\xa0Deepmind described\\xa0their ongoing efforts in developing and using a capable ecosystem of deep learning research libraries based on the functional differentiable programming library JAX. JAX is the conceptual progeny of what started as an academic project for simple but nigh-universal automatic differentiation in Python (especially NumPy) called\\xa0Autograd.\\nAfter Google scooped up several of the research programmers responsible for the original Autograd, they developed a new library and now we have JAX. JAX is an interesting package due in no small part to its emphasis on composable functional programming paradigms. It also pays attention to the more general concept of “differentiable programming” rather than focusing primarily on neural networks like TensorFlow or PyTorch. Although PyTorch and TensorFlow can both be used to build, say, differentiable physics models instead of neural networks, JAX is more readily amenable to flexible differentiable programming for scientific and other programming tasks from the start. The JAX offering is compelling enough, at least, to induce DeepMind to embark on a substantial adoption and development track, despite having previously spent significant time building TensorFlow-based tools like\\xa0Sonnet.\\nAs part of DeepMind’s efforts to develop a JAX-based ecosystem for deep learning research they’ve developed a graph learning library called Jraph.\\n\\xa0\\nOriginal image in the public domain from Wikimedia contributor\\xa0Scambelo\\n\\n\\xa0\\nUnlike some of the other libraries on this list, Jraph is a lightweight and minimalistic graph learning library that doesn’t in general prescribe a specific way for working with itself. Jraph inherited some design patterns from a spiritual predecessor,\\xa0Graph Nets, built with TensorFlow and Sonnet. Namely, Jraph uses the same GraphsTuple concept as Graph Nets, which is a data structure containing infromation describing nodes, edges, and edge directions. Another feature handled by Jraph, makes special accommodations for dealing with variable-structured graphs using masks and padding. That’s not a concern for most of the other Python libraries on this list, but it’s necessary due to the use of just-in-time compilation in JAX. This ensures that working with graphs in JAX doesn’t mean giving up the execution speedups JAX provides on both GPU and CPU hardware.\\n\\xa0\\n4. Spektral\\n\\xa0\\nSpektral logos used under the MIT License, from the\\xa0Spektral documentation.\\n\\n\\xa0\\nSpektral is a graph deep learning library based on Tensorflow 2 and Keras, and with a logo clearly inspired by the Pac-Man ghost villains. If you are set on using a TensorFlow-based library for your graph deep learning needs, Spektral may be your best option (although DGL, number 2 on our list, can support both PyTorch or TensorFlow back-ends). It’s designed to be easy-to-use and flexible, while retaining usage that is as close as possible to the familiar Keras API. This means that you can even train a model using the convenient model.fit() method, so long as you provide a Spetkral dataloader to handle the formation of TensorFlow friendly sparse matrices defining the graph. Unfortunately there is a trade-off for the ease-of-use of Spektral, and this comes in the form of noticeably slower training speeds for most tasks compared to the other major libraries DGL and PyTorch Geometric.\\nSpektral has significant adoption and it may be an appealing option should you want to build graph models with TensorFlow. It’s likely to be better supported than the Graph Nets library by Deepmind, which is next on our list but for all appearances is being phased out in favor of the JAX-based Jraph. Spektral is released under the Apache 2.0 open source license and has an active issues board with pull requests being integrated on a regular basis, making this an appealing library for someone wishing to not only use a good deep learning library, but contribute to one as well.\\n\\xa0\\n3. Graph Nets\\n\\xa0\\n\\nSource\\n\\xa0\\nGraph Nets is another graph deep learning from Deepmind. Built on TensorFlow and Sonnet (another DeepMind library), it may soon be largely superseded by the JAX-based Jraph described earlier. Graph Nets requires TensorFlow 1, and as a result it feels somewhat dated despite being only about 3 years old. As of this writing, It has an impressive 737 forks and nearly 5,000 stars on GitHub, and, like most other libraries from Google/DeepMind, is licensed under Apache 2.0. Graph Nets originated the GraphsTuple data structure used by Jraph.\\nWhile Graph Nets seems to be quite popular on GitHub, it is probably a less attractive option than the other libraries on this list, unless you are working on a pre-existing code base that already makes heavy use of the library. For new projects with TensforFlow, Spektral and DGL are probably a better bet, as they’re built with more up-to-date technology and likely to continue to receive decent support for a few years.\\n\\xa0\\n2. Deep Graph Library (DGL)\\n\\xa0\\n\\nSource\\n\\xa0\\nRather than being associated with a major tech company like Microsoft’s PTGNN or Google/DeepMind’s Jraph and Graph Nets, DGL is the product of a group of deep learning enthusiasts called the\\xa0Distributed Deep Machine Learning Community. It has over 100 contributors, over 1500 commits, and over 7,000 stars on GitHub. DGL is also unique in our list for offering a flexible choice of back-end. Models can have PyTorch, TensorFlow, or MXNet running under the hood, while offering a largely similar experience to the one driving an experiment. It’s one of the longer-lived libraries still under active development on our list, with a first commit going back to April 2018. DGL was used recently to build the\\xa0SE(3) transformer, a powerful graph transformer with both rotational and translational equivariance that is suspected to be a building block or inspiration for AlphaFold2. This model, the successor to the already impressive AlphaFold, was the star behind DeepMind’s impressive, show-winning performance at the\\xa02020 CASP14\\xa0protein structure prediction challenge. That event prompted some major\\xa0news outlets\\xa0to herald AlphaFold2 as the first AI project to solve a major scientific challenge.\\nDGL is built around the neural message passing paradigm described by\\xa0Gilmer et al.\\xa0in 2017. This provides a flexible framework and it covers most types of graph layers for building graph neural networks. As you’ll notice from reading through the code repository and documentation, DGL is an expansive project. This also means there are plenty (nearly 200) open issues, a ripe opportunity for someone looking to contribute to a graph deep learning project with a big impact. DGL is used for a number of specialized applications, to the extent where several additional libraries have been built on top of it.\\xa0DGL-LifeSci\\xa0is a library built specifically for deep learning graphs as applied to chem- and bio-informatics, while\\xa0DGL-KE\\xa0is built for working with knowledge graph embeddings. Both of those bonus libraries are developed by AWS Labs.\\n\\xa0\\n1. PyTorch Geometric\\n\\xa0\\n\\nSource\\n\\xa0\\nThe library topping our list is none other than\\xa0PyTorch Geometric. PyTorch Geometric, or PyG to friends, is a mature geometric deep learning library with over 10,000 stars and 4400 commits, most of these being the output of one very prolific PhD student\\xa0rusty1s. PyG sports a very long list of implemented graph neural network layers. Not only does it run deep graph networks quite quickly, but PyG is also built for other types of geometric deep learning such as point cloud and mesh-based models.\\nPyG has a well written\\xa0tutorial introduction by example, and having been developed since 2017, it’s pretty well established and well-supported by a community of users and just over 140 contributors. Using PyG will be very familiar for anyone who has worked with PyTorch before, with the most noticeable difference being some differences in the data input. Instead of the usual forward(x) programming pattern, you’ll instead get used to using forward(batch), where batch is a data structure that contains all the information describing graph features and connections.\\nFor new projects with a free hand in choosing a library, PyTorch Geometric is pretty tough to beat.\\nFor example, here’s how the libraries compare to each other:\\n\\n\\nName\\nLicense\\nStars\\nLanguage, Flavor\\nMain Contributor(s)\\n\\n\\nGeometricFlux.jl\\nMIT\\n180\\nJulia Language, Flux.jl\\nyuehua\\n\\n\\nPyTorch GNN\\nMIT\\n206\\nPython, PyTorch\\nMicrosoft\\n\\n\\nJraph\\nApache 2.0\\n536\\nPython, JAX\\nDeepMind\\n\\n\\nSpektral\\nMIT\\n1,700\\nPython, TF2/keras\\ndanielegrattarola\\n\\n\\nGraph Nets\\nApache 2.0\\n4,800\\nPython, PyTorch\\nDeepMind\\n\\n\\nDeep Graph Library\\nApache 2.0\\n7,000\\nPython, PyTorch, TF, MxNet\\nDistributed MLC\\n\\n\\nPyTorch Geometric\\nMIT\\n10,600\\nPython, PyTorch\\nrusty1s\\n\\n\\n\\n\\xa0\\nChoosing a Deep Learning Library\\n\\xa0\\nIn many cases, your choice of a deep graph learning library will be heavily influenced by a previous choice of deep learning library made by you, your employer, or maybe your supervising professor. If you are fond of the Keras API and TensorFlow or need to retain consistent dependencies with a pre-existing code base, for example, Spektral may be the right library for you. We wouldn’t recommend starting a new project with DeepMind’s Graph Nets and TensorFlow 1, but the library does still get occasional updates and may be a reasonable choice to support legacy projects.\\nIf you’re lucky enough to be starting from scratch, you have a couple of enticing options. If you think the deliberate productivity + execution speed prioritization of the Julia Programming is the future of machine learning and scientific programming, GeometricFlux.jl is an exciting prospect. If you are intrigued by functional programming paradigms and want to retain some of the speed advantages from just-in-time compilation (like Julia) but would prefer to stick with Python, the JAX-based Jraph is an attractive option. Finally, if you want a fast, capable library at a relatively established and mature state of development, it’s going to be hard to go wrong with PyTorch Geometric.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Build An Image Classifier in Few Lines of Code with Flash\\nFree From Stanford: Machine Learning with Graphs\\nA Graph-based Text Similarity Method with Named Entity Information in NLP',\n",
       " \"By Derrick Mwiti, Data Scientist.\\ncomments\\nAs an ML engineer or data scientist, sometimes you inevitably find yourself in a situation where you have hundreds of records for one class label and thousands of records for another class label.\\nUpon training your model you obtain an accuracy above 90%. You then realize that the model is predicting everything as if it’s in the class with the majority of records. Excellent examples of this are fraud detection problems and churn prediction problems, where the majority of the records are in the negative class. What do you do in such a scenario? That will be the focus of this post.\\n\\xa0\\nCollect More Data\\n\\xa0\\nThe most straightforward and obvious thing to do is to collect more data, especially data points on the minority class. This will obviously improve the performance of the model. However, this is not always possible. Apart from the cost one would have to incur, sometimes it's not feasible to collect more data. For example, in the case of churn prediction and fraud detection, you can’t just wait for more incidences to occur so that you can collect more data.\\n\\xa0\\nConsider Metrics Other than Accuracy\\n\\xa0\\nAccuracy is not a good way to measure the performance of a model where the class labels are imbalanced. In this case, it's prudent to consider other metrics such as precision, recall, Area Under the Curve (AUC) — just to mention a few.\\nPrecision\\xa0measures the ratio of the true positives among all the samples that were predicted as true positives and false positives. For example, out of the number of people our model predicted would churn, how many actually churned?\\n\\xa0\\n\\n\\xa0\\nRecall\\xa0measures the ratio of the true positives from the sum of the true positives and the false negatives. For example, the percentage of people who churned that our model predicted would churn.\\n\\xa0\\n\\n\\xa0\\nThe AUC is obtained from the Receiver Operating Characteristics (ROC) curve. The curve is obtained by plotting the true positive rate against the false positive rate. The false positive rate is obtained by dividing the false positives by the sum of the false positives and the true negatives.\\nAUC closer to one is better, since it indicates that the model is able to find the true positives.\\n\\xa0\\nEmphasize the Minority Class\\n\\xa0\\nAnother way to deal with imbalanced data is to have your model focus on the minority class. This can be done by computing the class weights. The model will focus on the class with a higher weight. Eventually, the model will be able to learn equally from both classes. The weights can be computed with the help of scikit-learn.\\n\\nfrom sklearn.utils.class_weight import compute_class_weight\\r\\nweights = compute_class_weight(‘balanced’, y.unique(), y)\\r\\narray([ 0.51722354, 15.01501502])\\n\\n\\nYou can then pass these weights when training the model. For example, in the case of logistic regression:\\n\\nclass_weights = {\\r\\n 0:0.51722354,\\r\\n 1:15.01501502\\r\\n}lr = LogisticRegression(C=3.0, fit_intercept=True, warm_start = True, class_weight=class_weights)\\n\\n\\nAlternatively, you can pass the class weights as\\xa0balanced\\xa0and the weights will be automatically adjusted.\\n\\nlr = LogisticRegression(C=3.0, fit_intercept=True, warm_start = True, class_weight=’balanced’)\\n\\n\\nHere’s the ROC curve before the weights are adjusted.\\n\\n\\xa0\\n\\n\\xa0\\nAnd here’s the ROC curve after the weights have been adjusted. Note the AUC moved from 0.69 to 0.87.\\n\\xa0\\n\\n\\xa0\\nTry Different Algorithms\\n\\xa0\\nAs you focus on the right metrics for imbalanced data, you can also try out different algorithms. Generally, tree-based algorithms perform better on imbalanced data. Furthermore, some algorithms such as\\xa0LightGBM\\xa0have hyperparameters that can be tuned to indicate that the data is not balanced.\\n\\xa0\\nGenerate Synthetic Data\\n\\xa0\\nYou can also generate\\xa0synthetic data\\xa0to increase the number of records in the minority class — usually known as oversampling. This is usually done on the training set after doing the train test split. In Python, this can be done using the\\xa0Imblearn\\xa0package. One of the strategies that can be implemented from the package is known as the\\xa0Synthetic Minority Over-sampling Technique (SMOTE). The technique is based on k-nearest neighbors.\\nWhen using SMOTE:\\n\\nThe first parameter is a\\xa0float\\xa0that indicates the ratio of the number of samples in the minority class to the number of samples in the majority class, once resampling has been done.\\nThe number of neighbors to be used to generate the synthetic samples can be specified via the\\xa0k_neighbors\\xa0parameter.\\n\\n\\nfrom imblearn.over_sampling import SMOTEsmote = SMOTE(0.8)X_resampled,y_resampled = smote.fit_resample(X.values,y.values)pd.Series(y_resampled).value_counts()0    9667\\r\\n1    7733 \\r\\ndtype: int64\\n\\n\\nYou can then fit your resampled data to your model.\\n\\nmodel = LogisticRegression()model.fit(X_resampled,y_resampled)predictions = model.predict(X_test)\\n\\n\\n\\xa0\\nUndersample the Majority Class\\n\\xa0\\nYou can also experiment on reducing the number of samples in the majority class. One such strategy that can be implemented is the\\xa0NearMiss\\xa0method. You can also specify the ratio just like in SMOTE, as well as the number of neighbors via\\xa0n_neighbors.\\n\\nfrom imblearn.under_sampling import NearMissunderSample = NearMiss(0.3,random_state=1545)pd.Series(y_resampled).value_counts()0  1110 1  333 dtype: int64\\n\\n\\n\\xa0\\nFinal Thoughts\\n\\xa0\\nOther techniques that can be used include using building\\xa0an ensemble\\xa0of weak learners to create a strong classifier. Metrics such as precision-recall curve and area under curve (PR, AUC) are also worth trying when the positive class is the most important.\\nAs always, you should experiment with different techniques and settle on the ones that give you the best results for your specific problems. Hopefully, this piece has given some insights on how to get started.\\nCode available here.\\n\\xa0\\nBio: Derrick Mwiti is a data scientist who has a great passion for sharing knowledge. He is an avid contributor to the data science community via blogs such as Heartbeat, Towards Data Science, Datacamp, Neptune AI, KDnuggets just to mention a few. His content has been viewed over a million times on the internet. Derrick is also an author and online instructor. He also trains and works with various institutions to implement data science solutions as well as to upskill their staff. Derrick’s studied Mathematics and Computer Science from the Multimedia University, he also is an alumnus of the Meltwater Entrepreneurial School of Technology. If the world of Data Science, Machine Learning, and Deep Learning interest you, you might want to check his Complete Data Science & Machine Learning Bootcamp in Python course.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to fix an Unbalanced Dataset\\nThe 5 Most Useful Techniques to Handle Imbalanced Datasets\\nPro Tips: How to deal with Class Imbalance and Missing Labels\",\n",
       " 'comments\\nBy Frederik Bussler, Growth at Apteo.\\n\\nBy author.\\nWe’ve all heard of Kaggle, but that also means there’s more competition — recently, Kaggle reached 5 million users. Further, not all competitions are open to everyone in the world. Here’s the policy of one competition, for instance:\\n\\n“Members of the Kaggle community who are not United States Citizens or legal permanent residents at the time of entry are allowed to participate in the Competition but\\xa0are not eligible to win prizes. If a team has one or more members who are not prize eligible, then the entire team is not prize eligible.”\\n\\nBy trying out other competition platforms, you can be a “big fish in a small pond,” as there are a lot fewer competitors.\\nKeep in mind that AI competitions aren’t the end-all-be-all if you want to enter the industry, as you’ll need knowledge in statistics, computing, communication, and more — not just knowing how to build models.\\nBesides ranking in competitions, you’ll want to work on practical projects that you can share with the world. Ideally, your projects can resonate with non-technical audiences as well, such as hiring managers who often don’t understand the intricacies of the field. To do so, you can use no-code analytics tools like\\xa0Apteo\\xa0that let you share very simple and easy-to-understand analyses.\\nThat being said, let’s dive in.\\n\\xa0\\n1.\\xa0DrivenData\\n\\xa0\\nDrivenData is open to everyone around the world (with the exception of OFAC sanctioned countries like Cuba, Iran, Iraq, North Korea, Sudan, and Syria).\\n\\nDrivenData Eligibility rules. Captured by author.\\nCurrent competitions include Alzheimer’s research, hateful meme detection, predicting flu vaccines, and more.\\n\\xa0\\n2.\\xa0CROWDANALYTIX\\n\\xa0\\nCROWDANALYTIX features competitions like cement quality forecasting, with participants from around the world. Currently, competitions are slowing down, but there’s also a great\\xa0blog\\xa0to learn more about data science topics in the meantime.\\nCheck out\\xa0Damian Boh’s experience working on a CrowdANALYTIX competition:\\xa0How I Won Top Five in a Deep Learning Competition\\n\\xa0\\n3.\\xa0Signate\\n\\xa0\\n\\nPhoto by\\xa0Louie Martinez\\xa0on\\xa0Unsplash.\\nSignate is basically Japan’s Kaggle and has current competitions about vehicle driving image recognition, flattening the curve, and more.\\n\\xa0\\n4.\\xa0Zindi\\n\\xa0\\nZindi is a pan-African data science competition platform with challenges including African language NLP, insurance recommendations, a mental health chatbot, and more.\\nGIZ AI4D Africa Language Challenge - Round 2\\n\\xa0\\n5.\\xa0Alibaba Cloud Tianchi\\n\\xa0\\nAlibaba’s Tianchi platform boasts 400,000 data scientists on its platform, working on challenges like image-based 3D shape retrieval, 3D object reconstruction, and instance segmentation.\\n\\xa0\\n6.\\xa0Analytics Vidhya\\n\\xa0\\nAnalytics Vidhya, besides being a great data science resource, is India’s go-to data science competition platform, with current challenges including loan prediction, sales prediction, times series forecasting, recommendation engines, and more.\\n\\xa0\\n7.\\xa0CodaLab\\n\\xa0\\nCodaLab is a French-based data science competition platform popular in Europe, created as a joint venture between Microsoft and Stanford University in 2013.\\nToday, CodaLab boasts 40,000 users and 1,000 competitions, including an on-going\\xa0robotics vision challenge.\\n\\xa0\\nConclusion\\n\\xa0\\nYou can’t win Kaggle prizes as a non-American, but there are fortunately many regional alternatives available.\\nWhile we didn’t cover regions like South or Central America — which don’t have their own local platforms — you can use truly open-access sites like DrivenData no matter where you are.\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nLessons From My First Kaggle Competition\\nWhat my first Silver Medal taught me about Text Classification and Kaggle in general?\\n3 Best Sites to Find Datasets for your Data Science Projects',\n",
       " 'By Matthew Mayo, KDnuggets.\\nData scientist may no longer be the \"sexiest\" job around, but perhaps it, along with related data roles, are still some of the most satisfying?\\nThe latest KDnuggets survey is looking to probe the data community\\'s job satisfaction, and is asking:\\n\\n\\nYour job satisfaction\\nHow many years have you been in this job?\\nAre you looking for a new job?\\nYour employment type\\nYour job title (choose closest, and ignore Senior/Junior/Principal, etc.)\\n\\n\\n\\nImage source: Open Sourced Workplace\\n\\xa0\\n\\n\\nThis poll is closed - results and analysis coming soon!\\nView the current results here.\\nThanks to everyone who has made time to participate.',\n",
       " 'comments\\nBy Philip Tannor, Co-Founder & CEO of Deepchecks.\\n\\nImage by Clker-Free-Vector-Images from Pixabay\\n\\xa0\\xa0\\nAI systems are becoming increasingly popular and central in many industries. They decide who might get a loan from the bank, whether an individual should be convicted, and we may even entrust them with our lives when using systems such as autonomous vehicles in the near future. Thus, there is a growing need for mechanisms to harness and control these systems so that we may ensure that they behave as desired.\\nOne important issue that has been gaining popularity in the last few years is fairness. While usually ML models are evaluated based on metrics such as accuracy, the idea of fairness is that we must ensure that our models are unbiased with regard to attributes such as gender, race and other selected attributes.\\nA classic example of an episode regarding racial bias in AI systems, is the COMPAS software system, developed by Northpointe, which aims to assist US courts with assessing the likelihood of a defendant becoming a recidivist. Propublica published an article which claims that this system is biased against blacks, giving them higher risk ratings.\\n\\nML system bias against African Americans? (source)\\n\\xa0\\nIn this post we will try to understand where biases in ML models originate, and explore methods for creating unbiased models.\\n\\xa0\\nWhere Does Bias Come From?\\n\\xa0\\n\\n\"Humans are the weakest link\"\\n—Bruce Schneier\\n\\n\\xa0\\nIn the field of Cybersecurity it is often said that “humans are the weakest link” (Schneier). This idea applies in our case as well. Biases are in fact introduced into ML models by humans unintentionally.\\nRemember, an ML model can only be as good as the data it’s trained on, and thus if the training data contains biases, we can expect our model to mimic those same biases. Some representative examples for this can be found in the field of word embeddings in NLP. Word embeddings are learned dense vector representations of words, that are meant to capture semantic information of a word, which can then be fed to ML models for different downstream tasks. Thus, embeddings of words with similar meanings are expected to be “close” to each other.\\n\\nWord embeddings can capture the semantic meaning of words. (source)\\n\\xa0\\nIt turns out that the embedded space can be used to extract relations between words, and to find analogies as well. A classic example for this is the well known king-man+woman=queen equation. However, if we substitute the word “doctor” for the word “king” we get “nurse” as the female equivalent of the “doctor”. This undesired result simply reflects existing gender biases in our society and history. If in most available texts doctors are generally male and nurses are generally female, that’s what our model will understand.\\n\\ndoctor = nlp.vocab[\\'doctor\\']\\r\\nman = nlp.vocab[\\'man\\']\\r\\nwoman = nlp.vocab[\\'woman\\']\\r\\nresult = doctor.vector - man.vector + woman.vector\\r\\nprint(most_similar(result))\\r\\n \\r\\nOutput: nurse\\n\\nCode example: man is to doctor as woman is to nurse according to gensim word2vec (source)\\n\\xa0\\nCulture Specific Tendencies\\n\\xa0\\nCurrently, the most used language on the internet is English. Much of the research and products in the field of Data Science and ML is done in English as well. Thus, many of the “natural” datasets that are used to create huge language models tend to match American thought and culture, and may be biased towards other nationalities and cultures.\\n\\nCultural bias: GPT-2 needs active steering in order to produce a positive paragraph with the given prompt. (source)\\n\\xa0\\nSynthetic Datasets\\n\\xa0\\nSome biases in the data may be created unintentionally in the process of the dataset’s construction. During construction and evaluation people are more likely to notice and pay attention to details they are familiar with. A well known example for an image classification mistake, is when Google Photos misclassified black people as gorillas. While a single misclassification of this sort may not have a strong impact on the overall evaluation metrics, it is a sensitive issue, and could have a large impact on the product and the way customers relate to it.\\xa0\\n\\nRacist AI algorithm? Misclassification of black people as gorillas. (source)\\n\\xa0\\nIn conclusion, no dataset is perfect. Whether a dataset is handcrafted or “natural”, it is likely to reflect the biases of it’s creators, and thus the resulting model will contain the same biases as well.\\n\\xa0\\n\\xa0\\nCreating fair ML models\\n\\xa0\\nThere are multiple proposed methods for creating fair ML models, which generally fall into one of the following stages.\\n\\xa0\\nPreprocessing\\n\\xa0\\nA naive approach to creating ML models that are unbiased with respect to sensitive attributes is to simply remove these attributes from the data, so that the model cannot use them for its prediction. However, it is not always straightforward to divide attributes into clear cut categories. For example, a person’s name may be correlated with their gender or ethnicity, nevertheless we would not necessarily want to regard this attribute as sensitive. More sophisticated approaches attempt to use dimensionality reduction techniques in order to eliminate sensitive attributes.\\n\\xa0\\n\\xa0\\nAt Training Time\\n\\xa0\\nAn elegant method for creating unbiased ML models is using adversarial debiasing. In this method we simultaneously train two models. The adversary model is trained to predict the protected attributes given the predictors prediction or hidden representation. The predictor is trained to succeed on the original task while making the adversary fail, thus minimizing the bias.\\n\\nAdversarial debiasing illustration: The predictor loss function consists of two terms, the predictor loss, and the adversarial loss. (source)\\n\\xa0\\nThis method can achieve great results for debiasing models without having to “throw away” the input data, however, it may suffer from difficulties that arise in general when training adversarial networks.\\n\\xa0\\n\\xa0\\nPost Processing\\n\\xa0\\nIn the post processing stage we get the model’s predictions as probabilities, but we can still choose how to act based on these outputs, for example we can move the decision threshold for different groups in order to meet our fairness requirements.\\nOne way to ensure model fairness in the post processing stage is to look at the intersection of the area under the ROC curve for all groups. The intersection represents TPRs and FPRs that can be achieved for all classes simultaneously. Note that in order to satisfy the desired result of\\xa0 equal TPRs and FPRs for all classes one might need to purposefully choose to get less good results on some of the classes.\\n\\nThe colored region is what’s achievable while fulfilling the separability criterion for fairness. (source)\\n\\xa0\\nAnother method for debiasing a model in the post processing stage involves calibrating the predictions for each class independently. Calibration is a method for ensuring that the probability outputs of a classification model indeed reflect the matching ratio of positive labels. Formally, a classification model is calibrated if for each value of r:\\n\\nWhen a model is properly calibrated error rates will be similar across the different values of protected attributes.\\n\\xa0\\n\\xa0\\nConclusion\\n\\xa0\\nTo sum it up, we have discussed the concepts of bias and fairness in the ML world, we have seen that model biases often reflect existing biases in society. There are various ways in which we could enforce and test for fairness in our models, and hopefully, using these methods will lead to more just decision making in AI assisted systems around the world.\\n\\xa0\\nFurther Reading\\n\\xa0\\nGender bias in word embeddings\\nPropublica article\\nAlekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, & Hanna Wallach. (2018). A Reductions Approach to Fair Classification.\\nBrian Hu Zhang, Blake Lemoine, & Margaret Mitchell. (2018). Mitigating Unwanted Biases with Adversarial Learning.\\nSolon Barocas, Moritz Hardt, & Arvind Narayanan (2019). Fairness and Machine Learning. fairmlbook.org.\\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, & Aram Galstyan. (2019). A Survey on Bias and Fairness in Machine Learning.\\n\\xa0\\nBio: Philip Tannor is Co-Founder & CEO of Deepchecks.\\nRelated:\\n\\nData Protection Techniques Needed to Guarantee Privacy\\nWhat Makes AI Trustworthy?\\nEthics, Fairness, and Bias in AI',\n",
       " 'By Devin Partida, Editor-in-Chief of ReHack.com.\\ncomments\\n\\nPhoto by Pixabay from Pexels\\n\\xa0\\nData security is a growing concern for any company as cybercrime continues to prosper. Since data scientists’ entire work revolves around potentially sensitive data, they face more pressure than most. If you suffer a data breach, you may be unable to perform your job, in addition to potential financial and reputational losses.\\nIBM estimates that a data breach costs $3.86 million on average. Depending on the severity of the incident, it could also cost your reputation or even your job. With that in mind, here are seven data security best practices to adopt this year.\\n\\xa0\\n1. Use Only What You Need\\n\\xa0\\nOne of the best ways to secure your data is to minimize what you store. While it’s tempting to collect as much data as possible, especially when training machine learning models, this makes you more vulnerable. You can only lose what you have, so go through your databases and get rid of anything that isn’t necessary.\\nPart of this principle is keeping an updated record of the data you have on hand. If you get to a point where you no longer need some information, purge it from your database. Holding on to legacy data doesn’t help you and only means you have more to lose.\\n\\xa0\\n2. Mask Sensitive Data\\n\\xa0\\nThe “use only what you need” principle applies to the type of data you store, too. Many data science operations don’t require user-specific information, so don’t store it if you don’t need it. If you must use sensitive data like personal identifiers, you should mask it.\\nOne of the most common ways to mask sensitive data is to use a substitution cipher. Tokenization, which substitutes real values with dummy data, is another option and generally safer, as it places the encrypted values in a separate database. No matter which method you use, make sure you scrub your data of all sensitive info that isn’t necessary first.\\n\\xa0\\n3. Collaborate Carefully\\n\\xa0\\nData science is often a collaborative process, and you should think about how you communicate with collaborators. While email may be convenient, it isn’t encrypted by default, so it’s unsuitable for sharing data or credentials to access databases. There are many available services made specifically for sensitive file-sharing, so these are a better option.\\nYou should also keep trust to a minimum, no matter who you’re working with. People should only be able to access what’s critical for their job. You may even consider obfuscating information before sharing it, if possible, to mitigate the impact of any potential breaches.\\n\\xa0\\n4. Encrypt as Much as Possible\\n\\xa0\\nWhen you do share data, you should encrypt it. You should also encrypt your data when it’s sitting in your database. While encryption isn’t a cure-all for all your security concerns, it’s a low-cost way to add another layer of protection.\\nMany of the best data encryption tools today won’t slow your processes much, either. Look through your options to find something that can encrypt your data at rest and in motion in all scenarios. While this won’t necessarily stop breaches from happening, it will mitigate their cost.\\n\\xa0\\n5. Secure More Than Your Databases\\n\\xa0\\nRemember that security applies to more than just where you store your data. Your databases should be the area you pay the most attention to, but they shouldn’t be your only concern. Backups, connected applications, and analytics servers can all serve as backdoors to your data, so they need protection too.\\nAny program, drive, or file that touches your data should be secure. As you work on this, it’s easier when your data has fewer connections. Minimizing the things that have access to your databases makes your job easier and offers more protection.\\n\\xa0\\n6. Take Care With Third-Party Cloud Vendors\\n\\xa0\\nIf you use a third-party cloud like AWS, be careful not to become complacent with security. Unfortunately, many users do, as a recent study revealed that 82% of companies give these vendors highly privileged access. Third-party clouds are not inherently risky, but you do need to take security into your own hands.\\nCheck your permissions to make sure you grant the least privilege to your vendor and other applications. Use strong credentials, including multi-factor authentication, and rotate these regularly. If you don’t know what to do, many of these vendors provide security best practices you can reference.\\n\\xa0\\n7. Establish a Clear Governance Policy\\n\\xa0\\nFinally, you should establish a clear and specific governance policy for your whole team. Having a written document of what people should and shouldn’t do will help ensure safe user behavior. If someone makes a mistake that jeopardizes security, you can refer to the policy to see what went wrong.\\nYour governance policy should define everybody’s role in security. You may have a rotating schedule for who monitors and documents incoming and outgoing data. You may give everyone a static role. Whatever you do, make it specific and clear, and ensure everyone understands it.\\n\\xa0\\nData Science Security Must Improve\\n\\xa0\\nData science is playing an increasingly central role in business today. As this trend continues, your work becomes a more valuable target for cybercriminals. Data science teams must embrace security in light of these rising threats.\\nStart with these best practices, then look for other, smaller areas where you can increase security. When your data is secure, you can work with confidence and impress potential clients.\\n\\xa0\\nBio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com.\\nRelated:\\n\\nHow Automation Is Improving the Role of Data Scientists\\nData Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions\\nHow Data Professionals Can Add More Variation to Their Resumes',\n",
       " \"By Benjamin Obi Tayo, Ph.D., DataScienceHub.\\ncomments\\n\\nImage by Benjamin O. Tayo.\\nThe scatter pairplot is a visualization of pairwise relationships in a dataset and is the first step for effective feature selection. It provides a qualitative analysis of the pairwise correlation between features and is a powerful tool for feature selection and dimensionality reduction. For an introduction of the pairplot using the seaborn package, see this link: https://seaborn.pydata.org/generated/seaborn.pairplot.html\\nIn this article, we will analyze a portfolio of stocks to examine the ones that are strongly correlated to the overall market. The portfolio contains 22 stocks (see Table 1) from different sectors such as Healthcare, Real Estate, Consumer Discretionary, Energy, Industrials, Telecommunication Services, Information Technology, Consumer Staples, and Financials.\\n\\xa0\\n\\n\\n\\nSymbol\\nName\\nSymbol\\nName\\nSymbol\\nName\\n\\n\\nAAL\\nAmerican Airlines\\nEDIT\\nEditas Medicine\\nUAL\\nUnited Airlines\\n\\n\\nAAPL\\nApple\\nHPP\\nHudson Pacific Properties\\nWEN\\nWendy\\n\\n\\nABT\\nAbbott Laboratories\\nJNJ\\nJohnson & Johnson\\nWFC\\nWells Fargo\\n\\n\\nBNTX\\nBioNTech\\nMRNA\\nModerna\\nWMT\\nWalmart\\n\\n\\nBXP\\nBoston Properties\\nMRO\\nMarathon Oil Corporation\\nXOM\\nExxon Mobile\\n\\n\\nCCL\\nCarnival Corporation\\nPFE\\nPfizer\\nSP500\\nStock Market Index\\n\\n\\nDAL\\nDelta Airlines\\nSLG\\nSL Green Realty\\n\\n\\n\\n\\nDVN\\nDevon Energy\\nTSLA\\nTesla\\n\\n\\n\\n\\n\\nTable 1. Portfolio of 22 stocks from diverse sectors.\\nOur goal is to answer the question: which stocks in the portfolio correlate strongly with the stock market? We will use the S&P 500 index as a measure of the total stock market. We will assume a threshold correlation coefficient of 70% for a stock to be considered to be strongly correlated to the S&P 500.\\n\\xa0\\nData Collection and Processing\\n\\xa0\\nRaw data were obtained from the yahoo finance website: https://finance.yahoo.com/\\nThe historical data for each stock has information on daily open price, high price, low price, and closing price. The CSV file was downloaded for each stock, and then the column “close” was extracted and combined to create the dataset, which can be found here: portfolio.csv\\n\\xa0\\nGenerate Scatter Pairplot\\n\\xa0\\n\\nimport numpy as np\\r\\nimport pandas as pd\\r\\nimport pylab\\r\\nimport matplotlib.pyplot as plt\\r\\nimport seaborn as sns\\r\\n\\r\\nurl = 'https://raw.githubusercontent.com/bot13956/datasets/master/portfolio.csv'\\r\\ndata = pd.read_csv(url)\\r\\ndata.head()\\r\\n\\r\\ncols = data.columns[1:24]\\r\\nsns.pairplot(data[cols], height=2.0)\\r\\n\\r\\n\\r\\n\\n\\n\\xa0\\nCalculate Covariance Matrix\\n\\xa0\\nThe scatter pairplot is the first step, which provides a qualitative analysis of pairwise correlations between features. To quantify the degree of correlation, the covariance matrix has to be computed.\\n\\nfrom sklearn.preprocessing import StandardScaler\\r\\nstdsc = StandardScaler()\\r\\nX_std = stdsc.fit_transform(data[cols].iloc[:,range(0,23)].values)\\r\\n\\r\\ncov_mat = np.cov(X_std.T, bias= True)\\r\\n\\r\\nimport seaborn as sns\\r\\nplt.figure(figsize=(13,13))\\r\\nsns.set(font_scale=1.2)\\r\\nhm = sns.heatmap(cov_mat,\\r\\n                 cbar=True,\\r\\n                 annot=True,\\r\\n                 square=True,\\r\\n                 fmt='.2f',\\r\\n                 annot_kws={'size': 12},\\r\\n                 yticklabels=cols,\\r\\n                 xticklabels=cols)\\r\\nplt.title('Covariance matrix showing correlation coefficients')\\r\\nplt.tight_layout()\\r\\nplt.show()\\r\\n\\r\\n\\r\\n\\n\\n\\xa0\\n\\xa0\\nCompressed Output Showing Pairplots and Correlation Coefficients\\n\\xa0\\nSince we are only interested in the correlations between the 22 stocks in the portfolio with the S&P 500, Figure 1 below shows the final output from our analysis.\\n\\nFigure 1. Scatter pairplots and correlation coefficients between portfolio stocks and the S&P 500.\\nFigure 1 shows that out of the 22 stocks, 8 have a correlation coefficient less than 70%. Interestingly, except for WEN stock, all the other stocks have a positive correlation with the S&P 500 index.\\nThe full covariance matrix is shown in Figure 2.\\n\\nFigure 2. Covariance matrix visualization.\\nIn summary, we’ve shown how the scatter pairplot can be used as a first step for feature selection. Other advanced methods for feature selection and dimensionality reduction include the following: PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis).\\n\\xa0\\nRelated:\\n\\nA simple static visualization can often be the best approach\\nHow to create stunning visualizations using python from scratch\\nCreating Good Meaningful Plots: Some Principles\",\n",
       " 'comments\\nBy Matthew Przybyla, Senior Data Scientist at Favor Delivery.\\n\\nPhoto by\\xa0rigel\\xa0on\\xa0Unsplash.\\nBoth of these tools are important to not only data scientists but also to those in similar positions\\xa0like data analytics and business intelligence. With that being said, when should data scientists specifically use pandas over SQL and vice versa? In some situations, you can get away with just using SQL, and some other times, pandas is much easier to use, especially for data scientists who focus on research in a Jupyter Notebook setting. Below, I will discuss when you should use SQL and when you should use pandas. Keep in mind that both of these tools have specific use cases, but there are many times where their functionality overlap, and that is what I will be comparing below as well.\\n\\xa0\\nPandas\\n\\xa0\\n\\nPhoto by\\xa0Kalen Kemp\\xa0on\\xa0Unsplash.\\nPandas\\xa0is an open-source data analysis tool in the Python programing language.\\xa0The benefit of pandas starts when you already have your main dataset, usually from a SQL query.\\xa0This main difference can mean that the two tools are separate. However, you can also perform several of the same functions in each respective tool. For example, you can create new features from existing columns in pandas, perhaps easier and faster than in SQL.\\nIt is important to note that I am not comparing what pandas does that SQL cannot do and vice versa. I will be picking the tool that can do the function more efficiently or preferable for data science work — in my opinion, from personal experience.\\nHere are times where using pandas is more beneficial than SQL — while also having the same functionality as SQL:\\n\\ncreating calculated fields from existing features\\n\\nWhen incorporating a more complex SQL query, you often are incorporating subqueries as well in order to divide values from different columns. In pandas, you can simply divide features much easier like the following:\\n\\ndf[\"new_column\"] = df[\"first_column\"]/df[\"second_column\"]\\r\\n\\r\\n\\n\\n\\xa0\\nThe code above is showing how you can divide two separate columns and assign those values to a new column. In this case, you are performing the feature creation on the whole entire dataset or dataframe. You can use this function in both feature exploration and feature engineering in the process of data science.\\n\\ngrouping by\\n\\nAlso referring to subqueries, grouping by in SQL can become quite complex and require lines and lines of code that can be visually overwhelming. In pandas, you can simply group by one line of code.\\xa0I am not referring to the ‘group by’ at the end of a simple select from table query, but one where there are multiple subqueries involved.\\n\\ndf.groupby(by=\"first_column\").mean()\\r\\n\\r\\n\\n\\n\\xa0\\nThis result would be returning the mean of the first_column for every column in the dataframe. There are many other ways to use this grouping function, which are outlined nicely in the pandas documentation.\\n\\nchecking data types\\n\\nIn SQL, you will often have to cast types, but sometimes it can be a little clearer to see the way pandas lays out data types in a vertical format rather than scrolling through a horizontal output in SQL. You can expect\\xa0some\\xa0examples of data types returned to be int64, float64, datetime64[ns], and object.\\n\\ndf.dtypes\\r\\n\\r\\n\\n\\n\\xa0\\nWhile these are all fairly simple functions of pandas and SQL, in SQL, they are particularly tricky and sometimes just much easier to implement in a pandas dataframe. Now, let’s look at what SQL is better at performing.\\n\\xa0\\nSQL\\n\\xa0\\n\\nPhoto by\\xa0Caspar Camille Rubin\\xa0on\\xa0Unsplash.\\nSQL is probably the language that is used most by the most amount of different positions. For example, a data engineer could use SQL, a Tableau developer, or a product manager. With that being said, data scientists tend to use SQL frequently. It is important to note that there are several different versions of SQL, usually all having a similar function, just slightly formatted differently.\\nHere are times where using SQL is more beneficial than pandas — while also having the same functionality as pandas:\\n\\nWHERE clause\\n\\nThis clause in SQL is used frequently and can also be performed in pandas. In pandas, however, it is slightly more difficult or less intuitive. For example, you have to write out redundant code, whereas, in SQL, you simply need the\\xa0WHERE.\\n\\nSELECT id\\r\\nFROM table\\r\\n   WHERE id > 100\\r\\n\\r\\n\\n\\n\\xa0\\nIn pandas, it would be something like:\\n\\ndf[df[\"id\"] > 100][\"id\"]\\r\\n\\r\\n\\n\\n\\xa0\\nYes, both are simple, but SQL is just a little more intuitive.\\n\\nJOINS\\n\\nPandas has a few ways to join, which can be a little overwhelming, whereas in SQL, you can perform simple joins like the following:\\xa0INNER, LEFT, RIGHT.\\n\\nSELECT\\r\\n   one.column_A,\\r\\n   two.column_B\\r\\nFROM first_table one\\r\\n   INNER JOIN second_table two ON two.id = one.id\\r\\n\\r\\n\\n\\n\\xa0\\nIn this code, joining is slightly easier to read than in pandas, where you have to merge dataframes, and especially as you merge more than two dataframes, it can be quite complex in pandas. SQL can perform multiple joins, whether it be INNER, etc., all in the same query.\\nAll of these examples, whether it be SQL or pandas, can be used in at least the exploratory data analysis portion of the data science process, as well as in feature engineering, and querying model results once they are stored in a database.\\n\\xa0\\nSummary\\n\\xa0\\nThis comparison of pandas versus SQL is more of a personal preference. With that being said, you may feel the opposite of my opinion. However, I hope it still sheds light on the differences between pandas and SQL, as well as what you can perform the same in both tools, using slightly different coding techniques and a different language altogether.\\nTo summarize, we have compared the benefits of using pandas over SQL and vice versa for a few of their shared functions:\\n\\n\\xa0creating calculated fields from existing features\\ngrouping by\\nchecking data types\\nWHERE clause\\nJOINS\\n\\n\\xa0\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nEvery Complex DataFrame Manipulation, Explained & Visualized Intuitively\\nData Preparation in SQL, with Cheat Sheet!\\nIntroduction to Pandas for Data Science',\n",
       " 'comments\\nBy Rebecca Vickery, Data Scientist\\n\\n\\nPhoto by\\xa0Daniel Schludi\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nStatistics is a fundamental skill that data scientists use every day. It is the branch of mathematics that allows us to collect, describe, interpret, visualise, and make inferences about data. Data scientists will use it for data analysis, experiment design, and statistical modelling.\\nStatistics is also essential for machine learning. We will\\xa0use statistics to understand the data prior to training a model. When we take samples of data for training and testing our models we need to employ statistical techniques to ensure fairness. When evaluating the performance of a model we need statistics to assess the variability of the predictions and assess accuracy.\\n\\n“If statistics are boring, you’ve got the wrong numbers.”, Edward Tufte\\n\\nThese are just some of the ways in which statistics are employed by data scientists. If you are studying data science it is therefore essential to develop a good understanding of these statistical techniques.\\nThis is one area where books can be a particularly useful study tool as detailed explanations of statistical concepts is essential to your understanding.\\nHere are my top 5 free books for learning statistics for data science.\\n\\xa0\\nPractical Statistics for Data Scientists\\n\\xa0\\nby Peter Bruce and Andrew Bruce\\n\\n\\nImage:\\xa0amazon.co.uk\\n\\n\\nRead for free here.\\nMain topics covered:\\n\\nData structures.\\nDescriptive statistics.\\nProbability.\\nMachine learning.\\n\\nSuitable for: Complete beginners.\\nStatistics is a very broad field, and only part of it is relevant to data science. This book is extremely good at only covering the areas related to data science. So if you are looking for a book that will quickly give you just enough understanding to be able to practice data science then this book is definitely the one to choose.\\nIt is filled with a lot of practical coded examples (written in R), gives very clear explanations for any statistical terms used and also links out to other resources for further reading.\\nThis is overall an excellent book to cover off the basics and is suitable for an absolute beginner to the field.\\n\\xa0\\nThink Stats\\n\\xa0\\nby Allen B. Downey\\n\\n\\nImage:\\xa0greenteapress.com\\n\\n\\xa0\\nRead for free here.\\nMain topics covered:\\n\\nStatistical thinking.\\nDistributions.\\nHypothesis testing.\\nCorrelation.\\n\\nSuitable for: Beginners with basic Python.\\nThe introduction for this book states that “this book is about turning knowledge into data” and it does a very good job of introducing statistical concepts through practical examples of data analysis.\\n\\n“this book is about turning knowledge into data”\\n\\nIt is another book that covers only the concepts directly related to data science and also contains lots of code examples, this time written in Python. It is aimed heavily at programmers and relies on using that skill to understand the key statistical concepts introduced. This book is therefore ideally suited to those who already have at least a basic grasp of Python.\\n\\xa0\\nBayesian Methods for Hackers\\n\\xa0\\nby Cameron Davidson-Pilon\\n\\n\\nImage:\\xa0amazon.com\\n\\n\\xa0\\nRead for free here.\\nMain topics covered:\\n\\nBayesian inference.\\nLoss functions.\\nBayesian machine learning.\\nPriors.\\n\\nSuitable for: Non-statisticians with a working knowledge of Python.\\nBayesian inference is a branch of statistics that deals with understanding uncertainty. As a data scientist uncertainty is something you will need to model on a very regular basis. If you are building a machine learning model, for example, you will need to be able to understand the uncertainty around the predictions that your model is delivering.\\nBayesian methods can be quite abstract and difficult to understand. This book aimed firmly at programmers (so some Python is a prerequisite), is the only material I have found that explains these concepts in a simple enough way for a non-statistician to understand. There are coded examples throughout and the Github repository, where the chapters are hosted, contains a large selection of notebooks. It is, therefore, an excellent hands-on introduction to this subject.\\n\\xa0\\nStatistics in Plain English\\n\\xa0\\nby Timothy C. Urdan\\n\\n\\nImage:\\xa0amazon.co.uk\\n\\n\\xa0\\nRead for free here.\\nMain topics covered:\\n\\nRegression.\\nDistributions.\\nFactor analysis.\\nProbability.\\n\\nSuitable for: Non-statisticians with any level of programming experience.\\nThis book covers general statistical techniques rather than just those aimed at data scientists or programmers. It is however written in a very straight forward style and covers a wide range and depth of statistical concepts in a very simple to understand way.\\nThe book was originally written for students studying a non-mathematics based course where an understanding of statistics is required, such as the social sciences. It, therefore, covers enough theory to understand the techniques but doesn’t assume an existing mathematical background. It is, therefore, an ideal book to read if you are coming into data science without a math-based degree.\\n\\xa0\\nComputer Age Statistical Inference\\n\\xa0\\nby Bradley Efron and Trevor Hastie\\n\\n\\nImage:\\xa0amazon.co.uk\\n\\n\\xa0\\nRead for free here.\\nMain topics covered:\\n\\nBayesian and frequentist inference.\\nLarge scale hypothesis testing.\\nMachine learning.\\nDeep learning.\\n\\nSuitable for: Someone with a basic understanding of statistics and statistical notation. No programming required.\\nThis book covers the theory behind most of the popular machine learning algorithms used by data scientists today. It also gives a thorough introduction to both Bayesian and Frequentist statistical inference methodologies.\\nThe second half of the book, which covers machine learning algorithms, is some of the best material I have seen on this subject. Each explanation is in-depth and uses practical examples such as the classification of spam data which makes quite complex ideas easier to digest. The book is most suited to those who have already covered the basics of statistics for data analysis and are familiar with some statistical notation.\\n\\xa0\\nThe books I included in this article cover enough topics for a complete beginner to learn all the statistics needed for data science. They can all be read for free online but most also have a print version that can be purchased if you prefer to read physical books. Statistics is an essential component of the data science toolset and something which often requires in-depth reading to truly understand the concepts. Something which these books can provide.\\nFor more data science reading lists please check out my previous articles below.\\n5 Free Books for Learning Python for Data Science\\nA completely free reading list for learning Python\\n\\xa0\\nCompletely Free Machine Learning Reading List\\n10 free books to read if you are studying machine learning\\n\\xa0\\nReading List for Applied AI\\nSix books to read if you are applying AI to your business in 2020\\n\\xa0\\nThanks for reading!\\nI send out a monthly newsletter if you would like to join please sign up via this link. Looking forward to being part of your learning journey!\\n\\xa0\\nBio: Rebecca Vickery is learning data science through self study. Data Scientist @ Holiday Extras. Co-Founder of alGo.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nThe Best Free Data Science eBooks: 2020 Update\\nTop 5 Free Machine Learning and Deep Learning eBooks Everyone should read\\nMathematics for Machine Learning: The Free eBook',\n",
       " 'By Terence Shin, Data Scientist | MSc Analytics & MBA student.\\ncomments\\n\\xa0\\n\\nImage created by Author.\\nI just wanted to start off by saying that this is heavily inspired by Jeff Hale’s articles that he wrote back in 2018/2019. I’m writing this simply because I wanted to get a more up-to-date analysis of what skills are in demand today, and I’m sharing this because I’m assuming that there are people out there that also want to see an updated version of the most in-demand skills for data scientists in 2021.\\nTake what you want from this analysis — it’s obvious that the insights gathered from web scraping job postings do not offer a perfect correlation to what data science skills are actually most demanded. However, I think this gives a good indication of what general skills you should focus more on, and likewise, stray away from.\\nWith that said, I hope you enjoy this, and let’s dive into it!\\n\\xa0\\nMethodology\\n\\xa0\\nFor this analysis, I webscraped and accumulated over 15,000 job postings from Indeed, Monster, and SimplyHired. I didn’t webscrape LinkedIn because I ran into Captcha issues trying to scrape it.\\nI then checked to see how many job postings included each term that I was searching. The list of terms that I was searching was as follows (if you want to see any other skills, please mention it in the comments so I can add it for next year’s analysis!):\\n\\nPython, SQL, R, Java, Git, C, MATLAB, Excel, C++, JavaScript, C#, Julia, Scala, SAS\\nScikit-learn, Pandas, NumPy, SciPy\\nMatplotlib, Looker, Tableau\\nTensorFlow, PyTorch, Keras\\nSpark, Hadoop, AWS, GCP, Hive, Azure, Google Cloud, MongoDB, BigQuery\\nDocker, Kubernetes, Airflow\\nNoSQL, MySQL, PostgreSQL\\nCaffe, Alteryx, Perl, Cassandra, Linux\\n\\nAfter getting the counts from each source, I summed them up and then divided it over the total number of data scientist job postings to get a percentage. For example, Python’s value of 0.77 means that 77% of the job postings had Python in it.\\nFinally, I compared the results to the analysis done by Jeff Hale in 2019 to get the percentage change from 2019 to 2021.\\n\\xa0\\nResults\\n\\xa0\\nTop Skills\\nBelow are the top 25 most in-demand data science skills in 2021, ranked from highest to lowest:\\n\\nImage created by Author.\\nTop Programming Languages\\nTo get a more granular look, the chart below shows the top programming languages for data scientists:\\n\\nImage created by Author.\\nIt’s no surprise that Python, SQL, and R are the top three programming languages.\\nPersonally, I also stand by the fact that you should know either Python or R as well as SQL. I started with Python, and I’ll probably stick with Python for the rest of my life. It’s so far ahead in terms of open source contributions, and it’s straightforward to learn. SQL is arguably the most important skill to learn across any\\xa0type of data-related profession, whether you’re a data scientist, data engineer, data analyst, business analyst, the list goes on.\\nTop Python Libraries\\nSimilarly, the chart below shows the top Python Libraries for Data Scientists:\\n\\nImage created by Author.\\nTensorFlow ranks first, as it is one of the most popular libraries of Python for deep learning. PyTorch is a strong alternative, hence its ranking not too far behind.\\nScikit-learn is arguably the most important library in Python for machine learning. After cleaning and manipulating your data with Pandas and/or NumPy, scikit-learn is used to build machine learning models as it has tons of tools used for predictive modelling and analysis.\\nIn my opinion, Pandas, NumPy, and SciPy are also essential for data scientists despite their representation above.\\nFastest Growing and Declining Skills\\nThe charts below show the fastest growing and declining skills from 2019 to 2021:\\n\\nImage created by Author.\\n\\nImage created by Author.\\nHere are a few takeaways from the two charts above:\\n\\nThere is a huge increase in skills related to the cloud, like AWS and GCP.\\nSimilarly, there is also a large increase in skills related to deep learning, like PyTorch and TensorFlow.\\nSQL and Python continue to grow in importance, while R remains stagnant.\\nApache products, like Hadoop, Hive, and Spark, continue to decline in importance.\\n\\n\\xa0\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\n7 Most Recommended Skills to Learn to be a Data Scientist\\n5 Supporting Skills That Can Help You Get a Data Science Job\\n9 Skills You Need to Become a Data Engineer\\nThe Most in Demand Skills for Data Scientists, by Jeff Hale, Nov 2018.',\n",
       " 'comments\\nBy Jürgen Schmidhuber (@SchmidhuberAI)\\n\\n\\xa0\\nAbstract.\\xa0In 2021, we are celebrating the 10-year anniversary of DanNet, named after my outstanding Romanian postdoc Dan Claudiu Cireșan (aka\\xa0Dan Ciresan). In 2011, DanNet was the first pure deep convolutional neural network (CNN) to win computer vision contests. For a while, it enjoyed a monopoly. From 2011 to 2012 it won every contest it entered,\\xa0winning four of them in a row (15 May 2011, 6 Aug 2011, 1 Mar 2012, 10 Sep 2012), driven by a very fast implementation based on graphics processing units (GPUs). Remarkably, already in 2011, DanNet achieved the first\\xa0superhuman performance\\xa0in a vision challenge, although compute was still 100 times more expensive than today. In July 2012, our\\xa0CVPR paper on DanNet\\xa0hit the computer vision community. The similar AlexNet (citing DanNet) joined the party in\\xa0Dec 2012. Our even much deeper\\xa0Highway Net\\xa0(May 2015) and its special case ResNet (Dec 2015) further improved performance (a ResNet is a Highway Net whose gates are always open). Today, a decade after DanNet, everybody is using fast deep CNNs for computer vision.\\nCNNs originated over 4 decades ago\\xa0[CNN1-4]. The basic CNN architecture with convolutional layers and downsampling layers is due to Kunihiko Fukushima (1979)\\xa0[CNN1, CNN1+]. In 1987, NNs with convolutions were combined by Alex Waibel\\xa0[CNN1a,b]\\xa0with weight sharing and\\xa0backpropagation, a technique from 1970\\xa0[BP1-4]\\xa0[R7]. Yann LeCun\\'s team later contributed important improvements of CNNs, especially for images, e.g.,\\xa0[CNN2]\\xa0[CNN4]\\xa0[T20]\\xa0(Sec. XVIII). The popular downsampling variant called \"max-pooling\" was introduced by Juyang Weng et al. (1993)\\xa0[CNN3]. In 2010, my own team at the Swiss AI Lab IDSIA showed\\xa0[MLP1]\\xa0that\\xa0unsupervised pre-training is not necessary to train deep NNs\\xa0(a reviewer called this a\\xa0\"wake-up call to the machine learning community\"— compare the\\xa0survey blog post\\xa0[MLP2]).\\nOne year later, our team with my postdocs Dan Cireșan & Ueli Meier and my PhD student Jonathan Masci (a fellow co-founder of\\xa0NNAISENSE) greatly sped up the training of deep CNNs. Our fast GPU-based\\xa0[GPUNN]\\xa0CNN of 1 Feb 2011\\xa0[GPUCNN1,2,6], often called \"DanNet,\" was a practical breakthrough. Published later that year at IJCAI\\xa0[GPUCNN1], it was much deeper and faster than earlier GPU-accelerated CNNs of 2006\\xa0[GPUCNN]. DanNet showed that deep CNNs worked far better than the existing state-of-the-art for recognizing objects in images\\xa0[GPUCNN2,2+,5,6].\\nOn a sunny day in Silicon Valley, at IJCNN 2011, DanNet blew away the competition and achieved the first\\xa0superhuman visual pattern recognition\\xa0in an international contest\\xa0[GPUCNN2-3,5]. Even the\\xa0New York Times\\xa0mentioned this. DanNet performed twice as good as human test subjects and three times better than the already impressive second place entry by LeCun\\'s team\\xa0[SER11]. Compare\\xa0Sec. D\\xa0&\\xa0Sec. XVIII\\xa0of\\xa0[T20].\\nSince 2011, DanNet has attracted tremendous interest from industry. Its temporary monopoly on winning computer vision competitions made it the first deep CNN to win: a\\xa0Chinese handwriting contest\\xa0(ICDAR, May 2011), a\\xa0traffic sign recognition contest\\xa0(IJCNN, Aug 2011), an\\xa0image segmentation contest\\xa0(ISBI, May 2012), and a\\xa0contest on object detection in large images\\xa0(ICPR, Sept 2012). The latter was actually a medical imaging contest on cancer detection\\xa0[GPUCNN8]. Our CNN image scanners were 1000 times faster than previous methods\\xa0[SCAN]. The significance of these kind of improvements for the health care industry is obvious. Today IBM, Siemens, Google, and many startups are pursuing this approach.\\nIn 2011, we also introduced our deep neural nets to Arcelor Mittal, the world\\'s largest steel producer, and were able to greatly improve steel defect detection\\xa0[ST]. To the best of my knowledge, this was the first deep learning breakthrough in heavy industry. A significant part of modern computer vision is extending our work of 2011, e.g.,\\xa0[DL1-4]\\xa0and\\xa0Sec. 19\\xa0of\\xa0[MIR].\\nA follow up\\xa0technical report on DanNet\\xa0in Feb 2012 summarized some of the recent breakthroughs. In July 2012, DanNet was also presented at CVPR, the leading computer vision conference\\xa0[GPUCNN3]. This helped to spread the word in the computer vision community. As of 2020, the CVPR article was the most cited DanNet paper, albeit not the first\\xa0[GPUCNN1-3,6].\\nAfter DanNet had won 4 image recognition competitions, the similar GPU-accelerated \"AlexNet\" won the ImageNet\\xa0[IM09]\\xa02012 contest\\xa0[GPUCNN4-5]\\xa0[R6]. Unlike DanNet, AlexNet used Christoph v. d. Malsburg\\'s rectified linear neurons (ReLUs)\\xa0[CMB]\\xa0(1973) and a variant of Stephen J. Hanson\\'s stochastic delta rule (1990) called \"dropout\"\\xa0[Drop1]\\xa0[T20]. While both of these techniques helped, they are not really required to win vision contests\\xa0[GPUCNN5]\\xa0[R6]. Back then, the only really important CNN-related task was to greatly accelerate known techniques for training CNNs through GPUs. Compare\\xa0Sec. XIV\\xa0of\\xa0[T20].\\nWe continued to make CNNs and other neural nets even deeper and better. Until 2015, deep networks had at most a few tens of layers, e.g., 20-30 layers. But in May 2015, our Highway Net [HW1]\\xa0[HW3]\\xa0[HW]\\xa0[R5]\\xa0was the first working extremely deep feedforward neural net with hundreds of layers. The Highway Net is based on the\\xa0LSTM\\xa0principle\\xa0[LSTM1-2]\\xa0which enables much deeper learning. Its special case called \"ResNet\"\\xa0[HW2]\\xa0(the ImageNet 2015 winner of Dec 2015) is a Highway Net whose gates are always open (compare\\xa0[HW]\\xa0&\\xa0Sec. 4\\xa0of\\xa0[MIR]).\\xa0Highway Nets\\xa0perform roughly as well as ResNets on ImageNet\\xa0[HW3]. Highway layers are also often used for natural language processing\\xa0[HW3]\\xa0(compare\\xa0[MIR]\\xa0[DEC]\\xa0[T20]).\\nThe original successes of DanNet required a precise understanding of the inner workings of GPUs\\xa0[GPUCNN1-3]. Today, convenient software packages shield the user from such details, and compute is roughly 100 times cheaper than 10 years ago when our results set the stage for the recent\\xa0decade of deep learning\\xa0[DEC]. Many current commercial neural net applications are based on what started in 2011\\xa0[DL1-4]\\xa0[DEC].\\n\\xa0\\nAcknowledgments\\n\\xa0\\nThanks to several expert reviewers for useful comments. (Let me know under\\xa0juergen@idsia.ch\\xa0if you can spot any remaining error.) The contents of this article may be used for educational and non-commercial purposes, including articles for Wikipedia and similar sites.\\n\\xa0\\nReferences\\n\\xa0\\n[MLP1] D. C. Ciresan, U. Meier, L. M. Gambardella, J. Schmidhuber. Deep Big Simple Neural Nets For Handwritten Digit Recognition. Neural Computation 22(12): 3207-3220, 2010.\\xa0ArXiv Preprint\\xa0(1 March 2010).\\xa0[Showed that plain backprop for deep standard NNs is sufficient to break benchmark records, without any unsupervised pre-training.]\\n[MLP2] J. Schmidhuber (Sep 2020).\\xa010-year anniversary of supervised deep learning breakthrough (2010). No unsupervised pre-training. The rest is history\\n[MIR] J. Schmidhuber (2019).\\xa0Deep Learning: Our Miraculous Year 1990-1991.\\xa0See also\\xa0arxiv:2005.05744.\\n[DEC] J. Schmidhuber (2020).\\xa0The 2010s: Our Decade of Deep Learning / Outlook on the 2020s.\\n[DL1] J. Schmidhuber, 2015. Deep Learning in neural networks: An overview. Neural Networks, 61, 85-117.\\xa0More.\\n[DL2] J. Schmidhuber, 2015.\\xa0Deep Learning. Scholarpedia, 10(11):32832.\\n[DL4] J. Schmidhuber, 2017.\\xa0Our impact on the world\\'s most valuable public companies: 1. Apple, 2. Alphabet (Google), 3. Microsoft, 4. Facebook, 5. Amazon ....\\n[T20] J. Schmidhuber (2020).\\xa0Critique of 2018 Turing Award for deep learning.\\n[CNN1] K. Fukushima: Neural network model for a mechanism of pattern recognition unaffected by shift in position—Neocognitron. Trans. IECE, vol. J62-A, no. 10, pp. 658-665, 1979.\\xa0[The first deep convolutional neural network architecture, with alternating convolutional layers and downsampling layers. In Japanese. English version:\\xa0[CNN1+].\\xa0More in Scholarpedia.]\\n[CNN1+] K. Fukushima: Neocognitron: a self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, vol. 36, no. 4, pp. 193-202 (April 1980).\\xa0Link.\\n[CNN1a] A. Waibel. Phoneme Recognition Using Time-Delay Neural Networks. Meeting of IEICE, Tokyo, Japan, 1987.\\xa0[First application of backpropagation\\xa0[BP1][BP2]\\xa0and weight-sharing to a convolutional architecture.]\\n[CNN1b] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano and K. J. Lang. Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37, no. 3, pp. 328-339, March 1989.\\n[CNN2] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel: Backpropagation Applied to Handwritten Zip Code Recognition, Neural Computation, 1(4):541-551, 1989.\\xa0PDF.\\n[CNN3] Weng, J., Ahuja, N., and Huang, T. S. (1993). Learning recognition and segmentation of 3-D objects from 2-D images. Proc. 4th Intl. Conf. Computer Vision, Berlin, Germany, pp. 121-128.\\xa0[A CNN whose downsampling layers use Max-Pooling (which has become very popular) instead of Fukushima\\'s Spatial Averaging\\xa0[CNN1].]\\n[CNN4] M. A. Ranzato, Y. LeCun: A Sparse and Locally Shift Invariant Feature Extractor Applied to Document Images. Proc. ICDAR, 2007\\n[IM09] J. Deng, R. Socher, L.J. Li, K. Li, L. Fei-Fei (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255). IEEE, 2009.\\n[GPUNN] Oh, K.-S. and Jung, K. (2004). GPU implementation of neural networks. Pattern Recognition, 37(6):1311-1314.\\xa0[Speeding up traditional NNs on GPU by a factor of 20.]\\n[GPUCNN] K. Chellapilla, S. Puri, P. Simard. High performance convolutional neural networks for document processing. International Workshop on Frontiers in Handwriting Recognition, 2006.\\xa0[Speeding up shallow CNNs on GPU by a factor of 4.]\\n[GPUCNN1] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, J. Schmidhuber. Flexible, High Performance Convolutional Neural Networks for Image Classification.\\xa0International Joint Conference on Artificial Intelligence (IJCAI-2011, Barcelona), 2011.\\xa0PDF.\\xa0ArXiv preprint\\xa0(1 Feb 2011).\\xa0[Speeding up deep CNNs on GPU by a factor of 60. Used to\\xa0win four important computer vision competitions 2011-2012\\xa0before others won any with similar approaches.]\\n[GPUCNN2] D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. A Committee of Neural Networks for Traffic Sign Classification.\\xa0International Joint Conference on Neural Networks (IJCNN-2011, San Francisco), 2011.\\xa0PDF.\\xa0HTML overview.\\xa0[At IJCNN 2011, DanNet achieved the first superhuman performance in a computer vision contest, with half the error rate of humans, and one third the error rate of the closest competitor. This led to massive interest from industry.]\\n[GPUCNN2+] D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. Multi-Column Deep Neural Network for Traffic Sign Classification.\\xa0Neural Networks\\xa032: 333-338, 2012.\\xa0PDF of preprint.\\n[GPUCNN3] D. C. Ciresan, U. Meier, J. Schmidhuber. Multi-column Deep Neural Networks for Image Classification. Proc.\\xa0IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012, p 3642-3649, July 2012.\\xa0PDF. Longer TR of Feb 2012:\\xa0arXiv:1202.2745v1 [cs.CV].\\xa0More.\\n[GPUCNN4] A. Krizhevsky, I. Sutskever, G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. NIPS 25, MIT Press, Dec 2012.\\xa0PDF.\\n[GPUCNN5] J. Schmidhuber.\\xa0History of computer vision contests won by deep CNNs on GPU.\\xa0March 2017.\\xa0[How IDSIA used deep and fast GPU-based CNNs to win four important computer vision competitions 2011-2012 before others won contests using similar approaches.]\\n[GPUCNN6] J. Schmidhuber, D. Ciresan, U. Meier, J. Masci, A. Graves. On Fast Deep Nets for AGI Vision. In Proc. Fourth Conference on Artificial General Intelligence (AGI-11), Google, Mountain View, California, 2011.\\xa0PDF.\\n[GPUCNN7] D. C. Ciresan, A. Giusti, L. M. Gambardella, J. Schmidhuber. Mitosis Detection in Breast Cancer Histology Images using Deep Neural Networks. MICCAI 2013.\\xa0PDF.\\n[GPUCNN8] J. Schmidhuber. First deep learner to win a contest on object detection in large images— first deep learner to win a medical imaging contest (2012).\\xa0HTML.\\xa0[How IDSIA used GPU-based CNNs to win the ICPR 2012 Contest on Mitosis Detection and the MICCAI 2013 Grand Challenge.]\\n[SER11] P. Sermanet, Y. LeCun. Traffic sign recognition with multi-scale convolutional networks. Proc. IJCNN 2011, p 2809-2813, IEEE, 2011\\n[SCAN] J. Masci, A. Giusti, D. Ciresan, G. Fricout, J. Schmidhuber. A Fast Learning Algorithm for Image Segmentation with Max-Pooling Convolutional Networks. ICIP 2013. Preprint arXiv:1302.1690.\\n[ST] J. Masci, U. Meier, D. Ciresan, G. Fricout, J. Schmidhuber Steel Defect Classification with Max-Pooling Convolutional Neural Networks. Proc. IJCNN 2012.\\xa0PDF.\\n[HW] J. Schmidhuber (2015): Overview of\\xa0Highway Networks: First working really deep feedforward neural networks with over 100 layers. (Updated 2020 for 5-year anniversary.)\\n[HW1] R. K. Srivastava, K. Greff, J. Schmidhuber. Highway networks. Preprints\\xa0arXiv:1505.00387\\xa0(May 2015) and\\xa0arXiv:1507.06228\\xa0(July 2015). Also at NIPS 2015.\\xa0[The first working very deep feedforward nets with over 100 layers. Let g, t, h, denote non-linear differentiable functions. Each non-input layer of a highway net computes g(x)x + t(x)h(x), where x is the data from the previous layer. (Like LSTM with forget gates\\xa0[LSTM2]\\xa0for RNNs.) Resnets\\xa0[HW2]\\xa0are a special case of this where the gates are always open: g(x)=t(x)=const=1. Highway Nets perform roughly as well as ResNets\\xa0[HW2]\\xa0on ImageNet\\xa0[HW3]. Highway layers are also often used for natural language processing, where the simpler residual layers do not work as well\\xa0[HW3].\\xa0More.]\\n[HW1a] R. K. Srivastava, K. Greff, J. Schmidhuber. Highway networks. Presentation at the Deep Learning Workshop, ICML\\'15, July 10-11, 2015.\\xa0Link.\\n[HW2] He, K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image recognition. Preprint\\xa0arXiv:1512.03385\\xa0(Dec 2015).\\xa0Residual nets are a special case of Highway Nets\\xa0[HW1]\\xa0where the gates are open: g(x)=1 (a typical highway net initialization) and t(x)=1.\\xa0More.\\n[HW3] K. Greff, R. K. Srivastava, J. Schmidhuber. Highway and Residual Networks learn Unrolled Iterative Estimation. Preprint\\xa0arxiv:1612.07771\\xa0(2016). Also at ICLR 2017.\\n[LSTM1] S. Hochreiter, J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735-1780, 1997.\\xa0PDF.\\xa0More.\\n[LSTM2] F. A. Gers, J. Schmidhuber, F. Cummins. Learning to Forget: Continual Prediction with LSTM. Neural Computation, 12(10):2451-2471, 2000.\\xa0PDF.\\xa0[The \"vanilla LSTM architecture\" with forget gates that everybody is using today, e.g., in Google\\'s Tensorflow.]\\n[BP1] S. Linnainmaa. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master\\'s Thesis (in Finnish), Univ. Helsinki, 1970.\\xa0See chapters 6-7 and FORTRAN code on pages 58-60.\\xa0PDF. See also BIT 16, 146-160, 1976.\\xa0Link.\\xa0[The first publication of \"modern\" backpropagation, also known as the reverse mode of automatic differentiation.]\\n[BP2] P. J. Werbos. Applications of advances in nonlinear sensitivity analysis. In R. Drenick, F. Kozin, (eds): System Modeling and Optimization: Proc. IFIP, Springer, 1982.\\xa0PDF.\\xa0[First application of backpropagation\\xa0[BP1]\\xa0to neural networks. Extending preliminary thoughts in his 1974 thesis.]\\n[BP4] J. Schmidhuber.\\xa0Who invented backpropagation?\\xa0More\\xa0[DL2].\\n[R5] Reddit/ML, 2019.\\xa0The 1997 LSTM paper by Hochreiter & Schmidhuber has become the most cited deep learning research paper of the 20th century.\\n[R6] Reddit/ML, 2019.\\xa0DanNet, the CUDA CNN of Dan Ciresan in J. Schmidhuber\\'s team, won 4 image recognition challenges prior to AlexNet.\\n[R7] Reddit/ML, 2019.\\xa0J. Schmidhuber on Seppo Linnainmaa, inventor of backpropagation in 1970.\\n[Drop1] Hanson, S. J. (1990). A Stochastic Version of the Delta Rule, PHYSICA D,42, 265-272.\\xa0[Dropout is a special case of the stochastic delta rule—compare preprint\\xa0arXiv:1808.03578, 2018.]\\n[CMB] C. v. d. Malsburg (1973). Self-Organization of Orientation Sensitive Cells in the Striate Cortex. Kybernetik, 14:85-100, 1973.\\xa0[See Table 1 for rectified linear units or ReLUs. Possibly this was also the first work on applying an EM algorithm to neural nets.]\\n\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\n5 Papers on CNNs Every Data Scientist Should Read\\nIntroduction to Convolutional Neural Networks\\nDeep Learning in Neural Networks: An Overview',\n",
       " 'comments\\nBy Paul Brebner, Technology Evangelist at Instaclustr\\nKafka Connect is a particularly powerful open source data streaming tool that makes it pretty darn painless to pair Kafka with other data technologies. As a distributed technology, Kafka Connect offers particularly high availability and elastic scaling independent of Kafka clusters. Using source or sink connectors to send data to and from Kafka topics, Kafka Connect enables integrations with multiple non-Kafka technologies with no code needed.\\n\\nRobust open source Kafka connectors are available for many popular data technologies, as is the opportunity to write your own. This article walks through a real-world, real-data use case for how to use Kafka Connect to integrate real-time streaming data from Kafka with Elasticsearch (to enable the scalable search of indexed Kafka records) and Kibana (in order to visualize those results).\\xa0\\n\\nFor an interesting use case that highlights the advantages of Kafka and Kafka Connect, I was inspired by the CDC’s COVID-19 data tracker. The Kafka-enabled tracker collects real-time COVID testing data from multiple locations, in multiple formats and using multiple protocols, and processes those events into easily-consumable, visualized results. The tracker also has the necessary data governance to make sure results arrive quickly and can be trusted.\\nI began searching for a similarly complex and compelling use case – but ideally one less fraught than the pandemic. Eventually I came upon an interesting domain, one that included publicly available streaming REST APIs and rich data in a simple JSON format: lunar tides.\\n\\xa0\\n\\xa0\\nLunar tide data\\n\\xa0\\nTides follow the lunar day, a 24-hour-50-minute period during which the planet fully rotates to the same point beneath the orbiting moon. Each lunar day has two high tides and two low tides caused by the moon’s gravitational pull:\\n\\nSource 1 National Oceanic and Atmospheric Administration\\n\\xa0\\nThe National Oceanic and Atmospheric Administration (NOAA) provides a REST API that makes it easy to retrieve detailed sensor data from its global tidal stations.\\xa0\\n\\nFor example, the following REST call specifies the station ID, data type (I chose sea level) and datum (mean sea level), and requests the single more recent result in metric units:\\xa0\\n\\nhttps://api.tidesandcurrents.noaa.gov/api/prod/datagetter?date=latest&station=8724580&product=water_level&datum=msl&units=metric&time_zone=gmt&application=instaclustr&format=json\\n\\n\\nThis call returns a JSON result with the latitude and longitude of the station, the time, and the water level value. Note that you must remember what your call was in order to understand the data type, datum, and units of the returned results!\\n\\xa0\\n\\n{\"metadata\": {\\r\\n   \"id\":\"8724580\",\\r\\n   \"name\":\"Key West\",\\r\\n   \"lat\":\"24.5508”,\\r\\n   \"lon\":\"-81.8081\"},\\r\\n \"data\":[{\\r\\n   \"t\":\"2020-09-24 04:18\",\\r\\n   \"v\":\"0.597\",\\r\\n      \"s\":\"0.005\", \"f\":\"1,0,0,0\", \"q\":\"p\"}]}\\n\\n\\n\\xa0\\nStarting the data pipeline (with a REST source connector)\\n\\xa0\\nTo begin creating the Kafka Connect streaming data pipeline, we must first prepare a Kafka cluster and a Kafka Connect cluster.\\xa0\\n\\nNext, we introduce a REST connector, such as this available open source one. We’ll deploy it to an AWS S3 bucket (use these instructions if needed). Then we’ll tell the Kafka Connect cluster to use the S3 bucket, sync it to be visible within the cluster, configure the connector, and finally get it running. This “BYOC” (Bring Your Own Connector) approach ensures that you have limitless options for finding a connector that meets your specific needs.\\n\\nThe following example demonstrates using a “curl” command to configure a 100% open source Kafka Connect deployment to use a REST API. Note that you’ll need to change the URL, name, and password to match your own deployment:\\n\\ncurl https://connectorClusterIP:8083/connectors -k -u name:password -X POST -H \\'Content-Type: application/json\\' -d \\'\\r\\n\\r\\n{\\r\\n    \"name\": \"source_rest_tide_1\",\\r\\n    \"config\": {\\r\\n      \"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\\r\\n      \"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\\r\\n      \"connector.class\": \"com.tm.kafka.connect.rest.RestSourceConnector\",\\r\\n      \"tasks.max\": \"1\",\\r\\n      \"rest.source.poll.interval.ms\": \"600000\",\\r\\n      \"rest.source.method\": \"GET\",\\r\\n      \"rest.source.url\": \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?date=latest&station=8454000&product=water_level&datum=msl&units=metric&time_zone=gmt&application=instaclustr&format=json\",\\r\\n      \"rest.source.headers\": \"Content-Type:application/json,Accept:application/json\",\\r\\n      \"rest.source.topic.selector\": \"com.tm.kafka.connect.rest.selector.SimpleTopicSelector\",\\r\\n      \"rest.source.destination.topics\": \"tides-topic\"\\r\\n    }\\r\\n}\\n\\n\\nThe connector task created by this code polls the REST API in 10-minute intervals, writing the result to the “tides-topic” Kafka topic. By randomly choosing five total tidal sensors to collect data this way, tidal data is now filling the tides topic via five configurations and five connectors.\\n\\n\\xa0\\n\\xa0\\nEnding the pipeline (with an Elasticsearch sink connector)\\n\\xa0\\nTo give this tide data somewhere to go, we’ll introduce an Elasticsearch cluster and Kibana at the end of the pipeline. We’ll configure an open source Elasticsearch sink connector to send Elasticsearch the data.\\n\\nThe following sample configuration uses the sink name, class, Elasticsearch index, and our Kafka topic. If an index doesn’t exist already, one with default mappings will be created.\\xa0\\n\\ncurl https://connectorClusterIP:8083/connectors -k -u name:password -X POST -H \\'Content-Type: application/json\\' -d \\'\\r\\n{\\r\\n  \"name\" : \"elastic-sink-tides\",\\r\\n  \"config\" :\\r\\n  {\\r\\n    \"connector.class\" : \"com.datamountaineer.streamreactor.connect.elastic7.ElasticSinkConnector\",\\r\\n    \"tasks.max\" : 3,\\r\\n    \"topics\" : \"tides\",\\r\\n    \"connect.elastic.hosts\" : ”ip\",\\r\\n    \"connect.elastic.port\" : 9201,\\r\\n    \"connect.elastic.kcql\" : \"INSERT INTO tides-index SELECT * FROM tides-topic\",\\r\\n    \"connect.elastic.use.http.username\" : ”elasticName\",\\r\\n    \"connect.elastic.use.http.password\" : ”elasticPassword\"\\r\\n  }\\r\\n}\\'\\n\\n\\nThe pipeline is now operational. However, all tide data arriving in the Tides index is a string, due to the default index mappings.\\n\\nCustom mapping is required to correctly graph our time series data. We’ll create this custom mapping for the tides-index below, using the JSON “t” field for the custom date, “v” as a double, and “name” as the keyword for aggregation:\\n\\ncurl -u elasticName:elasticPassword ”elasticURL:9201/tides-index\"  -X PUT -H \\'Content-Type: application/json\\' -d\\'\\r\\n{\\r\\n\"mappings\" : {\\r\\n  \"properties\" : {\\r\\n     \"data\" : {\\r\\n        \"properties\" : {\\r\\n             \"t\" : { \"type\" : \"date\",\\r\\n                     \"format\" : \"yyyy-MM-dd HH:mm\"\\r\\n             },\\r\\n             \"v\" : { \"type\" : \"double\" },\\r\\n             \"f\" : { \"type\" : \"text\" },\\r\\n             \"q\" : { \"type\" : \"text\" },\\r\\n             \"s\" : { \"type\" : \"text\" }\\r\\n             }\\r\\n       },\\r\\n       \"metadata\" : {\\r\\n          \"properties\" : {\\r\\n             \"id\" : { \"type\" : \"text\" },\\r\\n             \"lat\" : { \"type\" : \"text\" },\\r\\n             \"long\" : { \"type\" : \"text\" },\\r\\n             \"name\" : { \"type\" : ”keyword\" } }}}}         }\\'\\n\\n\\nElasticsearch “reindexing” (deleting the index and reindexing all data) is typically required each time you change an Elasticsearch index mapping. Data can either be replayed from an existing Kafka sink connector, as we have in this use case, or sourced using the Elasticsearch reindex operation.\\xa0\\n\\xa0\\n\\xa0\\nVisualizing data with Kibana\\n\\xa0\\nTo visualize the tide data, we’ll first create an index pattern in Kibana, with “t” configured as the timefilter field. We’ll then create a visualization, choosing a line graph type. Lastly, we’ll configure the graph settings such that the y-axis displays the average tide level over 30 minutes and the x-axis shows that data over time.\\xa0\\nThe result is a graph of changes in the tides for the five sample stations that the pipeline collects data from:\\n\\n\\xa0\\nResults\\n\\xa0\\nThe periodic nature of tides is plain to see in our visualization, with two high tides occurring each lunar day.\\n\\nMore surprisingly, the range between high and low tides is different at each global station. This is due to the influences of not just the moon, but the sun, local geography, weather, and climate change. This example Kafka Connect pipeline utilizes Kafka, Elasticsearch and Kibana to helpfully demonstrate the power of visualizations: they can often reveal what raw data cannot!\\n\\xa0\\nBio: Paul Brebner is the Technology Evangelist at Instaclustr, which provides a managed service platform of open source technologies such as Apache Cassandra, Apache Spark, OpenSearch, Redis, and Apache Kafka.\\nRelated:\\n\\n5 Python Data Processing Tips & Code Snippets\\nWhat’s ETL?\\nDate Processing and Feature Engineering in Python',\n",
       " 'Your analytical skills are incredibly valuable – especially in your work with data.\\nHowever, rational thinking alone isn\\'t enough.\\nHave you ever:\\n\\nTold people about your idea, but then no one seemed to care?\\nExplained your analysis, leaving your colleague confused?\\nWorked hard on a data product, only to discover that the business never uses it?\\n\\nYou’re not alone. Many data professionals have this experience. Luckily, there is a way to prevent the frustration: people skills.\\nResearch shows that people skills are becoming more important with the rise of AI.\\nA great way to boost these skills is by reading the new book of Gilbert Eijkelenboom: People Skills for Analytical Thinkers.\\n\\nThis is not just another book on communication: it’s written in an analytical language. Through data and algorithm metaphors, you’ll deepen your knowledge of human behavior. The writing style makes it easy to read.\\nIn the book, you’ll learn: \\n\\nTo better understand your own behavior\\nHow to say ‘no’ without offending your colleagues\\nHow to become persuasive and tell stories with data\\n\\nDo you want people to USE your data products and ACT on your insights?\\nThen this book, filled with academic insights, exercises, and stories, is exactly what you need.\\nAbout the author\\nAs a former professional poker player, you can find Gilbert Eijkelenboom wherever psychology and data meet. While Gilbert’s academic background is in Behavioral Science, he has built a career in Analytics consulting. Combining both worlds, Gilbert founded the company MindSpeaking: soft skills for data science & analytics.\\nHe loves to help data professionals make a bigger impact with their data skills. Next to training, Gilbert enjoys writing. Last year, he published the bestselling book \"People Skills for Analytical Thinkers\" and his online content reaches more than 1,000,000 online views each year.',\n",
       " 'comments\\nBy Parmeet Bhatia, Machine Learning Practitioner and Deep Learning Enthusiast\\nDeep Learning has evolved from simple neural networks to quite complex architectures in a short span of time. To support this rapid expansion, many different deep learning platforms and libraries are developed along the way. One of the primary goals for these libraries is to provide easy to use interfaces for building and training deep learning models, that would allow users to focus more on the tasks at hand. To achieve this, it may require to hide core implementation units behind several abstraction layers that make it difficult to understand basic underlying principles on which deep learning libraries are based. Hence the goal of this article is to provide insights on building blocks of deep learning library. We first go through some background on Deep Learning to understand functional requirements and then walk through a simple yet complete library in python using NumPy that is capable of end-to-end training of neural network models (of very simple types). Along the way, we will learn various components of a deep learning framework. The library is just under 100 lines of code and hence should be fairly easy to follow. The complete source code can be found at\\xa0https://github.com/parmeet/dll_numpy\\n\\xa0\\nBackground\\n\\xa0\\nTypically a deep learning computation library (like TensorFlow and PyTorch) consists of components shown in the figure below.\\n\\n\\nComponents of Deep Learning Framework\\n\\n\\xa0\\nOperators\\nAlso used interchangeably with layers, they are the basic building blocks of any neural network. Operators are vector-valued functions that transform the data. Some commonly used operators are layers like linear, convolution, and pooling, and activation functions like ReLU and Sigmoid.\\n\\xa0\\nOptimizers\\nThey are the backbones of any deep learning library. They provide the necessary recipe to update model parameters using their gradients with respect to the optimization objective. Some well-known optimizers are SGD, RMSProp, and Adam.\\n\\xa0\\nLoss Functions\\nThey are closed-form and differentiable mathematical expressions that are used as surrogates for the optimization objective of the problem at hand. For example, cross-entropy loss and Hinge loss are commonly used loss functions for the classification tasks.\\n\\xa0\\nInitializers\\nThey provide the initial values for the model parameters at the start of training. Initialization plays an important role in training deep neural networks, as bad parameter initialization can lead to slow or no convergence. There are many ways one can initialize the network weights like small random weights drawn from the normal distribution. You may have a look at\\xa0https://keras.io/initializers/\\xa0for a comprehensive list.\\n\\xa0\\nRegularizers\\nThey provide the necessary control mechanism to avoid overfitting and promote generalization. One can regulate overfitting either through explicit or implicit measures. Explicit methods impose structural constraints on the weights, for example, minimization of their L1-Norm and L2-Norm that make the weights sparser and uniform respectively. Implicit measures are specialized operators that do the transformation of intermediate representations, either through explicit normalization, for example, BatchNorm, or by changing the network connectivity, for example, DropOut and DropConnect.\\n\\nThe above-mentioned components basically belong to the front-end part of the library. By front-end, I mean the components that are exposed to the user for them to efficiently design neural network architectures. On the back-end side, these libraries provide support for automatically calculating gradients of the loss function with respect to various parameters in the model. This technique is commonly referred to as Automatic Differentiation (AD).\\n\\n\\xa0\\nAutomatic Differentiation (AD)\\nEvery deep learning library provides a flavor of AD so that a user can focus on defining the model structure (computation graph)and delegate the task of gradients computation to the AD module. Let us go through an example to see how it works. Say we want to calculate partial derivatives of the following function with respect to its input variables\\xa0X₁\\xa0and\\xa0X₂:\\n\\nY = sin(x₁)+X₁*X₂\\n\\n\\nThe following figure, which I have borrowed from\\xa0https://en.wikipedia.org/wiki/Automatic_differentiation, shows it’s computation graph and calculation of derivatives via chain-rule.\\n\\n\\nComputation graph and calculation of derivatives via chain-rule\\n\\n\\xa0\\nWhat you see in the above figure is a flavor of reverse-mode automatic differentiation (AD). The well known Back-propagation algorithm is a special case of the above algorithm where the function at the top is loss function. AD exploits the fact that every composite function consists of elementary arithmetic operations and elementary functions, and hence the derivatives can be computed by recursively applying the chain-rule to these operations.\\n\\xa0\\nImplementation\\n\\xa0\\nIn the previous section, we have gone through all the necessary components to come up with our first deep learning library that can do end-to-end training. To keep things simple, I will mimic the design pattern of\\xa0the\\xa0Caffe\\xa0Library. Here we define two abstract classes: A “Function” class and an “Optimizer” class. In addition, there is a “Tensor” class which is a simple structure containing two NumPy multi-dimensional arrays, one for holding the value of parameters and another for holding their gradients. All the parameters in various layers/operators will be of type “Tensor”. Before we dig deeper, the following figure provides a high-level overview of the library.\\n\\n\\nUML diagram of Library\\n\\n\\xa0\\nAt the time of this writing, the library comes with the implementation of the linear layer, ReLU activation, and SoftMaxLoss Layer along with the SGD optimizer. Hence the library can be used to train a classification model comprising of fully connected layers and ReLU non-linearity. Lets now go through some details of the two abstract classes we have.\\nThe “Function” abstract class provides an interface for operators and is defined as follows:\\n\\nAbstract Function class\\n\\xa0\\nAll the operators are implemented by inheriting the “Function” abstract class. Each operator must provide an implementation of\\xa0forward(…)\\xa0and\\xa0backward(…)\\xa0methods and optionally implement\\xa0getParams\\xa0function to provide access to its parameters (if any). The\\xa0forward(…)\\xa0method receives the input and returns its transformation by the operator. It will also do any house-keeping necessary to compute the gradients. The\\xa0backward(…)\\xa0method receives partial derivatives of the loss function with respect to the operator’s output and implements the partial derivatives of loss with respect to the operator’s input and parameters (if there are any). Note that\\xa0backward(…)\\xa0function essentially provides the capability for our library to perform automatic differentiation.\\nTo make things concrete let’s look at the implementation of the Linear function as shown in the following code snippet:\\n\\nImplementation of Linear function\\n\\xa0\\nThe\\xa0forward(…)\\xa0function implements the transformation of the form\\xa0Y = X*W+b\\xa0and returns it. It also stores the input X as this is needed to compute the gradients of\\xa0W\\xa0in the backward function. The\\xa0backward(…)\\xa0function receives partial derivatives\\xa0dY\\xa0of loss with respect to the output\\xa0Y\\xa0and implements the partial derivatives with respect to input\\xa0X\\xa0and parameters\\xa0W\\xa0and\\xa0b. Furthermore, it returns the partial derivatives with respect to the input\\xa0X, that will be passed on to the previous layer.\\nThe abstract “Optimizer” class provides an interface for optimizers and is defined as follows:\\n\\nAbstract Optimizer class\\n\\xa0\\nAll the optimizers are implemented by inheriting the “Optimizer” base class. The concrete optimization class must provide the implementation for the\\xa0step()\\xa0function. This method updates the model parameters using their partial derivatives with respect to the loss we are optimizing. The reference to various model parameters is provided in the\\xa0__init__(…)\\xa0function.\\xa0Note that the common functionality of resetting gradients is implemented in the base class itself.\\nTo make things concrete, let’s look at the implementation of stochastic gradient descent (SGD) with momentum and weight decay.\\n\\n\\xa0\\nGetting to the real stuff\\n\\xa0\\nTo this end, we have all the ingredients to train a (deep) neural network model using our library. To do so, we would need the following:\\n\\nModel:\\xa0This is our computation graph\\nData and Target:\\xa0This is our training data\\nLoss Function:\\xa0Surrogate for our optimization objective\\nOptimizer:\\xa0To update model parameters\\n\\nThe following pseudo-code depicts a typical training cycle:\\n\\nmodel #computation graph\\r\\ndata,target #training data\\r\\nloss_fn #optimization objective\\r\\noptim #optimizer to update model parameters to minimize lossRepeat:#until convergence or for predefined number of epochs\\r\\n   optim.zeroGrad() #set all gradients  to zero\\r\\n   output = model.forward(data) #get output from  model\\r\\n   loss   = loss_fn(output,target) #calculate loss\\r\\n   grad   = loss.backward() #calculate gradient of loss w.r.t output\\r\\n   model.backward(grad) #calculate gradients for all the parameters\\r\\n   optim.step() #update model parameters\\n\\n\\nThough not a necessary ingredient for a deep learning library, it may be a good idea to encapsulate the above functionality in a class so that we don’t have to repeat ourselves every time we need to train a new model (this is in line with the philosophy of higher-level abstraction frameworks like\\xa0Keras). To achieve this, let’s define a class “Model” as shown in the following code snippet:\\n\\nThis class serves the following functionalities:\\n\\nComputation Graph:\\xa0Through\\xa0add(…)\\xa0function, one can define a sequential model. Internally, the class will simply store all the operators in a list named\\xa0computation_graph.\\nParameter Initialization:\\xa0The class will automatically initialize model parameters with small random values drawn from uniform distribution at the start of training.\\nModel Training:\\xa0Through\\xa0fit(…)\\xa0function, the class provides a common interface to train the models. This function requires the training data, optimizer, and the loss function.\\nModel Inference:\\xa0Through\\xa0predict(…)\\xa0function, the class provides a common interface for making predictions using the trained model.\\n\\nSince this class does not serve as a fundamental building block for deep learning, I implemented it in a separate module called utilities.py. Note that the\\xa0fit(…)\\xa0function makes use of\\xa0DataGenerator\\xa0Class whose implementation is also provided in the utilities.py module. This class is just a wrapper around our training data and generate mini-batches for each training iteration.\\n\\xa0\\nTraining our first model\\n\\xa0\\nLet’s now go through the final piece of code that trains a neural network model using the proposed library. Inspired by the\\xa0blog-post\\xa0of\\xa0Andrej Karapathy, I am going to train a hidden layer neural network model on spiral data. The code for generating the data and it’s visualization is available in the utilities.py file.\\n\\n\\nSpiral data with three classes\\n\\n\\xa0\\nA three-class spiral data is shown in the above figure. The data is non-linearly separable. So we hope that our one hidden layer neural network can learn the non-linear decision boundary. Bringing it all together, the following code snippet will train our model.\\n\\nEnd to end code for training a neural network model\\n\\xa0\\nThe following figure shows the same spiral data together with the decision boundaries of the trained model.\\n\\n\\nSpiral data with the corresponding decision boundaries of the trained model\\n\\n\\xa0\\nConcluding remarks\\n\\xa0\\nWith the ever-increasing complexity of deep learning models, the libraries tend to grow at exponential rates both in terms of functionalities and their underlying implementation. That said, the very core functionalities can still be implemented in a relatively small number of lines of code. Although the library can be used to train end-to-end neural network models (of very simple types), it is still restricted in many ways that make deep learning frameworks usable in various domains including (but not limited to) vision, speech, and text. With that said, I think this is also an opportunity to fork the base implementation and add missing functionalities to get your hands-on experience. Some of the things you can try to implement are:\\n\\nOperators:\\xa0Convolution Pooling etc.\\nOptimizers:\\xa0Adam RMSProp etc.\\nRegularizers:\\xa0BatchNorm DropOut etc.\\n\\nI hope this article gives you a glimpse of what happens under the hood when you use any deep learning library to train your models. Thank you for your attention and I look forward to your comments or any questions in the comment section.\\n\\xa0\\nBio: Parmeet Bhatia is a Machine learning practitioner and deep learning enthusiast. He is an experienced Machine Learning Engineer and R&D professional with a demonstrated history of developing and productization of ML and data-driven products. He is highly passionate about building end-to-end intelligent systems at scale.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nAutograd: The Best Machine Learning Library You’re Not Using?\\n10 Things You Didn’t Know About Scikit-Learn\\nDeep Learning for Signal Processing: What You Need to Know',\n",
       " \"October was a record month for KDnuggets, with an all-time high number of visitors, who had an abundance of great content to choose.  Here are the most popular KDnuggets posts in October.\\r\\n\\nMost Viewed - Platinum Badge (>24,000 UPV)\\n\\nData Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science, by Benjamin Obi Tayo (*)\\r\\nfastcore: An Underrated Python Library, by Hamel Husain (*)\\r\\nGoodhart's Law for Data Science and what happens when a measure becomes a target?, by Jamil Mirabito (*)\\r\\n\\n\\nMost Viewed - Gold Badge (>12,000 UPV)\\n\\nHow to become a Data Scientist: a step-by-step guide, by Great Learning\\r\\n PerceptiLabs - A GUI and Visual API for TensorFlow, by PerceptiLabs (*)\\r\\n A step-by-step guide for creating an authentic data science portfolio project, by Felix Vemmer\\r\\n How to Explain Key Machine Learning Algorithms at an Interview, by Terence Shin\\r\\n\\n\\nMost Viewed - Silver Badge (> 6,000 UPV)\\n\\n Text Mining with R: The Free eBook, by Matthew Mayo (*)\\r\\n The unspoken difference between junior and senior data scientists, by Misra Turp (*)\\r\\n Roadmap to Natural Language Processing (NLP), by Pier Paolo Ippolito\\r\\n How to ace the data science coding challenge, by Benjamin Obi Tayo (*)\\r\\n Ain't No Such a Thing as a Citizen Data Scientist, by Venkat Raman (*)\\r\\n 10 Best Machine Learning Courses in 2020, by Ahmad Bin Shafiq\\r\\n Good-bye Big Data. Hello, Massive Data!, by Sqream (*)\\r\\n Free From MIT: Intro to Computational Thinking and Data Science, by Matthew Mayo\\r\\n\\n\\n\\n\\nMost Viewed - Platinum Badge (>1,000 shares)\\n\\nData Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science, by Benjamin Obi Tayo\\r\\n\\n\\nMost Shared - Gold Badge (>500 shares)\\n\\n 10 Best Machine Learning Courses in 2020, by Ahmad Bin Shafiq\\r\\nHow to become a Data Scientist: a step-by-step guide, by Great Learning\\r\\n How to Explain Key Machine Learning Algorithms at an Interview, by Terence Shin\\r\\n\\n\\nMost Shared - Silver Badge (>250 shares)\\n\\n Free From MIT: Intro to Computational Thinking and Data Science, by Matthew Mayo\\r\\n How LinkedIn Uses Machine Learning in its Recruiter Recommendation Systems, by Jesus Rodriguez (*)\\r\\n Software Engineering Tips and Best Practices for Data Science, by Ahmed Besbes (*)\\r\\n An Introduction to AI, updated, by Imtiaz Adam (*)\\r\\n fastcore: An Underrated Python Library, by Hamel Husain\\r\\n Free Introductory Machine Learning Course From Amazon, by Matthew Mayo (*)\\r\\n A step-by-step guide for creating an authentic data science portfolio project, by Felix Vemmer\\r\\n Roadmap to Natural Language Processing (NLP), by Pier Paolo Ippolito\\r\\n Annotated Machine Learning Research Papers, by Matthew Mayo (*)\\r\\n\\n\\r\\n(*) indicates that badge added or upgraded based on these monthly results.\\r\\n\\nMost Shareable (Viral) Blogs\\r\\nAmong the top blogs, here are the blogs with the highest ratio of shares/unique views, which suggests that people who read it really liked it. \\r\\n\\n An Introduction to AI, updated, by Imtiaz Adam\\r\\n How LinkedIn Uses Machine Learning in its Recruiter Recommendation Systems, by Jesus Rodriguez\\r\\n 5 Best Practices for Putting Machine Learning Models Into Production, by Sigmoid\\r\\n Annotated Machine Learning Research Papers, by Matthew Mayo\\r\\n Free Introductory Machine Learning Course From Amazon, by Matthew Mayo\",\n",
       " 'Sponsored Post.\\n\\nWith new technologies come new ways of doing business – and new skill sets. Should you hire new talent or train existing teams? SAS worked with HR Dive and Coursera on surveys that explore how managers are approaching upskilling, reskilling and cross-skilling – and what employees say they need – to build tech skills in 2021. \\nWith all the workforce changes last year, it is not surprising that employees lack the skills to meet new demands. Many short-term development plans are becoming irrelevant. In fact, almost nine out of 10 managers surveyed said their employee development plans needed to change to reflect the skills gaps that organizations face today. Here’s what they found:\\n\\n88% of managers said they believed their employee development plans needed to change in 2021.\\n50% of managers said employees needed more upskilling, reskilling and cross-skilling, and 41% said they themselves needed those same opportunities.\\n32% of managers said that when considering the types of skills employees should focus on increasing, employees needed more technical skills and soft skills, and 20% of managers said that they themselves could use more technical skills.\\n\\nDespite these challenges, managers and employees agree on many things when it comes to building new skills, including what to learn and how to learn it. In fact, both survey groups ranked technical skills, including data science, AI, machine learning, programming and advanced analytics, as most important.\\nTo be ready for today’s challenges, companies need sound methods to assess what skills their employees have, the ability to identify the gaps, and a plan to upskill them for success. They also need to offer ongoing learning and skills development opportunities to recruit new, top-performing employees. Do you feel ready? You can read the survey results here, along with predicted learning and development trends, and insights for upskilling, cross-skilling and reskilling your workforce.',\n",
       " \"comments\\nBy Przemek Chojecki, CEO Contentyze.\\n2020 was the year of remote work and growing automation. We learned to work from home, and many companies went completely digital. Artificial Intelligence got a huge boost last year — more data to learn on, more processes to automate and optimize. It seems 2021 and beyond will hold even more opportunities for the applications of AI. Let’s prepare for this bright new future with a list of books.\\n\\nBest Artificial Intelligence books to read in 2021. Prepare for the future of automation.\\n\\xa0\\nSociological AI books\\n\\xa0\\nThe most pressing issue is how AI will influence us as a human race and what future it will bring us. Elon Musk often warns about the policies — we have to make our policies right —to make sure that we will have a sustainable development of Artificial Intelligence rather than bad experiments bringing catastrophic consequences and deepening inequalities.\\n\\nHomo Deus: A Brief History of Tomorrow\\xa0is now a classic book about how technology, Artificial Intelligence, in particular, impacts societies. The author presents a historical perspective and discusses ramifications for our present and future. A must-read!\\n\\nThe Singularity is Near\\xa0popularized the term ‘Singularity’, a moment when artificial intelligence algorithms will be as intelligent as humans on average across all disciplines (think Alexa on steroids).\\nRay Kurzweil, the author, is a well-known futurist who lives by his own principles, trying to live to the singularity moment around 2040. Enter the world of the Singularity and what it will change in how we operate as humans.\\n\\xa0\\nPhilosophical AI books\\n\\xa0\\nLooking into the future, we can see the Singularity, a moment when Artificial Intelligence capabilities will surpass those of humans on average across all domains. This will be a moment of profound change, and though it might never come (or it’ll be here by 2040), it’s worth thinking from this future perspective about what we are doing right now, especially who we are as humans and where we’re going.\\n\\nSuperintelligence\\xa0is the classic reference for thinking about Artificial Intelligence and Artificial General Intelligence. Nick Bostrom analyses different levels of AI, arriving at the concept of AGI, an AI at the level and beyond general human intelligence, able to cope with any creative task. A dense book, but worth it!\\n\\nLife 3.0\\xa0is another take on Artificial General Intelligence and what it might mean for humans if AI were much more intelligent than us. It has really great examples and starts with a fascinating story of how AGI might enter the world and allow one company to dominate. A vivid narrative!\\n\\xa0\\nBusiness-oriented AI books\\n\\xa0\\nArtificial Intelligence wouldn’t be that popular if it hadn’t found so many applications across so many sectors in the business world, now more than ever. Using AI in the form of machine learning is ubiquitous. Here’s a very practical list of business books focused on Artificial Intelligence.\\n\\nAI Superpowers\\xa0is a book on how China jumped on the AI wagon and started innovating on a massive scale. Kai-Fu Lee demonstrates the differences between Silicon Valley and the Chinese start-up ecosystem. Great lessons for anyone interested in entrepreneurship as well as what it takes to build AI start-ups.\\n\\nArtificial Intelligence Business\\xa0is a concise guide to Artificial Intelligence for business people and commercially oriented data scientists. It goes through all the most popular applications across branches like finance, logistics, entertainment, and more. You’ll find many examples of companies using AI to their benefit, as well as methods for implementing AI in your organization.\\n\\nApplied Artificial Intelligence\\xa0is designed with business leaders in mind. It gives you a framework to think about Machine Learning, Deep Learning, AGI, ANI, and shows multiple use-cases in today's business world. Interesting for people interested in policy-making.\\n\\xa0\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nData Science Books You Should Start Reading in 2021\\n10 Best Machine Learning Textbooks that All Data Scientists Should Read\\nArtificial Intelligence Books to Read in 2020\",\n",
       " 'By Nicole Janeway Bills, Data Scientist at Atlas Research.\\ncomments\\n\\n\\nSwimming upstream to address data quality issues. Photo by alleksana on Pexels.\\n\\n\\xa0\\n**Update 10/12: I’m now recognized as a CDMP Associate after passing the Fundamentals Exam. Questions for me? Drop them in the comments or join the\\xa0study group.**\\n**Update 8/15: it’s recently come to my attention that the certification exams are\\xa0open book, which is extremely exciting because it means less time memorizing and more time working with data in a real world setting. Also, I am starting a\\xa0study group on Facebook\\xa0— join for help with your exam prep.**\\nEight years ago, data\\xa0science was proclaimed “the sexiest job of the 21st century.” Yet plodding through hours of data munging still feels\\xa0decidedly\\xa0unsexy. If anything, the storied rise of the data science career has illustrated just how poorly most organizations are doing when it comes to managing their data.\\nEnter the\\xa0Certified Data Management Professional (CDMP)\\xa0from Data Management Association International (DAMA). The CDMP is the best data strategy certification you’ve never heard of. (And honestly, when you consider the fact that you’re probably working a job that didn’t exist ten years ago, it’s not surprising that this certification isn’t widespread just yet.)\\nData strategy is a crucial discipline that spans\\xa0end-to-end management of the data lifecycle\\xa0as well as associated aspects of\\xa0data governance\\xa0and key considerations of\\xa0data ethics.\\nThis article outlines the\\xa0hows and whys\\xa0of getting the CDMP, which lays the groundwork for effective thought leadership on data strategy. It also includes a survey — you can offer your thoughts on the most important aspects of data management for data science and check out the consensus of the community.\\nIn this guide:\\n\\nAbout the CDMP Exam\\nHow to prepare for CDMP\\nWhat’s tested on the CDMP\\nSurvey —most important aspect of data management\\nWhy data scientists should get CDMP certified\\n\\nDisclaimer: this post is not sponsored by DAMA International — views reflected are mine alone. I’m including an affiliate link to the\\xa0DMBOK\\xa0on Amazon, the reference guide that is required for the exam, given that it’s an open book test. Buying the exam through this link helps support my writing on Data Science and Data Strategy — thanks in advance.\\n\\xa0\\nAbout the CDMP Exam\\n\\xa0\\nTraining for the CDMP confers expertise across 14 areas related to data strategy (which I’ll cover in more detail in a later section). The test is\\xa0open book, but the\\xa0100 questions\\xa0on the exam must be completed within\\xa090 minutes\\xa0— not a lot of time to be looking things up. Therefore, it’s important to be extremely familiar with the\\xa0reference material.\\nWhen you schedule the exam ($300), DAMA provides 40 practice questions that are pretty reflective of the difficulty of the actual exam. As a further resource,\\xa0check out this article about the process of studying for a certification.\\nIt’s possible to sit for the exam online while monitored via webcam ($11 proctoring fee). The format of the exam is multiple choice — choose the single correct option out of five. You can mark questions and come back to them. At the conclusion of test taking, you get immediate feedback on your score.\\nAnything over 60% is considered passing. This is just fine if you’re interested in getting your CDMP Associate certification and moving along. If you’re interested in the advanced tiers of CDMP certification, you’ll have to pass with a 70% (CDMP Practitioner) or 80% (CDMP Master). To get certified at the highest level, CDMP Fellow, you’ll need to attain the Master Certification and also demonstrate industry experience and contribution to the field. Each of these\\xa0advanced certifications also require passing two Specialist exams.\\nThis brings me to my final point, which is about why — purely from a\\xa0career advancement\\xa0standpoint — you should chose to put yourself through the studying and exam taking process for CDMP: certification from DAMA is associated with\\xa0high-end positions\\xa0in leadership, management, and data architecture. (Think of CDMP as getting credentialed into a semi-secret society of data ninjas.) Increasingly, enterprise roles and federal contracts related to data management are requesting CDMP certification.\\xa0Read more.\\n\\n\\nvia\\xa0CDMP\\n\\n\\xa0\\nPros:\\n\\nProvides well-rounded knowledge base on topics related to data strategy\\nOpen book test means less time spent on route memorization\\nFour tiers for different levels of data management professionals\\n60% score requirement to pass lowest level of certification\\nAssociated with elite roles\\nProvides 3 year membership to DAMA International\\n$311 exam fee is cheaper than other data-related certifications from Microsoft and The Open Group\\n\\nCons:\\n\\nDAMA is not backed by a major tech company (e.g. Amazon, Google, Microsoft) that is actively pushing marketing efforts and driving brand recognition for CDMP certification — this means that CDMP is likely to be recognized as valuable mainly among individuals who are already familiar with data management\\n$311 exam fee is relatively expensive compared to\\xa0AWS Cloud Practitioner cert\\xa0($100) or\\xa0GPC certs\\xa0($200)\\n\\nAlternatives:\\n\\nMicrosoft Certified Solutions Associate\\xa0(MCSA) — modularized certifications focusing on various Microsoft products ($330+)\\nMicrosoft Certified Solutions Expert\\xa0(MCSE) — builds on the MCSA with integrated certifications on topics such as\\xa0Core Infrastructure,\\xa0Data Management & Analytics, and\\xa0Productivity\\xa0($495+)\\nThe Open Group Architecture Framework\\xa0(TOGAF) —various levels of certification on high-level framework for software development and enterprise architecture methodology ($550+)\\nScaled Agile Framework\\xa0(SAFe) — role-based certifications for software engineering teams ($995)\\n\\n\\xa0\\nHow to prepare for CDMP\\n\\xa0\\nGiven that CDMP is an open book test, to study for the exam, all that’s needed is the DAMA Body of Knowledge book (DMBOK\\xa0$55). It’s around\\xa0600 pages, but if you mainly focus your study time on Chapter 1 (Data Management), diagrams & schemas, roles & responsibilities, and definitions, then this should get you 80% of the way toward a passing score.\\nIn terms of how to use\\xa0DMBOK, one test taker recommended 4–6 hours per weekend for 8–10 weeks. Another approach could be reading a couple pages each morning and evening. However you tackle it, make sure you’re incorporating\\xa0spaced repetition\\xa0into your studying methodology.\\nIn addition to being your study guide for the exam, the\\xa0DMBOK\\xa0is of course useful as reference book, and you can drop it on your colleague’s desk if they need to learn data strategy or if they’ve nodded off during a webinar.\\n\\xa0\\nWhat’s tested on the CDMP\\n\\xa0\\nThe CDMP covers 14 topics —I’ve listed them in order of the prevalence with which they occur on the exam and provided a brief definition for each.\\nData Governance\\xa0( 11%) — practices and processes to ensure formal management of data assets.\\xa0Read more.\\nData Quality\\xa0( 11%) — assuring data is fit for consumption based on its accuracy, completeness, consistency, integrity, reasonability, timeliness, uniqueness/deduplication, validity, and accessibility.\\xa0Read more.\\nData Modelling and Design\\xa0( 11%) — translation of business needs into technical specifications.\\xa0Read more.\\nMetadata Management\\xa0(11%) — information about data collected.\\xa0Read more.\\nMaster and Reference Data Management\\xa0(10%) — reference data is information used to categorize other data found in a database, or information that is solely for relating data in a database to information beyond the boundaries of the organization. Master reference data refers to information that is shared across a number of systems within the organization.\\xa0Read more.\\nData Warehousing and Business Intelligence\\xa0(10%) — a\\xa0data warehouse\\xa0stores information from operational systems (as well as other data resources, potentially) in a way that is optimized to support decision-making processes. Business intelligence refers to the use of technology to gather and analyze data, then translate it into useful information.\\xa0Read more.\\nDocument and Content Management\\xa0(6%) — technologies, methods, and tools used to organize and store an organization’s documents.\\xa0Read more.\\nData Integration and Interoperability\\xa0( 6%) — use of technical and business processes to merge data from different sources, with the goal of readily and efficiently providing access to valuable information.\\xa0Read more.\\nData Architecture\\xa0(6%) — specifications to describe existing state, define data requirements, guide data integration, and control data assets, according to the organization’s data strategy.\\xa0Read more.\\nData Security\\xa0( 6%) — implementation of policies and procedures to ensure people and things take the right actions with data and information assets, even in the presence of malicious inputs.\\xa0Read more.\\nData Storage and Operations\\xa0( 6%) — characterization of hardware or software that holds, deletes, backs up, organizes, and secures an organization’s information.\\xa0Read more.\\nData Management Process\\xa0( 2%) — end-to-end management of data, including collection, control, protection, delivery, and enhancement.\\xa0Read more.\\nBig Data\\xa0( 2%) — extremely large datasets, often composed of various structured, unstructured, and semi-structured data types.\\xa0Read more.\\nData Ethics\\xa0( 2%) — code of conduct encompassing data handling, algorithms, and other practices to ensure that data is used appropriately in a moral context.\\xa0Read more.\\n\\xa0\\nWhy data scientists should get CDMP certified\\n\\xa0\\nStill not convinced why data strategy is important? Let’s take a look from the perspective of a data scientist aiming to increase their knowledge and earning potential.\\n\\n\\nPhoto by\\xa0Franki Chamaki\\xa0on\\xa0Unsplash. The signage is a trademark of\\xa0Hivery, a company that leverages AI for the retail industry.\\n\\n\\xa0\\nIt’s been said\\xa0that a data scientist sits at the nexus of statistics, computer science, and domain knowledge. Why would you want to add one more thing to your plate?\\n\\nSuccesswise, you’re better off being good at two complementary skills than being excellent at one\\n\\nScott Adams, author and creator of the Dilbert comics, offers the\\xa0idea\\xa0that “every skill you acquire doubles your odds of success.” He acknowledges this may be somewhat of an oversimplification — “obviously some skills are more valuable than others, and the twelfth skill you acquire might have less value than each of the first eleven” — but the point is that sometimes\\xa0it’s\\xa0better to go wide\\xa0than to go deep.\\nSetting aside the relative magnitude of the benefit (because I seriously doubt it’s 2x per skill… thank you, law of diminishing marginal returns), it seems unquestionable that broadening your skillset can lead to more significant gains relative to toiling away at learning one specific skills. In a nutshell, this is why I think it’s important for a data scientist to learn data strategy.\\nGenerally speaking,\\xa0having diversity in your skillset allows you to:\\n\\nProblem solve\\xa0more effectively by drawing on cross-disciplinary learnings\\nCommunicate\\xa0better with your teammates from other specialties\\nGet your foot in the door\\xa0in terms of gaining access to new projects\\n\\nUnderstanding\\xa0data strategy transforms you from being a data consumer into an empowered data advocate at your organization. It’s worth putting up with all the tongue twister acronyms (DMBOK\\xa0— really? Couldn’t they have just called it\\xa0The Data Management Book?)\\xa0in order to deepen your appreciation for the end-to-end knowledge generating process.\\n\\xa0\\nOther articles to diversify your skills\\n\\xa0\\nUsing Java to Fix Your Data Science Problems\\nGet speedy access to quality data by understanding the basics of this widely-used programming language.\\n\\xa0\\nComprehensive Guide to the Data Warehouse\\nLearn about the role of the data warehouse as the master store of analysis-ready datasets.\\n\\xa0\\nHow to Ace the AWS Cloud Practitioner Certification with Minimal Effort\\nForecast: cloudy with a 100% chance of passing on your first try.\\n\\xa0\\nIf you enjoyed reading this article, follow me on\\xa0Medium,\\xa0LinkedIn, and\\xa0Twitter\\xa0for more ideas to advance your data science skills.\\xa0Join the study group for the CDMP Exam.\\xa0Buy the DMBOK.\\n\\xa0\\nBio: Nicole Janeway Bills\\xa0is Data Scientist with experience in commercial and federal consulting. She helps organizations leverage their top asset: a simple and robust Data Strategy. Sign up for more of her writing.\\nOriginal. Reposted with permission.\\nRelated:\\n\\n10 Underrated Python Skills\\n6 Lessons Learned in 6 Months as a Data Scientist\\n5 Must-Read Data Science Papers (and How to Use Them)',\n",
       " \"By Devin Partida, Editor-in-Chief of ReHack.com.\\ncomments\\n\\n\\nPhoto by Laurel and Michael Evans on Unsplash\\n\\n\\xa0\\nPeople often think of predictive analytics tools as primarily resourceful for white-collar work. However, these five examples show that it offers plenty of potential in blue-collar jobs, too.\\n\\xa0\\n1. Auto Mechanics Get Diagnostic Help\\n\\xa0\\nMost professionals who address problems with cars only see those vehicles once someone notices symptoms. By that time, the issue may be so extensive that it costs the vehicle owner hundreds or thousands of dollars to fix, plus increases the manual labor required by the mechanic.\\nThere's been a more recent move towards getting diagnostic information throughout the whole lifecycle of a vehicle. That approach lets technicians see problems earlier, often before they cause symptoms. With the help of data processing in the cloud and wireless networks, mechanics can get real-time status updates for individual vehicles or entire fleets and become proactive about keeping them at peak performance.\\nKnowing about a problem before a vehicle arrives in a shop also aids in better planning. For example, if a technician feels nearly certain about the cause of an issue, they could order a part and have it ready to install before ever seeing the vehicle.\\n\\xa0\\n2. Facility Maintenance Crews View Valuable Details\\n\\xa0\\nPredictive maintenance also assists the teams of people who oversee building maintenance. Facility managers often connect sensors on equipment used for building climate control. They can then see notifications about faulty components or other problems before breakdowns occur. People can collect more than 350,000 data points annually by analyzing 10 metrics every 15 minutes.\\nMoreover, this use of data science can show users the likely effects of specific tweaks before they make them. For example, if a building maintenance manager wants to focus on energy savings, a dashboard could reveal what changes would give the biggest payoffs. That knowledge removes costly and potentially frustrating guesswork.\\nMany customers know the importance of getting regular maintenance on their heating and air conditioning equipment. However, problems can still occur between appointments. Applying advanced predictions to the equation reduces the chances of service disruptions in commercial buildings.\\n\\xa0\\n3. Predictive Analytics Keep Truck Drivers Safer\\n\\xa0\\nAlong with the earlier discussion about relying on analytics to spot automotive malfunctions before they happen, people who manage or operate commercial fleets have another compelling reason to invest in predictive analytics. They can increase driver safety by predicting which employees are most likely to have accidents. The tools can also highlight the drivers with above-average safe driving practices.\\nMany of today's solutions that utilize data science examine factors both within and outside of a driver's control. For example, a tool may measure a person's driving habits, as well as environmental elements such as traffic and weather. This approach to risk management allows supervisors to intervene and provide coaching before mishaps occur.\\nPeople interested in using analytics this way have dozens of possible metrics to track. However, GPS tracking is especially advantageous in this use case. It allows showing the time spent in particular locations. If a driver takes a break or needs to stop and pick up a delivery, managers can see how long those pauses take. Similarly, they can monitor if drivers spend too much time in traffic and need to reroute.\\n\\xa0\\n4. Data Science Brings Increased Visibility to Construction\\n\\xa0\\nConstruction projects can be difficult to complete on time and under budget. However, predictive analytics could give an unprecedented level of transparency to such endeavors at all stages. Applying data science to the preconstruction phase helps managers understand where costs could get out of control or which aspects might take longer than expected to complete.\\nAnalyzing data at later stages also helps project managers take corrective action to keep things on track before problems happen. This capability makes it easy to keep clients pleased and in the loop, too.\\nMaking the most of analytics in construction requires taking the information out of silos. People also need to ensure that the imported data is in the right format for the tool they want to use. Taking care of those foundational necessities will help them have better overall outcomes.\\n\\xa0\\n5. Warehouse Management Improves With Predictive Analytics\\n\\xa0\\nPeople are also increasingly interested in using data-based predictions to enhance warehouse management. For example, a dashboard could calculate the possible number of hours saved if a company improves a process or reduces the distance a worker must travel when transporting goods around the facility.\\nAn advanced tool could also determine how a facility's labor needs will change as seasonal demands fluctuate. Knowing those details helps company leaders know when to hire more team members to avoid overstretching the current workforce.\\nHigh-tech analytics platforms play a crucial role in injury reduction, too. If a tool examines historical accident data, it could help decision-makers choose what to change so that people stay healthy and able to give their full contributions during a day at work.\\n\\xa0\\nIntriguing Reasons to Use Predictive Analytics\\n\\xa0\\nThese five use cases illustrate why people should not immediately assume that there is no place for data science in blue-collar work. Analyzing data in the right ways can improve effectiveness, productivity and safety, among other desirable aims.\\n\\xa0\\nBio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com.\\nRelated:\\n\\nTop 8 Data Science Use Cases in Construction\\nData Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions\\nHow Data Science Is Keeping People Safe During COVID-19\",\n",
       " 'By Derrick Mwiti, Data Scientist.\\ncomments\\nGradient boosted decision trees have proven to outperform other models. It’s because boosting involves implementing several models and aggregating their results.\\nGradient boosted models have recently become popular thanks to their performance in machine learning competitions on Kaggle.\\nIn this article, we’ll see what gradient boosted decision trees are all about.\\n\\xa0\\nGradient boosting\\n\\xa0\\nIn\\xa0gradient boosting, an ensemble of weak learners is used to improve the performance of a machine learning model. The weak learners are usually decision trees. Combined, their output results in better models.\\nIn case of regression, the final result is generated from the average of all weak learners. With classification, the final result can be computed as the class with the majority of votes from weak learners.\\nIn gradient boosting, weak learners work sequentially. Each model tries to improve on the error from the previous model. This is different from the bagging technique, where several models are fitted on subsets of the data in a parallel manner. These subsets are usually drawn randomly with replacement. A great example of bagging is in Random Forests®.\\nThe boosting process looks like this:\\n\\nBuild an initial model with the data,\\nRun predictions on the whole data set,\\nCalculate the error using the predictions and the actual values,\\nAssign more weight to the incorrect predictions,\\nCreate another model that attempts to fix errors from the last model,\\nRun predictions on the entire dataset with the new model,\\nCreate several models with each model aiming at correcting the errors generated by the previous one,\\nObtain the final model by weighting the mean of all the models.\\n\\n\\xa0\\nBoosting algorithms in machine learning\\n\\xa0\\nLet’s take a look at boosting algorithms in machine learning.\\n\\xa0\\nAdaBoost\\n\\xa0\\nAdaBoost fits a sequence of weak learners to the data. It then assigns more weight to incorrect predictions, and less weight to correct ones. This way the algorithm focuses more on observations that are harder to predict. The final result is obtained from the majority vote in classification, or the average in regression.\\nYou can implement this algorithm using Scikit-learn. The `n_estimators` argument can be passed to it to indicate the number of weak learners needed. You can control the contribution of each weak learner using the `learning_rate` argument.\\nThe algorithm uses decision trees as the base estimators by default. The base estimators and the parameters of the decision trees can be tuned to improve the performance of the model. By default, decision trees in AdaBoost have a single split.\\nClassification using AdaBoost\\nYou can use the `AdaBoostClassifier` from Scikit-learn to implement the AdaBoost model for classification problems. As you can see below, the parameters of the base estimator can be tuned to your preference. The classifier also accepts the number of estimators you want. This is the number of decision trees you need for the model.\\n\\nfrom sklearn.tree import DecisionTreeClassifier\\r\\nfrom sklearn.ensemble import AdaBoostClassifier\\r\\nbase_estimator=DecisionTreeClassifier(max_depth=1,criterion=\\'gini\\', splitter=\\'best\\', min_samples_split=2)\\r\\nmodel = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=100)\\r\\nmodel.fit(X_train, y_train)\\n\\n\\nRegression using AdaBoost\\nApplying AdaBoost to regression problems is similar to the classification process, with just a few cosmetic changes. First, you have to import the `AdaBoostRegressor`. Then, for the base estimator, you can use the `DecisionTreeRegressor`. Just like the previous one, you can tune the parameters of the decision tree regressor.\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\r\\nfrom sklearn.ensemble import AdaBoostRegressor\\r\\nbase_estimator = DecisionTreeRegressor(max_depth=1, splitter=\\'best\\', min_samples_split=2)\\r\\nmodel = AdaBoostRegressor(base_estimator=base_estimator,n_estimators=100)\\r\\nmodel.fit(X_train, y_train)\\n\\n\\n\\xa0\\nScikit-learn gradient boosting estimator\\n\\xa0\\nGradient boosting is different from AdaBoost, because the loss function optimization is done via gradient descent. Like AdaBoost, it also uses decision trees as weak learners. It also sequentially fits the trees. When adding subsequent trees, loss is minimized using gradient descent.\\nIn the Scikit-learn implementation, you can specify the number of trees. This is a parameter that should be looked at keenly, because specifying too many trees can lead to overfitting. On the other hand, specifying a very small number of trees can lead to underfitting.\\nThe algorithm lets you specify the learning rate. This dictates how fast the model will learn. A low learning rate will often require more trees in the model. This means more training time.\\nLet’s now take a look at the implementation of gradient boosted trees in Scikit-learn.\\nClassification with the Scikit-learn gradient boosting estimator\\nThis is implemented using the `GradientBoostingClassifier`. Some of the parameters expected by this algorithm include:\\n\\n`loss` defining the loss function to be optimized\\n`learning_rate` that determines the contribution of each tree\\n`n_estimatorst` dictates the number of decision trees\\n`max_depth` is the maximum depth of each estimator\\n\\n\\nfrom sklearn.ensemble import GradientBoostingClassifier\\r\\ngbc = GradientBoostingClassifier(loss=\\'deviance\\', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=\\'friedman_mse\\', min_samples_split=2, min_samples_leaf=1)\\r\\ngbc.fit(X_train,y_train)\\n\\n\\nAfter fitting the classifier, you can obtain the importance of the features using the `feauture_importances_` attribute. This is usually referred to as the Gini importance.\\n\\ngbc.feature_importances_\\n\\n\\n\\nThe higher the value, the more important the feature is. The values in the obtained array will sum to 1.\\nNote: Impurity-based importances are not always accurate, especially when there are too many features. In that case, you should consider using\\xa0permutation-based importances.\\nRegression with the Scikit-learn gradient boosting estimator\\nThe Scikit-learn gradient boosting estimator can be implemented for regression using `GradientBoostingRegressor`. It takes parameters that are similar to the classification one:\\n\\nloss,\\nnumber of estimators,\\nmaximum depth of the trees,\\nlearning rate…\\n\\n…just to mention a few.\\n\\nfrom sklearn.ensemble import GradientBoostingRegressor\\r\\nparams = {\\'n_estimators\\': 500,\\r\\n          \\'max_depth\\': 4,\\r\\n          \\'min_samples_split\\': 5,\\r\\n          \\'learning_rate\\': 0.01,\\r\\n          \\'loss\\': \\'ls\\'}\\r\\ngbc = GradientBoostingRegressor(**params)\\r\\ngbc.fit(X_train,y_train)\\n\\n\\nLike the classification model, you can also obtain the feature importances for the regression algorithm.\\n\\ngbc.feature_importances_\\n\\n\\n\\xa0\\nXGBoost\\n\\xa0\\nXGBoost\\xa0is a gradient boosting library supported for Java, Python, Java and C++, R, and Julia. It also uses an ensemble of weak decision trees.\\nIt’s a linear model that does tree learning through parallel computations. The algorithm also ships with features for performing cross-validation, and showing the feature’s importance. The main features of this model are:\\n\\naccepts sparse input for tree booster and linear booster,\\nsupports custom evaluation and objective functions,\\n`Dmatrix`, its optimized data structure improves its performance.\\n\\nLet’s take a look at how you can apply XGBoost in Python. The parameters accepted by the algorithm include:\\n\\n`objective` to define the type of task, say regression or classification;\\n`colsample_bytree` the subsample ratio of columns when constructing each tree. Subsampling happens once in every iteration. This number is usually a value between 0 and 1;\\n`learning_rate` that determines how fast or slow the model will learn;\\n`max_depth` indicates the maximum depth for each tree. The more the trees, the greater model complexity, and the higher chances of overfitting;\\n`alpha` is the\\xa0L1 regularization\\xa0on weights;\\n`n_estimators` is the number of decision trees to fit.\\n\\nClassification with XGBoost\\nAfter importing the algorithm, you define the parameters that you would like to use. Since this is a classification problem, the `binary: logistic` objective function is used. The next step is to use the `XGBClassifier` and unpack the defined parameters. You can tune these parameters until you obtain the ones that are optimal for your problem.\\n\\nimport xgboost as xgb\\r\\nparams = {\"objective\":\"binary:logistic\",\\'colsample_bytree\\': 0.3,\\'learning_rate\\': 0.1,\\r\\n                \\'max_depth\\': 5, \\'alpha\\': 10}\\r\\nclassification = xgb.XGBClassifier(**params)\\r\\nclassification.fit(X_train, y_train)\\n\\n\\nRegression with XGBoost\\nIn regression, the `XGBRegressor` is used instead. The objective function, in this case, will be the `reg:squarederror`.\\n\\nimport xgboost as xgb\\r\\nparams = {\"objective\":\"reg:squarederror\",\\'colsample_bytree\\': 0.3,\\'learning_rate\\': 0.1,\\r\\n                \\'max_depth\\': 5, \\'alpha\\': 10}\\r\\nregressor = xgb.XGBRegressor(**params)\\r\\nregressor.fit(X_train, y_train)\\n\\n\\nThe XGBoost models also allow you to obtain the feature importances via the `feature_importances_` attribute.\\n\\nregressor.feature_importances_\\n\\n\\n\\nYou can easily visualize them using Matplotlib. This is done using the `plot_importance` function from XGBoost.\\n\\nimport matplotlib.pyplot as plt\\r\\nxgb.plot_importance(regressor)\\r\\nplt.rcParams[\\'figure.figsize\\'] = [5, 5]\\r\\nplt.show()\\n\\n\\n\\nThe `save_model` function can be used for saving your model. You can then send this model to your model registry.\\n\\nregressor.save_model(\"model.pkl\")\\n\\n\\nCheck Neptune docs about integration with\\xa0XGBoost\\xa0and with\\xa0matplotlib.\\n\\xa0\\nLightGBM\\n\\xa0\\nLightGBM is different from other gradient boosting frameworks because it uses a leaf-wise tree growth algorithm. Leaf-wise tree growth algorithms are known to converge faster than depth-wise growth algorithms. However, they’re more prone to overfitting.\\nSource\\n\\n\\xa0\\nThe algorithm is\\xa0histogram-based, so it places continuous values into discrete bins. This leads to faster training and efficient memory utilization.\\nOther notable features from this algorithm include:\\n\\nsupport for GPU training,\\nnative support for categorical features,\\nability to handle large-scale data,\\nhandles missing values by default.\\n\\nLet’s take a look at some of the main parameters of this algorithm:\\n\\n`max_depth` the maximum depth of each tree;\\n`objective` which defaults to regression;\\n`learning_rate` the boosting learning rate;\\n`n_estimators` the number of decision trees to fit;\\n`device_type` whether you’re working on a CPU or GPU.\\n\\nClassification with LightGBM\\nTraining a binary classification model can be done by setting `binary` as the objective. If it’s a multi-classification problem, the `multiclass` objective is used.\\nThe dataset is also converted to LightGBM’s `Dataset` format. Training the model is then done using the `train` function. You can also pass the validation datasets using the `valid_sets` parameter.\\n\\nimport lightgbm as lgb\\r\\nlgb_train = lgb.Dataset(X_train, y_train)\\r\\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\\r\\nparams = {\\'boosting_type\\': \\'gbdt\\',\\r\\n              \\'objective\\': \\'binary\\',\\r\\n              \\'num_leaves\\': 40,\\r\\n              \\'learning_rate\\': 0.1,\\r\\n              \\'feature_fraction\\': 0.9\\r\\n              }\\r\\ngbm = lgb.train(params,\\r\\n    lgb_train,\\r\\n    num_boost_round=200,\\r\\n    valid_sets=[lgb_train, lgb_eval],\\r\\n    valid_names=[\\'train\\',\\'valid\\'],\\r\\n   )\\r\\n\\n\\n\\nRegression with LightGBM\\nFor regression with LightGBM, you just need to change the objective to `regression`. The boosting type is Gradient Boosting Decision Tree by default.\\nIf you like, you can change this to the random forest algorithm, `dart` — Dropouts meet Multiple Additive Regression Trees, or\\xa0 `goss` — Gradient-based One-Side Sampling.\\n\\nimport lightgbm as lgb\\r\\nlgb_train = lgb.Dataset(X_train, y_train)\\r\\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\\r\\nparams = {\\'boosting_type\\': \\'gbdt\\',\\r\\n              \\'objective\\': \\'regression\\',\\r\\n              \\'num_leaves\\': 40,\\r\\n              \\'learning_rate\\': 0.1,\\r\\n              \\'feature_fraction\\': 0.9\\r\\n              }\\r\\ngbm = lgb.train(params,\\r\\n    lgb_train,\\r\\n    num_boost_round=200,\\r\\n    valid_sets=[lgb_train, lgb_eval],\\r\\n    valid_names=[\\'train\\',\\'valid\\'],\\r\\n   )\\n\\n\\nYou can also use LightGBM to plot the model’s feature importance.\\n\\nlgb.plot_importance(gbm)\\n\\n\\n\\nLightGBM also has a built-in function for saving the model. That function is `save_model`.\\n\\ngbm.save_model(\\'mode.pkl\\')\\n\\n\\n\\xa0\\nCatBoost\\n\\xa0\\nCatBoost\\xa0is a depth-wise gradient boosting library developed by Yandex. The algorithm grows a balanced tree using oblivious decision trees.\\nIt uses the same features to make the right and left split at each level of the tree.\\nFor example in the image below, you can see that `297,value>0.5` is used through that level.\\n\\nOther notable features of\\xa0CatBoost\\xa0include:\\n\\nnative support for categorical features,\\nsupports training on multiple GPUs,\\nresults in good performance with the default parameters,\\nfast prediction via CatBoost’s model applier,\\nhandles missing values natively,\\nsupport for regression and classification problems.\\n\\nLet’s now mention a couple of training parameters from CatBoost:\\n\\n`loss_function` the loss to be used for classification or regression;\\n`eval_metric` the model’s evaluation metric;\\n`n_estimators` the maximum number of decision trees;\\n`learning_rate` determines how fast or slow the model will learn;\\n`depth` the maximum depth for each tree;\\n`ignored_features` determines the features that should be ignored during training;\\n`nan_mode` the method that will be used to deal with missing values;\\n`cat_features` an array of categorical columns;\\n`text_features` for declaring text-based columns.\\n\\nClassification with CatBoost\\nFor classification problems,`CatBoostClassifier` is used. Setting `plot=True` during the training process will visualize the model.\\n\\nfrom catboost import CatBoostClassifier\\r\\nmodel = CatBoostClassifier()\\r\\nmodel.fit(X_train,y_train,verbose=False, plot=True)\\n\\n\\n\\nRegression with CatBoost\\nIn the case of regression, the `CatBoostRegressor` is used.\\n\\nfrom catboost import CatBoostRegressor\\r\\nmodel = CatBoostRegressor()\\r\\nmodel.fit(X_train,y_train,verbose=False, plot=True)\\n\\n\\nYou can also use the `feature_importances_` to obtain the ranking of the features by their importance.\\n\\nmodel.feature_importances_\\n\\n\\n\\nThe algorithm also provides support for performing cross-validation. This is done using the `cv` function while passing the required parameters.\\nPassing `plot=”True”` will visualize the cross-validation process. The `cv` function expects the dataset to be in CatBoost’s `Pool` format.\\n\\nfrom catboost import Pool, cv\\r\\nparams = {\"iterations\": 100,\\r\\n          \"depth\": 2,\\r\\n          \"loss_function\": \"RMSE\",\\r\\n          \"verbose\": False}\\r\\ncv_dataset = Pool(data=X_train,\\r\\n                  label=y_train)\\r\\nscores = cv(cv_dataset,\\r\\n            params,\\r\\n            fold_count=2, \\r\\n            plot=True)\\n\\n\\nYou can also use CatBoost to perform a grid search. This is done using the `grid_search` function. After searching, CatBoost trains on the best parameters.\\nYou should not have fitted the model before this process. Passing the `plot=True` parameter will visualize the grid search process.\\n\\ngrid = {\\'learning_rate\\': [0.03, 0.1],\\r\\n        \\'depth\\': [4, 6, 10],\\r\\n        \\'l2_leaf_reg\\': [1, 3, 5, 7, 9]}\\r\\n\\r\\ngrid_search_result = model.grid_search(grid, X=X_train, y=y_train, plot=True)\\n\\n\\nCatBoost also enables you to visualize a single tree in the model. This is done using the `plot_tree` function and passing the index of the tree you would like to visualize.\\n\\nmodel.plot_tree(tree_idx=0)\\n\\n\\n\\n\\xa0\\nAdvantages of gradient boosting trees\\n\\xa0\\nThere are several reasons as to why you would consider using gradient boosting tree algorithms:\\n\\ngenerally more accurate compare to other modes,\\ntrain faster especially on larger datasets,\\nmost of them provide support handling categorical features,\\nsome of them handle missing values natively.\\n\\n\\xa0\\nDisadvantages of gradient boosting trees\\n\\xa0\\nLet’s now address some of the challenges faced when using gradient boosted trees:\\n\\nprone to overfitting: this can be solved by applying L1 and L2 regularization penalties. You can try a low learning rate as well;\\nmodels can be computationally expensive and take a long time to train, especially on CPUs;\\nhard to interpret the final models.\\n\\n\\xa0\\nFinal thoughts\\n\\xa0\\nIn this article, we explored how to implement gradient boosting decision trees in your machine learning problems. We also walked through various boosting-based algorithms that you can start using right away.\\nSpecifically, we’ve covered:\\n\\nwhat is gradient boosting,\\nhow gradient boosting works,\\nvarious types of gradient boosting algorithms,\\nhow to use gradient boosting algorithms for regression and classification problems,\\nthe advantages of gradient boosting trees,\\ndisadvantages of gradient boosting trees,\\n\\n…and so much more.\\nYou’re all set to start boosting your machine learning models.\\n\\xa0\\nResources\\n\\xa0\\n\\nGradient boosting in TensorFlow\\nHistogram-based gradient boosting\\xa0\\nClassification notebook\\xa0\\nRegression notebook\\xa0\\n\\n\\xa0\\nBio: Derrick Mwiti is a data scientist who has a great passion for sharing knowledge. He is an avid contributor to the data science community via blogs such as Heartbeat, Towards Data Science, Datacamp, Neptune AI, KDnuggets just to mention a few. His content has been viewed over a million times on the internet. Derrick is also an author and online instructor. He also trains and works with various institutions to implement data science solutions as well as to upskill their staff. You might want to check his Complete Data Science & Machine Learning Bootcamp in Python course.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nLightGBM: A Highly-Efficient Gradient Boosting Decision Tree\\nThe Best Machine Learning Frameworks & Extensions for Scikit-learn\\nFast Gradient Boosting with CatBoost',\n",
       " 'By Benjamin Obi Tayo, Ph.D., DataScienceHub.\\ncomments\\nMost academic training programs in data science are focused mostly on teaching hard skills. Time and time again, industry data, market trends, and insights from top business leaders highlight soft skills as a key component to success in the workplace. This article will discuss the essential hard and soft skills for success in data science practice.\\n\\xa0\\nHard Skills\\n\\xa0\\n1. Mathematics and Statistics Skills\\nMath skills are essential in data science and machine learning. For more about the basic math skills needed for data science and machine learning, please see this article:\\xa0How Much Math do I need in Data Science?\\n2. Essential Programming Skills\\nProgramming skills are essential in data science. Since Python and R are considered the 2 most popular programming languages in data science, essential knowledge in both languages is crucial. For more information on essential programming skills needed for data science, please see this article:\\xa0How Much Programming do I need in Data Science?\\n3. Data Wrangling and Preprocessing Skills\\nData is key for any analysis in data science, be it inferential analysis, predictive analysis, or prescriptive analysis. The predictive power of a model depends on the quality of the data that was used in building the model. Data comes in different forms such as text, table, image, voice, or video. Most often, data that is used for analysis has to be mined, processed, and transformed to render it to a form suitable for further analysis.\\ni)\\xa0Data Wrangling: The process of data wrangling is a critical step for any data scientist. Very rarely is data easily accessible in a data science project for analysis. It’s more likely for the data to be in a file, a database, or extracted from documents such as web pages, tweets, or PDFs. Knowing how to wrangle and clean data will enable you to derive critical insights from your data that would otherwise be hidden.\\nii)\\xa0Data Preprocessing: Knowledge about data preprocessing is very important and include topics such as:\\na) Dealing with missing data\\nb) Data imputation\\nc) Handling categorical data\\nd) Encoding class labels for classification problems\\ne) Techniques of feature transformation and dimensionality reduction such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).\\n4. Data Visualization Skills\\nUnderstand the essential components of good data visualization (see figure below). Be able to use several data visualization packages, such as matplotlib, seaborn, and ggplot2.\\n\\nTypical workflow for a data visualization project. Image by Benjamin O. Tayo.\\n5. Basic Machine Learning Skills\\nMachine Learning is a very important branch of data science. It is important to understand the machine learning framework: Problem Framing, Data Analysis, Model Building, Testing & Evaluation, and Model Application.\\n\\nTypical workflow for a machine learning project. Image by Benjamin O. Tayo.\\nFind out more about the machine learning framework from here:\\xa0Machine Learning Process Tutorial.\\n6. Skills from Real World Capstone Data Science Projects\\nSkills acquired from course work alone will not make you a data scientist. A qualified data scientist must be able to demonstrate evidence of successful completion of a real-world data science project that includes every stage in data science and machine learning process such as problem framing, data acquisition and analysis, model building, model testing, model evaluation, and deploying the model. Real-world data science projects could be found in the following:\\na) Kaggle Projects\\nb) Internships\\nc) From Interviews\\n\\xa0\\nSoft Skills\\n\\xa0\\n1. Communication Skills\\nData scientists need to be able to communicate their ideas with other members of the team or with business administrators in their organizations. Good communication skills would play a key role here to be able to convey and present very technical information to people with little or no understanding of technical concepts in data science. Good communication skills will help foster an atmosphere of unity and togetherness with other team members such as data analysts, data engineers, field engineers, etc.\\n2. Be a Lifelong Learner\\nData science is a field that is ever-evolving, so be prepared to embrace and learn new technologies. One way to keep in touch with developments in the field is to network with other data scientists. Some platforms that promote networking are LinkedIn, GitHub, and Medium (Towards Data Science\\xa0and\\xa0Towards AI publications). These platforms are very useful for up-to-date information about recent developments in the field.\\n3. Team Player Skills\\nAs a data scientist, you will be working in a team of data analysts, engineers, administrators, so you need good communication skills. You need to be a good listener too, especially during early project development phases where you need to rely on engineers or other personnel to be able to design and frame a good data science project. Being a good team player would help you to thrive in a business environment and maintain good relationships with other members of your team as well as administrators or directors of your organization.\\n4. Business Acumen Skills\\nA very important skill set that is essential for practical applications is business acumen. Business acumen is the ability to draw out meaningful conclusions from a model that can lead to important and cost-saving data-driven decision making. Acquiring business acumen skills is therefore essential for practical data scientists.\\n5. Ethical Skills in Data Science\\nUnderstand the implication of your project. Be truthful to yourself. Avoid manipulating data or using a method that will intentionally produce bias in results. Be ethical in all phases from data collection to analysis, to model building, analysis, testing, and application. Avoid fabricating results for the purpose of misleading or manipulating your audience. Be ethical in the way you interpret the findings from your data science project.\\n\\xa0\\nSummary and Conclusion\\n\\xa0\\nIn summary, we’ve discussed several essential skills needed for practicing data scientists. While academic training programs do a good job to teach hard skills, soft skills are essential for success in the real world.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nTop 5 must-have Data Science skills for 2020\\nThe Most In Demand Tech Skills for Data Scientists\\nWhich Data Science Skills are core and which are hot/emerging ones?',\n",
       " 'By Jesus Rodriguez, Intotheblock.\\ncomments\\n\\n\\nSource:\\xa0https://www.microsoft.com/en-us/research/blog/dowhy-a-library-for-causal-inference/\\n\\n\\xa0\\n\\nI recently started a new newsletter focus on AI education. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:\\n\\n\\n\\xa0\\nThe human mind has a remarkable ability to associate causes with a specific event. From the outcome of an election to an object dropping on the floor, we are constantly associating chains of events that cause a specific effect. Neuropsychology refers to this cognitive ability as causal reasoning. Computer science and economics study a specific form of causal reasoning known as causal inference which focuses on exploring relationships between two observed variables. Over the years, machine learning has produced many methods for causal inference but they remain mostly difficult to use in mainstream applications. Recently, Microsoft Research\\xa0open sourced DoWhy, a framework for causal thinking and analysis.\\nThe challenge with causal inference is not that is a new discipline, quite the opposite, but that the current methods represent a very small and simplistic version of causal reasoning. Most models that try to connect causes such as linear regression rely on empirical analysis that makes some assumption about the data. Pure causal inference relies on counterfactual analysis which is a closer representation to how humans make decisions. Imagine a scenario in which you are traveling with your families for vacations to an unknown destination. Before and after the vacation you are wrestling with a few counterfactual questions:\\n\\nAnswering these questions is the focus of causal inference. Unlike supervised learning, causal inference depends on estimation of unobserved quantities. This if often known as the “fundamental problem” of causal inference which implies that a model never has a purely objective evaluation through a held-out test set. In our vacation example, you can either observe the effects on going on vacation or not going on vacations but never both. This challenge forces causal inference to make critical assumptions about the data generation process. Traditional machine learning frameworks for causal inference try to take shortcuts around the “fundamental problem” resulting on a very frustrating experience for data scientists and developers.\\n\\xa0\\nIntroducing DoWhy\\n\\xa0\\n\\nMicrosoft’s DoWhy\\xa0is a Python-based library for causal inference and analysis that attempts to streamline the adoption of causal reasoning in machine learning applications. Inspired by Judea Pearl’s do-calculus for causal inference, DoWhy combines several causal inference methods under a simple programming model that removes many of the complexities of traditional approaches. Compared to its predecessors, DoWhy makes three key contributions to the implementation of causal inference models.\\n\\n\\nProvides a principled way of modeling a given problem as a causal graph so that all assumptions explicit.\\nProvides a unified interface for many popular causal inference methods, combining the two major frameworks of graphical models and potential outcomes.\\nAutomatically tests for the validity of assumptions if possible and assesses robustness of the estimate to violations.\\n\\nConceptually, DoWhy was created following two guiding principles: asking causal assumptions explicit and testing robustness of the estimates to violations of those assumptions. In other words, DoWhy separates the identification of a causal effect from the estimation of its relevance which enables the inference of very sophisticated causal relationships.\\nTo accomplish its goal, DoWhy models any causal inference problem in a workflow with four fundamental steps: model, identify, estimate and refute.\\n\\n\\nModel:\\xa0DoWhy models each problem using a graph of causal relationships. The current version of DoWhy supports two formats for graph input:\\xa0gml\\xa0(preferred) and\\xa0dot. The graph might include prior knowledge of the causal relationships in the variables but DoWhy does not make any immediate assumptions.\\nIdentify:\\xa0Using the input graph, DoWhy finds all possible ways of identifying a desired causal effect based on the graphical model. It uses graph-based criteria and do-calculus to find potential ways find expressions that can identify the causal effect\\nEstimate:\\xa0DoWhy estimates the causal effect using statistical methods such as matching or instrumental variables. The current version of DoWhy supports estimation methods based such as propensity-based-stratification or propensity-score-matching that focus on estimating the treatment assignment as well as regression techniques that focus on estimating the response surface.\\nVerify:\\xa0Finally, DoWhy uses different robustness methods to verify the validity of the causal effect.\\n\\n\\xa0\\nUsing DoWhy\\n\\xa0\\nDevelopers can start using DoWhy by installing the Python module using the following command:\\n\\npython setup.py install\\n\\n\\nLike any other machine learning program, the first step of a DoWhy application is to load the dataset. In this example, imagine that we are trying to infer the correlation between different medical treatments and outcomes represented by the following dataset.\\n\\nTreatment    Outcome        w0\\r\\n0   2.964978   5.858518 -3.173399\\r\\n1   3.696709   7.945649 -1.936995\\r\\n2   2.125228   4.076005 -3.975566\\r\\n3   6.635687  13.471594  0.772480\\r\\n4   9.600072  19.577649  3.922406\\n\\n\\nDoWhy relies on pandas dataframes to capture the input data:\\n\\nrvar = 1 if np.random.uniform() >0.5 else 0 \\r\\ndata_dict = dowhy.datasets.xy_dataset(10000, effect=rvar, sd_error=0.2) \\r\\ndf = data_dict[\\'df\\']\\r\\nprint(df[[\"Treatment\", \"Outcome\", \"w0\"]].head())\\n\\n\\nAt this point, we simply need about four steps to infer causal relationships between the variables. The four steps correspond to the four operations of DoWhy: model, estimate, infer and refute. We can start by modeling the problem as a causal graph:\\n\\nmodel= CausalModel(\\r\\n        data=df,\\r\\n        treatment=data_dict[\"treatment_name\"],\\r\\n        outcome=data_dict[\"outcome_name\"],\\r\\n        common_causes=data_dict[\"common_causes_names\"],\\r\\n        instruments=data_dict[\"instrument_names\"])\\r\\nmodel.view_model(layout=\"dot\")\\r\\nfrom IPython.display import Image, display\\r\\ndisplay(Image(filename=\"causal_model.png\"))\\n\\n\\n\\n\\nSource:\\xa0https://microsoft.github.io/dowhy/\\n\\n\\xa0\\nThe next step is to identify the causal relationships in the graph:\\n\\nidentified_estimand = model.identify_effect()\\n\\n\\nNow we can estimate the causal effect and determine if the estimation is correct. This example uses linear regression for simplicity:\\n\\nestimate = model.estimate_effect(identified_estimand,\\r\\n        method_name=\"backdoor.linear_regression\")\\r\\n# Plot Slope of line between treamtent and outcome =causal effect\\r\\ndowhy.plotter.plot_causal_effect(estimate, df[data_dict[\"treatment_name\"]], df[data_dict[\"outcome_name\"]])\\n\\n\\n\\n\\nSource:\\xa0https://microsoft.github.io/dowhy/\\n\\n\\xa0\\nFinally, we can use different technique to refute the causal estimator:\\n\\nres_random=model.refute_estimate(identified_estimand, estimate, method_name=\"random_common_cause\")\\n\\n\\nDoWhy is a very simple and useful framework to implement causal inference models. The current version can be used as a standalone library or integrated into popular deep learning frameworks such as TensorFlow or PyTorch. The combination of multiple causal inference methods under a single framework and the four-step simple programming model makes DoWhy incredibly simple to use for data scientist tackling causal inference problems.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nFacebook Uses Bayesian Optimization to Conduct Better Experiments in Machine Learning Models\\nLearning by Forgetting: Deep Neural Networks and the Jennifer Aniston Neuron\\nNetflix’s Polynote is a New Open Source Framework to Build Better Data Science Notebooks',\n",
       " 'Sponsored Post.\\n\\n\\xa0\\nHiring or Looking to Get Hired in Data Science/Analytics? The INFORMS Virtual Career Fair is for You\\n\\xa0\\nThe 2020 INFORMS Virtual Career Fair saw hundreds of highly qualified job seekers in data science, analytics, AI/ML, and more, match with leading employers across both industry and academia. This year, INFORMS is looking to repeat – and build upon – the success of last year’s event with our next Virtual Career Fair, hosted alongside the Virtual 2021 INFORMS Business Analytics Conference. \\n\\xa0\\nEmployers: Expect Top Talent and Diverse Skill Sets\\n\\xa0\\nThe Virtual Career Fair provides employers with the opportunity to meet with and collect resumes from interested job seekers ranging from seasoned professionals in academia or industry to freshly-minted PhD and master’s students just entering the workforce. \\nParticipation in the INFORMS Virtual Career Fair will raise the visibility of your organization with both Business Analytics Conference participants and registered job seekers. All Career Fair events are widely advertised – not only to job seekers already enrolled in INFORMS Career Center – but also to thousands of professionals attending the meeting.\\nTop benefits of participation include:\\n\\nThe ability to arrange public and private LIVE chats from your own virtual booth\\nUnrestricted access to candidate profile details and basic contact info (before, during, and after the event)\\nUnlimited job postings\\n\\n\\xa0\\nJobseekers: Meet with Organizations Who Value Your Expertise and Skills\\n\\xa0\\nSign up for FREE and create your profile today. Registrants can browse available job openings and meet with individual recruiters through public and private chats. Once you register and upload a resume to your profile, organizations may send messages to your account regarding available openings – even before the Virtual Career Fair opens. Register today so employers can review your qualifications and begin reaching out.\\nTips for success: \\n\\nPREPARE FOR THE FAIR: Review participating organizations’ profiles as they are added to the website and formulate questions before joining chat sessions.\\nReview job listings to be informed about positions that interest you.\\nBe aware: Not all employers post positions in the Virtual Career Fair system and may refer you to their websites instead.\\n\\n\\xa0\\nStill Charting Your Course? Let INFORMS Be Your Career Guide\\n\\xa0\\nWant to learn more about career paths in our industry? We examined job titles, years of experience, and analytic skills to create a framework that defines technical job skills, requirements, and career pathways under the “analytics” umbrella. \\nVisit the Analytics Career Pathways page and download the report to check out what we found!',\n",
       " 'comments\\nBy Hadrien Jean, Machine Learning Scientist\\n\\n\\xa0\\nAs you can see in\\xa0Essential Math for Data Science, being able to manipulate vectors and matrices is critical to create machine learning and deep learning pipelines, for instance for reshaping your raw data before using it with machine learning libraries.\\nThe goal of this chapter is to get you to the next level of understanding of vectors and matrices. You’ll start seeing matrices, not only as operations on numbers, but also as a way to transform vector spaces. This conception will give you the foundations needed to understand more complex linear algebra concepts like matrix decomposition. You’ll build up on what you learned about vector addition and scalar multiplication to understand linear combinations of vectors.\\n\\xa0\\nLinear Transformations\\n\\xa0\\nIntuition\\n\\xa0\\nA\\xa0linear transformation\\xa0(or simply\\xa0transformation, sometimes called\\xa0linear map) is a mapping between two vector spaces: it takes a vector as input and\\xa0transforms\\xa0it into a new output vector. A function is said to be linear if the properties of additivity and scalar multiplication are preserved, that is, the same result is obtained if these operations are done before or after the transformation. Linear functions are synonymously called linear transformations.\\n\\nLinear transformations notation \\nYou can encounter the following notation to describe a linear transformation:\\xa0T(v). This refers to the vector\\xa0v\\xa0transformed by\\xa0T. A transformation\\xa0T\\xa0is associated with a specific matrix. Since additivity and scalar multiplication must be preserved in linear transformation, you can write:\\n\\nand\\n\\xa0\\n\\n\\n\\n\\xa0\\nLinear Transformations as Vectors and Matrices\\n\\xa0\\nIn linear algebra, the information concerning a linear transformation can be represented as a matrix. Moreover, every linear transformation can be expressed as a matrix.\\nWhen you do the linear transformation associated with a matrix, we say that you\\xa0apply\\xa0the matrix to the vector. More concretely, it means that you calculate the matrix-vector product of the matrix and the vector. In this case, the matrix can sometimes be called a\\xa0transformation matrix. For instance, you can apply a matrix\\xa0A\\xa0to a vector\\xa0v\\xa0with their product\\xa0Av.\\n\\nApplying matrices Keep in mind that, to apply a matrix to a vector, you\\xa0left multiply\\xa0the vector by the matrix: the matrix is on the left to the vector.\\nWhen you multiply multiple matrices, the corresponding linear transformations are combined in the order from right to left.\\nFor instance, let\\'s say that a matrix\\xa0A\\xa0does a 45-degree clockwise rotation and a matrix\\xa0B\\xa0does a stretching, the product\\xa0BA\\xa0means that you first do the rotation and then the stretching.\\nThis shows that the matrix product is:\\n\\nNot commutative (AB ≠ BA): the stretching then the rotation is a different transformation than the rotation then the stretching.\\nAssociative (A(BC)) = ((AB)C): the same transformations associated with the matrices A, B and C are done in the same order.\\n\\nA matrix-vector product can thus be considered as a way to transform a vector. You can see in\\xa0Essential Math for Data Science\\xa0that the shape of\\xa0A\\xa0and\\xa0v\\xa0must match for the product to be possible.\\n\\n\\n\\xa0\\nGeometric Interpretation\\n\\xa0\\nA good way to understand the relationship between matrices and linear transformations is to actually visualize these transformations. To do that, you’ll use a grid of points in a two-dimensional space, each point corresponding to a vector (it is easier to visualize points instead of arrows pointing from the origin).\\nLet’s start by creating the grid using the function\\xa0meshgrid()\\xa0from Numpy:\\n\\nx = np.arange(-10, 10, 1)\\r\\ny = np.arange(-10, 10, 1)\\r\\n\\r\\nxx, yy = np.meshgrid(x, y)\\n\\n\\nThe\\xa0meshgrid()\\xa0function allows you to create all combinations of points from the arrays x and y. Let’s plot the scatter plot corresponding to\\xa0xx\\xa0and\\xa0yy.\\n\\nplt.scatter(xx, yy, s=20, c=xx+yy)\\r\\n# [...] Add axis, x and y with the same scale\\n\\n\\n\\nFigure 1: Each point corresponds to the combination of x and y values.\\n\\xa0\\nYou can see the grid in Figure 1. The color corresponds to the addition of\\xa0xx\\xa0and\\xa0yy\\xa0values. This will make transformations easier to visualize.\\n\\xa0\\nThe Linear Transformation associated with a Matrix\\n\\xa0\\nAs a first example, let’s visualize the transformation associated with the following two-dimensional square matrix.\\n\\nConsider that each point of the grid is a vector defined by two coordinates (x\\xa0and\\xa0y).\\nLet’s create the transformation matrix\\xa0T:\\n\\nT = np.array([\\r\\n    [-1, 0],\\r\\n    [0, -1]\\r\\n])\\n\\n\\nFirst, you need to structure the points of the grid to be able to apply the matrix to each of them. For now, you have two 20 by 20 matrices (xx\\xa0and\\xa0yy) corresponding to\\xa0\\xa0points, each having a\\xa0x\\xa0value (matrix\\xa0xx) and a\\xa0y\\xa0value (yy). Let’s create a 2 by 400 matrix with xx flatten as the first column and yy as the second column.\\n\\nxy =  np.vstack([xx.flatten(), yy.flatten()])\\r\\nxy.shape\\n\\n\\n\\n(2, 400)\\n\\n\\nYou have now 400 points, each with two coordinates. Let’s apply the transformation matrix\\xa0T\\xa0to the first two-dimensional point (xy[:, 0]), for instance:\\n\\nT @ xy[:, 0]\\n\\n\\n\\narray([10, 10])\\n\\n\\nYou can similarly apply\\xa0TT\\xa0to each point by calculating its product with the matrix containing all points:\\n\\ntrans = T @ xy\\r\\ntrans.shape\\n\\n\\n\\n(2, 400)\\r\\n\\n\\n\\nYou can see that the shape is still\\xa0(2,400). Each transformed vector (that is, each point of the grid) is one of the column of this new matrix. Now, let’s reshape this array to have two arrays with a similar shape to xx and yy.\\n\\nxx_transformed = trans[0].reshape(xx.shape)\\r\\nyy_transformed = trans[1].reshape(yy.shape)\\r\\n\\n\\n\\nLet’s plot the grid before and after the transformation:\\n\\nf, axes = plt.subplots(1, 2, figsize=(6, 3))\\r\\naxes[0].scatter(xx, yy, s=10, c=xx+yy)\\r\\naxes[1].scatter(xx_transformed, yy_transformed, s=10, c=xx+yy)\\r\\n# [...] Add axis, x and y witht the same scale\\n\\n\\n\\nFigure 2: The grid of points before (left) and after (right) its transformation by the matrix\\xa0TT.\\n\\xa0\\nFigure 2 shows that the matrix\\xa0T\\xa0rotated the points of the grid.\\n\\xa0\\nShapes of the Input and Output Vectors\\n\\xa0\\nIn the previous example, the output vectors have the same number of dimensions than the input vectors (two dimensions).\\nYou might notice that the shape of the transformation matrix must match the shape of the vectors you want to transform.\\n\\nFigure 3: Shape of the transformation of the grid points by\\xa0TT.\\n\\xa0\\nFigure 3 illustrates the shapes of this example. The first matrix with a shape (2, 2) is the transformation matrix T and the second matrix with a shape (2, 400) corresponds to the 400 vectors stacked. As illustrated in blue, the number of rows of the\\xa0T\\xa0corresponds to the number of dimensions of the output vectors. As illustrated in red, the transformation matrix must have the same number of columns as the number of dimensions of the matrix you want to transform.\\nMore generally, the size of the transformation matrix tells you the input and output dimensions. An m\\xa0by n transformation matrix transforms\\xa0n-dimensional vectors to\\xa0mm-dimensional vectors.\\n\\xa0\\nStretching and Rotation\\n\\xa0\\nLet’s now visualize the transformation associated with the following matrix:\\n\\nLet’s proceed as in the previous example:\\n\\nT = np.array([\\r\\n    [1.3, -2.4],\\r\\n    [0.1, 2]\\r\\n])\\r\\ntrans = T @ xy\\r\\n\\r\\nxx_transformed = trans[0].reshape(xx.shape)\\r\\nyy_transformed = trans[1].reshape(yy.shape)\\r\\n\\r\\nf, axes = plt.subplots(1, 2, figsize=(6, 3))\\r\\naxes[0].scatter(xx, yy, s=10, c=xx+yy)\\r\\naxes[1].scatter(xx_transformed, yy_transformed, s=10, c=xx+yy)\\r\\n# [...] Add axis, x and y witht the same scale\\n\\n\\n\\nFigure 4: The grid of points before (left) and after (right) the transformation by the new matrix\\xa0T.\\n\\xa0\\nFigure 4 shows that the transformation is different from the previous rotation. This time, there is a rotation, but also a stretching of the space.\\n\\nAre these transformations linear? You might wonder why these transformations are called \"linear\". You saw that a linear transformation implies that the properties of additivity and scalar multiplication are preserved.\\nGeometrically, there is linearity if the vectors lying on the same line in the input space are also on the same line in the output space, and if the origin remains at the same location.\\n\\n\\n\\xa0\\nSpecial Cases\\n\\xa0\\nInverse Matrices\\n\\xa0\\nTransforming the space with a matrix can be reversed if the matrix is invertible. In this case, the inverse T−1 of the matrix\\xa0T\\xa0is associated with a transformation that takes back the space to the initial state after\\xa0T\\xa0has been applied.\\nLet’s take again the example of the transformation associated with the following matrix:\\n\\nYou’ll plot the initial grid of point, the grid after being transformed by\\xa0T, and the grid after successive application of\\xa0T\\xa0and\\xa0T−1\\xa0(remember that matrices must be left-multiplied):\\n\\nT = np.array([\\r\\n    [1.3, -2.4],\\r\\n    [0.1, 2]\\r\\n])\\r\\ntrans = T @ xy\\r\\n\\r\\nT_inv = np.linalg.inv(T)\\r\\n\\r\\nun_trans = T_inv @ T @ xy\\r\\n\\r\\nf, axes = plt.subplots(1, 3, figsize=(9, 3))\\r\\naxes[0].scatter(xx, yy, s=10, c=xx+yy)\\r\\naxes[1].scatter(trans[0].reshape(xx.shape), trans[1].reshape(yy.shape), s=10, c=xx+yy)\\r\\naxes[2].scatter(un_trans[0].reshape(xx.shape), un_trans[1].reshape(yy.shape), s=10, c=xx+yy)\\r\\n\\r\\n# [...] Add axis, x and y witht the same scale\\r\\n\\n\\n\\n\\nFigure 5: Inverse of a transformation: the initial space (left) is transformed with the matrix\\xa0T\\xa0(middle) and transformed back using\\xa0T−1\\xa0(right).\\n\\xa0\\nAs you can see in Figure 5, the inverse\\xa0T−1\\xa0of the matrix\\xa0T\\xa0is associated with a transformation that reverses the one associated with\\xa0T.\\nMathematically, the transformation of a vector\\xa0v\\xa0by\\xa0T\\xa0is defined as:\\nTv\\nTo transform it back, you multiply by the inverse of\\xa0T:\\nT−1Tv\\n\\nOrder of the matrix products Note that the order of the products is from right to left. The vector on the right of the product is first transformed by\\xa0T\\xa0and then the result is transformed by\\xa0T−1.\\n\\n\\nAs you can see in\\xa0Essential Math for Data Science,\\xa0, so you have:\\n\\nmeaning that you get back the initial vector\\xa0v.\\n\\xa0\\nNon Invertible Matrices\\n\\xa0\\nThe linear transformation associated with a singular matrix (that is a non invertible matrix) can’t be reversed. It can occur when there is a loss of information with the transformation. Take the following matrix:\\n\\nLet’s see how it transforms the space:\\n\\nT = np.array([\\r\\n    [3, 6],\\r\\n    [2, 4],\\r\\n])\\r\\ntrans = T @ xy\\r\\n\\r\\nf, axes = plt.subplots(1, 2, figsize=(6, 3))\\r\\naxes[0].scatter(xx, yy, s=10, c=xx+yy)\\r\\naxes[1].scatter(trans[0].reshape(xx.shape), trans[1].reshape(yy.shape), s=10, c=xx+yy)\\r\\n# [...] Add axis, x and y witht the same scale\\r\\n\\n\\n\\n\\nFigure 6: The initial space (left) is transformed into a line (right) with the matrix T. Multiple input vectors land on the same location in the output space.\\n\\xa0\\nYou can see in Figure 6 that the transformed vectors are on a line. There are points that land on the same place after the transformation. Thus, it is not possible to go back. In this case, the matrix\\xa0T\\xa0is not invertible: it is singular.\\n\\xa0\\nBio: Hadrien Jean is a machine learning scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog (hadrienj.github.io).\\nOriginal. Reposted with permission.\\nRelated:\\n\\nEssential Math for Data Science: Probability Density and Probability Mass Functions\\nEssential Math for Data Science: Integrals And Area Under The Curve\\nEssential Math for Data Science: The Poisson Distribution',\n",
       " \"comments\\nBy Venkat Raman, Co-Founder at Aryma Labs\\n\\n\\nSource: Unsplash\\n\\n\\xa0\\nWe often come across YouTube videos, posts, blogs and private courses wherein they say “We accept the Null Hypothesis” instead of saying “We fail to reject the Null hypothesis”.\\nIf you correct them, they would say what's the big difference? “The opposite of ‘Rejecting the Null’ is ‘Accepting’ isn’t it?”.\\nWell, it is not so simple as it is construed. We need to rise above antonyms and understand one crucial concept. That crucial concept is\\xa0‘Popperian falsification’.\\nThis concept or philosophy also holds key to why we use the language “Fail to reject the Null”.\\nBasically, the Popperian falsification implies that\\xa0‘Science is never settled’. It keeps changing or evolving. Theories held sacrosanct today could be refuted tomorrow.\\n\\n\\nThe Popperian falsification implies that ‘Science is never settled’. It keeps changing or evolving. Theories held sacrosanct today could be refuted tomorrow.\\n\\n\\nSo under this principle, scientists never proclaim “X theory is true”. Instead what they try to do is, they try to prove that “the theory X is wrong”. This is called the principle of falsification.\\nNow having tried your best and you still could not prove the theory X is wrong, what would you say? You would say “I failed to prove theory X is wrong”.\\xa0Ah.. now can you see the parallels between “I failed to prove theory X is wrong” and “We fail to reject the Null ”.\\nNow lets come to why you can’t say “we accept the Null hypothesis”.\\nWe could not prove theory X is wrong. But does that really mean theory X is correct? No, somebody more smarter than in the future could prove theory x is wrong. There always exists that possibility. Remember above that we said “science is never settled”.\\nA more classic example is that of the ‘Black Swan’. “Suppose a theory proposes that all swans are white. The obvious way to prove the theory is to check that every swan really is white — but there’s a problem. No matter how many white swans you find, you can never be sure there isn’t a black swan lurking somewhere. So, you can never prove the theory is true. In contrast, finding one solitary black swan guarantees that the theory is false.”\\nNote: The post is merely to drive home the point how the language “we fail to reject” came about. It is not a post favoring inductive reasoning over deductive reasoning or vice versa. Neither it is an effort to prove or disprove Karl Popper’s falsification principle.\\nReference (black swan example):\\xa0https://www.newscientist.com/people/karl-popper/#ixzz70d4aPeIj\\nYour comments and opinions are welcome.\\nYou can reach out to me on: Linkedin and Twitter\\n\\xa0\\nBio: Venkat Raman is Co-Founder at Aryma Labs.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nAbstraction and Data Science: Not a great combination\\n5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist\\nManaging Your Reusable Python Code as a Data Scientist\",\n",
       " \"By Matthew Mayo, KDnuggets.\\ncomments\\n\\n\\xa0\\nAmazon's Machine Learning University is making its online courses, previously only available to Amazon employees, freely-available to the public.\\nOne of the first such courses made publicly available is Accelerated Computer Vision, a course which describes itself as follows:\\n\\nThis repository contains slides, notebooks, and datasets for the Machine Learning University (MLU) Computer Vision class. Our mission is to make Machine Learning accessible to everyone. We have courses available across many topics of machine learning and believe knowledge of ML can be a key enabler for success. This class is designed to help you get started with Computer Vision, learn about widely used Machine Learning techniques and apply them to real-world problems.\\n\\nThis concise course, taught by Rachel Hu, an applied scientist at AWS Deep Engine Science, is made up of a playlist of video lectures, along with a GitHub repository of course notebooks, slides, and data. The short videos and accompanying courseware material is collected into into 3 overarching lessons, organized as follows:\\n\\n\\xa0\\nLesson notebooks for the course focus on acquiring the skills for practical CV implementation, and have the following focuses, providing insight into what you can expect to learn along the way:\\n\\nNeural Networks\\nConvolutional Neural Networks\\nFinal Project Overview\\nAlexNet\\nAutoGluon for Computer Vision\\nResNet\\nYOLO\\n\\nThe course also includes the implementation of a final project in practical computer vision, the Jupyter notebook for which begins as follows, providing additional hints of expected learning outcomes:\\n\\n\\xa0\\nThis appears to be a great introductory course for computer vision beginners. Amazon has done the community a service by opening their Machine Learning University courses to the public, and between the Accelerated Computer Vision course and the other currently available offerings, it appears that those wishing to learn machine learning from a world class machine learning organization now have another option.\\n\\xa0\\nRelated:\\n\\nAccelerated Natural Language Processing: A Free Course\\nGoing Beyond Superficial: Data Science MOOCs with Substance\\nAwesome Machine Learning and AI Courses\",\n",
       " 'comments\\nBy Elise Devaux, Statice\\nThe economics, legal, and corporate implications of data privacy are now too strong to be ignored. In the last decades, different privacy-enhancing techniques were proposed to respond to ever-increasing requirements of technical and societal nature. But how do you decide which approach to use? \\n\\xa0\\nThe scope of data privacy\\n\\xa0\\nTwo years after the General Data Protection Regulation (GDPR) introduction, personal data is at the epicenter of today’s privacy requirements. Over 60 jurisdictions modern privacy and data protection laws to regulate personal data processing. \\nUnder the GDPR, any information relating to an identified or identifiable natural person is personal data. This inclusive definition encompasses both direct and indirect identifiers. Direct identifiers are information that explicitly identifies a person, such as a name, a social security number, or biometric data. Indirect, or quasi, identifiers refer to information that can be combined with additional data to identify a person. They are, for example, medical records, dates and places of birth, or financial information.\\xa0\\n\\n\\xa0\\nTraditionally, a risk hierarchy existed between these two types of attributes. Direct identifiers were perceived as more “sensitive” than quasi-identifiers. In many data releases, only the former attributes were subject to some privacy protection mechanism, while the latter were released in clear. Such releases were often followed by prompt re-identification of the supposedly ‘protected’ subjects. It soon became apparent that quasi-identifiers could be just as ‘sensitive’ as direct identifiers. With the GDPR, this notion has finally made it into law: both types of attributes are put on the same level, identifiers and quasi-identifiers attributes are personal data and present an equally important privacy breach risk.\\nNowadays protection laws strictly regulate personal data processing. This makes a strong case for implementing privacy protection techniques. Indeed, failure to comply exposes companies to severe penalties. Besides, implementing proper privacy protections might lead to customer trust increase. In a world plagued by data breaches and privacy violations, people are increasingly concerned about what happens to their data. And finally, data breaches targeting personal data are costing companies money. Personal data remains the most expensive item to lose in a breach.\\n\\xa0\\nMost often, it is personal data that we seek to protect. However, any sensitive data can require privacy protection. For example, any business information, such as financial information or trade secrets, can need to remain private. Protecting business or classified information is usually a matter of holding on to corporate value.\\nThese elements could incite companies to decrease the amount of personal data they collect and process. However, information is at the core of product and service development for many companies. By deleting personal data, companies would pass on many of the benefits and values they could derive from it. For all these reasons, companies must find privacy protection mechanisms to remain data-driven while safeguarding individuals\\' privacy.\\xa0\\xa0\\n\\xa0\\nAnonymization as means of privacy protection\\n\\xa0\\nA way to guarantee the privacy of personal and sensitive data is to anonymize it. Anonymization refers to the process of irreversibly transforming data to prevent the re-identification of individuals. Meaning that if a company releases an anonymized dataset, it’s theoretically impossible to re-identify a person from it, either directly or indirectly. Anonymization represents the highest form of privacy protection. However, perfect anonymity of data is rarely achieved, as it would render the data almost useless.\\nFor some time, the middle ground has been to use lighter privacy protection mechanisms, mechanisms such as data masking or pseudonymization. These processes aim at protecting data by removing or altering its direct, sometimes indirect, identifiers. It\\'s quite frequent to see the term \"anonymization\" in references to these methods. However, the two have clear legal and technical implications.\\xa0\\n\\n\\xa0\\nOn the technical side, these techniques produce data with different levels of privacy protection. Masking or pseudonymization techniques can be reversed. They complicate the identification of individuals but don\\'t remove the possibility of re-identifying someone. They\\'re a weaker privacy protection mechanism than anonymization, which minimizes to greater extent re-identification risks.\\nOn the legal side, modern data privacy laws, such as the GDPR and the California Consumer Privacy Act (CCPA), differentiate between pseudonymization and anonymization processes. The latest is considered private enough that anonymized data isn’t subject to personal data protection laws anymore. Pseudonymized data, on the other hand, still represents a risk for individual privacy and must be handled as personal data.\\xa0\\n\\xa0\\nPrivacy protection techniques\\n\\xa0\\nDifferent privacy protection techniques offer different protection levels. It is essential to know and understand how they work. Depending on the nature of the application and the data, you can choose one technique or another.\\nAs mentioned above, pseudonymization, or data masking, is commonly used to protect data privacy. It consists of altering data, most of the time, direct identifiers, to protect individuals\\' privacy in the datasets. There are several techniques to produce pseudonymized data:\\n\\nEncryption: hiding sensitive data using a cipher protected by an encryption key.\\nShuffling: scrambling data within a column to disassociate its original other attributes.\\nSuppression: nulling or removing from the dataset the sensitive columns.\\nRedaction: masking out parts of the entirety of a column’s values.\\n\\n\\n\\xa0\\nData generalization is another approach to protect privacy. The underlying idea is to prevent a person’s identification by reducing the details available in the data. There are, as well, various approaches to generalizing data. For example, it\\'s possible to generate an aggregated value from the sensitive data or use a value range in place of a numerical value. Among data generalization techniques, we can find popular privacy models such as k-anonymity, l-diversity, t-closeness expressly designed to mitigate data disclosure risks.\\xa0\\n\\n\\xa0\\nSynthetic data offers an alternative approach to privacy protection. Instead of altering or masking the original data, one generates completely new artificial data. Machine learning models help create this new dataset that mimics the statistical properties of the original data.\\xa0\\n\\n\\xa0\\n\\xa0\\nThe shortcomings of traditional data protection techniques\\n\\xa0\\nData masking and generalization techniques are popular protection mechanisms. However, they present some drawbacks that it’s essential to know.\\xa0\\nPseudonymization presents limitations when it comes to data privacy. In multiple instances, researchers proved that this technique could lead to re-identification and disclosure of individual\\' identities. Most of the time, when you mask or remove data identifiers, the quasi-identifiers are still present. They can link individuals across secondary data sources, ultimately disclosing people\\'s identity and breaching privacy.\\xa0\\n\\n\\xa0\\nOn the other hand, data generalization methods are known to deteriorate the value of the data. For example, when values are highly aggregated, the resulting data loses its statistical granularity. This granularity could have brought value to analyses. Researchers showed that some generalization methods, such as k-anonymity, had privacy loopholes.\\xa0\\nSynthetic data allows finding a balance between the data privacy and data utility shortcomings of other methods. However, it can also present limitations and risks depending on the technology and the process used to generate the synthetic data. For example, some techniques can fail at properly mimicking data that holds unusual data points. Synthetic datasets can also suffer from privacy breaches when proper protections aren’t ensured.\\xa0\\nThere are more and more opportunities to address the privacy shortcomings of traditional techniques. Technologies are showing increasing signs of maturity. The number of PrivacyTech companies is rising quickly, and the industry is quickly attracting large fundings. Public bodies are starting to promote privacy-preserving technology. For instance, the UK government acknowledged in its National Data Strategy that synthetic data and other privacy-enhancing technologies represent an opportunity for innovation. Leading industry analysts such as Gartner also recognizes the rise of these technologies. \\n\\xa0\\nBio: Elise Devaux (@elise_deux) is a tech enthusiast digital marketing manager, working at Statice, a startup specialized in synthetic data as a privacy-preserving solution.\\nRelated:\\n\\nHow “Anonymous” is Anonymized Data?\\n10 Use Cases for Privacy-Preserving Synthetic Data\\n10 Steps for Tackling Data Privacy and Security Laws in 2020',\n",
       " 'comments\\nBy JABDE, Journal of Astrological Big Data Ecology.\\n\\nFor those new to dealing with engineers and data scientists, it’s very tough to understand what flavor statistician you may be dealing with, how to manage them, and how much to trust them. Statistics can be closer to an art than a science. When it comes to the different types of statisticians, there are almost as varied as sexual identities. Many statisticians and analysts can have multiple identities, but they all generally follow the Dunning-Kruger Effect. Experience and confidence are not one-to-one relationships.\\nHere is a guide to the top 8 most common types of data scientists.\\n\\xa0\\n1. The Unashamed Frequentist\\n\\xa0\\nThis type of statistician worships the p-value and is often the most confident of their answer. You can spot these analysts by a lack of predictive behavior and overuse of null hypothesis testing. This flavor of analyst will report what happened and not much else. While the unashamed frequentist is easy to understand and works the fastest, they are very boring and prone to accidental\\xa0and sometimes intentional\\xa0p-hacking.\\n\\xa0\\n2. The Data Bro\\n\\xa0\\nReady for the new hotness of the most popular statistical library? The data bro typically does not have a heavy math background, but they know how to code by google. The easiest way to spot a data bro is by their flashy presentations, fancy visualizations, and their blind use of open-source statistics libraries. The data bro is fantastic at presentations and, under the right circumstances, can quickly turn around a beautifully intuitive data product. Unfortunately, the data bro is the most susceptible to using faulty online libraries developed by non-statisticians or misapplying statistical tests because the graph just looked too cool.\\n\\xa0\\n3. The Novice\\n\\xa0\\nWhen a statistician is new to the game, you can tell by their obsession with doing things by the book and an unwillingness to take guesses. The novice will spend a week researching the best performance metric to determine if their analysis is working. With no experience or intuition, their days are spent trying multiple new methods on one data set and writing a thesis until all possible options have been exhausted. They won’t be novices for long but will never be sure of themselves until they have more experience.\\n\\xa0\\n4. The Reacher\\n\\xa0\\nEver want to design a neural network for a simple regression problem? The Reacher has. The Reacher will use the most complicated method possible for analysis for the sake of trying something cool. The Reacher is a high risk, medium to low reward analyst who will take way too long to create a simple data product by providing analysis that nobody asked for that will only be appreciated and understood by a niche audience.\\n\\xa0\\n5. The one-trick pony\\n\\xa0\\nSome tools are so great that with a little bit of creativity, you can solve just about any problem you may encounter. However, just because anything can be solved with a nonlinear algorithm doesn’t mean that it should. The one-trick pony has learned one of these adaptive methods very well. While having a complete understanding of the method, the one-trick pony may forget to check assumptions and misapply their favorite method. A screwdriver may not be meant to hammer a nail, but you can with enough force.\\n\\xa0\\n6. The Philosopher\\n\\xa0\\nIf you ever ask a statistician what one of their metrics or results means only to receive a sermon on the different ways of interpreting results, you’re dealing with a philosopher. For the philosopher, everything is a figment of their imagination, and nothing is real. You can debate the meaning of one word with the philosopher for an hour only to discover that they don’t believe in words or that the word meant something else 200 years ago, and that’s the definition they use. None of their models are correct, but some of them are ‘useful’ as long as you understand the technical risk. The philosopher is similar to the novice in that they will never feel like they have confidently answered the question.\\n\\xa0\\n7. The Fake Bayesian\\n\\xa0\\nIf you ever hear an analyst mention that the difference between Frequentists and Bayesians is that Bayesians win in Vegas, but you’ve never heard them talk about priors, you’re talking to a fake Bayesian. Nate Silver is their god. These analysts will religiously read FiveThirtyEight and assume that their methods are inherently better if they just use Bayes Theorem without realizing that nearly every statistical method employs Bayes Theorem. The fake Bayesian is a sign of a small amount of experience but is more prone to overconfidence.\\n\\xa0\\n8. The True Bayesian\\n\\xa0\\nI don’t know any of these types, so I’m going to have to get back to you. I assume they’re pretty cool people.\\n\\xa0\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nCartoon: Data Scientist vs Data Engineer\\nCartoon: Machine Learning takes a vacation\\nCartoon: What Else Can AI Guess From Your Face?',\n",
       " 'comments\\nBy Hrvoje Smolic, Founder at Graphite Note, Qualia Data Sciences and Qualia d.o.o..\\n\\xa0\\nYes, the charts are important...\\n\\xa0\\nFor every executive who is about to pull up the first slide of a presentation in a boardroom filled with eager associates, there is usually that moment just before speaking when nerves fray, last-minute doubts fill the mind, and fear of failure can weaken the resolve. Did I choose the right image to start with? Is the text too small to read? Is the right tone conveyed throughout the presentation? The usual recommendations of how best to present data most effectively crowd into the mind — don’t clutter, choose relevant KPIs, keep it simple, choose layouts carefully, less is more, and be cautious with colors.\\n\\nSource: Image by Nong Vang,\\xa0unsplash.com.\\n\\xa0\\n...but the key is emotion.\\n\\xa0\\nThese are all helpful ideas, but none of them can hold a candle to the most important concept of all, emotion.\\nYou see, whereas brains are built for visuals, hearts turn on stories.\\n\\nVisualization might be important, but emotion is key. If you want to sell a story, send a missile to the heart.\\n\\nDon’t get me wrong, I believe in the power of graphs and charts. Stacked columns, line charts, waterfall, and scatter plots all have their place in data presentation. However, at its core, data visualization is a tool that transforms data into actionable insight by using it to tell a story.\\n\\nCharts can be the start, not the end of the communication.\\n\\nThe psychologist Jerome Bruner claims that we are 22 times more likely to remember a fact when it has been wrapped in a story. Data-driven storytelling is a powerful force as it takes stats and metrics and puts them into context through a narrative that everyone inside or outside of an organization can grasp.\\nWithout question, human beings are, first and foremost, visual creatures. Our brains are built for visual information, as the following data shows:\\n\\n90% of the information processed by the brain is visual.\\nIt takes\\xa0only 13 milliseconds\\xa0for the human brain to process an image.\\nThe human brain processes images 60,000 times faster than text.\\n80% of people remember what they see, compared to ten percent what they hear and 20 percent of what they read.\\nIn response to a recent survey, 95% of B2B buyers said that they wanted shorter and highly visual content.\\nPublishers that feature visual content grow traffic 12 times faster than those who don’t.\\n\\nHowever, as anyone who has sat in a darkened movie theater and felt a warm tear roll down her cheek knows, it is the emotion that makes life’s moments unforgettable.\\n\\nSource: Image by\\xa0Robina Weermeijer,\\xa0unsplash.com.\\n\\xa0\\nThe use of emotion can make data presentation unforgettable\\n\\xa0\\nAs Carl Bucher once stated, “They may forget what you said, but they will never forget how you made them feel.”\\n“Out of clutter, find simplicity,” said Albert Einstein. This is a great motto for presenters. Simple messages resonate deeply if extraneous items are being stripped away.\\n\\nEmotions cut through the clutter like nothing else.\\n\\nBefore putting any design elements in place, think about the end goal. What are the most important elements that need to be showcased? Who is the audience? What is the emotional punch you want to be conveyed on the final slide? To build successful dashboards, a designer needs to put himself in the audience’s shoes.\\nDesigners should always try to provide maximum information.\\xa0Without the right context, numbers that might seem extremely obvious to one person might be perplexing to others.\\nAll the axes should be named, and titles should be added to all charts. Comparison values should also be included. The rule of thumb here is to use the most common comparisons, for example, comparison against a set target, against a preceding period, or against a projected value.\\nComparisons also add emotion as they provide benchmarks of understanding.\\nIt is easy to recognize the emotional power of a line chart that reveals sales are falling off a cliff—or taking off into the ether.\\nThe artist\\xa0Kenneth Noland once said, “For me, context is the key — from that comes the understanding of everything.”\\n\\nWithout providing context, it’s impossible to know whether numbers are good or bad, typical or atypical.\\n\\nWithout comparison values, numbers on a dashboard are meaningless for viewers. And more importantly, users won’t know if any action is warranted.\\n\\nSource: Image by Author.\\n\\xa0\\nDesign: Intelligence Made Visible\\n\\xa0\\nDashboard design best practices concern more than just good metrics and well-thought-out charts. The second step of dashboard design is the placement of charts on a dashboard. If your dashboard is well organized visually, information can easily be found.\\nPoor layout forces users to think more as they try to grasp a point the presenter is trying to make. Nobody likes to look for data in a jungle of charts and a maze of tangled numbers.\\nThe general rule is that the key information should be displayed first — on the top of the screen, upper left-hand corner. Most cultures read from left to right, then from top to bottom, which means viewers intuitively look at the upper-left part of a page first and read down from there.\\n“Color is a power which directly influences the soul,”\\xa0states the famous Russian artist Wassily Kandinsky. Who can argue with one of the greatest painters and art theorists of the 20th century? Color is used by advertisers everywhere to convey an emotional message that resonates deeply within any audience. Without a shadow of a doubt, the use of color is one of the most important best practices in dashboard design.\\n\\xa0\\nGraphite Note Proposition\\n\\xa0\\nWe propose data analytics to “go vertical,”\\xa0so there will be much more natural, easier to explain insights and conclusions\\xa0by design.\\n\\nSource: Graphite Note Notebook.\\nYou have a title text block, graph block, predictive analytics block, explanation text block, several graph blocks, etc. A notebook-style data analysis.\\nThis will allow data analysts to explain the whole process beautifully, tell a data story, and engage the audience (your team). And that will, in turn,\\xa0supercharge decision-making because decisions are made on an emotional level, where the story hits.\\n\\xa0\\nData Storytelling\\n\\xa0\\nOf course, you don’t need to go all Joseph Campbell Hero of a Thousand Faces with your presentations. Charting a mythical story filled with a mixture of heroes, mentors, tricksters, shapeshifters, guardians, and shadows isn’t the goal, but recognizing how myth can turn each story point — or chart— is. Understanding the power of the story adds a subtextual level to any data presentation that, although not recognized by an audience, will pull on their emotions.\\nDale Carnegie once said, “There are always three speeches for every one you actually gave. The one you practiced, the one you gave, and the one you wish you gave.” Utilizing proper data presentation techniques and wrapping emotion into the power of storytelling will help ensure that the speech you give is not separated into three but combined into one.\\nAbove all else, it will be the speech you had wished you given, and you won’t suffer pangs of regret or wishful thinking afterward.\\nEmotion is a tricky element to include, and it must always, always, always be united with the truth. However, if you can wrap your data presentation in a compelling story, you’ll not only touch your audience’s heart but also have them eating out of the palm of your hand.\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nTelling a Great Data Story: A Visualization Decision Tree\\nWhy data analysts should choose stories over statistics\\nHow to frame the right questions to be answered using data',\n",
       " 'comments\\nBy Vegard Flovik\\n\\n\\nA graph visualization issued from the\\xa0Opte Project, a tentative cartography of the Internet\\n\\n\\xa0\\nGraph theory might sound like an intimidating and abstract topic to you, so why should you even spend your time reading an article about it? However, although it might not sound very applicable, there are actually an abundance of useful and important applications of graph theory! In this article, I will try to explain briefly what some of these applications are. In doing so, I will do my best to convince you that having at least some basic knowledge of this topic can be useful in solving some interesting problems you might come across.\\nIn this\\xa0article, I will through a concrete example show how a route planning/optimization task can be formulated and solved using graph theory. More specifically, I will consider a large warehouse consisting of 1000s of different items in various locations/pickup points. The challenge here is, given a list of items, which path should you follow through the warehouse to pickup all items, but at the same time minimize the total distance traveled? For those of you familiar with these kind of problems, this has quite some resemblance to the famous\\xa0traveling salesman problem. (A well known problem in\\xa0combinatorial optimization, important in\\xa0theoretical computer science\\xa0and\\xa0operations research).\\nAs you might have realized, the goal of this article is not to give a comprehensive introduction to graph theory (which would be quite a tremendous task). Through a real-world example, I will rather try to convince you that knowing at least some\\xa0basics\\xa0of graph theory can prove to be very useful!\\nI will start with a brief historical introduction to the field of graph theory, and highlight the importance and the wide range of useful applications in many vastly different fields. Following this more general introduction, I will then shift focus to the warehouse optimization example discussed above.\\n\\xa0\\nThe history of Graph Theory\\n\\xa0\\nThe basic idea of graphs were first introduced in the 18th century by the Swiss mathematician Leonhard Euler, one of the most eminent mathematicians of the 18th century (and of all time, really). His work on the famous “Seven Bridges of Königsberg problem”, are commonly quoted as origin of\\xa0graph theory.\\nThe city of Königsberg in Prussia (now Kaliningrad, Russia) was set on both sides of the Pregel River, and included two large islands — Kneiphof and Lomse — which were connected to each other, or to the two mainland portions of the city, by seven bridges (as illustrated in the below figure to the left). The problem was to devise a walk through the city that would cross each of those bridges once and only once.\\nEuler, recognizing that the relevant constraints were the four bodies of land & the seven bridges, drew out the first known visual representation of a modern graph. A modern graph, as seen in bottom-right image, is represented by a set of points, known as\\xa0vertices or nodes, connected by a set of lines known as edges.\\n\\n\\nCredit:\\xa0Wikipedia\\n\\n\\xa0\\nThis abstraction from a concrete problem concerning a city and bridges etc. to a graph makes the problem tractable mathematically, as this abstract representation includes only the information important for solving the problem. Euler actually proved that this specific problem has no solution. However, the difficulty he faced was the\\xa0development of a suitable technique of analysis, and of subsequent tests that established this assertion with mathematical rigor. From there, the branch of math known as graph theory lay dormant for decades. In modern times, however, it’s applications are finally exploding.\\n\\xa0\\nIntroduction to Graph Theory\\n\\xa0\\nAs mentioned previously, I do not aim to give a comprehensive introduction to graph theory. The following section still contains some of the basics when it comes to different kind of graphs etc., which is of relevance to the example we will discuss later on path optimization.\\nGraph Theory\\xa0is ultimately the study of\\xa0relationships. Given a set of nodes & connections, which can abstract anything from city layouts to computer data, graph theory provides a helpful tool to quantify & simplify the many moving parts of dynamic systems. Studying graphs through a framework provides answers to many arrangement, networking, optimization, matching and operational problems.\\nGraphs can be used to model many types of relations and processes in physical, biological, social and information systems, and has a wide range of useful applications such as e.g.\\n\\nFinding communities in networks, such as social media (friend/connection recommendations), or in the recent days for possible spread of COVID19 in the community through contacts.\\nRanking/ordering hyperlinks in search engines.\\nGPS/Google maps to find the shortest path home.\\nStudy of molecules and atoms in chemistry.\\nDNA sequencing\\nComputer network security\\n….. and many more….\\n\\n\\n\\nA simple example of a graph with 6 nodes\\n\\n\\xa0\\n\\n\\nA slightly more complex social media network. Credit:\\xa0Martin Grandjean\\xa0Wikimedia\\n\\n\\xa0\\nAs mentioned, there are several types of graphs that describe different kind of problems (and the constraints within them). A nice walk-through of various types of graphs can also be found in a\\xa0previous article\\xa0by\\xa0Kelvin Jose,\\xa0and the below section represents a subset of that article.\\n\\xa0\\nTypes of Graphs\\n\\xa0\\nThere are different types of graph representations available and we have to make sure that we understand the kind of graph we are working with when programmatically solving a problem which includes graphs.\\n\\nUndirected Graphs\\n\\nAs the name shows, there won’t be any specified directions between nodes. So an edge from node A to B would be\\xa0identical\\xa0to the edge from B to A.\\n\\nIn the above graph, each node could represent different cities and the edges show the bidirectional roads.\\n\\nDirected Graphs (DiGraphs)\\n\\nUnlike undirected graphs, directed graphs have orientation or\\xa0direction\\xa0among different nodes. That means if you have an edge from node A to B, you can move only from A to B.\\n\\n\\nCredit:\\xa0WikiMedia\\n\\n\\xa0\\nLike the previous example, if we consider nodes as cities, we have a direction from city 1 to 2. That means, you can drive from city 1 to 2 but not back to city 1, because there is no direction back to city 1 from 2. But if we closely examine the graph, we can see cities with bi-direction. For example cities 3 and 4 have directions to both sides.\\n\\nWeighted Graphs\\n\\nMany graphs can have edges containing a weight associated to represent a real world implication such as cost, distance, quantity etc …\\n\\n\\nCredit:\\xa0Estefania Cassingena Navone\\xa0via\\xa0freecodecamp.org\\n\\n\\xa0\\nWeighted graphs\\xa0could be either directed or undirected graph. The one we have in this example is an undirected weighted graph. The cost (or distance) from the green to the orange node (and vice versa) is 3. Like our previous example, if you want to travel between two cities, say city green and orange, we would have to drive 3 miles. These metrics are self-defined and could be changed according to the situations. For a more elaborated example, consider you have to travel to city pink from green. If you look at the city graph, we can’t find any direct roads or edges between the two cities. So what we can do is to travel via another city. The most promising routes would be starting from green to pink via orange and blue. If the weights are costs between cities, we would have to spend 11$ to travel via blue to reach pink but if we take the other route via orange, we would only have to pay 10$ for the trip.\\nThere may be several weights associated with each edge, including distance, travel time, or monetary cost. Such weighted graphs are commonly used to program GPS’s, and travel-planning search engines that compare flight times and costs.\\n\\xa0\\nGraph Theory → Route optimization\\n\\xa0\\nHaving (hopefully) convinced you that graph theory is worth knowing something about, it is now time to focus on our example case of route planning when picking items in our warehouse.\\n\\xa0\\nChallenge:\\n\\xa0\\nThe challenge here is that given a “picking list” as input, we should find the shortest route that passes all the pickup points, but also complies to the restrictions with regard to where it is possible/allowed to drive. The assumptions and constraints here are that crossing between corridors in the warehouse is only allowed at marked “turning points”. Also, the direction of travel must follow the specified legal driving direction for each corridor.\\n\\xa0\\nSolution:\\n\\xa0\\nThis problem can be formulated as an optimization problem in graph theory. All pickup points in the warehouse form a “node” in the graph, where the edges represent permitted lanes/corridors and distances between the nodes. To introduce the problem more formally, let us start from a simplified example.\\nThe graph below represents 2 corridors with 5 shelves/pickup-points per corridor. All shelves are here represented as a node in the graph, with an address ranging from 1–10. The arrows indicate the permitted driving direction, where the double arrows indicate that you can drive either way. Simple enough, right?\\n\\nBeing able to represent the permitted driving routes in the form of a graph, means that we can use mathematical techniques known from graph theory to find the optimal “driving route” between the nodes (i.e., the stock shelves in our warehouse).\\nThe example graph above can be described mathematically through an «adjacency matrix». The adjacency matrix to the right in the below figure is thus a representation of our «warehouse graph», which indicates all permitted driving routes between the various nodes.\\n\\n\\nExample 1:\\xa0You are allowed to travel from node 2 → 3, but not from 3 → 2. This is indicated by the “1” in the adjacency matrix to the right.\\nExample 2:\\xa0You are allowed to go from both node 8 → 3, and from 3 → 8, again indicated by the “1”`s in the adjacency matrix (which in this case is symmetric when it comes to travel direction).\\n\\n\\xa0\\nBack to our warehouse problem:\\n\\xa0\\nA real warehouse is of course bigger and more complex than the above example. However, the main principles of how to represent the problem through a graph remains the same. To make the real problem slightly simpler (and more visually suitable for this article), I have reduced the total number of shelves/pickup-points (approximately every 50th shelf included, marked with black squares in the below figure). All pickup points are given an address (“node number”) from 1–74. The other relevant constraints mentioned earlier, such as permitted driving directions in each of the corridors, as well as the allowed “turning points” and shortcuts between the corridors are also indicated in the figure..\\n\\n\\nGraph representation of our simplified warehouse\\n\\n\\xa0\\nThe next step is then to represent this graph in the form of a adjacency matrix. Since we are here interested in finding both the optimal route and total distance, we must also include the driving distance between the various nodes in the matrix.\\n\\n\\nAdjacency matrix for the “warehouse graph”\\n\\n\\xa0\\nThis matrix indicates all constraints with regard to both the permitted direction of travel, which “shortcuts” are permitted, any other restrictions as well as the driving distance between the nodes (illustrated through the color). As an example, the “shortcut” between nodes 21 and 41 shown in the graph representation can clearly be identified also in the adjacency matrix. The “white areas” of the matrix represents the paths that are not allowed, indicated through an “infinite” distance between those nodes.\\n\\xa0\\nFrom graph representation to path optimization\\n\\xa0\\nJust having an abstracted representation of our warehouse in the form of a graph, does of course not solve our actual problem. The idea is rather that through this graph representation, we can now use the mathematical framework and algorithms from graph theory to solve it!\\nSince graph optimization is a well-known field in mathematics, there are several methods and algorithms that can solve this type of problem. In this example case, I have based the solution on the “Floyd-Warshall algorithm”, which is a well known\\xa0algorithm\\xa0for finding\\xa0shortest paths\\xa0in a\\xa0weighted graph. A single execution of the algorithm will find the lengths (summed weights) of shortest paths between all pairs of nodes. Although it does not return details of the paths themselves, it is possible to reconstruct the paths with simple modifications to the algorithm.\\nIf you give this algorithm as input a “picking order list” where you go through a list of items you want to pick, you should then be able to obtain the optimal route which minimize the total driving distance to collect all items on the list.\\nExample:\\xa0Let us start by visualizing the results for a (short) picking list as follows: Start from node «0», pick up items at location/node 15, 45, 58 and 73 (where these locations are illustrated in the figure below). The algorithm finds the shortest allowable route between these points through calculating the “distance matrix”,\\xa0D, which can then be used to determine the total driving distance between all locations/nodes in the picking list.\\n\\nStep 1:\\xa0D[0][15] → 90 m\\nStep 2:\\xa0D[15][45] →52 m\\nStep 3:\\xa0D[45][58] → 34 m\\nStep 4:\\xa0D[58][73] → 92 m\\n\\nTotal distance = 268m\\n\\n\\nOptimized driving route from picking list\\n\\n\\xa0\\nHave tested several “picking lists” as input and verifying the proposed driving routes and calculated distance, the algorithm has been able to find the optimal route in all cases. The algorithm respects all the imposed constraints, such as the permitted direction of travel, and uses all permitted “shortcuts” to minimize the total distance.\\n\\xa0\\nFrom path optimization to useful insights\\n\\xa0\\nAs shown through the above example, we have developed an optimization algorithm that calculates the optimal driving route via all points on a picking order list (for a simplified version of the warehouse). By providing a list of picking orders as input, one can thus relatively easily calculate statistics on typical mileage per. picking order. These statistics can then also be filtered on various information such as item type, customer, date, etc. In the following section, I have thus picked a few examples on how one can extract interesting statistics from such a path optimization tool.\\nIn doing this, I first generated 10.000 picking order lists where the number of items per list ranges from 1–30 items, located at random pickup points in the warehouse (address 3–74 in the figure above). By performing the path optimization procedure over all these picking list, we can then extract some interesting statistics.\\nExample 1:\\xa0Calculate mileage as a function of the number of units per. picking order list. Here, you would naturally assume that the total mileage increases the more items you have to pick. But, at some level, this will start to flatten out. This is due to the fact that one eventually has to stop by all the corridors in the warehouse to pick up goods, which then prevents us from making use of clever “shortcuts” to minimize the total driving distance. This tendency can be illustrated in the figure below to the left, which illustrates that for more than approximately 15–20 units per picking order, adding extra items does not make the total mileage much longer (as you have to drive through all corridors of the warehouse anyway). Note that the figures show a “density plot” of the distribution of typical mileage per. picking orders list.\\nAnother interesting statistic, which shows the same trend, is the distribution of driving distance per picked item in the figure to the right. Here, we see that for picking lists with few items, the typical mileage per. item is relatively high (with a large variance, depending on how “lucky” we are with some items being located in the same corridor etc.). For picking lists with several items though, the mileage per. item is gradually decreasing. This type of statistic can thus be interesting to investigate closer, in order to optimize how many items each picking order list should contain in order to minimize the mileage per picked item.\\n\\n\\nEstimating driving distance per list/item vs. number of items per list.\\n\\n\\xa0\\nExample 2:\\xa0Here I have used real-world data that also contains additional information in the form of a customer ID (here shown for only two customers). We can then take a closer look at the distribution in mileage per. picking order list for the two customers. For example, do you typically have to drive longer distances to pick the goods of one customer versus another? And, should you charge that customer extra for this additional cost?\\nThe below figure to the left shows the distribution in mileage for «Customer 1» and «Customer 2» respectively. One of the things we can interpret from this is that for customer 2, most picking order lists have a noticeably shorter driving distance compared to customer 1. This can also be shown by calculating the average mileage per. picking order list for the two customers (figure to the right).\\n\\nThis type of information can e.g. be used to implement pricing models where the product price to the customer is also based on mileage per order. For customers where the order involves more driving (and thus also more time and higher cost) you can consider invoicing extra compared to orders that involve short driving distances.\\n\\xa0\\nSummary:\\n\\xa0\\nIn the end, I hope I have convinced you that graph theory is not just some abstract mathematical concept, but that it actually has many useful and interesting applications! Hopefully, the examples above will be useful for some of you in solving similar problems later, or at least satisfy some of your curiosity when it comes to graph theory and some of its applications.\\nThe cases discussed in the article covers just a few examples that illustrate some of the possibilities that exist. If you have previous experience and ideas on the topic, it would be interesting to hear your thoughts in the comments below!\\nDid you find the article interesting? If so, you might also like some of my other articles on topics such as AI, Machine Learning, physics, etc., which you can find in the links below and on my medium author profile:\\xa0https://medium.com/@vflovik\\n\\nDeep learning based reverse image search for industrial applications\\nDeep Transfer Learning for Image Classification\\nBuilding an AI that can read your mind\\nThe hidden risk of AI and Big Data\\nHow to use machine learning for anomaly detection and condition monitoring\\nHow (not) to use Machine Learning for time series forecasting: Avoiding the pitfalls\\nHow to use machine learning for production optimization: Using data to improve performance\\nHow do you teach physics to AI systems?\\nCan we build artificial brain networks using nanoscale magnets?\\nArtificial Intelligence in Supply Chain Management: Utilizing data to drive operational performance\\n\\nI also discuss various topics related to AI/machine learning in the workshop presentation below: “ From hype to real-world applications”. I hope you found these resources interesting and useful!\\n\\xa0\\nBio:\\xa0Vegard Flovik is working on machine learning and advanced analytics at Kongsberg Digital.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nGraph Representation Learning: The Free eBook\\nBuilding a Deep Learning Based Reverse Image Search\\nHow (not) to use Machine Learning for time series forecasting: The sequel',\n",
       " 'By Jesus Rodriguez, Intotheblock.\\ncomments\\n\\n\\nSource:\\xa0https://bdtechtalks.com/2021/07/15/openai-codex-ai-programming/\\n\\n\\xa0\\nA couple of weeks ago, OpenAI astonished the artificial intelligence(AI) world with the release of Codex, a massive model that can translate natural language into code. Codex can effectively generate end to end from basic language instructions. If you don’t believe me, you should watch this video which can be considered one of the best AI demos of all time 😉\\n\\n\\nVideo Credit: OpenAI\\n\\n\\xa0\\nA lot has been written about Codex’s capabilities since its initial launch.\\nHowever, I have been more intrigued by the small requirements that become incredibly relevant to build a model of this magnitude. Deep diving into Codex, there are a few interesting things I found that thought would be good to highlight:\\n\\xa0\\n1.\\xa0Codex is proficient in about a dozen languages but it was trained for Python\\n\\xa0\\nI found this incredibly insightful. The original goal of OpenAI was to make Codex proficient in Python but it turns out that the model picked up other languages during the pretraining process. This speaks to the unique capabilities of language pretrained models.\\n\\xa0\\n2.\\xa0Testing Codex’s was more than tricky\\n\\xa0\\nThe AI community has been amazed by the research behind Codex but I think the engineering side has been as impressive. One aspect that I was particularly intrigued about was the testing part. How in the world do you test live code without taking massive risks. It turns out that the OpenAI team put a ton of work building very sophisticated sandboxes to test the outputs from Codex in isolation.\\n\\xa0\\n3.\\xa0Matching semantics to code is far from trivial\\n\\xa0\\nTraining a model in all the source code in the world sounds cool but its far from trivial. After all, not all code is created equal. Code in Github can be poorly documented while notebooks can have rich semantic information. Similarly, code snippets in Stack Overflow have richer levels of semantic information. Mapping code sections to language semantics was one of the challenges of building Codex.\\n\\xa0\\n4.\\xa0Codex still struggles with task decomposition\\n\\xa0\\nIf you think how programmers work, we tend to decompose a problem into smaller tasks and produce code for those. It turns out that Codex is great at the latter but still struggles in problem decomposition tasks. This shouldn’t be surprising if we think that problem decomposition requires very complex cognitive skills.\\n\\xa0\\n5.\\xa0Supervised Fine-Tuning was a huge part of building Codex\\n\\xa0\\nCode in the internet appears in all sorts of levels of completeness, documentation, syntactic richness etc. Training a model in such a diverse code sets can produce unreliable results. In that sense OpenAI had to undergo a massive supervised fine-tuning effort.\\n\\xa0\\nThese are some of the aspects about Codex that are not super well-known but that have been major contributors to the success of the first version of the model. Codex success was both due to advanced ML research as a massive ML engineering and infrastructure efforts.\\n\\xa0\\nBio: Jesus Rodriguez is currently a CTO at Intotheblock. He is a technology expert, executive investor and startup advisor. Jesus founded Tellago, an award winning software development firm focused helping companies become great software organizations by leveraging new enterprise software trends.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nGitHub Copilot Open Source Alternatives\\nJurassic-1 Language Models and AI21 Studio\\nHow to Train a BERT Model From Scratch',\n",
       " 'comments\\nBy Sigmoid\\n\\xa0\\n1. What is DataOps?\\n\\xa0\\nIn simple terms, DataOps can be defined as a methodology that offers speed and agility to data pipelines, thereby enhancing the quality of data and delivery practices.\\xa0DataOps enables\\xa0greater collaboration within organizations and drives data initiatives at scale. With the help of\\xa0automation, DataOps improves the availability, accessibility, and integration of data. It brings people, processes, and technology together to deliver reliable and high-quality data to all stakeholders. Rooted in the agile methodology, DataOps aims at offering optimal consumer experience by continuous delivery of analytic insights.\\n\\xa0\\n2. How is DataOps different from DevOps?\\n\\xa0\\nDataOps is often considered DevOps applied to data analytics. However, DataOps is more than just that. It also combines key capabilities of data engineers and data scientists to offer a robust, process-driven structure to data-focused enterprises. DevOps combines software development and IT operations to ensure continuous delivery in the systems development lifecycle. Whereas, DataOps also brings niche capabilities of key contributors in the data chain – data developers, data analysts, data scientists, and data engineers – to ensure greater collaboration in the development of data flows.\\xa0 Also, while comparing DataOps and DevOps, it is worth noting that DevOps focuses on transforming the delivery capability of software development teams whereas DataOps emphasizes the transformation of analytics models and intelligence systems with the help of data engineers.\\n\\xa0\\n3. Why is DataOps integral to data engineering?\\n\\xa0\\nData engineers play an important role in ensuring that data is properly managed across the entire analytics trail. Additionally, they are entrusted with the responsibility of optimal use and safety of data.\\xa0 DataOps help facilitate the key functional areas of data engineers by enabling them with end-to-end orchestration of tools, data, codes, and organizational data environment. It can boost the collaboration and communication within the teams to adapt with evolving customer needs. In simple terms, DataOps strengthens the hands of data engineers by offering greater collaboration between various data stakeholders and helping them achieve reliability, scalability, and agility.\\n\\xa0\\n4. What role do DataOps engineers play in enabling advanced enterprise analytics?\\n\\xa0\\nNow, the role of a DataOps engineer is slightly different from that of a data engineer. The DataOps engineer meticulously defines and manages the environment in which the data is developed. The role also includes offering guidance and design support to data engineers around workflows. As far as\\xa0advanced enterprise analytics\\xa0is concerned, DataOps engineers play a significant role in automating data development and integration. With their in-depth knowledge of software development and agile methodologies, DataOps engineers contribute to enterprise analytics by tracking document sources through metadata cataloging as well as building metric platforms to standardize calculations. Some of the key roles of a DataOps engineer are:\\n\\nTest automation\\nCreation of code repositories\\nFramework orchestration\\nCollaboration and workflow management\\nLineage and impact analysis\\nData preparation and integration\\n\\n\\n\\xa0\\n5. What are the popular technology platforms commonly used by DataOps teams?\\n\\xa0\\nIt is fair to say that DataOps is still an evolving discipline and data-focused organizations are learning more about it every day. However, with technological innovations, a number of platforms have already made a mark and are growing their impact across the industry. Here are some of the popular platforms used by DataOps teams.\\n\\nKubernetes\\xa0\\nKubernetes is an open-source orchestration platform that allows companies to combine multiple docker containers into a single unit. This makes the development process much faster and simple.\\xa0Kubernetes\\xa0can help teams manage data scheduling on nodes within a single cluster, simplify workloads and categorize containers into logical units for easy discovery and management.\\nELK (Elasticsearch, Logstash, Kibana)\\nThe ELK Stack of Elastic is a widely preferred log management platform which comprises three distinct open-source software packages – Elasticsearch, Logstash and Kibana. While Elasticsearch is a NoSQL database that runs on Lucene search engine, Logstash is a log pipeline solution that accepts data inputs from multiple sources and performs data transformations. Kibana on the other hand, is essentially a visualization layer which operates on top of elastic search.\\nDocker\\nThe Docker platform is often regarded as the simplest and straight forward tool which can help companies scale high-end applications using containers and securely run them on the cloud. Security is one of the key aspects that differentiates the platform as it provides a secure environment for testing and execution.\\nGit\\xa0\\nGit is a leading version-control application that allows companies to effectively manage and store version updates in data files. It can control and define a particular data analytics pipeline such as source code, algorithms, HTML, parameter files, configuration files, containers and logs. Since these data artefacts are simply source code, Git makes them easily discoverable and manageable.\\nJenkins\\xa0\\nJenkins is an open-source server-based application which helps companies seamlessly orchestrate a range of activities to achieve continuous automated integration. The platform supports the end-to-end development lifecycle of an application from development to deployment while speeding up the process through automated testing.\\nDatadog\\nDatadog is an open-source cloud monitoring platform that facilitates full visibility across an application stack by allowing companies to monitor metrics, traces and logs through a unified dashboard. The platform comes with around 400 built-in integrations and a predefined dashboard that simplifies the process.\\n\\n\\xa0\\nRounding Up\\n\\xa0\\nWith time the complexity and scale of enterprise AI and ML applications will only increase, giving rise to the need of convergence in data management practices. In order to meet customer demands and deploy applications faster, organizations need to reconcile data cataloging and accessibility functions while ensuring integrity. And, this is where DataOps practices can help companies create a difference.\\n\\xa0\\nBio: Jagan is a Dev Ops evangelist who leads the Dev Ops practice at Sigmoid. He has instrumental in maintaining and supporting highly critical data systems for clients across CPG, Retail, AdTech, BFSI, QSR and Hi-Tech verticals.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nModel Experiments, Tracking and Registration using MLflow on Databricks\\nData Scientist, Data Engineer & Other Data Careers, Explained\\nWhy You Should Consider Being a Data Engineer Instead of a Data Scientist',\n",
       " 'comments\\nBy Nitin Kumar, Sigmoid\\n\\n\\xa0\\nBig data storage with cloud – doubling down on the opportunities\\n\\xa0\\nThe last decade has seen the rapid growth of cloud adoption rates across industries. Today, cloud data storage accounts for\\xa045% of all enterprise data\\xa0and by Q2 2021, that number could grow to 53%.\\nThe writing’s on the wall – given the direction in which the industry is moving, there’s no better time to embrace cloud than now. And, one thing that is going to play a critical role in enterprise cloud transition strategy is cloud storage or a cloud data warehouse.\\n\\n\\xa0\\nThe advantages of a cloud data warehouse\\n\\xa0\\nThe ability to integrate data from multiple channels allows organizations to effectively leverage Business Intelligence (BI) tools and derive meaningful insights. It’s important to remember that BI tools are highly limited in their data preparation capabilities. Cloud data warehouses can be a game changer here as they help BI tools leverage the wealth of reliable and correctly structured data to generate actionable business insights.\\nThe present-day cloud data warehouse (CDW) forms the core of the data analytics architecture. Having a data warehouse on the cloud enables businesses to store very large amounts of data sustainably and gain several advantages by leveraging advanced data analytics. These include:\\n\\nCost reduction:\\xa0Providers take care of hardware, upgrades, maintenance, and outage management, which makes cloud less expensive than on-premise infrastructure.\\nData security:\\xa0A single point of access simplifies the cloud data security conundrum, making it arguably safer than on-premise data storage. It also allows the integration of additional safety measures such as VPNs and cloud encryption.\\nReliability: Leading cloud warehouse providers like Amazon, Microsoft, and Google\\xa0report\\xa099.99% uptime. Cloud promises to make anywhere and anytime access to services, tools and data a reality. This level of service and reliability enhances customer experiences and provides a robust platform for business continuity.\\nScalability and enhanced accessibility:\\xa0\\xa0One of the defining features of cloud infrastructure is its vertical and horizontal scalability. It is ready to meet the data requirements as per organizational demands removing the data volume restrictions effectively.\\n\\n\\nMoreover, efficient data integration and data governance facilitates seamless data management, which in turn enhances data accessibility. Businesses that work with massive data for quick decision making, have\\xa0achieved benefits\\xa0including faster access to data and reduced infrastructure cost. Considering these features, it is safe to say that cloud data warehouses will define how organizations access and leverage Business Intelligence (BI) going forward.\\nBut even as we venture into a cloud-essential future, enterprises have their work cut out for them. They must fortify their cloud strategies and conduct a thorough cloud readiness assessment. One of the first steps to achieve that is facilitating the seamless migration of data to a cloud data warehouse.\\n\\xa0\\nData migration: how to go about it?\\n\\xa0\\nThe benefits of cloud data warehouses make them an indispensable driver of digital transformation. But how do enterprises go about migrating business data to a cloud infrastructure from a legacy environment? There is no one right way to develop an effective migration strategy, but there are a few essential steps that enterprises need to take:\\n\\nDetermining the type of data storage:\\xa0It is important to understand that the enterprise data transformation journey is a gradual process. There are two parts to this preliminary step:\\n\\nCreating data lake or data warehouse: One of the most fundamental aspects of choosing a cloud storage destination is to understand the differences between a data lake and a data warehouse. Even though the terms are often used interchangeably, there are vast differences in terms of structure and purpose. A data lake is a vast pool of unstructured data, the purpose of which is not yet determined. A data warehouse on the other hand, is a repository for structured and filtered data stored with a definite purpose.\\nThe data stored within data lakes are highly accessible, whereas it is more complicated and cost intensive to change or update the data within data warehouses. Moreover, since the data within data lakes is unprocessed, it requires specialized tools and data scientists to make good use of it. Organizations need to account for these differences and choose what suits their purpose. In many cases, organizations need to choose both.\\n\\nChoosing the right cloud provider:\\xa0When it comes to choosing the right cloud warehouse provider, the options are plenty. And each provider has their pros and cons. Businesses need to identify the cloud warehouse that uniquely matches their specific requirements. This is an important aspect of selecting and operationalizing\\xa0cloud migration services. Organizations can try migrating a small data set to multiple cloud warehouses to determine which options work best for them, both in terms of performance and cost. Doing so can give them a better perspective in terms of congruity.\\nCopying all existing data:\\xa0This step depends on the sheer volume of the data they have. But it is also important to consider the data schema and format before the final transfer. The schema must be transferred before loading and used while setting up the replication process.\\nEnabling continuous replication:\\xa0The next step is setting up data synchronization. This can be done either manually or by using data pipeline services to manage schema and data replication. This is a critical step since the rest of the components can only be transferred once the synchronization is secure.\\nBuilding the analytics infrastructure and data application migration:\\xa0Business Intelligence and analytics infrastructure setup should follow once the migration pipeline is created. This is a relatively low risk task. However, legacy applications may pose a unique challenge. In some cases, ensuring a smooth transition and an optimum performance involves replacing ODBC drivers, rewriting queries, or even changing the data model.\\nMigrating transformations:\\xa0The final step is to recreate the transformations to produce final data models in the new cloud ecosystem. It is recommended that enterprises follow the ELT (extract, load, transform) model of transfer as an alternative to ETL (extract, transform, load) to accelerate data processing in a cloud environment.\\n\\n\\n\\xa0\\nAccelerating business growth with cloud warehouses\\n\\xa0\\nCloud data warehouse adoptions\\xa0are growing\\xa0at a CAGR of nearly 15%. To keep up, business leaders must recalibrate their cloud strategy and leverage the benefits of an ever-expanding cloud ecosystem through cloud warehouses. This will allow them to harness the power of business intelligence and embrace emerging technology and trends like edge computing and AI/ML. This, in turn, will boost their capability and preparedness to enter new markets, and promises a significant business advantages.\\nThe recent economic turmoil of the pandemic emphasizes the need for alternatives to legacy systems. It has become imperative for businesses to embrace cloud consulting and move to cloud warehouses. This will help them in steadying the ship in the short-term while targeting sustained growth and expansion in the long-term.\\n\\xa0\\nBio: Nitin Kumar is Engineering Manager at Sigmoid and has a decade of experience working with Big Data technologies. He is passionate about solving business problems across Banking, CPG, Retail, and QSR domains through his expertise in open source and cloud technologies.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nMeet whale! The stupidly simple data discovery tool\\nFeature Store vs Data Warehouse\\n4 Myths of Big Data and 4 Ways to Improve with Deep Data',\n",
       " 'By Gregory Piatetsky, KDnuggets.\\nKDnuggets Top Blog Rewards Program now has the winners for the month of June - congratulations to all winners below! Here are the top 6 blogs published in June 2021 whose authors will share the $2,000 (USD) reward amount: \\n\\n 5 Tasks To Automate With Python, by Dylan Roy\\n Data Scientists Will be Extinct in 10 Years, by Michael Mew\\n How to Generate Automated PDF Documents with Python, by Mohammad Khorasani\\n How I Doubled My Income with Data Science and Machine Learning, by Terence Shin\\n Pandas vs SQL: When Data Scientists Should Use Each Tool, by Matthew Przybyla\\n Top 10 Data Science Projects for Beginners, by Natassha Selvaraj\\n\\nBut don\\'t give up on Data Science yet because of the popularity of the opinion blog that Data Scientists will be extinct in 10 years.\\nIn the latest KDnuggets Poll, a majority of respondents were optimistic about Data Scientist prospects - see Relax! Data Scientists will not go extinct in 10 years, but the role will change.\\nWe started the rewards program to encourage more high-quality and especially original (unpublished) contributions to KDnuggets.\\nFor each month, beginning in May 2021, we will determine 6 most viewed blogs published on KDnuggets that month (excluding blogs by KDnuggets Editors Matthew Mayo and Gregory Piatetsky), and will distribute $2,000 (USD) among the authors of these top blogs. \\nWhile we appreciate and publish the reposts, the goal of this program is to get more great original (not published previously) articles, which will be rewarded at the rate of 3X the reposts. \\nNote: An original submission first published on KDnuggets may be reposted elsewhere two weeks after KDnuggets publication, provided there is a link to KDnuggets blog and text \"Originally published on KDnuggets\".\\nHere are the details on KDnuggets Top Blogs Reward Program .\\nTo submit a blog to KDnuggets, please follow the submission guidelines.',\n",
       " 'comments\\nBy M. Sebastian Metti, Founder and CEO of Saturn Cloud.\\n\\xa0\\nWhy Python?\\n\\xa0\\nPython is the clear winning programming language in data science & machine learning (DSML). With its rich and dynamic open-source software ecosystem, Python stands unmatched in how adaptable, reliable, and functional it is. If you disagree with this premise, then please take a quick detour here.\\n\\nPython has over 8 million users (SlashData) (Image Credit: HackerNoon).\\n\\xa0\\nThe Purpose of Your Data Science & Machine Learning Capability\\n\\xa0\\nYour goal as a lead of a DSML team is to deliver the best return on investment to the business. The business invests in the DSML capability with a budget for staff and resources, while your job is to deliver the maximum business impact you can.\\nYour business impact can be measured in many ways. The most high-level objectives are cost optimization, risk optimization, and revenue growth. You may focus on a variety of specific metrics within each objective, such as customer acquisition cost optimization, churn prediction, fraud detection, patient health outcomes, or personalized product recommendations.\\n\\nAnything that diverts goal-setting, budget, and execution from this purpose drives down the ROI your team can deliver. Where the attention goes, the energy flows, to quote a self-improvement guru.\\n\\xa0\\nRenting vs. Owning\\n\\xa0\\nThis a re-framing of the classic Buy vs. Build discussion in the context of many DSML platforms offering “pay as you go” pricing now, much like Amazon Web Services. I feel it’s necessary to rephrase the discussion because, unlike “Buying” where you pay a fixed cost, whether or not you use it, “Renting” implies that you only pay for when you use it. This is much more convenient for the end-user.\\nAs you begin to set up your DSML platform in Python, you can own the internal architecture, or you can rent it from a vendor. I’ll use Saturn Cloud as the primary vendor, but depending on your needs, you might want to check out Domino Data Lab (fixed annual license fee model) or Databricks (DS&ML platform for Scala and Spark, not Python).\\nThe Hidden Cost of Owning\\nOwning a DSML capability carries inherent “scope creep” issues that are not in plain view from the outset. It is all too easy to expect owning the capability as simply integrating your favorite open source tools: Jupyter, Dask or PySpark, Prefect or Airflow, Kubernetes, NVIDIA RAPIDS, Bokeh, Plotly, Streamlit, etc.\\nHere is a shortlist of “scope creep” dealbreakers we hear from our customers who have previously tried to own a DSML capability:\\n\\nSetting up and managing cloud hosting and support for AWS, Azure, GCP, or on-premise\\nEnsuring enterprise-grade security of code and data; even more burdensome if you are in a highly regulated industry\\nConfiguration: executing work on the proper infrastructure which exposes the appropriate resources and libraries for the task at hand\\nMonitoring, e.g., ensuring minimal downtime\\nUser management: managing employee access to systems and information\\nAccess control: controlling what users can do and see within an application\\nManaging existing OSS package versioning and integrating new OSS packages\\nSupport for end-users; managing consultations with OSS experts\\n\\nEach of these bullets has a list of further burdens that may not be attractive. In fact, some of it is so painful that our Saturn Cloud co-founder and CTO, Hugo Shi, wrote an article on Kubernetes just to vent.\\nThe Obvious Cost of Owning\\nHere are the cost components of ownership that you need to consider as you build your DSML capability.\\nExample 1: Owning Results in Higher Total Cost\\nYour team is tasked with developing a customer churn model. If you could predict churn, sales could take proactive measures to retain more accounts. Your company generates $100M in annual sales, and there’s an opportunity to reduce churn from 10% to 5%, or by $5M annually. To keep it simple, we’ll assume you’re a SaaS company with 100% gross margins.\\nFigure 1: Renting = Automated DevOps.\\n\\nAssumes FTE cost of $150K.\\nGiven the cost savings in automating DevOps, the renting scenario generates higher ROI due to less total spend.\\nExample 2: Owning Carries High Opportunity Cost\\nNow let’s assume in both scenarios your team is 9 FTEs, but in the renting scenario, all 9 are dedicated to Data Science & ML. A team of 9 FTEs can produce 50% more output than a team of 6 FTEs, so with the spare capacity, you take on a second project around customer personalization. Let’s assume this project could result in 5% higher software sales in year 1.\\nFigure 2: Renting = Force Multiplier.\\n\\nAssumes FTE cost of $150K.\\nNotice that in the renting scenario, you’re actually spending more money, but with the same team size, you can generate higher ROI. By shifting labor spend to Data Science & ML from DevOps, your team is more efficient and can tackle more positive ROI projects in the same period. The owning scenario carries an inherent opportunity cost, which is not inherent in the renting scenario.\\nIn both scenarios, the ROI of renting outperforms that of owning a DSML capability. It is also worth noting that cloud computing pricing has dropped significantly over the past decade, whereas labor costs for data science, machine learning, and DevOps have increased significantly.\\n\\xa0\\nA Cautionary Tale\\n\\xa0\\nNot every organization needs to rent DSML architecture. But, it is much easier and less risky to rent first before you own.\\n“Rent before you own.”\\nI have spoken with hundreds of DSML leaders in the past couple of years. A good portion of them lead their teams into owning DSML architecture without renting, and without assessing the obvious and hidden costs of owning. All too often, they turn back halfway, realizing renting is cheaper, easier, more flexible, and allows them to stay focused. Furthermore, many developers on the teams expected they would be only part of building the architecture upfront, but later had to serve in full-time support roles, spending much less time on interesting scientific projects they joined the company for!\\n\\xa0\\nIt’s Somebody Else’s Problem Now\\n\\xa0\\n...is what you’ll be saying when you rent the architecture. Yes, all the integration of open-source tools, open-source version management, building state-of-the-art security around data and code, building enterprise administration architecture, cloud hosting, support services, open-source expert consultations — say it with me — somebody else’s problem!\\nNot only is that offloaded, but you get some pretty great benefits from a dedicated team working on it.\\n\\nGreater Performance: Saturn’s tooling offers up to 100x faster runtime than Apache Spark, Pandas, and other data processing tools\\nInstant Delivery: You subscribe, you have it immediately in your virtual private cloud\\nExpert Support: Leading committers of Python OSS available to support you\\nSmooth Experience: Immediate integration and updating of open source tools\\nNative Integrations: Amazon Web Services, Snowflake, and other cloud services\\nSeamless Teamwork Tools: Interactive and Collaborative DSML Capabilities\\nAutomation: Data Pipelines and Workflow Orchestration with Prefect\\nBeautiful: Intuitive, State-of-the-art User Interface\\nFlexibility: Pay As You Go and Cancel Whenever\\n\\n\\xa0\\nConcluding: Your Pythonic DSML Capability\\n\\xa0\\nOwnership Model: Team and budget are divided in using DSML capability to create value and supporting DSML capability.\\n\\nSource: Saturn Cloud.\\nRent Model: Entire team and budget are streamlined towards using rented DSML capability to create value.\\n\\nSource: Saturn Cloud.\\nThe purpose of your DSML capability is to maximize its ROI. You want as much of your budget going towards that target: whether the endpoint is faster stock market trading decision-making, recommending new marketing investment, running more drug discovery models, and so on.\\nMy advice is:\\n\\nChoose Python for its unmatched open source ecosystem\\nChoose to rent before you buy\\n\\nGood luck, and if you are curious about Saturn Cloud, please check us out here.\\nRelated:\\n\\nAlternative Cloud Hosted Data Science Environments\\nUnderstanding Cloud Data Services\\nData Science Tools Popularity, animated',\n",
       " \"By Kevin Vu, Exxact Corp.\\ncomments\\n\\n\\xa0\\nHow GPUs Accelerate Data Science & Data Analytics\\n\\xa0\\nArtificial intelligence (AI)\\xa0is set to transform global productivity, working patterns, and lifestyles and create enormous wealth. Research firm Gartner expects the global AI economy to increase from about $1.2 trillion last year to about\\xa0$3.9 Trillion by 2022, while McKinsey sees it delivering global economic activity of around\\xa0$13 trillion by 2030. In many ways, at its core, this transformation is fueled by powerful Machine Learning (ML) tools and techniques.\\nIt is now well established that the modern AI/ML systems’ success has been critically dependent on their ability to\\xa0process massive amounts of raw data in a parallel fashion using task-optimized hardware. Therefore, use of specialized hardware like Graphics Processing Units (GPUs) played a significant role in this early success. Since then, a lot of emphasis has been given on building highly optimized software tools and customized mathematical processing engines (both hardware and software) to leverage the power and architecture of GPUs and parallel computing.\\nWhile the use of GPUs and distributed computing is widely discussed in the academic and business circles for core AI/ML tasks (e.g. running a 100-layer deep neural network for image classification or billion-parameter BERT speech synthesis model), they find less coverage when it comes to their utility for regular data science and data engineering tasks. These\\xa0data-related tasks are the essential precursor to any ML workload in an AI pipeline\\xa0and they often constitute a majority percentage of the time and intellectual effort spent by a data scientist or even a ML engineer.\\nIn fact, recently, the famous AI pioneer Andrew Ng talked about\\xa0moving from a model-centric to a data-centric approach for AI\\xa0tools development. This means spending much more time with the raw data and preprocessing it before an actual AI workload executes on your pipeline.\\nYou can watch Andrew’s interview here: https://www.youtube.com/watch?v=06-AZXmwHjo\\n\\nThis brings us to an important question...\\n\\xa0\\nCan we leverage the power of GPU and distributed computing for regular data processing jobs too?\\n\\xa0\\nThe answer is not trivial, and needs some special consideration and knowledge sharing. In this article, we will try to show some of the tools and platforms that can be used for this purpose.\\n\\nImage source\\n\\xa0\\nRAPIDS: Leverage GPU for Data Science\\n\\xa0\\nThe\\xa0RAPIDS\\xa0suite of open source software libraries and APIs gives you the ability to execute end-to-end data science and analytics pipelines entirely on GPUs. NVIDIA incubated this project and built tools to take advantage of CUDA primitives for low-level compute optimization. It specifically focuses on\\xa0exposing GPU parallelism and high-bandwidth memory speed features through the friendly Python language\\xa0popular with all the data scientists and analytics professionals.\\nCommon data preparation and wrangling tasks\\xa0are highly valued in the RAPIDS ecosystem as they take up a significant amount of time in a typical data science pipeline. A familiar\\xa0dataframe-like API\\xa0has been developed with a lot of optimization and robustness built-in. It has also been customized to integrate with a variety of ML algorithms for end-to-end pipeline accelerations with incurring serialization costs.\\nRAPIDS also includes a significant amount of internal\\xa0support for multi-node, multi-GPU deployment and distributed processing. It integrates with other libraries which make\\xa0out-of-memory\\xa0(i.e. dataset size larger than individual computer RAM) data processing easy and accessible for individual data scientists.\\nHere are the most prominent libraries that are included in the RAPIDS ecosystem.\\n\\xa0\\nCuPy\\n\\xa0\\nA CUDA-powered array library that looks and feels like Numpy, the foundation of all numerical computing and ML with Python. It uses CUDA-related libraries including cuBLAS, cuDNN, cuRand, cuSolver, cuSPARSE, cuFFT and NCCL to make full use of the GPU architecture with the goal of providing GPU-accelerated computing with Python.\\nCuPy’s interface is highly similar to that of NumPy and can be used as a simple drop-in replacement for most use cases. Here is the module-level detailed list of API compatibility between CuPy and NumPy.\\nView the\\xa0CuPy Comparison Table.\\nThe speedup over NumPy can be mind-boggling depending on the data type and use case. Here is a speedup comparison between CuPy and NumPy for two different array sizes and for various common numerical operations - FFT, slicing, sum and standard deviation, matrix multiplication, SVD - that are widely used by almost all ML algorithms.\\n\\nCuPy speeds compared to NumPy,\\xa0Image source\\n\\xa0\\nCuDF\\n\\xa0\\nBuilt based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data. It provides a\\xa0pandas-like API\\xa0that will be familiar to almost all data engineers & data scientists, so they can use it to easily accelerate their workflows using powerful GPUs without going into the details of CUDA programming.\\nCurrently, cuDF is supported only on Linux, and with Python versions 3.7 and later. Other requirements are,\\n\\nCUDA 11.0+\\nNVIDIA driver 450.80.02+\\nPascal architecture or better (Compute Capability >=6.0)\\n\\nView more about this powerful library in the\\xa0API docs for CuDF.\\nFinally,\\xa0data scientists and analysts (i.e. those who do not necessarily use\\xa0deep learning\\xa0in any of their daily tasks) can rejoice and use powerful AI-workstations\\xa0like the following to enhance their productivity.\\n\\nData science\\xa0workstation from Exxact Corporation,\\xa0Image source\\n\\xa0\\nCuML\\n\\xa0\\ncuML enables data scientists, analysts, and researchers to run traditional/ classical ML algorithmic tasks with (mostly) tabular datasets on GPUs without going into the details of CUDA programming. In most cases, cuML's Python API matches that of the popular Python library Scikit-learn to make the transition to GPU hardware fast and painless.\\nView the\\xa0GitHub repo for CuML\\xa0documentation to learn more.\\nCuML also integrates with\\xa0Dask, wherever it can, to offer\\xa0multi-GPU and multi-node-GPU support\\xa0for an ever-increasing set of algorithms that takes advantage of such distributed processing.\\n\\xa0\\nCuGraph\\n\\xa0\\nCuGraph is a collection of GPU accelerated graph algorithms that process data found in\\xa0GPU DataFrames. The vision of cuGraph is to make graph analysis ubiquitous to the point that users just think in terms of analysis and not technologies or frameworks.\\nData scientists familiar with Python will quickly pick up how cuGraph integrates with the Pandas-like API of cuDF. Likewise, users familiar with NetworkX will quickly recognize the NetworkX-like API provided in cuGraph, with the goal to allow existing code to be ported with minimal effort into RAPIDS.\\nCurrently, it supports all kinds of graph analytics algorithms,\\n\\nCentrality\\nCommunity\\nLink analysis\\nLink prediction\\nTraversal\\n\\nMany scientific and business analytics tasks involve use of extensive graph algorithms on large datasets. Libraries like cuGraph\\xa0lend the assurance of higher productivity to those engineers when they invest in GPU-powered workstations.\\n\\nEmpower social graph analytics using GPU-accelerated computing,\\xa0Image source\\n\\xa0\\nThe Overall Pipeline for GPU Data Science\\n\\xa0\\nRAPIDS envisions a whole pipeline for GPU-powered data science task flow as follows. Note that\\xa0deep learning, which has traditionally been the primary focus of GPU-based computing, is only a sub-component of this system.\\n\\nThe GPU Data Science Pipeline,\\xa0Image source\\n\\xa0\\nDask: Distributed Analytics With Python\\n\\xa0\\nAs we observed, modern data processing pipelines can often benefit from distributed processing of large data chunks.\\xa0This is slightly different from the parallelism offered by the thousands of cores in a single GPU. This is more about how to split up a mundane data processing (which may occur much before the dataset is ready for ML algorithms) into chunks and process in using multiple compute nodes.\\nThese computing nodes can be GPU cores or they can even be simple logical/ virtual cores of CPU.\\nBy design,\\xa0most widely popular data science libraries like Pandas, Numpy, and Scikit-learn cannot take advantage of truly distributed processing easily. Dask tries to solve this problem by bringing the features of intelligent task scheduling and big data chunk handling into regular Python code. Naturally, it is composed of two parts:\\n\\nDynamic task scheduling\\xa0optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.\\n“Big Data” collections\\xa0like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of the aforementioned dynamic task schedulers.\\n\\nHere is an illustrative diagram of a typical Dask task-flow.\\n\\nOfficial Dask Flow Documentation,\\xa0Image source\\n\\xa0\\nEasy to Convert Existing Codebase\\n\\xa0\\nFamiliarity is at the core of the Dask design, so that a typical data scientist can\\xa0just pick up his/her existing Pandas/Numpy based codebase and convert it to Dask code\\xa0following a minimal learning curve. Here are some of the canonical examples from their official documentation.\\n\\nDask Documentation comparing Pandas and NumPy,\\xa0Image source\\n\\xa0\\nDask-ML for Scalability Challenges\\n\\xa0\\nThere are different kinds of scalability challenges for ML engineers. The following figure illustrates them. The machine learning library Dask-ML offers something for each of these scenarios. Therefore, one can focus on either model-centric exploration or data-centric development based on the unique business or research requirements.\\nMost importantly, focus on familiarity again plays a role here and the DASK-ML API is designed to mimic that of the widely popular Scikit-learn API.\\n\\nDask Documentation for XGBRegressor,\\xa0Image source\\n\\xa0\\n\\n\\xa0\\nDask Benefits from Multi-Core CPU Systems\\n\\xa0\\nIt is to be noted that the primary attractiveness of Dask comes from its role as a high-level, efficient task scheduler that can work with any Python code or data structure. Consequently, it is not dependent on a GPU to boost existing data science workloads with distributed processing.\\nEven multi-core CPU systems can take full advantage of Dask if the code is written to focus on that. Major changes in the code are not required.\\nYou can distribute your convex optimization routine or hyperparameter search among many cores of your laptop using Dask. Or, you can just process different parts of a simple DataFrame based on some filtering criteria using the full multi-core parallelism. This opens up the possibility of boosting the productivity of all the data scientists and analysts who do not need to buy expensive graphics cards for their machine but can just invest in a\\xa0workstation\\xa0with 16 or 24 CPU cores.\\n\\xa0\\nSummary of Distributed Data Science Powered by GPUs\\n\\xa0\\nIn this article, we discussed some exciting new developments in the Python data science ecosystem which enables common data scientists, analysts, science researchers, academics, to use GPU-powered hardware systems for a much wider variety of data related tasks than just what is related to image classification and natural language processing. This will surely broaden the appeal of such hardware systems to these large sections of users and democratize the data science user base even more.\\nWe also touched upon the possibilities of distributed analytics with the Dask library which can leverage multi-core CPU workstations.\\nHopefully, this kind of convergence of powerful hardware and modern software stack will open up endless possibilities for highly efficient data science workflows.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Use NVIDIA GPU Accelerated Libraries\\nGood-bye Big Data. Hello, Massive Data!\\nAbstraction and Data Science: Not a great combination\",\n",
       " 'comments\\nBy Ankush Kundaliya & Aditya Aggarwal, Abzooba\\nThis article outlines a machine learning approach to detect and diagnose anomalies in the context of machine maintenance. However, before we dive into the approach, we will gather a brief understanding of machine maintenance. This article is arranged as below:\\n\\nIntroduction to machine maintenance\\nWhat is predictive maintenance? \\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\nApproaches for machine diagnosis\\nMachine diagnosis using machine learning\\n\\n\\xa0\\n1) Introduction to Machine Maintenance\\n\\xa0\\nFor any industrial machinery equipment, owners want to increase operational flexibility and reduce operating costs. To achieve this objective, system engineers mainly focuses on 3 attributes of the machinery.\\n\\nReliability (R):\\xa0It is defined as the probability of a machine or machine component operating as expected without failure for a given period of time. The commonly used metric for it is \"Mean time between failure\" i.e. \"Total operating time\" / \"Total failures\"\\xa0\\nMaintainability (M):\\xa0It is\\xa0defined as the probability that a machine or machine component can be repaired within a specified period of time. The commonly used metric for it is \"Mean time to repair\" i.e. \"Total downtime\" / \"Total outages\"\\nAvailability (A):\\xa0It is defined as the probability that a machine or machine component is functional at a given point in time. Availability depends on reliability and maintainability, defined as \"Total operating time\"/(\"Total operating time\" + \"Total downtime\")\\n\\nMaintenance strategy significantly improve the reliability and availability of\\xa0assets and, as a result, decreases the number of unpredicted breakdowns. With advancements in technology, maintenance strategies have also evolved over time as summarized in Table 1.\\n\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\n\\n\\n\\n\\n\\n\\nBreakdown Maintenance or Run to Failure\\n\\n\\nPreventive Maintenance or Scheduled Maintenance\\n\\n\\nPredictive Maintenance or Condition-based Maintenance\\n\\n\\n\\n\\nDefinition\\n\\n\\nMaintenance actions are taken only after a breakdown happened.\\n\\n\\nPlanned maintenance actions after specific time intervals.\\n\\n\\nMaintenance actions are taken according to the actual condition of the operating equipment assessed through condition monitoring procedures.\\n\\n\\n\\n\\nPrinciple\\n\\n\\nFail and fix\\xa0\\nReactive\\nUnscheduled\\n\\n\\nTime-based\\nPreventive\\nScheduled at regular intervals\\n\\n\\nPredictive\\nPreventive\\nCondition-based Just-in-time\\n\\n\\n\\n\\nPros\\n\\n\\nCost-effective for small, non-critical equipment\\n\\n\\nA proactive strategy that helps to minimize downtime, prevent costly repairs caused by secondary damages.\\n\\n\\nReduces maintenance costs, downtime, secondary damage, and avoids unnecessary parts replacement\\n\\n\\n\\n\\nCons\\n\\n\\nCostly downtime, extensive secondary damages\\n\\n\\nThe cost of maintenance is very high. Unplanned breakdowns can still occur.\\n\\n\\nNone\\n\\n\\n\\n\\n\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200bTable 1: Maintenance strategies (evolved with time in the order from left to right)\\nIn this article, we will be talking about a machine learning approach that aligns with the\\xa0predictive maintenance strategy. Hence, let\\'s understand \"what is predictive maintenance?\" before getting into the actual approach.\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\n\\xa0\\n2) What is Predictive Maintenance?\\n\\xa0\\nPredictive maintenance is determined based on the actual\\xa0condition of the machine and its components also known as condition-based maintenance (CBM). CBM suggests maintenance action only when there is evidence of abnormal behaviours from a component.\\nCBM heavily relies upon diagnostic (what is current condition?) and prognostic (what will be the condition in future?) information from the machine and its components. Both serve as different objective as shown in table 2.\\n\\n\\n\\n\\n\\nDiagnostic (What is condition currently?)\\nPrognostic (What will be the condition in future?)\\n\\n\\nDefinition\\nDiagnostics is the process of determining the current health status and the equipment deterioration using information delivered by the condition-monitoring system.\\nPrognostics is the ability to forecast the machine deterioration using information gathered from the machine and its components (like vibrations, change in temperature, change in pressure, current consumption, etc).\\n\\n\\nObjective\\n\\ni. fault detection – fault is about to happen;\\nii. fault isolation - locates the faulty component\\niii. fault identification - determine the root cause of the fault\\n\\n\\ni. forecasting the impending failures and\\nii. estimating the remaining useful life\\n\\n\\n\\n\\nTable 2: Success of CBM relies on diagnostic and prognostic ability\\xa0both\\nIn this article, we will be addressing the\\xa0diagnosis process of the CBM approach which includes anomaly detection, isolation, and identification to assist root cause analysis and plan maintenance.\\n\\xa0\\n3) Approaches for machine\\xa0diagnosis\\n\\xa0\\nThe first goal of the diagnostics is to identify the malfunctioning components. When observations from an operating machine differ from the expected behaviour then the real need for diagnostics arises. There are many approaches to do diagnostics\\xa0and a few commonly used ones are listed in Table 3.\\n\\n\\n\\n\\n\\n#\\n\\n\\nDiagnostic approach\\n\\n\\nDescription\\n\\n\\nLimitations / Disadvantages\\n\\n\\n\\n1\\nFault Trees[1]\\n\\nFault tree analysis is a top-down approach that was originally developed in Bell laboratories in the year 1962.\\nIt uses predefined logics to identify the component level failures that lead to occur system-level failure.\\n\\n1) Need a lot of domain expertise \\xa0\\xa0\\n2) Expensive to build and maintain\\n\\n\\n2\\nRule based[2]\\n\\nAs the name suggests, knowledge for diagnosis is captured in the form of IF-THEN rules. Rule-based systems are built with the help of expert diagnosticians to capture associations between the symptoms of an abnormal system and the underlying failures/faults.\\n\\n\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b1) Need a lot of domain expertise\\n2) Expensive to build and maintain\\n\\n\\n3\\nModel based\\n\\nA machine learning based model\\xa0learns how the system components are connected and how they normally behave.\\xa0Model is then tasked to identify those machine components which, when assumed to function abnormally, will account for the difference between the observed and expected behaviour.\\n\\n1) Need high computing resources\\n2) Requires a good amount of historical data\\n\\n\\n\\nTable 3: Commonly used\\xa0approaches to do machine diagnostics\\nIn this article, we will discuss modelling based approach with a case study.\\n\\xa0\\n4)\\xa0Machine diagnosis using\\xa0machine learning\\n\\xa0\\nWe will be explaining the ML approach using a case study on \"Condition-based predictive maintenance of Gas Turbines in a Power Plant\". The solution is using the ideas discussed in the paper [3] from AAAI Conference on Artificial Intelligence, Jul 2019.\\nThis solution is designed to address the most commonly faced challenges as listed below\\xa0-\\xa0\\n\\nIn most real-world scenarios, it is very difficult to get a sufficient amount of anomaly events data points in historical data. This makes\\xa0supervised learning techniques infeasible to detect or classify anomalies from normal behaviour.\\nIn multivariate time series data, it not only requires to capture the temporal dependency in each time series but also needs to encode the inter-correlations between different pairs of time series.\\nIn real-world applications, it is common to have noise which may not eventually lead to a true system failure. Therefore, an anomaly detection system should provide operators with an anomaly scores indicating the severity of incident.\\n\\n\\xa0\\n4.1) Modelling methodology\\n\\xa0\\nWe attempt to model an accurate short-term estimate of gas turbine engine performance and integrity conditions which can be invaluable for maintenance strategy and planning. We build our model based on the operational data of a gas turbine engine collected from different sensors deployed for monitoring the engine\\'s status.\\nWe have the historical data from n sensors, monitoring the engine\\'s status for a period T, \\ni.e., 𝑋 = (𝑥1, … , 𝑥𝑛\\xa0) 𝑇 ∈ ℝ𝑛∗𝑇\\xa0,\\nDuring this period T, we assume that there were no anomalies or fault events and engine was operated in normal operating condition. Given this data, we train a model to learn different statuses of an engine during its normal operations and detect difference in engine’s status during abnormal operations. We aim to detect anomaly events during operations and diagnose the severity and root cause of the anomaly.\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\nThe modelling methodology is unsupervised learning using\\xa0auto-encoders that learns how to represent original data into a compressed encoded representation and then learns how to reconstruct the original input data from\\xa0the encoded representation. More details about model is given in next section 4.1.1.\\n\\xa0\\n4.1.1) Multi-Scale Convolutional Recurrent Encoder-Decoder (MSCRED)\\n\\xa0\\nMSCRED is\\xa0an unsupervised learning technique that learns the normal operating conditions of the equipment from operational data by learning the signature matrices representing the different states\\xa0of operation of the machine in normal conditions. We train our model only on the normal signature matrices and assume that the signature matrices of machines\\xa0in abnormal operations differ from the normal operations.\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\nWhat is a Signature Matrix?\\nA signature matrix is a way of representing the data wherein the\\xa0multivariate time series data is transformed into correlation matrices to characterize the system status. The inter-correlations between different pairs of time series in a multivariate time series segment capture the shape similarities and value scale correlation between pairs of\\xa0time series.\\xa0\\n\\n\\nFig 1. Signature Matrices\\n\\n\\xa0\\nCharacterizing\\xa0System Status with Signature Matrices:\\n\\nWe first fix the window-sizes for different resolutions and the step-size\\xa0by which to slide these windows. For example, suppose we have\\xa0w1, w2, w3\\xa0windows such that\\xa0w3>w2>w1\\xa0and step-size\\xa0t, we right-align these windows on the time series and divide it into segments while sliding by step-size.\\nNext\\xa0for a segment, for\\xa0each window-size, we calculate\\xa0the correlations\\xa0between different pairs of multivariate time series to get\\xa0n\\xa0*\\xa0n\\xa0matrices\\xa0Mt,\\xa0where\\xa0n\\xa0is the number of sensors/time-series. For e.g. if we have 30 sensors then it will give us a matrix of 30*30 for a window.\\nWe stack these correlation matrices from segments of different resolutions (window-size) together to form the signature matrices.\\xa0For e.g. for 3 different window-size, the signature matrix\\xa0is of dimension 30*30*3.\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\n\\nMSCRED modelling framework:\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b Here are steps to create an unsupervised model using MSCRED modelling framework.\\nStep 1: Construct multi-resolution signature matrices to characterize multiple levels of the system operational statuses across different time steps (segments) as discussed in the section above. Multi-resolution signatures are used to reduce the\\xa0operational noise\\xa0in data and help us indicate the severity of abnormal incidents.\\n\\n\\nFig 2. MSCRED Model Framework: (a) Signature matrices encoding via CNN. (b) Temporal patterns modelling by attention based convLSTM. (c) Signature matrices decoding via deconvolution neural networks. (d) Loss function.\\n\\n\\xa0\\nStep 2: Use a convolutional encoder to capture and encode the\\xa0inter-sensor correlation\\xa0patterns from the signature matrices (as shown in part (a) of Fig 2).\\xa0\\nStep 3: Use an attention-based Convolutional Long-Short Term Memory (ConvLSTM) network to\\xa0capture the\\xa0temporal patterns\\xa0(as shown in part (b) of Fig 2).\\nStep 4: Use a\\xa0convolutional decoder to reconstruct the signature matrices\\xa0from\\xa0the feature maps which encode the\\xa0inter-sensor correlations and temporal information\\xa0(as shown in part (c) of Fig 2).\\xa0\\xa0\\nStep 5: The residual error between reconstructed signature matrices and original signature matrices is then utilized to detect and diagnose anomalies (as shown in part (d) of Fig 2).\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\n\\xa0\\n4.1.2) Anomaly Detection and Root Cause Identification\\n\\xa0\\nSteps to detect anomaly and identify the root cause is as below.\\n\\nWe utilize the residual error matrix to detect the anomaly and identify the root cause(s).\\nAnomaly score is calculated for each window by adding up the absolute value of residuals in the residual matrix.\\nAnomaly score greater than a defined threshold is marked as an anomaly. Analysing the corresponding residual matrix to find the rows and columns with the higher error\\xa0in the residual matrix give us the root cause or affected components.\\xa0\\nThe signature matrices of operational data includes channels\\xa0(s = 3 windows) that capture system status at different scales. Anomaly severity is given by computing the anomaly scores from residual error matrices of three channels, i.e., small, medium and large with size w – 10, 30, and 60, respectively as shown in fig 3.\\n\\n\\n\\nFig 3. Anomaly Diagnosis Results\\n\\n\\xa0\\nAnalysis of anomalous residual matrices helps operator in\\xa0root cause analysis and identify affected components. This method captures the temporal patterns in the time series as well as the inter-sensor correlation patterns. For\\xa0machine diagnosis, this method is claimed to outperform other state-of-the-art models.\\nReferences\\n\\nRamana PV, Fault Tree Analysis,\\xa0https://sixsigmastudyguide.com/fault-tree-analysis/\\nXiao-Wen \\xa0Deng, Qing-Shui \\xa0Gao, Chu \\xa0Zhang, Di \\xa0Hu, Tao \\xa0Yang, \"Rule - based Fault Diagnosis Expert System for Wind Turbine\",\\xa0ITM Web Conf. 11 07005 (2017),\\xa0DOI: 10.1051/itmconf/20171107005\\nChuxu Zhang and Dongjin Song and Yuncong Chen and Xinyang Feng and Cristian Lumezanu and Wei Cheng and Jingchao Ni and Bo Zong and Haifeng Chen and Nitesh V. Chawla, A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data,\\xa0\\xa0AAAI\\xa02019:\\xa01409-1416\\n\\n\\xa0\\nAnkush Kundaliya is a Data Scientist at Abzooba. With more than 5 years of extensive experience in the field of data science, Ankush has expertise in building data-driven solutions to complex business problems using advance deep learning and machine learning algorithms. Having\\xa0worked across multiple domains of industries including IT Service Management, Human Resources, Manufacturing, Life Sciences, and Financial Services, he has diverse knowledge and business acumen.\\nAditya Aggarwal serves as Data Science – Practice Lead at Abzooba Inc. With more than 12+ years experience in driving business goals through data driven solutions, Aditya specializes in predictive analytics, machine learning, business intelligence & business strategy across range of industries.\\nRelated:\\n\\nTop Obstacles to Overcome when Implementing Predictive Maintenance\\nMoving Predictive Maintenance from Theory to Practice\\nThe Potential of Predictive Analytics in Labor Industries',\n",
       " 'Sponsored Post.\\n\\n\\xa0\\nThe DATAcated Expo is coming up on October 5, 2021 from 11am - 6pm ET. This innovative, virtual conference is live-streamed on LinkedIn, and hosted by Kate Strachnyi, founder of DATAcated. The event is free to attend and provides the community with an opportunity to explore and discover innovative technologies in data science & analytics.\\nSPEAKERS: Expert speakers will provide real-life case studies and demos of technologies across a variety of data science & analytics processes. You’ll hear from Scott Taylor – The Data Whisperer, Greg Coquillo – Technology Manager at Amazon, Aishwarya Srinivasan - AI & Innovation Leader- Business Development at IBM, Susan Walsh – The Classification Guru, George Firican – founder of LightsOnData, Tina Huang - Data Scientist at Goldman Sachs, Bruno Aziza - Head of Data & Analytics, Google Cloud, and many more!\\nTOPICS: The presentations are centered around the data analytics process and will take participants on a journey from identifying the business use case to collecting, cleaning, exploring, visualizing, and ultimately communicating insights.\\xa0\\xa0We’ll also cover ethical use of data, as well as other special topics.\\nNETWORKING: This live event will provide several opportunities for networking and building relationships in the data community. \\nGIVEAWAYS: The DATAcated Expo is partnered with amazing organizations that will provide fun giveaways throughout the event. \\nREGISTRATION: Make sure you sign up to this FREE event and add it to your calendar here:\\xa0https://datacated.com/expo/ - registration for the event also enables you to receive the recordings from all sessions. \\nWe hope to see you there!',\n",
       " \"comments\\nBy Jim Dowling, CEO of Logical Clocks, Associate Professor at KTH Royal Institute of Technology.\\nTLDR; Machine learning models are only as good as the data (features) they are trained on. In enterprises, data scientists can often train very effective models in the lab - when given a free hand on which data to use. However, many of those data sources are not available in production environments due to disconnected systems and data silos. An AI-powered product that is limited to the data available within its application silo cannot recall historical data about its users or relevant contextual data from external sources. It is like a jellyfish - its autonomic system makes it functional and useful, but it lacks a brain. You can, however, evolve your models from brain-free AI to Total Recall AI with the help of a Feature Store, a centralized platform that can provide models with low latency access to data spanning the whole enterprise.\\n\\n\\xa0\\nAutonomic AI\\n\\xa0\\nJellyfish are undoubtedly complex creatures with sophisticated behaviour - they move, mate, and munch. They eat and discard waste from the same opening. Yet, they have no brain - their autonomic system suffices for their needs. The biggest breakthroughs in AI in recent years have been enabled by deep learning, which requires large volumes of data and specialized compute hardware (e.g., GPUs). However, just like a jellyfish, recent successes in image processing and NLP with deep learning required no brain - no working memory, history, or context.\\nMuch of deep learning today is Jellyfish AI. We have made incredible progress in identifying objects in images and translating natural language. Yet, such deep learning models typically only require the immediate input - the image or the text - to make their predictions. The\\xa0input signal is information-rich. These image and NLP models seldom require a 'brain' to augment the input with context or memories. Google translate doesn’t need to know the historical enmity between the Scots and the Irish in whether it's spelled Whisky or Whiskey. Jellyfish AI is impressive - the input data is information-rich, and models can learn fantastically advanced behaviour from labeled examples. All “knowledge” needed to make predictions is embedded in the model. The model does not need to have working memory (e.g., it doesn’t need to know the user has clicked 10 times on your website during the last minute).\\nNow compare using AI for image classification or NLP to building a web application that will use AI to interact with a user browsing a website. The immediate input data your application receives from your web browser are clicks on a mouse or a keyboard. The\\xa0input signal is information-light\\xa0- it is difficult to train a useful model using only user clicks. However, large Internet companies collect reams of information about users from many different sources and transform that user data into features (information-rich signals that are ready to be used for either training models or making predictions with models). Models can then combine the click features with historical features about users and contextual features to build information-rich inputs to models. For example, you could augment the user’s action with everything you know about a user’s history and context to increase the user's engagement with the product. The feature store for machine learning (ML) stores and serves these features to models. We believe that AI-powered products that can easily access historical and contextual features will lead the next wave of AI in the enterprise, and those products will need a feature store for ML.\\n\\xa0\\nData Scientist and ML Engineer Disconnect\\n\\xa0\\nA frequent source of tension in enterprises is between “naive” data scientists and “street-wise” ML engineers. Motivated by good software engineering practices, many ML engineers believe that ML models should be self-contained, and tension can arise with data scientists who want to include features in their models that are “obviously not available in the production system.”\\nHowever, data scientists are tasked with building the best models they can to add to the bottom line - engage more users, increase revenue, reduce costs. They know they can train better models with more data and more diverse sources of data. For example, a data scientist trying to predict if a financial transaction is suspected of money laundering or not might discover that a powerful feature is the graph of financial transfers related to this individual in the previous day/week/month.\\xa0They can reduce false alerts of money launder by a factor of 100*, reducing the costs of investigating the false alerts, saving the business millions of dollars per year. The data scientist hands the model over the wall to the ML engineer, who dismisses the idea of including the graph-based features in the production environment, and tension arises when communicating what is possible and what is not possible in production. The data scientist is crestfallen - but need not be.\\nThe Feature Store is now the de facto enterprise platform for storing historical and contextual features for AI-powered products.\\xa0The Feature Store is, in effect, the brain for AI-powered products, the three-eyed Raven that enables the model to access the history and state of the whole enterprise, not just the local state in the application.\\nFeature Stores enable applications or model serving infrastructure to take information-light inputs (such as a cookie identifying a user or a shopping cart session) and enrich it with features harvested from anywhere in the enterprise or beyond to build feature vectors capable of making better predictions. And\\xa0as we know from Deep Learning, model accuracy improves predictably with more features and data, so there will be an increasing trend towards adding more and more features to models to improve their accuracy. Andrew Ng has recently been advocating this approach that he calls\\xa0data-centric development instead of the more traditional model-centric development. Another noticeable trend in large enterprises is building faster and more scalable Feature stores that can supply those features within the time budget available to the AI-powered product. But\\xa0AI is going to revolutionize enterprise software products, so how do we make sure our AI-enabled products are not just Jellyfish AI?\\n\\xa0\\nEnabling AI-enabled Products with a Feature Store\\n\\xa0\\n\\nAnti-Pattern:\\xa0Re-implementing the “feature engineering” code for the serving layer is non-DRY. This introduces the risk of ‘skew’ between the features used to train models and the features served to operational models.\\nHow do we avoid limiting AI-enabled products to only using the input features collected by the application itself? Models will benefit from access to all data that the enterprise has collected about the user, product, or its context. A potential source of friction here, however, is the dominant architectural preference for microservices and data stove-pipes. Models themselves are being deployed as microservices in model-serving infrastructures, like\\xa0KFServing,\\xa0TensorFlow Serving, or\\xa0Nvidia Triton. How can we give these models access to more features?\\n\\nAnti-Pattern: Microservice-based Online Feature Store.\\xa0Microservices can be used to compute features in real-time from raw input data. When features can be pre-computed, microservices is an anti-pattern. This architecture adds latency, needs to be made highly available, handles hotspots, and microservices consume resources even when they are not needed. Serverless functions might be acceptable in the case where seconds of warmup latency is tolerable. But the microservices should still be reused to compute the training data - otherwise, there is a risk of training/serving skew.\\nWithout a Feature Store, applications could contact microservices or databases to compute or retrieve the historical and contextual features (data), respectively. Computing the features in the application itself is an anti-pattern as it duplicates the feature engineering code - that code should already exist to generate the training data for the model. Re-implementing feature engineering logic in applications also introduces the risk of skew between the features computed in the application and the features computed for training. If serving and training environments use the same programming language, they could avoid non-DRY code by reusing a versioned library that computes the features. However, even if feature engineering logic is written in Python in both training and serving, it may use PySpark for training and Python for serving or different versions of Python. Versioned libraries can help but are not a general solution to the feature skew problem.\\nThe Feature Store solves the training/serving skew problem by computing the features once in a feature pipeline. The feature pipeline is then reused to (1) create training data and (2) save those pre-engineered features to the Feature Store. The serving infrastructure can then retrieve those features when needed to make predictions. For example, when an application wants to make a prediction about a user, it would supply the user’s ID, shopping cart ID, session ID, or location to retrieve pre-engineered features from the Feature Store. The features are retrieved as a feature vector, and the feature vector is sent to the model that makes the prediction. The Feature Store service for retrieving feature vectors is commonly known as the\\xa0Online Feature Store. The logic for retrieving features from the Online Feature Store can also be implemented in model serving infrastructure, not just in applications. The advantage of looking up features in serving infrastructure is that it keeps the application logic cleaner, and the application just sends IDs and real-time features to the model serving infrastructure, that in turn, builds the feature vector, sends it to the model for prediction, and returns the result to the application. Low latency and high throughput are important properties for the online feature store - the faster you can retrieve features and the more features you can include in a given time budget, the more accurate models you should be able to deploy in production. To quote DoorDash:\\n\\u200d“Latency on feature stores is a part of model serving, and model serving latencies tend to be in the low milliseconds range. Thus, read latency has to be proportionately lower.”\\xa0\\u200d\\n\\nAI-enabled products use models to make predictions, and those models need an Online Feature Store to provide them with historical and contextual data (features) to make better predictions.\\n\\xa0\\nData-Centric AI with the Online Feature Store\\n\\xa0\\nSo, to summarize, if you want to give your ML models a brain, connect them up to a feature store. For enterprises building personalized services, the feature store can enrich their models with a 360-degree enterprise-wide view of the customer - not just a product-specific view of the customer. The feature store enables more accurate predictions through more data being available to make those predictions, and this ultimately enables products with better user experience, increased engagement, and the product intelligence now expected by users.\\n*\\xa0This is based on\\xa0a true story.\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Jim Dowling (@jim_dowling) is the CEO at Logical Clocks and an Associate Professor at KTH Royal Institute of Technology. He is lead architect of the open-source Hopsworks platform, the world's first enterprise Feature Store along with an advanced end-to-end ML platform.\\nRelated:\\n\\nFeature Store as a Foundation for Machine Learning\\nFeature Store vs Data Warehouse\\nFeature stores – how to avoid feeling that every day is Groundhog Day\",\n",
       " 'comments\\nBy Claire D. Costa, Content Writer and Strategist at Digitalogy LLC.\\n\\nPhoto by\\xa0luis gomes\\xa0from\\xa0Pexels.\\nPython is an experiment in how much freedom programmers need. Too much freedom and nobody can read another’s code; too little and expressiveness is endangered. - Guido van Rossum\\nSince its creation,\\xa0Python has rapidly evolved into a multi-faceted programming language, becoming the choice of several diverse projects ranging from web applications to being deployed into\\xa0Artificial Intelligence,\\xa0Machine Learning,\\xa0Deep Learning, and more.\\nPython\\xa0comes with numerous\\xa0features such as its simplicity, enormous collection of packages and libraries,\\xa0with relatively faster execution of programs, to list a few.\\nGitHub’s second-most popular language and the most popular language for machine learning.\\nFor a programmer,\\xa0a Code Editor or an IDE\\xa0is the first point of contact with any programming language, making its selection one of the most crucial steps in the journey ahead.\\xa0Throughout this article, we’ll discuss some of the top Python IDEs and Code Editors, along with the reasons why you should and shouldn’t pick them for your next project.\\nAccording to StackOverflow, Python is the fastest-growing major programming language:\\xa0Stack Overflow Developer Survey 2019\\n\\xa0\\nWhat is an Integrated Development Environment (IDE)?\\n\\xa0\\nAn IDE stands for\\xa0Integrated Development Environment and includes not just the standard code editor for managing the code but also provides a comprehensive set of tools for its debugging, execution, and testing, which is an absolute must for software development. Some IDEs also come with built-in compilers and interpreters. Listed below are some of the\\xa0standard features\\xa0common IDEs offer within a single dedicated environment:\\n\\nSyntax highlighting\\nBuild automation\\nVersion control\\nVisual programming\\nCode formatting and completion\\nCode refactoring\\nSupport for integration with external tools\\n\\n\\xa0\\nIDE vs. Code Editor\\n\\xa0\\nA Code Editor or an IDE is the most fundamental piece of software for any programmer, and it is what they start and end their day. To achieve its maximum potential, the best starting point is a Code Editor or an IDE that essentially lets you work with Python, but that’s not all.\\xa0A host of\\xa0programming languages\\xa0can work entirely without an IDE, while some are IDE-dependent.\\nCode Editor —\\xa0A Code Editor is a core piece of software that programmers use for application development. Think of it as a simple text editor but with additional programming-specific advanced features such as:\\n\\nSyntax highlighting\\nCode formatting\\nSplit file viewing and editing\\nInstant project switching\\nMultiple selections\\nCross-platform support\\nLight-weight\\n\\nIDE —\\xa0On the other hand, an IDE comes with a suite of tools that help in not just developing the application but also in its testing, debugging, refactoring, and automating builds. Needless to say, in most cases, an IDE can offer all features of a Code Editor, but a Code Editor cannot replace an IDE.\\n\\xa0\\nBest Python IDEs and Code Editors in 2020\\n\\xa0\\nChoosing the right tools for a job is critical. Similarly, when starting a new project, as a programmer, you have a lot of options when it comes to selecting the perfect Code Editor or IDE. There are loads of IDEs and Code Editors out there for Python, and in this section, we’ll discuss some of the best ones available with their benefits and weaknesses.\\n\\nPyCharm\\n\\n\\nImage Source —\\xa0PyCharm.\\n\\nCategory:\\xa0IDE\\nFirst Release Date:\\xa02010\\nPlatform Compatibility:\\xa0Windows, macOS, Linux\\nWho It’s For:\\xa0Intermediate to advanced Python users\\nSupporting Languages:\\xa0Python, Javascript, CoffeeScript, etc.\\nPrice:\\xa0Freemium (free limited feature community version, paid full-featured professional version)\\nDownload:\\xa0PyCharm Download Link\\nPopular Companies using Pycharm Python IDE -\\xa0Twitter, HP, Thoughtworks, GROUPON, and Telephonic.\\n\\nDeveloped by JetBrains, PyCharm is a\\xa0cross-platform IDE\\xa0that offers a variety of features such as version control, graphical debugger, integrated unit tester, and pairs well for web development and Data Science tasks. With PyCharm’s API, developers can create their custom plugins for adding new features to the IDE.\\xa0Other features include:\\n\\nCode completion\\nLive updates to code changes\\nPython refactoring\\nSupport for full-stack web development\\nSupport for scientific tools such as matplotlib, numpy, and scipy\\nSupport for Git, Mercurial and more\\nComes with paid and community editions\\n\\nAdvantages —\\n\\nCan boost productivity and code quality\\nHighly active community for support\\n\\nDisadvantages\\xa0—\\n\\nCan be slow to load\\nRequires changing default settings for existing projects for best compatibility\\nThe initial installation might be difficult\\n\\nScreenshot for Reference\\n\\nImage Source —\\xa0PyCharm.\\n\\nSpyder\\n\\n\\nImage Source —\\xa0Spyder.\\n\\nCategory:\\xa0IDE\\nFirst Release Year:\\xa02009\\nPlatform Compatibility:\\xa0Windows, macOS, Linux\\nWho It’s For:\\xa0Python data scientists\\nPrice:\\xa0Free\\nDownload:\\xa0Spyder Download Link\\n\\nSpyder\\xa0comes with support for packages like NumPy, SciPy, Matplotlib, and Pandas. Targeted towards scientists, engineers, and data analysts, Spyder offers advanced data exploration, analysis, and visualization tools.\\xa0Features of this cross-platform IDE include:\\n\\nCode completion\\nSyntax highlighting\\nCode benchmarking via Profiler\\nMulti-project handling\\nFind in Files feature\\nHistory log\\nInternal console for introspection\\nThird-party plugins support\\n\\nAdvantages —\\n\\nIncludes support for numerous scientific tools\\nComes with an amazing community support\\nInteractive console\\nLightweight\\n\\nDisadvantages —\\n\\nComes with execution dependencies\\nCan be a bit challenging at first for newcomers\\n\\nScreenshot for Reference\\n\\nImage Source —\\xa0Spyder.|\\nspyder-ide/spyder\\n\\nEclipse + Pydev\\n\\n\\n\\nCategory:\\xa0IDE\\nFirst Release Year:\\xa02001 —\\xa0for Eclipse, 2003 —\\xa0for Pydev\\nPlatform Compatibility:\\xa0Windows, macOS, Linux\\nWho It’s For:\\xa0Intermediate to advanced Python users\\nSupporting Languages:\\xa0Python, (Eclipse supports Java and many other programming languages)\\nPrice:\\xa0Free\\nDownload:\\xa0PyDev Download Link\\nPopular Companies using PyDev and Eclipse Python IDE\\xa0— Hike, Edify, Accenture, Wongnai, and Webedia.\\n\\nEclipse\\xa0is one of the top IDEs available, supporting a broad range of programming languages for application development, including Python. Primarily created for developing Java applications, support for other programming languages is introduced via plugins. The plugin used for Python development is Pydev and offers\\xa0additional benefits\\xa0over Eclipse IDE, such as:\\n\\nDjango, Pylint, and unit test integration\\nInteractive console\\nRemote debugger\\nGo to definition\\nType hinting\\nAuto code completion with auto import\\n\\nAdvantages —\\n\\nEasy to use\\nProgrammer friendly features\\nFree\\n\\nDisadvantages —\\n\\nComplex user interface makes it challenging to work with\\nIf you’re a beginner, then using Eclipse will be difficult\\n\\nScreenshot for Reference\\n\\nImage Source —\\xa0Pydev.\\n\\nIDLE\\n\\n\\nImage Source —\\xa0Python.\\n\\nCategory:\\xa0IDE\\nFirst Release Year: 1998\\nPlatform Compatibility:\\xa0Windows, macOS, Linux\\nWho It’s For:\\xa0Beginning Python users\\nPrice:\\xa0Free\\nDownload:\\xa0IDLE Download Link\\nPopular Companies using IDLE Python IDE —\\xa0Google, Wikipedia, CERN, Yahoo, and NASA.\\n\\nShort for\\xa0Integrated Development and Learning Environment,\\xa0IDLE has been bundled with Python as its default IDE for more than 15 years. IDLE is a\\xa0cross-platform IDE\\xa0and offers a basic set of features to keep it unburdened.\\xa0The features offered include:\\n\\nShell window with colorized code, input, output, and error messages\\nSupport for a multi-window text editor\\nCode auto-completion\\nCode formatting\\nSearch within files\\nDebugger with breakpoints\\nSupports smart indentation\\n\\nAdvantages —\\n\\nPerfect for beginners and educational institutions\\n\\nDisadvantages —\\n\\nLacks features offered by more advanced IDEs, such as project management capabilities\\n\\nIDLE - Python 3.8.3 documentation\\n\\nWing\\n\\n\\nImage Source —\\xa0Wing.\\n\\nCategory:\\xa0IDE\\nFirst Release Year:\\xa0September 7, 2000\\nPlatform:\\xa0Windows, Linux, and Mac\\nWho It’s For:\\xa0Intermediate to advanced Python users\\nPrice:\\xa0$179 per user for a year of commercial use, $245 per user for a permanent commercial use license\\nDownload:\\xa0Wing Download Link\\nPopular Companies using Wing Python IDE\\xa0— Facebook, Google, Intel, Apple, and NASA\\n\\nThe feature-rich IDE for Python, Wing, was developed to make development faster with the introduction of intelligent features such as smart editor and simple code navigation. Wing comes in 101, Personal, and Pro variants with Pro being the most feature-rich and the only paid one.\\xa0Other notable features by Wing include:\\n\\nCode completion, error detection, and quality analysis\\nSmart refactoring capabilities\\nInteractive debugger\\nUnit tester integration\\nCustomizable interface\\nSupport for remote development\\nSupport for frameworks such as Django, Flask, and more\\n\\nAdvantages —\\n\\nWorks well with version control systems such as Git\\nStrong debugging capabilities\\n\\nDisadvantages —\\n\\nLacks a compelling user interface\\n\\n\\xa0\\n\\nCloud9 IDE\\n\\n\\nImage Source —\\xa0AmazonCloud9.\\n\\nCategory: IDE\\nFirst Release Year:\\xa02010\\nPlatform: Linux/MacOS/Windows\\nPopular Companies using Cloud9 Python IDE —\\xa0Linkedin, Salesforce, Mailchimp, Mozilla, Edify, and Soundcloud.\\n\\nPart of\\xa0Amazon’s Web Services,\\xa0Cloud9 IDE\\xa0gives you access to a cloud-based IDE, requiring just a browser. All the code is executed on Amazon’s infrastructure, translating to a seamless and lightweight development experience.\\xa0Features include:\\n\\nRequires minimal project configuration\\nPowerful code editor\\nCode highlight, formatting, and completion capabilities\\nBuilt-in terminal\\nStrong debugger\\nReal-time pair programming capabilities\\nInstantaneous project setup, covering most programming languages and libraries\\nUnobstructed access to several AWS services via terminal\\n\\nAdvantages —\\n\\nEnables painless development of serverless applications\\nRemarkably robust and globally accessible infrastructure\\n\\nDisadvantages —\\n\\nDepends entirely on internet access\\n\\n\\xa0\\n\\nSublime Text 3\\n\\n\\nImage Source —\\xa0Sublime.\\n\\nCategory:\\xa0Code Editor\\nFirst Release Year:\\xa02008\\nPlatform Compatibility:\\xa0Windows, macOS, Linux\\nWho It’s For:\\xa0Beginner, Professional\\nSupporting Languages:\\xa0Python and C#\\nPrice:\\xa0Freemium\\nDownload:\\xa0Sublime text 3 Download Link\\nPopular Companies using Sublime Text Python IDE\\xa0—\\xa0Starbucks, Myntra, Trivago, Stack, and Zapier.\\n\\nSublime Text\\xa0is one of the most commonly used cross-platform Code Editors and supports several programming languages, including Python. Sublime offers various features such as plenty of themes for visual customization, a clean and distraction-free user interface, and supports package manager for extending the core functionality via plugins.\\xa0Other features include:\\n\\nUp-to-date plugins via Package Manager\\nFile auto-save\\nMacros\\nSyntax highlight and code auto-completion\\nSimultaneous code editing\\nGo to anything, definition, and symbol\\n\\nAdvantages —\\n\\nUncluttered user interface\\nSplit editing\\nFast and high-performance editor\\n\\nDisadvantages —\\n\\nAnnoying popup to buy sublime license\\nConfusingly large number of shortcuts\\nComplicated package manager\\n\\n\\n\\nVisual Studio Code\\n\\n\\nImage Source — Visual Studio Code.\\n\\nCategory:\\xa0IDE\\nFirst Release Year:\\xa02015\\nPlatform Compatibility:\\xa0Windows, macOS, Linux\\nWho It’s For: Professional\\nSupporting Languages:\\xa0All the major programming languages (Python, C++, C#, CSS, Dockerfile, Go, HTML, Java, JavaScript, JSON, Less, Markdown, PHP, PowerShell, Python, SCSS, T-SQL, TypeScript.)\\nPrice:\\xa0Free\\nDownload:\\xa0Visual Studio Code Download Link\\nPopular Companies using Visual Source Code (Python IDE) —\\xa0The Delta Group, TwentyEight, Inc., Focus Ponte Global, Creative Mettle, and National Audubon Society, Inc.\\n\\nDeveloped by Microsoft, Visual Studio Code is an acclaimed cross-platform code editor that is highly customizable and allows development in several programming languages, including Python. It offers a wide variety of features to programmers, such as smart debugging, customizability, plugin support for extending core features.\\xa0Key highlights include:\\n\\nBuilt-in support for Git and version control\\nCode refactoring\\nIntegrated terminal\\nIntelliSense for smarter code highlight and completion\\nIntuitive code debugging capabilities\\nSeamless deployment to Azure\\n\\nAdvantages —\\n\\nRegularly updated with active community support\\nFree\\n\\nDisadvantages —\\n\\nVast collection of plugins can make finding the right one challenging\\nLackluster handling of large files\\nLonger launch time\\n\\nScreenshot for Reference\\n\\nImage Source —\\xa0Visual Studio Code.\\nPython in Visual Studio Code\\n\\nAtom\\n\\n\\nImage Source —\\xa0Atom.\\n\\nCategory:\\xa0Code Editor\\nFirst Release Year:\\xa02014\\nPlatform Compatibility:\\xa0Windows, macOS, Linux\\nWho It’s For:\\xa0Beginner, Professional\\nSupporting Languages:\\xa0Python, HTML, Java and 34 other languages.\\nPrice:\\xa0Free\\nDownload:\\xa0Atom Download Link\\nPopular Companies using Atom (Python IDE) —\\xa0Accenture, Hubspot, Figma, Lyft, and Typeform.\\n\\nDeveloped by Github,\\xa0the top dog in source-code hosting and software version controlling, Atom is a lightweight and cross-platform Code Editor for Python and many other programming languages. Atom provides a lot of features in the form of packages, that enhances its core features. It’s built on HTML, JavaScript, CSS, and Node.js, with the underlying framework being Electron.\\xa0Features offered include:\\n\\nSupport for third-party packages via built-in Package Manager\\nSupports developer collaboration\\nOver 8000 feature and user experience-extending packages\\nSupport for multi-pane file access\\nSmart code completion\\nCustomizability options\\n\\nAdvantages —\\n\\nLightweight code editor\\nCommunity-driven development and support\\n\\nDisadvantages —\\n\\nRecent updates have increased RAM usage\\nSome tweaking required in settings before use\\n\\n\\n\\nJupyter\\n\\n\\nImage source —\\xa0Jupyter.\\n\\nCategory:\\xa0IDE\\nFirst Release Year:\\xa0February 2015\\nBrowser Compatibility:\\xa0Chrome, Firefox, Safari\\nPrice:\\xa0Free\\nDownload:\\xa0Jupyter Download Link\\nPopular Companies of Using Jupyter Python IDE\\xa0—\\xa0Google, Bloomberg, Microsoft, IBM, and Soundcloud.\\n\\nAlso known as\\xa0Project Jupyter,\\xa0it is an\\xa0open-source\\xa0and cross-platform IDE\\xa0that many data scientists and analysts prefer over other tools.\\xa0Perfect for working on technologies such as AI, ML, DL, along with several programming languages, Python included.\\xa0Jupyter Notebooks offer seamless creation and sharing of code, text, and equations for various purposes, including analysis, visualization, and development.\\xa0Features offered include:\\n\\nCode formatting and highlight\\nEasy sharing via email, Dropbox\\nProduces interactive output\\nPlays well with Big Data\\nCan be run from local and cloud machines\\n\\nAdvantages —\\n\\nRequires minimal setup\\nPerfect for quick data analysis\\n\\nDisadvantages —\\n\\nInexperienced users may find Jupyter complicated\\n\\nScreenshot for Reference\\n\\nImage source —\\xa0Jupyter.\\n\\xa0\\nHow to Choose Best Python IDEs and Code Editors for Yourself\\n\\xa0\\nPicking the right IDE or Code Editor can mean the difference in saving time with quicker development or losing it due to reckless decisions. We have mentioned a lot of IDEs and Code Editors in the previous section with some of its noteworthy features. If you’re confused about which one you should pick for your next Python project, then we recommend you give it a quick read. After all, what would a programmer be without a proper set of IDEs and Code Editors?\\nNote:\\xa0To eliminate problems of different kinds, I want to alert you to the fact this article represents just my personal opinion I want to share, and you possess every right to disagree with it.\\nOriginal. Reposted with permission.\\n\\xa0\\nBio: Claire D. Costa\\xa0is a Content Crafter and Marketer at\\xa0Digitalogy,\\xa0a tech sourcing and custom matchmaking marketplace that connects people with pre-screened and top-notch developers and designers based on their specific needs across the globe.\\nRelated:\\n\\nHere are the Most Popular Python IDEs/Editors\\nNew Poll: What Python IDE / Editor you used the most in 2020?\\nNetflix’s Polynote is a New Open Source Framework to Build Better Data Science Notebooks',\n",
       " \"By Derrick Mwiti, Data Scientist.\\ncomments\\n\\n\\nPhoto by Element5 Digital on Unsplash\\n\\n\\xa0\\nFeature selection\\xa0is an important task for any machine learning application. This is especially crucial when the data in question has many features. The optimal number of features also leads to improved model accuracy. Obtaining the most important features and the number of optimal features can be obtained via feature importance or feature ranking. In this piece, we’ll explore feature ranking.\\n\\xa0\\nRecursive Feature Elimination\\n\\xa0\\nThe first item needed for recursive feature elimination is an estimator; for example, a linear model or a decision tree model.\\nThese models have coefficients for linear models and feature importances in decision tree models. In selecting the optimal number of features, the estimator is trained and the features are selected via the coefficients, or via the feature importances. The least important features are removed. This process is repeated recursively until the optimal number of features is obtained.\\n\\xa0\\nApplication in Sklearn\\n\\xa0\\nScikit-learn makes it possible to implement recursive feature elimination via the sklearn.feature_selection.RFE class. The class takes the following parameters:\\n\\nestimator\\xa0— a machine learning estimator that can provide features importances via the\\xa0coef_\\xa0or\\xa0feature_importances_\\xa0attributes.\\nn_features_to_select\\xa0— the number of features to select. Selects\\xa0half\\xa0if it's not specified.\\nstep\\xa0— an integer that indicates the number of features to be removed at each iteration, or a number between 0 and 1 to indicate the percentage of features to remove at each iteration.\\n\\nOnce fitted, the following attributes can be obtained:\\n\\nranking_\\xa0— the ranking of the features.\\nn_features_\\xa0— the number of features that have been selected.\\nsupport_\\xa0— an array that indicates whether or not a feature was selected.\\n\\n\\xa0\\nApplication\\n\\xa0\\nAs noted earlier, we’ll need to work with an estimator that offers a\\xa0feature_importance_s\\xa0attribute or a\\xa0coeff_\\xa0attribute. Let’s work through a quick example. The dataset has 13 features—we’ll work on getting the optimal number of features.\\n\\nimport pandas as pddf = pd.read_csv(‘heart.csv’)df.head()\\n\\n\\n\\nLet’s obtain the\\xa0X\\xa0and\\xa0y\\xa0features.\\n\\nX = df.drop([‘target’],axis=1)\\r\\ny = df[‘target’]\\n\\n\\nWe’ll split it into a testing and training set to prepare for modeling:\\n\\nfrom sklearn.model_selection import train_test_split\\r\\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\\n\\n\\nLet’s get a couple of imports out of the way:\\n\\nPipeline\\xa0— since we’ll perform some cross-validation. It’s best practice in order to avoid data leakage.\\nRepeatedStratifiedKFold\\xa0— for repeated stratified cross-validation.\\ncross_val_score\\xa0— for evaluating the score on cross-validation.\\nGradientBoostingClassifier\\xa0— the estimator we’ll use.\\nnumpy\\xa0— so that we can compute the mean of the scores.\\n\\n\\nfrom sklearn.pipeline import Pipeline\\r\\nfrom sklearn.model_selection import RepeatedStratifiedKFold\\r\\nfrom sklearn.model_selection import cross_val_score\\r\\nfrom sklearn.feature_selection import RFE\\r\\nimport numpy as np\\r\\nfrom sklearn.ensemble import GradientBoostingClassifier\\n\\n\\nThe first step is to create an instance of the\\xa0RFE\\xa0class while specifying the estimator and the number of features you’d like to select. In this case, we’re selecting 6:\\n\\nrfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=6)\\n\\n\\nNext, we create an instance of the model we’d like to use:\\n\\nmodel = GradientBoostingClassifier()\\n\\n\\nWe’ll use a\\xa0Pipeline\\xa0to transform the data. In the\\xa0Pipeline\\xa0we specify\\xa0rfe\\xa0for the feature selection step and the model that’ll be used in the next step.\\nWe then specify a\\xa0RepeatedStratifiedKFold\\xa0with 10 splits and 5 repeats. The stratified K fold ensures that the number of samples from each class is well balanced in each fold. RepeatedStratifiedKFold repeats the stratified K fold the specified number of times, with a different randomization in each repetition.\\n\\npipe = Pipeline([(‘Feature Selection’, rfe), (‘Model’, model)])\\r\\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=36851234)\\r\\nn_scores = cross_val_score(pipe, X_train, y_train, scoring=’accuracy’, cv=cv, n_jobs=-1)\\r\\nnp.mean(n_scores)\\n\\n\\nThe next step is to fit this pipeline to the dataset.\\n\\npipe.fit(X_train, y_train)\\n\\n\\nWith that in place, we can check the support and the ranking. The support indicates whether or not a feature was chosen.\\n\\nrfe.support_\\r\\narray([ True, False,  True, False,  True, False, False,  True, False,True, False,  True,  True])\\n\\n\\nWe can put that into a dataframe and check the result.\\n\\npd.DataFrame(rfe.support_,index=X.columns,columns=[‘Rank’])\\n\\n\\n\\nWe can also check the relative rankings.\\n\\nrf_df = pd.DataFrame(rfe.ranking_,index=X.columns,columns=[‘Rank’]).sort_values(by=’Rank’,ascending=True)rf_df.head()\\n\\n\\n\\n\\xa0\\nAutomatic Feature Selection\\n\\xa0\\nInstead of manually configuring the number of features, it would be very nice if we could automatically select them. This can be achieved via recursive feature elimination and cross-validation. This is done via the\\xa0sklearn.feature_selection.RFECV\\xa0class. The class takes the following parameters:\\n\\nestimator\\xa0— similar to the\\xa0RFE\\xa0class.\\nmin_features_to_select\\xa0— the minimum number of features to be selected.\\ncv— the cross-validation splitting strategy.\\n\\nThe attributes returned are:\\n\\nn_features_\\xa0— the optimal number of features selected via cross-validation.\\nsupport_\\xa0— the array containing information on the selection of a feature.\\nranking_\\xa0— the ranking of the features.\\ngrid_scores_\\xa0— the scores obtained from cross-validation.\\n\\nThe first step is to import the class and create its instance.\\n\\nfrom sklearn.feature_selection import RFECVrfecv = RFECV(estimator=GradientBoostingClassifier())\\n\\n\\nThe next step is to specify the pipeline and the cv. In this pipeline we use the just created\\xa0rfecv.\\n\\npipeline = Pipeline([(‘Feature Selection’, rfecv), (‘Model’, model)])\\r\\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=36851234)\\r\\nn_scores = cross_val_score(pipeline, X_train, y_train, scoring=’accuracy’, cv=cv, n_jobs=-1)\\r\\nnp.mean(n_scores)\\n\\n\\nLet’s fit the pipeline and then obtain the optimal number of features.\\n\\npipeline.fit(X_train,y_train)\\n\\n\\nThe optimal number of features can be obtained via the\\xa0n_features_\\xa0attribute.\\n\\nprint(“Optimal number of features : %d” % rfecv.n_features_)Optimal number of features : 7\\n\\n\\nThe rankings and support can be obtained just like last time.\\n\\nrfecv.support_rfecv_df = pd.DataFrame(rfecv.ranking_,index=X.columns,columns=[‘Rank’]).sort_values(by=’Rank’,ascending=True)\\r\\nrfecv_df.head()\\n\\n\\nWith the\\xa0grid_scores_\\xa0we can plot a graph showing the cross-validated scores.\\n\\nimport matplotlib.pyplot as plt\\r\\nplt.figure(figsize=(12,6))\\r\\nplt.xlabel(“Number of features selected”)\\r\\nplt.ylabel(“Cross validation score (nb of correct classifications)”)\\r\\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\\r\\nplt.show()\\n\\n\\n\\n\\nNumbers of features against the accuracy plot\\n\\n\\xa0\\nFinal Thoughts\\n\\xa0\\nThe process for applying this in a regression problem is the same. Just ensure to use regression metrics instead of accuracy. I hope this piece has given you some insight on selecting the optimal number of features for your machine learning problems.\\nmwitiderrick/Feature-Ranking-with-Recursive-Feature-Elimination\\nFeature Ranking with Recursive Feature Elimination - mwitiderrick/Feature-Ranking-with-Recursive-Feature-Elimination\\n\\xa0\\n\\xa0\\nBio: Derrick Mwiti is a data analyst, a writer, and a mentor. He is driven by delivering great results in every task, and is a mentor at Lapid Leaders Africa.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow I Consistently Improve My Machine Learning Models From 80% to Over 90% Accuracy\\nLightGBM: A Highly-Efficient Gradient Boosting Decision Tree\\nFast Gradient Boosting with CatBoost\",\n",
       " 'By Nicole Janeway Bills, Data Scientist at Atlas Research.\\ncomments\\n\\nPhoto by\\xa0Rabie Madaci\\xa0on\\xa0Unsplash.\\nData science might be a young field, but that doesn’t mean you won’t face expectations about having an awareness of certain topics. This article covers several of the most important recent developments and influential thought pieces.\\nTopics covered in these papers range from the\\xa0orchestration of the DS workflow\\xa0to\\xa0breakthroughs in faster neural networks\\xa0to a\\xa0rethinking of our fundamental approach to problem solving with statistics. For each paper, I offer ideas for how you can apply these ideas to your own work\\n\\xa0\\n#1 —\\xa0Hidden Technical Debt in Machine Learning Systems\\n\\xa0\\nThe team at Google Research provides\\xa0clear instructions on antipatterns to avoid\\xa0when setting up your data science workflow. This paper borrows the metaphor of technical debt from software engineering and applies it to data science.\\n\\nvia\\xa0DataBricks.\\nAs the next paper explores in greater detail, building a machine learning product is a highly specialized subset of software engineering, so it makes sense that many lessons drawn from this discipline will apply to data science as well.\\nHow to use: follow the experts’\\xa0practical tips\\xa0to streamline development and production.\\n\\xa0\\n#2 —\\xa0Software 2.0\\n\\xa0\\nThis classic post from\\xa0Andrej Karpathy\\xa0articulated the paradigm that machine learning models are\\xa0software applications with code based on data.\\nIf data science is software, what exactly are we building towards? Ben Bengafort explored this question in an influential blog post called “The Age of the Data Product.”\\n\\nThe data product represents the operationalization phase of an ML project. Photo by\\xa0Noémi Macavei-Katócz\\xa0on\\xa0Unsplash.\\nHow to use: read more about how the data product fits into the\\xa0model selection process.\\n\\xa0\\n#3 —\\xa0BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\n\\xa0\\nIn this paper, the team at Google Research put forward the natural language processing (NLP) model that represented a step-function increase in our capabilities in for text analysis.\\nThough there’s\\xa0some controversy\\xa0over exactly why BERT works so well, this is a great reminder that the machine learning field may have uncovered successful approaches without fully understanding how they work.\\xa0As with nature, artificial neural networks are steeped in mystery.\\n\\n In this delightful clip, the Director of Data Science at Nordstrom explains how artificial neural nets draw inspiration from nature.\\nHow to use:\\n\\nThe\\xa0BERT paper is imminently readable and contains some suggested default hyperparameter settings as a valuable starting point (see Appendix A.3).\\nWhether or not you’re new to NLP, check out Jay Alammar’s\\xa0“A Visual Guide to Using BERT for the First Time” for a charming illustration of BERT’s capabilities.\\nAlso, check out ktrain, a package that sits atop Keras (which in turn sits atop TensorFlow) that allows you to effortlessly implement BERT in your work.\\xa0Arun Maiya\\xa0developed this powerful library to enable speed to insight for NLP, image recognition, and graph-based approaches.\\n\\n\\xa0\\n#4 —\\xa0The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\\n\\xa0\\nWhile NLP models are getting larger (see GPT-3’s 175 billion parameters), there’s been an orthogonal effort to find smaller, faster, more efficient neural networks. These networks promise quicker runtimes, lower training costs, and less demand for compute resources.\\nIn this groundbreaking paper, machine learning wiz kids Jonathan Frankle and Michael Carbin outline a pruning approach to uncover sparse sub-networks that can attain comparable performance to the original, significantly larger neural network.\\n\\nvia\\xa0Nolan Day’s “Breaking down the Lottery Ticket Hypothesis.”\\nThe Lottery Ticket refers to the connections with initial weights that make them particularly effective. The finding offers many advantages in storage, runtime, and computational performance - and won a\\xa0best paper award at ICLR 2019. Further research has built on this technique,\\xa0proving its applicability\\xa0and\\xa0applying it to an originally sparse network.\\nHow to use:\\n\\nConsider\\xa0pruning your neural nets before putting them into production. Pruning network weights can reduce the number of parameters by 90%+ while still achieving the same level of performance as the original network.\\nAlso, check out this\\xa0episode of the Data Exchange podcast where Ben Lorica talks to\\xa0Neural Magic, a startup that’s looking to capitalize on techniques such as\\xa0pruning and quantization\\xa0with a slick UI that makes achieving sparsity easier.\\n\\nRead more:\\n\\nCheck out this interesting sidebar\\xa0from one of the “The Lottery Ticket” authors about flaws in how the machine learning community evaluates good ideas.\\n\\n\\xa0\\n#5 —\\xa0Releasing the death-grip of null hypothesis statistical testing (p\\xa0< .05)\\n\\xa0\\n\\nClassical hypothesis testing leads to over-certainty and produces the false idea that causes have been identified via statistical methods. (Read more)\\n\\nHypothesis testing predates the use of computers. Given the challenges associated with this approach (such as the fact that\\xa0even statisticians find it nearly impossible to explain p-value), it may be time to consider alternatives such as somewhat precise outcome testing (SPOT).\\n\\n“Significant” via\\xa0xkcd.\\nHow to use:\\n\\nCheck out this blog post, “The Death of the Statistical Tests of Hypotheses,” where a frustrated statistician outlines some of the challenges associated with the classical approach and explains an alternative utilizing confidence intervals.\\n\\nSign up to get notified when “Resources to Supercharge your Data Science in the Last Months of 2020” comes out\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Nicole Janeway Bills\\xa0is a machine learning engineer with experience in commercial and federal consulting. Proficient in Python, SQL, and Tableau, Nicole has business experience in natural language processing (NLP), cloud computing, statistical testing, pricing analysis, and ETL processes, and aims to use this background to connect data with business outcomes and continue to develop technical skillsets.\\nRelated:\\n\\nAI Papers to Read in 2020\\nMust-read NLP and Deep Learning articles for Data Scientists\\n13 must-read papers from AI experts',\n",
       " \"comments\\nBy Tyler Richards, Data Scientist @ Facebook\\n\\n\\xa0\\nAround once a month, I get emailed by a student of some type asking how to get into Data Science, I've answered it enough that I decided to write it out here so I can link people to it. So if you’re one of those students, welcome!\\nI'll segment this into basic advice, which can be found quite easily if you just google 'how to get into data science' and advice that is less common, but advice that I've found very useful over the years. I'll start with the latter, and move on to basic advice. Obviously take this with a grain of salt as all advice comes with a bit of survivorship bias.\\n\\xa0\\nLess Basic Advice:\\n\\xa0\\n1. Find a solid community\\nIf you’re at a university, half the point of being there is to find smart, ambitious, and motivated people like yourself to learn and grow with. For my alma mater, that community was the\\xa0Data Science and Informatics club. Communities/networks help you get started, keep you motivated, and are key for scoring internships and full time offers in the long term.\\n2. Apply Data Science to Things you Enjoy\\nGetting good at anything is difficult (duh), and applying data science to a field or area you care about helps you stay motivated and stand out. A couple of my examples of this are: using\\xa0UF's (alma mater) student government elections\\xa0to learn about machine learning approaches, or tracking my friends' Elo scores by\\xa0recording our games of ping pong. These projects taught me essential skills without explicitly feeling like work.\\nGetting useful practice that is representative of the job you want to perform in the future is crucial because out of this practice you can only get one of two things:\\na. The realization that you don't actually like this type of data science in which case you should stop reading immediately\\nb. Valuable experience that you can easily write about (blog) or talk about (to people who want to pay you money)\\nThis brings me to my next point.\\n3. Minimize the ‘Clicks to Proof of Competence’\\nRecruiters will spend 15 seconds on your resume, potential teams will spend 1-5 minutes (at most) on your resume + website/Github (on average, visitors to\\xa0my portfolio site\\xa0spend 2 minutes and 16 seconds before moving on). Both groups often use proxies for competence like GPA, school quality, or experience in data from a tech firm (I call these: proof of status). As a result, you should very closely think about the time needed to signal to the reader that you can do whatever job they’re looking to hire for. A rough metric to consider for this is Clicks to Proof of Competence.\\nIf the recruiter has to click on the right repository in your Github and then click through files until they find the Jupyter notebook with unreadable code (without comments nonetheless), you’ve already lost. If the recruiter sees Machine Learning on your resume, but it takes 5 clicks to see any ML product or code that you've made, you've already lost. Anyone can lie on a resume; make a point to direct the reader’s attention quickly, and you’ll be in a significantly better spot.\\nThe way i've thought about optimizing for this metric is pretty clear on\\xa0my website. It roughly takes 10 seconds to skim the text (I would bet that most people don't read it all the way through), and then immediately people can choose a Data Science project to view, which are ordered by how well they show the work I can do. For starting off in DS, I would highly recommend making a website (even a bootstrap template website is fine) and hosting it on Github pages or heroku with your own domain.\\n4. Learn Through Research or Entry Level Jobs\\nAfter you do those three things, see if you can convince someone to pay you to learn data science. There is a great election data science group at UF that I loved (Dr McDonald and Dr Smith run it currently), but if you go to any research group and interview with them they might pay you for your work. Eventually, with experience like that, then you can apply for internships and get paid super well. The key here is to not start out looking for the incredibly fancy DS internships, but locally at companies or research groups that have Data Science tasks but not enough money to hire a full time Data Scientist. Data Science learning compounds quickly, so start now! Given all of that, let’s move on to the more basic advice.\\n\\xa0\\nExtremely Basic Advice:\\n\\xa0\\nData Science is mostly programming + statistics applied to whatever field you're in, so a background in those two areas is crucial.\\n1. Statistics\\nGet a good background in stats as quickly as possible (take classes, learn on your own online). Textbooks will take you far, curiosity will take you farther.\\nBooks/resources:\\n\\nNaked Statistics\\xa0(basic, paid)\\nISLR\\xa0(Introduction to Statistical Learning in R) (textbook, free)\\nStatistics and Probability: Khan Academy (basic, free)\\n\\n2. Programming\\nLearn either Python or R and get really good at it. Do something new every day, spend at least 5-10 hours per week on it as soon as possible. Learn SQL after this. You cannot skip around this.\\nBooks/resources:\\n\\nR for Data Science\\xa0(free)\\nMachine Learning Python Cookbook\\xa0(paid)\\nData Science From Scratch\\xa0(paid)\\nThe SQL Tutorial for Data Analysis\\xa0(free)\\nIntro to Comp Sci and Programming in Python\\xa0(MIT Course, free)\\n\\n3. Business Experience\\nAt P&G, my data science work was applied to retail. At Facebook, to integrity problems. At Protect Democracy, to, uh, Democracy. Learning about applications of data science into some business context is hard and takes practice, and often involves a solid understanding of metrics, product analytics and incentive structures. This fits in very well with #2 from the less basic advice.\\n\\xa0\\nFin\\n\\xa0\\nLearning data science is hard but I’ve found it to be incredibly rewarding. My final offer to you, in exchange for reading to the bottom of this long-ish piece, is to say that once you finish applying data science to a problem you’re passionate about and posting it somewhere online, DM it to me on\\xa0Twitter\\xa0and I promise to read it and retweet it. Good luck!\\n\\xa0\\nBio: Tyler Richards is a Data Scientist at Facebook.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow To Decide What Data Skills To Learn\\nThe unspoken difference between junior and senior data scientists\\n6 Lessons Learned in 6 Months as a Data Scientist\",\n",
       " 'comments\\nBy Walid Amamou, Founder of UBIAI\\n\\n\\nPhoto by\\xa0JJ Ying\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nIntroduction\\n\\xa0\\nOne of the most useful applications of NLP technology is information extraction from unstructured texts — contracts, financial documents, healthcare records, etc. — that enables automatic data query to derive new insights. Traditionally, named entity recognition has been widely used to identify entities inside a text and store the data for advanced querying and filtering. However, if we want to semantically understand the unstructured text,\\xa0NER alone is not enough since we don’t know how the entities are related to each other. Performing joint NER and relation extraction will open up a whole new way of information retrieval through knowledge graphs where you can navigate across different nodes to discover hidden relationships. Therefore, performing these tasks jointly will be beneficial.\\nBuilding on my\\xa0previous article\\xa0where we fine-tuned a BERT model for NER using spaCy3, we will now add relation extraction to the pipeline using the new Thinc library from spaCy. We train the relation extraction model following the steps outlined in\\xa0spaCy’s documentation. We will compare the performance of the relation classifier using transformers and tok2vec algorithms. Finally, we will test the model on a job description found online.\\n\\xa0\\nRelation Classification:\\n\\xa0\\nAt its core, the relation extraction model is a classifier that predicts a relation\\xa0r\\xa0for a given pair of entity\\xa0{e1, e2}. In case of transformers, this classifier is added on top of the output hidden states. For more information about relation extraction, please read this excellent\\xa0article\\xa0outlining the theory of fine tuning transformer model for relation classification.\\nThe pre-trained model that we are going to fine-tune is the roberta-base model but you can use any pre-trained model available in huggingface library by simply inputting the name in the config file (see below).\\nIn this tutorial we are going to extract the relationship between the two entities {Experience, Skills} as\\xa0Experience_in\\xa0and between {Diploma, Diploma_major} as\\xa0Degree_in. The goal is to extract the years of experience required in a specific skills and the diploma major associated to the required diploma. You can of course, train your own relation classifier for your own use case such as finding the cause/effect of symptoms in health records or company acquisitions in financial documents. The possibilities are limitless…\\nIn this tutorial, we will only cover the entity relation extraction part. For fine-tuning BERT NER using spaCy 3, please refer to my\\xa0previous article.\\n\\xa0\\nData Annotation:\\n\\xa0\\nAs in my\\xa0previous article, we use\\xa0UBIAI\\xa0text annotation tool to perform the joint entity and relation annotation because of its versatile interface that allows us to switch between entity and relation annotation easily (see below):\\n\\n\\nUBIAI’s joint entity and relation annotation interface\\n\\n\\xa0\\nFor this tutorial, I have only annotated around 100 documents containing entities and relations. For production, we will certainly need more annotated data.\\n\\xa0\\nData Preparation:\\n\\xa0\\nBefore we train the model, we need to convert our annotated data to a binary spacy file. We first split the annotation generated from UBIAI into training/dev/test and save them separately. We modify the\\xa0code\\xa0that is provided in spaCy’s tutorial repo to create the binary file for our own annotation (conversion code).\\nWe repeat this step for the training, dev and test dataset to generate three binary spacy files (files available in github).\\n\\xa0\\nRelation Extraction Model Training:\\n\\xa0\\nFor training, we will provide the entities from our golden corpus and train the classifier on these entities.\\n\\nOpen a new Google Colab project and make sure to select GPU as hardware accelerator in the notebook settings. Make sure GPU is enabled by running: !nvidia-smi\\nInstall spacy-nightly:\\n\\n\\n!pip install -U spacy-nightly --pre\\n\\n\\n\\nInstall the wheel package and clone spacy’s relation extraction repo:\\n\\n\\n!pip install -U pip setuptools wheel\\r\\n!python -m spacy project clone tutorials/rel_component\\n\\n\\n\\nInstall transformer pipeline and spacy transformers library:\\n\\n\\n!python -m spacy download en_core_web_trf\\r\\n!pip install -U spacy transformers\\n\\n\\n\\nChange directory to rel_component folder: cd rel_component\\nCreate a folder with the name “data” inside rel_component and upload the training, dev and test binary files into it:\\n\\n\\n\\nTraining folder\\n\\n\\xa0\\n\\nOpen project.yml file and update the training, dev and test path:\\n\\n\\ntrain_file: \"data/relations_training.spacy\"dev_file: \"data/relations_dev.spacy\"test_file: \"data/relations_test.spacy\"\\n\\n\\n\\nYou can change the pre-trained transformer model (if you want to use a different language, for example), by going to the configs/rel_trf.cfg and entering the name of the model:\\n\\n\\n[components.transformer.model]@architectures = \"spacy-transformers.TransformerModel.v1\"name = \"roberta-base\" # Transformer model from huggingfacetokenizer_config = {\"use_fast\": true}\\n\\n\\n\\nBefore we start the training, we will decrease the max_length in configs/rel_trf.cfg from the default 100 token to 20 to increase the efficiency of our model. The max_length corresponds to the\\xa0maximum distance\\xa0between two entities above which they will not be considered for relation classification. As a result, two entities from the same document will be classified, as long as they are within a maximum distance (in number of tokens) of each other.\\n\\n\\n[components.relation_extractor.model.create_instance_tensor.get_instances]@misc = \"rel_instance_generator.v1\"max_length = 20\\n\\n\\n\\nWe are finally ready to train and evaluate the relation extraction model; just run the commands below:\\n\\n\\n!spacy project run train_gpu # command to train train transformers\\r\\n!spacy project run evaluate # command to evaluate on test dataset\\n\\n\\nYou should start seeing the P, R and F score start getting updated:\\n\\n\\nModel training in progress\\n\\n\\xa0\\nAfter the model is done training, the evaluation on the test data set will immediately start and display the predicted versus golden labels. The model will be saved in a folder named “training” along with the scores of our model.\\nTo train the non-transformer model tok2vec, run the following command instead:\\n\\n!spacy project run train_cpu # command to train train tok2vec\\r\\n!spacy project run evaluate\\n\\n\\nWe can compare the performance of the two models:\\n\\n# Transformer model\\r\\n\"performance\":{\"rel_micro_p\":0.8476190476,\"rel_micro_r\":0.9468085106,\"rel_micro_f\":0.8944723618,}\\r\\n# Tok2vec model\\r\\n  \"performance\":{\"rel_micro_p\":0.8604651163,\"rel_micro_r\":0.7872340426,\"rel_micro_f\":0.8222222222,}\\n\\n\\nThe transformer based model’s precision and recall scores are significantly better than tok2vec and demonstrate the usefulness of transformers when dealing with low amount of annotated data.\\n\\xa0\\nJoint Entity and Relation Extraction Pipeline:\\n\\xa0\\nAssuming that we have already trained a transformer NER model as in my\\xa0previous post, we will extract entities from a job description found online (that was not part of the training nor the dev set) and feed them to the relation extraction model to classify the relationship.\\n\\nInstall spacy transformers and transformer pipeline\\nLoad the NER model and extract entities:\\n\\n\\nimport spacynlp = spacy.load(\"NER Model Repo/model-best\")Text=[\\'\\'\\'2+ years of non-internship professional software development experience\\r\\nProgramming experience with at least one modern language such as Java, C++, or C# including object-oriented design.1+ years of experience contributing to the architecture and design (architecture, design patterns, reliability and scaling) of new and current systems.Bachelor / MS Degree in Computer Science. Preferably a PhD in data science.8+ years of professional experience in software development. 2+ years of experience in project management.Experience in mentoring junior software engineers to improve their skills, and make them more effective, product software engineers.Experience in data structures, algorithm design, complexity analysis, object-oriented design.3+ years experience in at least one modern programming language such as Java, Scala, Python, C++, C#Experience in professional software engineering practices & best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operationsExperience in communicating with users, other technical teams, and management to collect requirements, describe software product features, and technical designs.Experience with building complex software systems that have been successfully delivered to customersProven ability to take a project from scoping requirements through actual launch of the project, with experience in the subsequent operation of the system in production\\'\\'\\']for doc in nlp.pipe(text, disable=[\"tagger\"]):   print(f\"spans: {[(e.start, e.text, e.label_) for e in doc.ents]}\")\\n\\n\\n\\nWe print the extracted entities:\\n\\n\\nspans: [(0, \\'2+ years\\', \\'EXPERIENCE\\'), (7, \\'professional software development\\', \\'SKILLS\\'), (12, \\'Programming\\', \\'SKILLS\\'), (22, \\'Java\\', \\'SKILLS\\'), (24, \\'C++\\', \\'SKILLS\\'), (27, \\'C#\\', \\'SKILLS\\'), (30, \\'object-oriented design\\', \\'SKILLS\\'), (36, \\'1+ years\\', \\'EXPERIENCE\\'), (41, \\'contributing to the\\', \\'SKILLS\\'), (46, \\'design\\', \\'SKILLS\\'), (48, \\'architecture\\', \\'SKILLS\\'), (50, \\'design patterns\\', \\'SKILLS\\'), (55, \\'scaling\\', \\'SKILLS\\'), (60, \\'current systems\\', \\'SKILLS\\'), (64, \\'Bachelor\\', \\'DIPLOMA\\'), (68, \\'Computer Science\\', \\'DIPLOMA_MAJOR\\'), (75, \\'8+ years\\', \\'EXPERIENCE\\'), (82, \\'software development\\', \\'SKILLS\\'), (88, \\'mentoring junior software engineers\\', \\'SKILLS\\'), (103, \\'product software engineers\\', \\'SKILLS\\'), (110, \\'data structures\\', \\'SKILLS\\'), (113, \\'algorithm design\\', \\'SKILLS\\'), (116, \\'complexity analysis\\', \\'SKILLS\\'), (119, \\'object-oriented design\\', \\'SKILLS\\'), (135, \\'Java\\', \\'SKILLS\\'), (137, \\'Scala\\', \\'SKILLS\\'), (139, \\'Python\\', \\'SKILLS\\'), (141, \\'C++\\', \\'SKILLS\\'), (143, \\'C#\\', \\'SKILLS\\'), (148, \\'professional software engineering\\', \\'SKILLS\\'), (151, \\'practices\\', \\'SKILLS\\'), (153, \\'best practices\\', \\'SKILLS\\'), (158, \\'software development\\', \\'SKILLS\\'), (164, \\'coding\\', \\'SKILLS\\'), (167, \\'code reviews\\', \\'SKILLS\\'), (170, \\'source control management\\', \\'SKILLS\\'), (174, \\'build processes\\', \\'SKILLS\\'), (177, \\'testing\\', \\'SKILLS\\'), (180, \\'operations\\', \\'SKILLS\\'), (184, \\'communicating\\', \\'SKILLS\\'), (193, \\'management\\', \\'SKILLS\\'), (199, \\'software product\\', \\'SKILLS\\'), (204, \\'technical designs\\', \\'SKILLS\\'), (210, \\'building complex software systems\\', \\'SKILLS\\'), (229, \\'scoping requirements\\', \\'SKILLS\\')]\\n\\n\\nWe have successfully extracted all the skills, number of years of experience, diploma and diploma major from the text! Next we load the relation extraction model and classify the relationship between the entities.\\nNote: Make sure to copy rel_pipe and rel_model from the scripts folder into your main folder:\\n\\n\\nScripts folder\\n\\n\\xa0\\n\\nimport randomimport typerfrom pathlib import Pathimport spacyfrom spacy.tokens import DocBin, Docfrom spacy.training.example import Examplefrom rel_pipe import make_relation_extractor, score_relationsfrom rel_model import create_relation_model, create_classification_layer, create_instances, create_tensors# We load the relation extraction (REL) modelnlp2 = spacy.load(\"training/model-best\")# We take the entities generated from the NER pipeline and input them to the REL pipelinefor name, proc in nlp2.pipeline:\\r\\n          doc = proc(doc)# Here, we split the paragraph into sentences and apply the relation extraction for each pair of entities found in each sentence.for value, rel_dict in doc._.rel.items():\\r\\n        for sent in doc.sents:\\r\\n          for e in sent.ents:\\r\\n            for b in sent.ents:\\r\\n              if e.start == value[0] and b.start == value[1]:\\r\\n                if rel_dict[\\'EXPERIENCE_IN\\'] >=0.9 :\\r\\n                  print(f\" entities: {e.text, b.text} --> predicted relation: {rel_dict}\")\\n\\n\\nHere we display all the entities having a relationship\\xa0Experience_in\\xa0with confidence score higher than 90%:\\n\\n\"entities\":(\"2+ years\", \"professional software development\"\") --> predicted relation\":\\r\\n{\"DEGREE_IN\":1.2778723e-07,\"EXPERIENCE_IN\":0.9694631}\"entities\":\"(\"\"1+ years\", \"contributing to the\"\") -->\\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":1.4581254e-07,\"EXPERIENCE_IN\":0.9205434}\"entities\":\"(\"\"1+ years\",\"design\"\") --> \\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":1.8895419e-07,\"EXPERIENCE_IN\":0.94121873}\"entities\":\"(\"\"1+ years\",\"architecture\"\") --> \\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":1.9635708e-07,\"EXPERIENCE_IN\":0.9399484}\"entities\":\"(\"\"1+ years\",\"design patterns\"\") --> \\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":1.9823732e-07,\"EXPERIENCE_IN\":0.9423302}\"entities\":\"(\"\"1+ years\", \"scaling\"\") --> \\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":1.892173e-07,\"EXPERIENCE_IN\":0.96628445}entities: (\\'2+ years\\', \\'project management\\') --> \\r\\npredicted relation:\\r\\n{\\'DEGREE_IN\\': 5.175297e-07, \\'EXPERIENCE_IN\\': 0.9911635}\"entities\":\"(\"\"8+ years\",\"software development\"\") -->\\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":4.914319e-08,\"EXPERIENCE_IN\":0.994812}\"entities\":\"(\"\"3+ years\",\"Java\"\") -->\\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":9.288566e-08,\"EXPERIENCE_IN\":0.99975795}\"entities\":\"(\"\"3+ years\",\"Scala\"\") --> \\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":2.8477e-07,\"EXPERIENCE_IN\":0.99982494}\"entities\":\"(\"\"3+ years\",\"Python\"\") -->\\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":3.3149718e-07,\"EXPERIENCE_IN\":0.9998517}\"entities\":\"(\"\"3+ years\",\"C++\"\") -->\\r\\npredicted relation\":\\r\\n{\"DEGREE_IN\":2.2569053e-07,\"EXPERIENCE_IN\":0.99986637}\\n\\n\\nRemarkably, we were able to extract almost all the years of experience along with their respective skills correctly with with no false positives or negatives!\\nLet’s look at the entities having relationship\\xa0Degree_in:\\n\\nentities: (\\'Bachelor / MS\\', \\'Computer Science\\') -->\\r\\npredicted relation: \\r\\n{\\'DEGREE_IN\\': 0.9943974, \\'EXPERIENCE_IN\\':1.8361954e-09} entities: (\\'PhD\\', \\'data science\\') --> predicted relation: {\\'DEGREE_IN\\': 0.98883855, \\'EXPERIENCE_IN\\': 5.2092592e-09}\\n\\n\\nAgain, we successfully extracted all the relationships between diploma and diploma major!\\nThis again demonstrates how easy it is to fine tune transformer models to your own domain specific case with low amount of annotated data, whether it is for NER or relation extraction.\\nWith only a hundred of annotated documents, we were able to train a relation classifier with good performance. Furthermore, we can use this initial model to auto-annotate hundreds more of unlabeled data with minimal correction. This can significantly speed up the annotation process and improve model performance.\\n\\xa0\\nConclusion:\\n\\xa0\\nTransformers have truly transformed the domain of NLP and I am particularly excited about their application in information extraction. I would like to give a shoutout to explosion AI(spaCy developers) and huggingface for providing open source solutions that facilitates the adoption of transformers.\\nIf you need data annotation for your project, don’t hesitate to try out\\xa0UBIAI\\xa0annotation tool. We provide numerous programmable labeling solutions (such as ML auto-annotation, regular expressions, dictionaries, etc…) to minimize hand annotation.\\nLastly, checkout\\xa0this article\\xa0to learn how to leverage the NER and relation extraction models to build knowledge graphs and extract new insights.\\nIf you have any comment, please leave a note below or email at admin@ubiai.tools!\\n\\xa0\\nBio: Walid Amamou is the Founder of UBIAI, an annotation tool for NLP applications, and holds a PhD in Physics.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHow to Fine-Tune BERT Transformer with spaCy 3\\nBuilding a Knowledge Graph for Job Search Using BERT\\nFine-Tuning Transformer Model for Invoice Recognition',\n",
       " \"Sponsored Post.\\n\\n\\xa0\\nData Science is a hot topic these days! So it stands to reason that data science and analytics related careers are becoming more and more popular (So much so that Data Scientist was even called the hottest job of the 21st century!) There are many new trends in these fields as it is a growing technology and changing every year. These range from the addition of data science to various new industries like marketing, finance, etc. to new innovations such as Decision Intelligence, Data as a Service, etc.\\xa0\\nMany of these innovations and trends impact careers related to data science and analytics. This is especially true as data science is a multidisciplinary field that also interacts with various other technologies like Artificial Intelligence, Machine Learning, Deep Learning, the Internet of Things, etc. So let's check out what are the new data science and analytics career trends for 2021 that may also shape the career options in the future.\\n\\xa0\\nCommon Data Science and Analytics Career Trends\\n\\xa0\\nThere are many emerging trends that can shape your data science and analytics career in 2021 and in the future. Some of these are terms that you might have never heard before but may become common technologies in the future. After all, who had heard of Data Science 10 years ago?! There are also some of these trends that may die out in the future but that is not known right now.\\xa0 So let's check out these popular career trends that may become popular in 2021 here:\\n\\nRise of Decision Intelligence\\nThere is no doubt that data science and its related fields like artificial intelligence are becoming more and more popular over time. In fact, any big companies you can think of, whether it's Google, Facebook, Apple (Even Netflix!) all use these technologies heavily. But in the future, many more companies will adopt decision intelligence. This means using the data and advanced methods like machine learning to obtain insights and make decisions based on those insights. It is estimated that more than 33% of organizations will use decision intelligence, whether they are tech companies or not.\\nThe popularity of Data Stories\\nData dashboards are extremely popular within companies these days. They allow the decision-makers to understand the data with the help of different graphs and charts. But data stories are becoming even more popular than data dashboards and they may play a critical role in the future. These data stories are crafted as stories that take the viewers on a journey of the data and easily explain all the conclusions that are available from the data analysis. Unlike dashboards, the users don't need any detailed technical knowledge to understand these stories.\\xa0\\nIncrease of Cloud Services\\nMore and more organizations are adopting data science and artificial intelligence these days but this technology is expensive and the data is not suitable for traditional storage models. Hence the increase in cloud Services! Data as a Service (DaaS) is becoming more and more popular these days because companies can use cloud storage to obtain actionable insights from their data even if they don't have data infrastructure in house. This is making data science more democratic and providing opportunities to small companies who would never be able to afford the cost of data analytics otherwise.\\xa0\\nBranching of Data Analytics\\nWhen you think of data, what comes to mind? Mainly data is associated with rows and rows of numbers in the form of tables. But this is not the only type of data that companies possess. In fact, data also include audio, video, texts, customer feedback, etc. that is much more difficult to analyze. But soon enough, data analytics will branch from merely textual data to all the other forms of data. This is much more difficult to do but will also provide companies with a three-dimensional view of their data which they can analyze to obtain maximum profit. This is already used in Sentiment Analysis, which is a branch of artificial intelligence used to analyze the general sentiment from the data.\\n\\n\\xa0\\nDifferent Industries that are Using Data Science and Analytics\\n\\xa0\\nNow, these are some general trends in Data Science and Analytics that will be observed in 2021 and the coming years. However, there are many ways in which Data Science is changing the shape of different industries like marketing, finance, etc. Given enough time, Data Science might be a part of all the industries in the world, not just tech! So let's understand the role of this technology in different industries.\\n\\nMarketing and Retail\\nData Science already plays a huge role in marketing and retail. The most basic thing that almost every company uses is marketing and retail dashboards that visualize the data to see the hidden patterns and trends. Data Science and Machine Learning are also very useful for customer analysis wherein customer data is collected to understand the demographic of customers, the products that are popular with different types of customers, their likes and dislikes, and how to market a particular product to a section of customers.\\xa0\\nWeb and Social Media Analytics\\nThe internet and social media are a treasure trove of data! Google alone processes around 20 petabytes of data every day (That's approximately 1 followed by 15 zeros) Companies can use the web and social media analytics to obtain data about their customers and feedback on their performance which can be used to improve their bottom line. Sentiment analysis is a great example of this where companies can obtain their customer reviews from the internet or social media and to understanding the sentiment of the customers towards the company.\\xa0\\nSupply Chain and Logistics\\nSupply Chain and Logistics may sound boring but it is a critical aspect for companies. Can you imagine Amazon working if their system for transporting products from point A to point B crashed? No! Therefore, Data Science and Analytics is an extremely important part of the Supply Chain and Logistics that companies can use for Inventory Management, Procurement Analysis, Inventory Classification, etc. For example, data analytics algorithms can be used to understand the correlation between demand and supply for companies and create methods to increase sales by always ensuring in-demand items are available.\\nFinance and Risk Analytics\\nFinTech is a technology trend that is becoming more and more popular with time. It involved finance companies using cutting edge technology like Data Science and Artificial Intelligence to improve in areas like risk analytics, fraud detection, algorithmic trading, etc. Many big banks and finance companies use Data Science to analyze their large store of data to optimize their risk scoring models and decrease their risks. This data can include financial transactions, lending schemes, interest rates, customer interactions, customer trustworthiness, etc.\\n\\n\\xa0\\nHow to Leverage these Data Science and Analytics trends in 2021?\\n\\xa0\\nAs you have seen, there are various new Data Science and Analytics trends emerging in 2021. You can leverage them to learn more about Data Science and improve your career using the data science courses offered by GreatLearning in collaboration with The University of Texas. These programs will teach you right from the basics of Data Science such as Python, Business Statistica, and Data Visualization to various techniques of Machine Learning such as Supervised and Unsupervised algorithms. You will also get direct domain exposure by doing projects relating to Data Science in different industries like Marketing and Retail, Web and Social Media Analytics, Supply Chain and Logistics, and Finance and Risk Analytics. Some of these projects include Facebook Comments Prediction, Retail Sales Prediction, Insurance Data Visualization, etc.\",\n",
       " 'By Emma Ding, Data Scientist & Software Engineer at Airbnb.\\ncomments\\n\\xa0\\nNote: This is the second part of this article. You can read the first part here.\\n\\xa0\\nAnalyzing Tests Results\\n\\n\\nPhoto by\\xa0Scott Graham\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nNovelty and Primacy Effects\\n\\xa0\\nWhen there’s a change in the product, people react to it differently. Some are used\\xa0to the way a product works and are reluctant to change. This is called the\\xa0primacy effect\\xa0or change aversion. Others may welcome changes, and a new feature attracts them to use the product more. This is called the\\xa0novelty effect. However, both effects will not last long as people’s behavior will stabilize after a certain amount of time. If an A/B test has a larger or smaller initial effect, it’s probably due to novel or primacy effects. This is a common problem in practice, and many interview questions are about this topic. A sample interview question is:\\n\\n\\nWe ran an A/B test on a new feature and the test won, so we launched the change to all users. However, after launching the feature for a week, we found that the treatment effect quickly declined. What is happening?\\n\\n\\nThe answer is the novelty effect. Over time, as the novelty wears off, repeat usage will be decreased so we observe a declining treatment effect.\\nNow you understand both novelty and primacy effects,\\xa0how do we address the potential issues? This is a typical follow-up question during interviews.\\nOne way to deal with such effects is to completely rule out the possibility of those effects. We could run tests only on first-time users because the novelty effect and primacy effect obviously doesn’t affect such users. If we already have a test running and we want to analyze if there’s a novelty or primacy effect, we could 1) compare new users’ results in the control group to those in the treatment group to evaluate novelty effect 2) compare first-time users’ results with existing users’ results in the treatment group to get an actual estimate of the impact of the novelty or primacy effect.\\n\\xa0\\nMultiple testing problem\\n\\xa0\\nIn the simplest form of an A/B test, there are two variants: Control (A) and treatment (B). Sometimes, we run a test with multiple variants to see which one is the best amongst all the features. It can happen when we want to test multiple colors of a button or test different home pages. Then we’ll have more than one treatment group. In this case, we should not simply use the same significance level of 0.05 to decide whether the test is significant because we are dealing with more than 2 variants, and the probability of false discoveries increases. For example, if we have 3 treatment groups to compare with the control group, what is the chance of observing at least 1 false positive (assume our significance level is 0.05)?\\nWe could get the probability that there is no false positives (assuming the groups are independent),\\n\\xa0\\nPr(FP = 0) = 0.95 * 0.95 * 0.95 = 0.857\\n\\xa0\\nthen obtain the probability that there’s at least 1 false positive\\n\\xa0\\nPr(FP >= 1) = 1 — Pr(FP = 0) = 0.143\\n\\xa0\\nWith only 3 treatment groups (4 variants), the probability of a false positive (or Type I error) is over 14%. This is called the “multiple testing” problem. A sample interview question is\\n\\n\\nWe are running a test with 10 variants, trying different versions of our landing page. One treatment wins and the p-value is less than .05. Would you make the change?\\n\\n\\nThe answer is no because of the multiple testing problem. There are several ways to approach it. One commonly used method is\\xa0Bonferroni correction. It divides the significance level 0.05 by the number of tests. For the interview question, since we are measuring 10 tests, then the significance level for the test should be 0.05 divided by 10 which is 0.005. Basically, we only claim a test if significant if it shows a p-value of less than 0.005. The drawback of Bonferroni correction is that it tends to be too conservative.\\nAnother method is to control the\\xa0false discovery rate\\xa0(FDR):\\n\\xa0\\nFDR = E[# of false positive / # of rejections]\\n\\xa0\\nIt measures out of all of the rejections of the null hypothesis, that is, all the metrics that you declare to have a statistically significant difference. How many of them had a real difference as opposed to how many were false positives. This only makes sense if you have\\xa0a huge number of metrics, say hundreds. Suppose we have 200 metrics and cap FDR at 0.05. This means we’re okay with seeing false positives 5 of the time. We will observe at least 10 false positive in those 200 metrics every time.\\n\\xa0\\nMaking Decisions\\n\\n\\nPhoto by\\xa0You X Ventures\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nIdeally, we see practically significant treatment results, and we could consider launching the feature to all users. But sometimes, we see\\xa0contradicting results, such as one metric goes up while another one goes down, so we need to make a win-lost tradeoff. A sample interview question is:\\n\\n\\nAfter running a test, you see the desired metric, such as the click-through rate is going up while the number of impressions is decreasing. How would you make a decision?\\n\\n\\nIn reality, it can be very involved to make product launch decisions because various factors are taken into consideration, such as the complexity of implementation, project management effort, customer support cost, maintenance cost, opportunity cost, etc.\\nDuring interviews, we could provide a simplified version of the solution, focusing on the\\xa0current objective\\xa0of the experiment. Is it to maximize engagement, retention, revenue, or something else? Also, we want to quantify the negative impact, i.e. the negative shift in a non-goal metric, to help us make the decision. For instance, if revenue is the goal, we could choose it over maximizing engagement assuming the negative impact is acceptable.\\n\\xa0\\nResources\\n\\xa0\\nLastly, I’d like to recommend two resources for you to learn more about A/B testing.\\n\\nUdacity’s free A/B testing course\\xa0covers all the fundamentals of A/B testing.\\nTrustworthy online controlled experiments — A practical guide to A/B testing\\xa0by Ron Kohavi, Diane Tang, and Ya Xu. It has in-depth knowledge on how to run A/B tests in industry, the potential pitfalls, and solutions. It contains a lot of useful stuff, so I actually plan to write a post to summarize the content of the book. Stay tuned if you are interested!\\n\\n\\xa0\\nBio: Emma Ding is a Data Scientist & Software Engineer at Airbnb.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nA/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1\\n5 Things to Know About A/B Testing\\nHow to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals',\n",
       " 'By Nicole Janeway Bills, Data Scientist at Atlas Research.\\ncomments\\n\\n\\nPhoto by\\xa0Cameron Casey\\xa0from\\xa0Pexels\\n\\n\\xa0\\nThe distribution of data refers to the way the data is spread out. In this article, we’ll discuss the essential concepts related to the normal distribution:\\n\\nWays to measure normality\\nMethods to transform a dataset to fit the normal distribution\\nUse of the normal distribution to represent naturally occurring phenomena and offer statistical insights\\n\\n\\xa0\\nOverview\\n\\xa0\\nData distribution is of great importance in statistics because we are pretty much always sampling from a population where the full distribution is unknown. The distribution of our sample may put limitations on the statistical techniques available to us.\\n\\n\\nNormal distribution, where f(x) = probability density function, σ = standard deviation, and μ = mean\\n\\n\\xa0\\nThe normal distribution is a frequently observed continuous probability distribution. When a dataset conforms to the normal distribution, it is possible to utilize many handy techniques to explore the data:\\n\\nKnowledge of the percentage of data within each standard deviation\\nLinear least squares regression\\nInference based on the sample mean (e.g., t-test)\\n\\nIn some cases, it’s beneficial to transform a skewed dataset so that it conforms to the normal distribution, thereby unlocking the use of this set of statistical techniques. This is more likely to be relevant when your data is almost normally distributed except for some distortion. More on this in a moment.\\nNormal distributions have the following features:\\n\\nSymmetric bell shape\\nMean and median are equal (at the center of the distribution)\\n≈68% of the data falls within 1 standard deviation of the mean\\n≈95% of the data falls within 2 standard deviations of the mean\\n≈99.7% of the data falls within 3 standard deviations of the mean\\n\\n\\n\\nM.W. Toews\\xa0via\\xa0Wikipedia\\n\\n\\xa0\\nHere are some terms you should be familiar with relevant to a general overview of the normal distribution:\\n\\nNormal Distribution: a symmetric probability distribution that is frequently used to represent real-valued random variables; sometimes called the bell curve or Gaussian distribution\\nStandard Deviation: measure of the amount of variation or dispersion of a set of values; calculated as the square root of variance\\nVariance: the distance of each data point from the mean\\n\\n\\xa0\\nHow to use the Normal Distribution\\n\\xa0\\nIf your dataset does not conform to the normal distribution, here are some suggestions:\\n\\nCollect more data: a small sample size or lack of data quality could be distorting your otherwise normally distributed dataset. As is often the case in Data Science, the solution could be to collect more data.\\nReduce sources of variance:\\xa0reduction of outliers\\xa0could result in normally distributed data.\\nApply a power transform: for skewed data, you might choose to apply the\\xa0Box-Cox method, which refers to taking the square root and the log of the observation.\\n\\nIn the sections that follow, we’ll explore some measures of normality and how you would use them in a Data Science project.\\n\\xa0\\nSkewness\\n\\xa0\\nSkewness is a measure of asymmetry relative to the mean. Here’s a graph of a\\xa0left skewed distribution.\\n\\n\\nRodolfo Hermans\\xa0via\\xa0Wikipedia\\n\\n\\xa0\\n💡 I’ve always found this to be a bit counterintuitive, so it’s worth paying close attention here. This graph has\\xa0negative skewness. This means that the\\xa0tail\\xa0of the distribution is longer on the left. The counterintuitive bit (to me at least) is that most of the data points are clustered to the right. Do not be tempted to confuse with right or positive skewness, which would be represented by this graph’s mirror image.\\n\\xa0\\nHow to use Skewness\\n\\xa0\\nUnderstanding skewness is important because it is a key factor in model performance. To measure skewness, use\\xa0skew\\xa0from the\\xa0scipy.stats\\xa0module.\\n\\n\\nvia\\xa0SciPy\\n\\n\\xa0\\nThe skewness measure can clue us in to potential deviation in model performance across the feature values. A positively skewed feature, like the second array above, will enable better performance on lower values, given that we’re providing more data in that range (opposed to higher value outliers).\\n\\xa0\\nKurtosis\\n\\xa0\\nFrom Greek\\xa0kurtos, meaning curved, kurtosis is a measure of the tailedness of the distribution. Kurtosis is typically measured relative to 0, the kurtosis value of the normal distribution using\\xa0Fisher’s definition. A positive kurtosis value indicates “fatter” tails (i.e., a slimmer bell curve with more outliers).\\n\\n\\nThe Laplace Distribution has kurtosis > 0. via\\xa0John D. Cook Consulting.\\n\\n\\xa0\\nHow to use Kurtosis\\n\\xa0\\nUnderstanding kurtosis provides a lens to the presence of outliers in a dataset. To measure kurtosis, use\\xa0kurtosis\\xa0from the\\xa0scipy.stats\\xa0module.\\n\\n\\nvia\\xa0SciPy\\n\\n\\xa0\\nA negative kurtosis value indicates data that is more tightly grouped around the mean with fewer outliers.\\n\\xa0\\nA caveat about the Normal Distribution\\n\\xa0\\nYou may have heard that many naturally occurring datasets conform to the normal distribution. This claim has been made for everything from IQ to human heights.\\nWhile it’s true that the normal distribution is drawn from observations of nature and does occur frequently, we risk oversimplification by applying this assumption too liberally.\\n\\nThe normal model often doesn’t fit well in the extremes. It often underestimates the probability of rare events.\\xa0The Black Swan\\xa0by Nassim Nicholas Taleb gives numerous examples of rare events that were not\\xa0as\\xa0rare as a normal distribution would predict.\\n\\nWhy heights are not normally distributed\\nIn my previous post, I speculated on why heights are normally distributed, that is, why their statistical distribution...\\n\\xa0\\n\\xa0\\nSummary\\n\\xa0\\nIn this brief article on the normal distribution, we covered some fundamental concepts, how it is measured, and how it is used. Be careful not to overapply the normal distribution or you risk discounting the likelihood of outliers. Hope this article provided some insight on this commonly observed and highly useful statistical concept.\\n\\xa0\\nMore Articles You Might Enjoy\\n\\xa0\\nHow to Use Clustering to Create a Neighborhood Explorer Tool\\nA step-by-step walkthrough of using sklearn’s clustering algorithm to create an interactive dashboard for your city.\\n\\xa0\\nData Science for the New Normal — Lessons from a $1.4B Startup\\nPost-COVID, machine learning is increasingly crucial for business success.\\n\\xa0\\n10 Python Skills They Don’t Teach in Bootcamp\\nAscend to new heights in Data Science and Machine Learning with this list of coding tips.\\n\\xa0\\n\\xa0\\nBio: Nicole Janeway Bills\\xa0is Data Scientist with experience in commercial and federal consulting. She helps organizations leverage their top asset: a simple and robust Data Strategy. Sign up for more of her writing.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nOverview of data distributions\\nEssential Math for Data Science: The Poisson Distribution\\nLooking Normal(ly Distributed)',\n",
       " \"Advance your data science career with Northwestern\\r\\n      \\n\\r\\n\\tThe integration of data science and business strategy has created a demand for \\r\\n\\tprofessionals who can make sense of big data. Build the essential technical, \\r\\n\\tanalytical, and leadership skills needed for careers in today's data-driven world in \\r\\n\\tNorthwestern's Master of Science in Data Science program. \\r\\n      \\n\\r\\n\\tYou'll learn from an accomplished faculty of leading industry experts. You can \\r\\n\\tchoose from a wide range of specializations and electives to suit your goals as you \\r\\n\\tearn your Northwestern University master's degree entirely online.\\r\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t      Summer Application Deadline\\r\\n\\t    \\n\\r\\n\\t      April 15\\r\\n\\t    \\n\\n\\n\\n\\n\\n\\r\\n\\t      Fall Application Deadline\\r\\n\\t    \\n\\r\\n\\t      July 15\\r\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0Sports reporting is dense withstatistics. Northwestern has helpedme present difficult concepts so thatviewers aren't put off by the math. Inbroadcasting, the need tocommunicate data is critical.”  \\r\\n\\t    \\n\\r\\n\\t      Edward Egros \\r\\n\\t    \\n\\r\\n\\t      SPORTS REPORTER AND ANCHOR FORKDFW DALLAS \\r\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\tRelated Programs\\r\\n      \\n\\r\\n\\t  Master of Science in Information Systems\\n\\n\\r\\n\\t  Master of Science in Health Analytics\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\tNorthwestern UniversitySchool of Professional Studies\\n\\n\\r\\n\\t      339 East Chicago Avenue\\r\\n\\t      Chicago, Illinois 60611\\n\\n\\n312-503-2579\\n\\n\\n\\n\\r\\n\\t      CONNECT WITH US\",\n",
       " 'By Matthew Mayo, KDnuggets.\\ncomments\\n\\nPhoto by Chris Ried on Unsplash\\n\\xa0\\nThere are lots of different approaches to managing your own code, which will differ depending on your requirements, personality, technical know-how, role, and numerous other factors. While a highly-experienced developer may have an incredibly regimented method of organizing their code across multiple languages, projects, and use cases, a data analyst that rarely writes their own code may be much more ad hoc and lackadaisical out of lack of necessity. There really is no right or wrong, it\\'s simply a matter of what works — and is appropriate — for you.\\nTo be specific, what I\\'m referring to by \"managing code\" is how you organize, store, and recall different pieces of code you, yourself, have written and found useful as long-term additions to your programming toolbox. Programming is all about automating, and so if, as someone who writes code, you find that you are performing similar tasks repetitively, it\\'s only makes sense that you somehow automated the recalling of the code associated with that task. \\nThis is why you are already using third-party libraries. No need to re-implement a support vector machine code base from scratch every time you want to use it; instead, you make use of a library — perhaps Scikit-learn — and take advantage of the collective work of numerous folks perfecting some code over time.\\nExtending this idea to the personal programming sphere only makes sense. You may already be doing this (I hope you are), but if not, here are a few approaches that I have settled on for managing my own reusable Python code as a data scientist, presented from most to least general code use.\\n\\xa0\\nFull-blown libraries\\n\\xa0\\nThis is the most general approach there is, and what could be argued is the most \"professional\"; however, this alone does not make it the right choice all the time.\\nIf you find that you are using the same functionality in numerous use cases, and doing so regularly, this is the way to go. This also makes sense if the functionality you want to reuse is easily parameterizable; that is, the task can be handled over and over again by writing and calling a generalized function with variables you can define each time you call.\\nFor example, I often find that I want to find the nth occurrence of some substring in a string, and there is no Python standard library function for this. Thus, I have a simple piece of code that accepts a string, and substring, and the nth occurrence I am looking for as input, and returns the position in the string which this nth occurrence begins (lifted long ago from here).\\n\\ndef find_nth(haystack, needle, n):\\r\\n    start = haystack.find(needle)\\r\\n    while start >= 0 and n > 1:\\r\\n        start = haystack.find(needle, start+len(needle))\\r\\n        n -= 1\\r\\n    return start\\n\\n\\nSince I deal with a lot of text processing, I have collected this with numerous other text processing functions I regularly use and created a library that resides on my computer as any other Python library would, and am able to import this library as any other. The steps for creating the library are somewhat lengthy, though straightforward, and so I will not cover them here, but this article is one of very many that does so well.\\nSo now that I have a textproc library, I can import and use my find_nth function easily, and as often as I like, without having to copy and paste the function into each and every program I write that I use it in. \\n\\nfrom textproc import find_nth\\r\\n\\r\\nsegment = line[:find_nth(line, \\',\\', 4)].strip()\\n\\n\\nAlso, if I want to extend the library to add more functions, or change the existing find_nth code, I can do so in one spot and just re-import.\\n\\xa0\\nProject-specific shared scripts\\n\\xa0\\nPerhaps you don\\'t need a full-blown library, as the code you want to reuse doesn\\'t seem to have a use beyond the project you are currently working on, but you do need to reuse it within a specific project. In this case, you can place the functions together in one script, and simply import that script by name. It\\'s the poor woman\\'s library, but it is often just what is needed.\\nIn my graduate work I had to write a lot of code related to unsupervised learning, specifically k-means clustering. I wrote what became functions for  initializing centroids, computing distances between data points and centroids, recalculating centroids, etc., and doing numerous of these tasks using different algorithms. I soon found that keeping a separate script with copies of some of these algorithm functions was not optimal, and so moved them out into their own scripts to be imported. It worked nearly the same way as a library, but the process was path-specific, and was meant for this project only.\\nSoon I had scripts for different centroid initialization functions and distance computation functions, and for data-loading and processing functions as well. As this code all became more and more parameterized and generally useful, the code eventually made its way into a legitimate library.\\nThis seems to be how things usually progress, at least in my experience: You write a function in your script that you need to use now, and you use it. The project expands, or you move on to a similar project, and you realize that same function would be handy to have now. So that function gets dropped down to a script of its own, and you import it to use. If this usefulness continues beyond the near term, and you find that function having more general and longer term use, that function now gets added to an existing library, or is the basis for a new one. \\nHowever, another specific useful aspect of importing simple scripts is when using Jupyter notebooks. Given the ad hoc, exploratory, and experimental nature of much of what goes on in Jupyter notebooks, I\\'m not a fan of importing notebooks into other notebooks as modules. If I find that more than one notebook is making regular use of some code excerpt, that code goes gets dropped down into a script stored in the same folder which then gets imported into the notebook(s). This approach makes much more sense to me, and provides more stability by knowing that one notebook another notebook relies on is not being edited in a harmful manner.\\n\\xa0\\nTask-specific templates\\n\\xa0\\nI find that I often perform some of the same tasks over and over again which do not lend well to being parameterized, or are tasks which could be parameterized but with more effort than it is worth. In such cases, I employ code templating, or boiler-plating. This is much more the copying and pasting of code that I wanted to avoid in all cases at the outset of this article, but sometimes it\\'s the right choice.\\nFor example, I often need to \"listify,\" for lack of a better word, the contents of a Pandas DataFrame, and while writing a function that could determine the number of columns, could accept as input the columns to use, etc., often the output also needs to be tweaked, all of which points to  writing a function being far too time consuming.\\nIn this case, I just write up a script template that can easily be changed, and keep it handy in a folder of similar templates. Here\\'s an excerpt of listify_df, which goes from CSV file to Pandas DataFrame, to the desired HTML output.\\n\\nimport pandas as pd\\r\\n\\r\\n# Read CSV file into dataframe\\r\\ncsv_file = \\'data.csv\\'\\r\\ndf = pd.read_csv(csv_file)\\r\\n\\r\\n# Iterate over df, creating numbered list entries\\r\\ni = 1\\r\\nfor index, row in df.iterrows():\\r\\n\\tentry = \\'<b>\\' + str(i) + \\\\\\r\\n\\t\\t\\t\\'. <a href=\"\\' + \\\\\\r\\n\\t\\t\\trow[\\'url\\'] + \\\\\\r\\n\\t\\t\\t\\'\">\\' + \\\\\\r\\n\\t\\t\\trow[\\'title\\'] + \\\\\\r\\n\\t\\t\\t\\'</a> + \\\\\\r\\n\\t\\t\\t\\'\\\\n\\\\n<blockquote>\\\\n\\' + \\\\\\r\\n\\t\\t\\trow[\\'description\\'] + \\\\\\r\\n\\t\\t\\t\\'\\\\n</blockquote>\\\\n\\'\\r\\n\\ti += 1\\r\\n\\tprint(entry)\\n\\n\\nIn this case, clear filenames and folder organization are helpful for managing these often useful snippets.\\n\\xa0\\nShort one-liners and blocks\\n\\xa0\\nLastly, there are a lot of repetitive snippets you probably type regularly. So why do you do that?\\nYou should be making use of a text expansion tool to insert short \"phrases\" when needed. I use AutoKey to manage such short phrases, which are associated with trigger keywords and then inserted when those keywords are typed.\\nFor example, do you import a lot of the same libraries for all of your projects of a particular type? I do. For instance, you could set up all of the imports you would need for working on a particular task by typing, say, #nlpimport which, once typed, is recognized as a trigger keyword and is replaced with the following:\\n\\nimport sys, requests\\r\\n\\r\\nimport numpy as np\\r\\nimport pandas as pd\\r\\n\\r\\nimport texthero\\r\\nimport scattertext as st\\r\\n\\r\\nimport spacy\\r\\nfrom spacy.lang.en.stop_words import STOP_WORDS\\r\\n\\r\\nfrom datasets import load_metric, list_metrics\\r\\nfrom transformers import pipeline\\r\\nfrom fastapi import FastAPI\\n\\n\\nIt should be noted that some IDEs have these capabilities. I, myself, generally use glorified text editors to code, and so AutoKey is necessary (and incredibly useful) in my case. If you have an IDE which takes care of this, great. The point is, you shouldn\\'t need to be typing these over and over all the time.\\n\\xa0\\nThis has been an overview of approaching the management of your reusable Python code as a data scientist. I hope that you have found it useful.\\n\\xa0\\nRelated:\\n\\nMachine Learning Pipeline Optimization with TPOT\\nData Scientists, You Need to Know How to Code\\nData Scientist, Data Engineer & Other Data Careers, Explained',\n",
       " \"comments\\nBy Mohammad Khorasani, Data Scientist/Engineer Hybrid\\n\\n\\nPhoto by\\xa0Austin Distel\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nWhen was the last time you grappled with a PDF document? You probably don’t have to look too far back\\xa0to find the answer to that question. We deal with a multitude of documents on a daily basis in our lives and an overwhelmingly large number of those are indeed PDF documents. It is fair to claim that a lot of these documents are tediously repetitive and agonizingly painful to formulate. It is about time we consider leveraging the power of automation with Python to mechanize the tedious so that we may reallocate our precious time to more pressing tasks in our lives.\\nMind you, there is absolutely no need to be tech savvy and what we are going to do here should be trivial enough that our inner unsavvy laymen can tackle in short order. After reading this tutorial you will learn how to automatically generate PDF documents with your own data, charts and images all bundled together with a dazzling look and structure.\\nSpecifically, in this tutorial we will automate the following actions:\\n\\nCreating PDF documents\\nInserting images\\nInserting text and numbers\\nVisualizing data\\n\\n\\xa0\\nCreating PDF Documents\\n\\xa0\\nFor this tutorial, we will be using\\xa0FPDF\\xa0which is one of the most versatile and intuitive packages used to generate PDF’s in Python. Before we proceed any further, fire up Anaconda prompt or any other Python IDE of your choice and install FPDF:\\n\\npip install FPDF\\n\\n\\nThen import the stack of libraries that we’ll be using to render our document:\\n\\nimport numpy as np\\r\\nimport pandas as pd\\r\\nfrom fpdf import FPDF\\r\\nimport matplotlib as mpl\\r\\nimport matplotlib.pyplot as plt\\r\\nfrom matplotlib.ticker import ScalarFormatter\\n\\n\\nSubsequently proceed with creating the first page of your PDF document and set the font with its size and color:\\n\\npdf = FPDF(orientation = 'P', unit = 'mm', format = 'A4')\\r\\npdf.add_page()\\r\\npdf.set_font('helvetica', 'bold', 10)\\r\\npdf.set_text_color(255, 255, 255)\\n\\n\\nYou can however change the font whenever you like if you need to have various typefaces.\\n\\xa0\\nInserting Images\\n\\xa0\\nThe next logical step would be to give our document a background image that sets the structure for the rest of our page. For this tutorial I used Microsoft PowerPoint to render the formatting for my background image. I simply used text boxes and other visuals to create the desired format and once I was done I grouped everything together by selecting all the elements and hitting Ctrl-G. Finally I saved the grouped elements as a PNG image by right clicking on them and selecting ‘save as picture’.\\n\\n\\nBackground image. Image by author.\\n\\n\\xa0\\nAs you can see above, the background image sets the structure for our page and includes space for charts, figures, text and numbers that will be generated later on. The specific PowerPoint file used to generate this image can be downloaded\\xa0here.\\nSubsequently insert the background image into your PDF document and configure its position with the following:\\n\\npdf.image('C:/Users/.../image.png', x = 0, y = 0, w = 210, h = 297)\\n\\n\\nPlease note that you can insert as many images as you like by extending the method shown above.\\n\\xa0\\nInserting Text and Numbers\\n\\xa0\\nAdding text and numbers can be done in two ways. We can either specify the exact location we want to place the text:\\n\\npdf.text(x, y, txt)\\n\\n\\nOr alternatively, we can create a cell and then place the text within it. This method would be more suitable for aligning or centering variable or dynamic text:\\n\\npdf.set_xy(x, y)\\r\\npdf.cell(w, h, txt, border, align, fill) \\n\\n\\nPlease note that in the methods above:\\n\\n‘x’ and ‘y’ refer to the specified location on our page\\n‘w’ and ‘h’ refer to the dimensions of our cell\\n‘txt’ is the string or number that is to be displayed\\n‘border’ indicates if a line must be drawn around the cell (0: no, 1: yes or L: left, T: top, R: right, B: bottom)\\n‘align’ indicates the alignment of the text (L: left, C: center, R: right)\\n‘fill’ indicates whether the cell background should be filled or not (True, False).\\n\\n\\xa0\\nVisualizing Data\\n\\xa0\\nIn this part we are going to create a bar chart that will display a timeseries dataset of our credit, debit and balance values versus time. For this we will use Matplotlib to render our figures as such:\\n\\nIn the snippet above, credit, debit and balance are 2-dimensional lists with values for date and transaction amount respectively. Once the chart is generated and saved, it can then be inserted into our PDF document using the method shown in the previous sections.\\nSimilarly, we can generate donut charts with the following snippet of code:\\n\\nAnd once you are all done, you can wrap it up by generating the automated PDF document as such:\\n\\npdf.output('Automated PDF Report.pdf')\\n\\n\\n\\xa0\\nConclusion\\n\\xa0\\nAnd there you have it, your very own automatically generated PDF report! Now you’ve learnt how to create PDF documents, insert text and images into them and you’ve also learnt how to generate and embed charts and figures. But you are by no means limited to just that, in fact you can extend these techniques to include other visuals with multiple page documents too. The sky is truly the limit.\\n\\n\\nImage by author.\\n\\n\\xa0\\nIf you want to learn more about Python and data visualization, then feel free to check out the following (affiliate linked) courses:\\xa0Python for Everybody Specialization\\xa0and\\xa0Data Visualization with Python. In addition, feel free to explore more of my tutorials\\xa0here.\\n\\xa0\\nBio: Mohammad Khorasani is a hybrid of a data scientist and an engineer. Logistician. Candid. Realpolitik. Unlearning dogma one belief at a time. Read more of Mohammad's writings.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nData Scientists, You Need to Know How to Code\\n5 Tasks To Automate With Python\\nHow to Make Python Code Run Incredibly Fast\",\n",
       " 'Sponsored Post.\\n\\nAs the world evolves, there are significant observable effects of climate change on the environment. Glaciers are melting and shrinking, sea levels are rising, the oceans are warming, and droughts are becoming more frequent and severe.\\nThe University of Florida, IBM, and the Florida Tech Council are looking for people who want to be part of the solution.\\nAt this hackathon, we are asking you to combat climate change and create solutions that make a difference while utilizing IBM technologies, AI/ML, and/or responsible AI. Tackle sustainability issues and compete for a share of the $100k prize pool, including a $30k grand prize.\\nSee https://floridahackswithibm.bemyapp.com/\\nIn 6 weeks, September 13 through October 22, you and your team will build an application in one of 6 challenge areas that can change the world by using IBM AI and machine learning technologies and with support by leaders in technology and climate from the University of Florida, IBM, and other partner organizations.\\nAs a hackathon participant, you will be provided with $200 in cloud credits to access to all of IBM’s services on the cloud.\\nThis hackathon is free to join and open to US residents only, participants in all 50 states are encouraged to join. All experience levels are encouraged, no past experience with conservation issues is needed.\\nRegister today to join and access exclusive data sets, IBM mentors, technical webinars, Cloud Credits, and more. https://floridahackswithibm.bemyapp.com/',\n",
       " 'Sponsored Post.\\n\\n\\xa0\\nDeriving insights from and building models on sensitive data is challenging as it must be protected in a way that maximizes value, but minimizes risk. Those who achieve this balance between data utility and privacy will have a huge competitive advantage.\\nListen to this on-demand webinar to hear how data science company WorldQuant Predictive achieves this and scales quantitative research in a complex, sensitive data environment.',\n",
       " 'By Terence Shin, Data Scientist | MSc Analytics & MBA student.\\ncomments\\n\\n\\nPhoto by\\xa0Ryan Harvey\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nI just want to say that whether you choose data science or data engineering should ultimately depend on your interests and where your passion lies. However, if you’re sitting on the fence, unsure of which to choose because they are of equal interest, then keep reading!\\nData science has been a hot topic for a while, but a new king of the jungle has arrived — data engineers. In this article, I’m going to share with you several reasons why you might want to consider pursuing data engineering over data science.\\nNote that\\xa0this IS an opinionated article and take what you want from this. That being said, I hope you enjoy!\\n\\xa0\\n1. Data engineering is fundamentally more important than data science.\\n\\xa0\\nWe’ve all heard the saying “garbage in, garbage out”, but only now are companies starting to truly understand the meaning of this. Machine learning and deep learning can be powerful but only in very special circumstances. Aside from the fact that there needs to be a substantial amount of data and a practical use for ML and DL, companies need to satisfy the\\xa0data hierarchy of needs\\xa0from the bottom up.\\n\\n\\nImage created by Author\\n\\n\\xa0\\nThe same way that we have physical needs (i.e. food and water) before social needs (i.e. the need for relationships), companies need to satisfy several requirements which generally fall under the data engineering umbrella. Notice how data science, specifically machine learning and deep learning, are the very last things that matter.\\nSimply put, there can be no data science without data engineering.\\xa0Data engineering is the foundation for a successful data-driven company.\\n\\xa0\\n2. The demand for data engineers is growing… by a lot.\\n\\xa0\\nLike I previously said, companies are realizing the need for data engineers. Hence, there is a growing demand for data engineers at the moment and there’s proof.\\nAccording to\\xa0Interview Query’s Data Science Interview report, the number of data science interviews only grew by 10% from 2019 to 2020, while the\\xa0number of data engineering interviews grew by 40%\\xa0in the same period of time!\\nAs well,\\xa0Mihail Eric conducted an analysis on Y-Combinator job postings\\xa0and found that there were\\xa0roughly 70% more data engineering roles for hire than data scientist roles.\\nYou might be wondering, “sure the growth is much higher, but what about in terms of absolute numbers?”\\nI took the liberty of webscraping all Data Scientist and all Data Engineer job postings from Indeed, Monster, and SimplyHired, and I found that the number of job listings is about the same for both!\\nOverall there were 16577 data scientist job listings and 16262 data engineer job listings.\\n\\n\\nImage created by Author\\n\\n\\xa0\\n3. Data engineering skills are extremely useful as a data scientist.\\n\\xa0\\nIn more established companies, the work is typically segregated so that data scientists can focus on data science work and data engineers can focus on data engineering work.\\nBut this is generally not the case for most companies.\\xa0I would say that the majority of companies actually require their data scientists to know some amount of data engineering skills.\\n\\n\\nA lot of data scientists end up requiring data engineering skills.\\n\\n\\nIt’s also incredibly beneficial to know data engineering skills as a data scientist and I’ll give an example: If you’re a business analyst that doesn’t know SQL, you’ll have to ask a data analyst to query information every time you want to gather insights, which creates a bottleneck in your workflow. Similarly, if you’re a data scientist without the fundamental knowledge of a data engineer, there will certainly be times when you’ll have to rely on someone else to fix an ETL pipeline or clean data as opposed to doing it on your own.\\n\\xa0\\n4. Data science is easier to learn than data engineering.\\n\\xa0\\nIn my opinion, it’s much easier to learn data science as a data engineer than learn data engineering skills as a data scientist. Why? Well there’s simply more resources available for data science, and\\xa0there are a number of tools and libraries that have been built to make data science easier.\\nAnd so, if you’re starting out your career, I personally think it’s more worthwhile investing your time learning data engineering than data science because you have more time to invest. When you’re working a full time job and a couple of years into your career, you might find that you don’t have the capacity or energy to invest as much time in learning. So from that perspective, I think it’s better to learn the harder realm first.\\n\\xa0\\n5. It encompasses an untapped market of opportunities.\\n\\xa0\\nI’m not just talking about job opportunities, but opportunities to innovate and make data engineering easier with new tools and methodologies.\\nWhen data science was initially hyped up, people found several barriers to learning data science, like data modeling and model deployment. Later, companies like PyCaret and Gradio emerged to solve these problems.\\nCurrently, we are in that initial stage with data engineering, and I foresee a number of opportunities to make data engineering easier.\\n\\xa0\\nThanks for Reading!\\n\\xa0\\nWhile this is an opinionated article, I hope that this sheds a bit of light as to why you may want to be a data engineer. I want to reiterate that whether you choose data science or data engineering should ultimately depend on your interests and where your passion lies. As always, I wish you the best of luck in your endeavors!\\nNot sure what to read next? I’ve picked another article for you:\\n4 Reasons Why You Shouldn’t Be a Data Scientist\\nWhy a data science job might not be the right fit for you\\n\\xa0\\nand another one!\\nWant to Be a Data Scientist? Don’t Start With Machine Learning.\\nThe biggest misconception aspiring data scientists have\\n\\xa0\\nTerence Shin\\n\\nIf you enjoyed this,\\xa0follow me on Medium\\xa0for more\\nInterested in collaborating? Let’s connect on\\xa0LinkedIn\\nSign up for my email list\\xa0here!\\n\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nWant to Be a Data Scientist? Don’t Start With Machine Learning\\n7 Most Recommended Skills to Learn to be a Data Scientist\\nThe Most In-Demand Skills for Data Scientists in 2021',\n",
       " 'Sponsored Post.\\n\\nFor over a year, those of us who love to travel have missed that special feeling of going “airside” – when we pass security and have no more obstacles between us and our destination. Now you can go AIRSIDE with your data by discovering how to eliminate the obstacles to data security, privacy and governance in an age of ever-evolving regulations.\\n\\xa0\\nVirtual Summit Summary\\n\\xa0\\nAIRSIDE LIVE 2021 kicks off on June 3rd, 2021 and explores industry challenges and solutions related to big data, data management, security, privacy and governance. Presented by Okera, this inaugural virtual summit features thought leaders and practitioners from CNN, AWS, Microsoft, Gartner, AES, Credit Suisse, Inspire Brands, 4A’s, BigID, First San Francisco Partners, vArmour, The Bloor Group, and more. Free and open to everyone, AIRSIDE LIVE takes place from 7:30 am - 4:00 pm PT.\\xa0\\n\\xa0\\nThe Opening Keynote\\n\\xa0\\nMike Rogers, National Security Veteran, host of CNN’s Declassified, and former chairman of the U.S. House Permanent Select Committee on Intelligence, will deliver the opening keynote, Breached Data Is Fueling a Cyber War.\\xa0\\n\\xa0\\nThe Scope\\n\\xa0\\nAIRSIDE LIVE will explore how organizations can align the overlapping and often competing priorities of:\\n\\nData Management - DataOps, data lifecycle management, and managing cloud, multi-cloud and hybrid environments.\\nData Security - Data discovery, identity access control to data, and gaining full visibility into application and data access.\\nData Privacy and Data Governance - Ever-evolving compliance regulations, data governance, building data privacy as a feature and more.\\xa0\\n\\n\\xa0\\nThe Itinerary\\n\\xa0\\nTalks, industry-focused panels, technical sessions and real-world case studies will cover the following topics and more:\\n\\nBuilding a framework for a successful digital transformation\\nCreating a culture of data and ethics\\nThe battle between data agility, data security and data privacy\\xa0\\nThe evolution of advertising and privacy\\xa0\\nEnforcing zero-trust with identity access to data\\xa0\\nFuture-proofing regulatory gray space\\nEmbarking on the journey to trusted data quality\\xa0\\n\\n\\xa0\\nThe Crew (so far)\\n\\nAjita Abraham, Nitin Agrawal, Ed Amoroso, Sonali Bhavsar, Charles Blauner, Krish Das, David Fairman, John Fowlkes, Sarah Gadd, Arun Ganesan, Nick Halsey, Katie Hyman, Eric Kavanagh, Judy Ko, Nong Li, Dmytro Lugovyi, Sanjeev Mohan, Kelle O’Neal, Sean Otto, Alison Pepper, Stacey Rolland, Nimrod Vax, Marc Woolward, and others.\\n\\nAIRSIDE LIVE will also offer multiple networking events, including giveaways where attendees have the opportunity to win a two-night stay at the TWA Airport Hotel in New York, gift cards, and swag.\\nReady to help build a future where all data is protected, governed, and used responsibly? Go AIRSIDE LIVE with us for free on June 3rd! Register here.',\n",
       " \"comments\\nBy Tessa Xie, Senior Data Scientist at Cruise.\\n\\nPhoto by\\xa0bruce mars\\xa0on\\xa0Unsplash.\\nWhen I first made the transition from finance to data science, I felt like I was on the top of the world — I got a job in my dream field, my career track is set, I will just keep my head down and work hard, what could go wrong? Well, there were a couple of things… For the following year as a data scientist, there were several mistakes that I’m glad I caught myself making early in my career. This way, I had time to reflect and course-correct before it was too late. After a while, I realized that these mistakes are quite common. In fact, I have observed a lot of DS around me still making these mistakes, unaware that they can hurt their data career in the long run.\\nIf my 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist were what I learned from the best, the lessons in this article are those that I learned the hard way, and I hope I can help you avoid making the same mistakes.\\n\\xa0\\nMistake 1: Seeing yourself as a foot soldier instead of a thought partner\\n\\xa0\\nGrowing up, we have always been evaluated based on how well we can follow the rules and orders, especially in school. You will be the top student if you follow the textbook and practice exams and just put in the hard work. A lot of people seem to carry this “foot soldier” mindset into their working environment. In my opinion, this is the exact mindset that’s hindering a lot of data scientists from maximizing their impact and standing out from their peers. I have observed a lot of DS, especially junior ones, think they have nothing to contribute to the decision-making process and would rather retreat to the background and passively implement decisions made for them. This kicks off a vicious cycle — the less you contribute to those discussions, the less likely stakeholders will involve you in future meetings, and the less opportunity you will get to contribute in the future.\\nLet me give you a concrete example of the difference between a foot soldier and a thought partner in the case of model development. In the data collection and feature brainstorming meetings, the old me used to passively take notes on stakeholders’ suggestions so I can implement them “perfectly” later on. When someone proposed a feature that I knew we didn’t have data for, I would not say anything based on the assumption that they are more senior and they must know something that I overlooked. But guess what, they didn’t. I would later face the situation that 50% of the features we brainstormed would require additional data collection that would put our project deadline at risk. As a result, I often found myself in the undesirable position of the bad-news-bearing messenger in the end. Striving to be a thought partner nowadays, I involve myself early in the conversation and leverage my unique position as the person that’s closest to the data. This way, I can manage the expectations of stakeholders early on and make suggestions to help the team move forward.\\nHow to avoid this:\\n\\nMake sure you don’t hold back in meetings in which you can contribute something from the data perspective: are stakeholders’ definitions of metrics sufficient for what they want to measure? Is data available for measuring the set of metrics? If not, can we find proxies for the data we DO have?\\nImposter syndrome\\xa0is real, especially among junior DS. Make sure you are aware of this, and whenever you are questioning whether you should say something that “others might have already thought of” or ask a “stupid clarifying question,” YOU SHOULD.\\nMaintain a level of curiosity about what other people are working on. There are a lot of occasions where I found I could add value by noticing gaps other people may have overlooked due to their lack of understanding of the company’s data.\\n\\n\\xa0\\nMistake 2: Pigeonhole yourself into a specific area of data science\\n\\xa0\\nDo I want to be a data engineer or a data scientist? Do I want to work with marketing & sales data or do the geospatial analysis? You may have noticed that I have been using the term DS so far in this article as a general term for a lot of\\xa0data-related career paths\\xa0(e.g., data engineer, data scientist, data analyst, etc.). That’s because the lines are so blurred between these titles in the data world these days, especially in smaller companies. I have observed a lot of data scientists see themselves as ONLY data scientists building models and don’t pay attention to any business aspects or data engineers who only focus on data pipelining and don’t want to know anything about the modeling that’s going on in the company.\\nThe best data talents are the ones who can wear multiple hats or are at least able to understand the processes of other data roles. This comes in especially handy if you want to work in an early stage or growth stage startup, where functions might not be as specialized yet, and you are expected to be flexible and cover a variety of data-related responsibilities. Even if you are in a clearly defined job profile, as you get more experience over time, you might discover that you are interested in transitioning into a different type of data role. This pivot will be much easier if you did not pigeonhole yourself and your skillset into the narrow focus of one specific role.\\nHow to avoid this:\\n\\nAgain, be curious about the projects other data roles are working on. Schedule periodic meetings with colleagues to talk to each other about interesting projects or have different data teams share their work/projects with each other periodically.\\nIf you can’t get exposure to other data roles at work, try to keep up/practice the data skills you don’t use during your free time. For example, if you are a data analyst and haven’t touched modeling in a while, consider practicing the skills through outside projects like a Kaggle competition.\\n\\n\\xa0\\nMistake 3: Not keeping up with the development in the field\\n\\xa0\\nComplacency Kills\\nEvery soldier knows this, and every DS should, too. Being complacent about your data skills and not putting in the time to learn new ones is a common mistake. Doing this in the data field is more dangerous than in some other areas because data science is a field that’s relatively new and is still experiencing drastic changes and developments. There are constantly new algorithms, new tools, and even new programming languages being introduced.\\nIf you don’t want to be that one data scientist who still only knows how to use STATA in 2021 (he exists, I worked with him), then you need to keep up with the developments in the field.\\n\\ufeff\\nDon’t let this be you (GIF\\xa0by GIPHY).\\nHow to avoid this:\\n\\nSign up for online classes to learn about new concepts and algorithms or to brush up on the ones you already know but haven’t used in a while on the job. The ability to learn is a muscle everyone should keep practicing, and being a life-long learner is probably the best gift you can give to yourself.\\nSign up for a DS newsletter or follow a DS blogger/publication on Medium and develop a habit of following the DS “news.”\\n\\n\\xa0\\nMistake 4: Overflexing your analytical muscle\\n\\xa0\\nIf all you have is a hammer, everything looks like a nail. Don’t be that DS who tries to use ML on everything. When I first entered the world of data science, I was so excited about all the fancy models I learned in school and couldn’t wait to try all of them on real-world problems. But the real world is different from academic research, and the\\xa080/20 rule\\xa0is always at play.\\nIn my previous article about “5 Lessons McKinsey Taught Me,” I wrote about how\\xa0business impact and interpretability sometimes are more important than the extra several percentage points of your model’s accuracy. Sometimes maybe an assumptions-driven Excel model makes more sense than a multi-layered neural net. In those cases, don’t over-flex your analytical muscle and make your approach overkill. Instead, flex your business muscle and be the DS who also has business acumen.\\nHow to avoid this:\\n\\nHave a full range of analytical skills/tools in your armory, from simple Excel to advanced ML modeling skills, so you can always assess which tool is the best to use in the situation and not bring a gun to a knife fight.\\nUnderstand the business needs before delving into the analysis. Sometimes stakeholders would request an ML model because it’s a popular concept, and they have unrealistic expectations about what ML models can do. It’s your job as a DS to manage the expectations and help them find better and simpler ways to achieve their goals. Remember?\\xa0Be a thought partner, not a foot soldier.\\n\\n\\xa0\\nMistake 5: Think building a data culture is someone else’s job\\n\\xa0\\nIn my article “6 Essential Steps to Building a Great Data Culture,” I wrote about how the lives of data scientists can be horrible and unproductive if the company doesn’t have a great data culture. In fact, I have heard a lot of DS complaining about unproductive ad hoc data requests that should be easily handled by stakeholders in a self-sufficient fashion (for example, changing an aggregation from monthly to daily in Looker, which literally consists of two clicks). Don’t think changing that culture is someone else’s job. If you want to see changes, make them. After all, who is better positioned to build the data culture and educate stakeholders about data than data scientists themselves? Helping to build up the data culture in the company will make your life a lot easier down the road as well as your stakeholders.\\nHow to avoid this:\\n\\nMake it your responsibility to conduct training for the non-analytical stakeholders and develop self-serve resources.\\nMake sure you start practicing what you are preaching, start linking queries to slides, link data sources of truth to documents, and start documenting your code and databases. You can’t build up a data culture overnight, so it definitely takes patience.\\n\\nI do want to point out that it’s OKAY to make mistakes in your career. The most important thing is to learn from those mistakes and to avoid them in the future. Or even better, write them down to help others avoid making the same mistakes.\\n\\xa0\\nOriginal. Reposted with permission.\\n\\xa0\\nBio:\\xa0Tessa Xie\\xa0is an experienced Advanced Analytics Consultant skilled in data science, SQL, R, Python, Consumer Research and Economic Research with a strong engineering background following a Master's degree focused in Financial Engineering from MIT.\\nRelated:\\n\\nHow a Single Mistake Wasted 3 Years of My Data Science Journey\\nData Scientists think data is their #1 problem. Here’s why they’re wrong.\\nLearning from 3 big Data Science career mistakes\",\n",
       " 'comments\\nBy Antoni Baum, Core Contributor to PyCaret and Contributor to Ray Tune\\n\\n\\n\\nHere’s a situation every\\xa0PyCaret\\xa0user is familiar with: after selecting a promising model or two from\\xa0compare_models(), it’s time to tune its hyperparameters to squeeze out all of the model’s potential with\\xa0tune_model().\\n\\nfrom pycaret.datasets import get_data\\r\\nfrom pycaret.classification import *\\r\\n\\r\\ndata = get_data(\"juice\")\\r\\n\\r\\nexp = setup(\\r\\n    data,\\r\\n    target = \"Purchase\",\\r\\n)\\r\\nbest_model = compare_models()\\r\\ntuned_best_model = tune_model(best_model)\\n\\n\\n(If you would like to learn more about PyCaret — an open-source, low-code machine learning library in Python,\\xa0this guide\\xa0is a good place to start.)\\nBy default,\\xa0tune_model()\\xa0uses the tried and tested\\xa0RandomizedSearchCV\\xa0from scikit-learn. However, not everyone knows about the various advanced options\\xa0tune_model()provides.\\nIn this post, I will show you how easy it is to use other state-of-the-art algorithms with PyCaret thanks to\\xa0tune-sklearn, a drop-in replacement for scikit-learn’s model selection module with cutting edge hyperparameter tuning techniques. I’ll also report results from a series of benchmarks, showing how tune-sklearn is able to easily improve classification model performance.\\n\\xa0\\nRandom search vs Bayesian optimization\\n\\xa0\\nHyperparameter optimization algorithms can vary greatly in efficiency.\\nRandom search has been a machine learning staple and for a good reason: it’s easy to implement, understand and gives good results in reasonable time. However, as the name implies, it is completely random — a lot of time can be spent on evaluating bad configurations. Considering that the amount of iterations is limited, it’d make sense for the optimization algorithm to focus on configurations that it considers promising by taking into account already evaluated configurations.\\nThat is, in essence, the idea of Bayesian optimization (BO). BO algorithms keep track of all evaluations and use the data to construct a “surrogate probability model”, which can be evaluated a lot faster than a ML model. The more configurations have been evaluated, the more informed the algorithm becomes, and the closer the surrogate model becomes to the actual objective function. That way the algorithm can make an informed choice about which configurations to evaluate next, instead of merely sampling random ones. If you would like to learn more about Bayesian optimization, check out this\\xa0excellent article by Will Koehrsen.\\nFortunately, PyCaret has built-in\\xa0wrappers\\xa0for several optimization libraries, and in this article, we’ll be focusing on\\xa0tune-sklearn.\\n\\xa0\\ntune-sklearn in PyCaret\\n\\xa0\\ntune-sklearn\\xa0is a drop-in replacement for scikit-learn’s model selection module. tune-sklearn provides a scikit-learn based unified API that gives you access to various popular state of the art optimization algorithms and libraries, including Optuna and scikit-optimize. This unified API allows you to toggle between many different hyperparameter optimization libraries with just a single parameter.\\ntune-sklearn is powered by\\xa0Ray Tune, a Python library for experiment execution and hyperparameter tuning at any scale. This means that you can scale out your tuning across multiple machines without changing your code.\\nTo make things even simpler, as of version 2.2.0, tune-sklearn has been integrated into PyCaret. You can simply do\\xa0pip install \"pycaret[full]\"\\xa0and all of the optional dependencies will be taken care of.\\n\\n\\nHow it all works together\\n\\n\\n\\n!pip install \"pycaret[full]\"\\r\\n\\r\\nfrom pycaret.datasets import get_data\\r\\nfrom pycaret.classification import *\\r\\n\\r\\ndata = get_data(\"juice\")\\r\\n\\r\\nexp = setup(\\r\\n    data,\\r\\n    target = \"Purchase\",\\r\\n)\\r\\nbest_model = compare_models()\\r\\ntuned_best_model_hyperopt = tune_model(\\r\\n    best_model,\\r\\n    search_library=\"tune-sklearn\",\\r\\n    search_algorithm=\"hyperopt\",\\r\\n    n_iter=20\\r\\n)\\r\\ntuned_best_model_optuna = tune_model(\\r\\n    best_model,\\r\\n    search_library=\"tune-sklearn\",\\r\\n    search_algorithm=\"optuna\",\\r\\n    n_iter=20\\r\\n)\\n\\n\\nJust by adding two arguments to\\xa0tune_model()you can switch from random search to tune-sklearn powered Bayesian optimization through\\xa0Hyperopt\\xa0or\\xa0Optuna. Remember that PyCaret has built-in search spaces for all of the included models, but you can always pass your own, if you wish.\\nBut how well do they compare to random search?\\n\\xa0\\nA simple experiment\\n\\xa0\\nIn order to see how Bayesian optimization stacks up against random search, I have conducted a very simple experiment. Using the\\xa0Kaggle House Prices dataset, I have created two popular regression models using PyCaret —\\xa0Random Forest\\xa0and\\xa0Elastic Net. Then, I tuned both of them using scikit-learn’s Random Search and tune-sklearn’s Hyperopt and Optuna Searchers (20 iterations for all, minimizing RMSLE). The process was repeated three times with different seeds and the results averaged. Below is an abridged version of the code — you can find the full code\\xa0here.\\n\\nfrom pycaret.datasets import get_data\\r\\nfrom pycaret.regression import *\\r\\n\\r\\ndata = get_data(\"house\")\\r\\nexp = setup(\\r\\n    data,\\r\\n    target = \"SalePrice\",\\r\\n    test_data=data, # so that the entire dataset is used for cross validation - do not normally do this!\\r\\n    session_id=42,\\r\\n    fold=5\\r\\n)\\r\\nrf = create_model(\"rf\")\\r\\nen = create_model(\"en\")\\r\\n\\r\\ntune_model(rf, search_library = \"scikit-learn\", optimize=\"RMSLE\", n_iter=20)\\r\\ntune_model(rf, search_library = \"tune-sklearn\", search_algorithm=\"hyperopt\", n_iter=20)\\r\\ntune_model(rf, search_library = \"tune-sklearn\", search_algorithm=\"optuna\", optimize=\"RMSLE\", n_iter=20)\\r\\n\\r\\ntune_model(en, search_library = \"scikit-learn\", optimize=\"RMSLE\", n_iter=20)\\r\\ntune_model(en, search_library = \"tune-sklearn\", search_algorithm=\"hyperopt\", n_iter=20)\\r\\ntune_model(en, search_library = \"tune-sklearn\", search_algorithm=\"optuna\", optimize=\"RMSLE\", n_iter=20)\\n\\n\\nIsn’t it great how easy PyCaret makes things? Anyway, here are the RMSLE scores I have obtained on my machine:\\n\\n\\nExperiment’s RMSLE scores\\n\\n\\nAnd to put it into perspective, here’s the percentage improvement over random search:\\n\\n\\nPercentage improvement over random search\\n\\n\\nAll of that using the same number of iterations in comparable time. Remember that given the stochastic nature of the process, your mileage may vary. If your improvement is not noticeable, try increasing the number of iterations (n_iter)from the default 10. 20–30 is usually a sensible choice.\\nWhat’s great about Ray is that you can effortlessly scale beyond a single machine to a cluster of tens, hundreds or more nodes. While PyCaret doesn’t support full Ray integration yet, it is possible to initialize a\\xa0Ray cluster\\xa0before tuning — and tune-sklearn will automatically use it.\\n\\nexp = setup(\\r\\n    data,\\r\\n    target = \"SalePrice\",\\r\\n    session_id=42,\\r\\n    fold=5\\r\\n)\\r\\n\\r\\nrf = create_model(\"rf\")\\r\\n\\r\\ntune_model(rf, search_library = \"tune-sklearn\", search_algorithm=\"optuna\", optimize=\"RMSLE\", n_iter=20) # Will run on Ray cluster!\\n\\n\\nProvided all of the necessary configuration is in place (\\xa0RAY_ADDRESSenvironment variable), nothing more is needed in order to leverage the power of Ray’s distributed computing for hyperparameter tuning. Because hyperparameter optimization is usually the most performance-intensive part of creating an ML model, distributed tuning with Ray can save you a lot of time.\\n\\xa0\\nConclusion\\n\\xa0\\nIn order to speed up hyperparameter optimization in PyCaret, all you need to do is install the required libraries and change two arguments in\\xa0tune_model()\\xa0— and thanks to built-in tune-sklearn support, you can easily leverage Ray’s distributed computing to scale up beyond your local machine.\\nMake sure to check out the documentation for\\xa0PyCaret,\\xa0Ray Tune\\xa0and\\xa0tune-sklearn\\xa0as well as the GitHub repositories of\\xa0PyCaret\\xa0and\\xa0tune-sklearn. Finally, if you have any questions or want to connect with the community, join\\xa0PyCaret’s Slack\\xa0and\\xa0Ray’s Discourse.\\nThanks to Richard Liaw and Moez Ali for proofreading and advice.\\n\\xa0\\nBio: Antoni Baum is a Computer Science and Econometrics MSc student, as well as a core contributor to PyCaret and contributor to Ray Tune.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nAlgorithms for Advanced Hyper-Parameter Optimization/Tuning\\n5 Tools for Effortless Data Science\\n5 Things You Are Doing Wrong in PyCaret',\n",
       " 'Sponsored Post.\\n\\nGoogle recently launched a Data Analytics Professional Certificate on Coursera. This program is designed to prepare learners for an entry-level role in under six months and offers a stepping stone to well-paying careers. This Certificate doesn’t have any prerequisites and is great for anyone, regardless of background or experience. The program is completely online, self-paced, and costs $39 per month.\\xa0\\nThis Professional Certificate also includes resources to help learners enhance their resumes and prepare for interviews. Upon completion, learners can share their information with 100+ partners committed to sourcing talent from Google certificate programs, such as Deloitte, Anthem, Verizon, Snap Inc, and other top brands. Google is also launching apprenticeships in the certificate field of data analytics and offering 200,000 scholarships for their new programs to learners across the U.S., Europe, Middle East, and Africa. \\nThe Google Data Analytics Professional Certificate is a seven-course certificate exploring analytical skills, concepts, and tools used in many introductory data analytics roles – including SQL, Tableau, RStudio, and Kaggle. With nearly 15,000 open entry-level data analytics roles in the U.S. and an annual median entry-level salary of over $63,000, data analytics is a growing field filled with opportunities.\\nInterested in preparing for a new career in a high-growth field? Check out the Google Data Analytics Professional Certificate.',\n",
       " 'By Nagesh Singh Chauhan, Data Science Enthusiast.\\ncomments\\n\\ncredit\\n\\xa0\\nTransformers\\n\\xa0\\nThe Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. The Transformer was proposed in the paper Attention Is All You Need. It is recommended reading for anyone interested in NLP.\\nNLP-focused startup Hugging Face recently released a major update to their popular “PyTorch Transformers” library, which establishes compatibility between PyTorch and TensorFlow 2.0, enabling users to easily move from one framework to another during the life of a model for training and evaluation purposes.\\nThe Transformers package contains over 30 pre-trained models and 100 languages, along with eight major architectures for natural language understanding (NLU) and natural language generation (NLG):\\n\\nBERT (from Google);\\nGPT (from OpenAI);\\nGPT-2 (from OpenAI);\\nTransformer-XL (from Google/CMU);\\nXLNet (from Google/CMU);\\nXLM (from Facebook);\\nRoBERTa (from Facebook);\\nDistilBERT (from Hugging Face).\\n\\nThe Transformers library no longer requires PyTorch to load models, is capable of training SOTA models in only three lines of code, and can pre-process a dataset with less than 10 lines of code. Sharing trained models also lowers computation costs and carbon emissions.\\nI am assuming that you are aware of Transformers and its attention mechanism. The prime aim of this article is to show how to use Hugging Face’s transformer library with TF 2.0,\\nInstallation (You don\\'t explicitly need PyTorch)\\n\\n!pip install transformers \\r\\n\\n\\n\\xa0\\nGetting started on a task with a pipeline\\n\\xa0\\nThe easiest way to use a pre-trained model on a given task is to use pipeline(). 🤗 Transformers provides the following tasks out of the box:\\n\\nSentiment analysis: is a text positive or negative?\\nText generation (in English): provide a prompt, and the model will generate what follows.\\nName entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\\nQuestion answering: provide the model with some context and a question, extract the answer from the context.\\nFilling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.\\nSummarization: generate a summary of a long text.\\nLanguage Translation: translate a text into another language.\\nFeature extraction: return a tensor representation of the text.\\n\\nPipelines encapsulate the overall process of every NLP process:\\n\\nTokenization: Split the initial input into multiple sub-entities with … properties (i.e., tokens).\\nInference: Maps every token into a more meaningful representation.\\nDecoding: Use the above representation to generate and/or extract the final output for the underlying task.\\n\\nThe overall API is exposed to the end-user through the pipeline() method with the following structurs.\\n\\xa0\\nGPT-2\\n\\xa0\\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text.\\nSince the goal of GPT-2 is to make predictions, only the decoder mechanism is used. So GPT-2 is just transformer decoders stacked above each other.\\nGPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where the model is comfortable with large input and can generate lengthy output.\\nText generation\\n\\nfrom transformers import pipeline, set_seed\\r\\ngenerator = pipeline(\\'text-generation\\', model=\\'gpt2\\')\\r\\ngenerator(\"Hello, I like to play cricket,\", max_length=60, num_return_sequences=7)\\r\\n\\n\\n\\xa0\\nOutput:\\n\\n[{\\'generated_text\\': \"Hello, I like to play cricket, but I\\'d rather play football! So I\\'ve decided to create a new game, The Super Bombers, on the Xbox One.\\\\n\\\\nAnd as a reward, you will hear the official announcement for this game!\\\\n\\\\nHere is what you can expect\"},\\r\\n {\\'generated_text\\': \\'Hello, I like to play cricket, but sometimes it\\\\\\'s like being a bad sportsman,\" he says. \"Sometimes I try and make cricket harder but sometimes I am just very happy and I always try to enjoy my cricket.\"\\\\n\\\\nWhile at Middlesex, Hautek was inspired by the\\'},\\r\\n {\\'generated_text\\': \\'Hello, I like to play cricket, but I can\\\\\\'t really understand what \"good\" and \"bad\" is. Do you have a definition of \"good\" and \"bad\"?\\\\n\\\\nYes, I think so. I mean, people who are well trained probably don\\\\\\'t have that problem with\\'},\\r\\n {\\'generated_text\\': \\'Hello, I like to play cricket, I play the game of cricket.\" The next day, he joined the family tour with his friends. It might have been a brief break for them both that he was so involved. A few days later they met at his cricket training centre, at which the pair\\'},\\r\\n {\\'generated_text\\': \"Hello, I like to play cricket, so I wanted to play English cricket... so I called up a friend of mine and, I remember, it wasn\\'t really English, but it actually has lots of good stuff about it.\\\\n\\\\n\\\\nDUPY: It\\'s very interesting, especially as you\"},\\r\\n {\\'generated_text\\': \\'Hello, I like to play cricket, but I don\\\\\\'t really like playing cricket in a stadium full of tourists; there\\\\\\'s not really any point in playing. We played that game almost three years ago for cricket.\\\\n\\\\n\"My favourite time about being here was last year in England. It was\\'},\\r\\n {\\'generated_text\\': \\'Hello, I like to play cricket, too. The kids of the city always play a good match, I mean, the cricket team is always very young.\"\\'}]\\r\\n\\n\\n\\xa0\\n\\ngenerator(\"The Indian man worked as a\", max_length=10, num_return_sequences=5)\\r\\n\\n\\n\\xa0\\nOutput:\\n\\n[{\\'generated_text\\': \\'The Indian man worked as a waiter in Delhi.\\'},\\r\\n {\\'generated_text\\': \\'The Indian man worked as a security guard for the\\'},\\r\\n {\\'generated_text\\': \\'The Indian man worked as a waiter for around ten\\'},\\r\\n {\\'generated_text\\': \\'The Indian man worked as a waiter on a Sunday\\'},\\r\\n {\\'generated_text\\': \\'The Indian man worked as a barista in the\\'}\\r\\n\\n\\n\\xa0\\nSentiment analysis\\n\\n# Allocate a pipeline for sentiment-analysis\\r\\nclassifier = pipeline(\\'sentiment-analysis\\')\\r\\nclassifier(\\'The secret of getting ahead is getting started.\\')\\r\\n\\n\\n\\xa0\\nOutput:\\n\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9970657229423523}]\\r\\n\\n\\n\\xa0\\nQuestion Answering\\n\\n# Allocate a pipeline for question-answering\\r\\nquestion_answerer = pipeline(\\'question-answering\\')\\r\\nquestion_answerer({\\r\\n    \\'question\\': \\'What is Newton\\'s third law of motion?\\',\\r\\n    \\'context\\': \\'Newton\\'s third law of motion states that, \"For every action there is equal and opposite reaction\"\\'})\\r\\n\\n\\n\\xa0\\nOutput:\\n\\n{\\'score\\': 0.6062518954277039,\\r\\n \\'start\\': 42,\\r\\n \\'end\\': 96,\\r\\n \\'answer\\': \\'\"For every action there is equal and opposite reaction\"\\'}\\r\\n\\n\\n\\xa0\\n\\nnlp = pipeline(\"question-answering\")\\r\\n\\r\\ncontext = r\"\"\"\\r\\nMicrosoft was founded by Bill Gates and Paul Allen in 1975.\\r\\nThe property of being prime (or not) is called primality.\\r\\nA simple but slow method of verifying the primality of a given number n is known as trial division.\\r\\nIt consists of testing whether n is a multiple of any integer between 2 and itself.\\r\\nAlgorithms much more efficient than trial division have been devised to test the primality of large numbers.\\r\\nThese include the Miller-Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical.\\r\\nParticularly fast methods are available for numbers of special forms, such as Mersenne numbers.\\r\\nAs of January 2016, the largest known prime number has 22,338,618 decimal digits.\\r\\n\"\"\"\\r\\n\\r\\n#Question 1\\r\\nresult = nlp(question=\"What is a simple method to verify primality?\", context=context)\\r\\n\\r\\nprint(f\"Answer 1: \\'{result[\\'answer\\']}\\'\")\\r\\n\\r\\n#Question 2\\r\\nresult = nlp(question=\"When did Bill gates founded Microsoft?\", context=context)\\r\\n\\r\\nprint(f\"Answer 2: \\'{result[\\'answer\\']}\\'\")\\r\\n\\n\\n\\xa0\\nOutput:\\n\\nAnswer 1: \\'trial division.\\'\\r\\nAnswer 2: \\'1975.\\'\\r\\n\\n\\n\\xa0\\nBERT\\n\\xa0\\nBERT (Bidirectional Encoder Representations from Transformers) makes use of a Transformer, which learns contextual relations between words in a text. In its vanilla form, Transformer includes two separate mechanisms\\u200a—\\u200aan encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is used. So BERT is just transformer encoders stacked above each other.\\nText prediction\\n\\nunmasker = pipeline(\\'fill-mask\\', model=\\'bert-base-cased\\')\\r\\nunmasker(\"Hello, My name is [MASK].\")\\r\\n\\n\\n\\xa0\\nOutput:\\n\\n[{\\'sequence\\': \\'[CLS] Hello, My name is David. [SEP]\\',\\r\\n  \\'score\\': 0.007879073731601238,\\r\\n  \\'token\\': 1681,\\r\\n  \\'token_str\\': \\'David\\'},\\r\\n {\\'sequence\\': \\'[CLS] Hello, My name is Kate. [SEP]\\',\\r\\n  \\'score\\': 0.007307342253625393,\\r\\n  \\'token\\': 5036,\\r\\n  \\'token_str\\': \\'Kate\\'},\\r\\n {\\'sequence\\': \\'[CLS] Hello, My name is Sam. [SEP]\\',\\r\\n  \\'score\\': 0.007054011803120375,\\r\\n  \\'token\\': 2687,\\r\\n  \\'token_str\\': \\'Sam\\'},\\r\\n {\\'sequence\\': \\'[CLS] Hello, My name is James. [SEP]\\',\\r\\n  \\'score\\': 0.006197025533765554,\\r\\n  \\'token\\': 1600,\\r\\n  \\'token_str\\': \\'James\\'},\\r\\n {\\'sequence\\': \\'[CLS] Hello, My name is Charlie. [SEP]\\',\\r\\n  \\'score\\': 0.006146721541881561,\\r\\n  \\'token\\': 4117,\\r\\n  \\'token_str\\': \\'Charlie\\'}]\\r\\n\\n\\n\\xa0\\nText Summarization\\n\\n#Summarization is currently supported by Bart and T5.\\r\\nsummarizer = pipeline(\"summarization\")\\r\\n\\r\\nARTICLE = \"\"\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.\\r\\nFirst conceived during Dwight D. Eisenhower\\'s administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,\\r\\nApollo was later dedicated to President John F. Kennedy\\'s national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress.\\r\\nProject Mercury was followed by the two-man Project Gemini (1962-66).\\r\\nThe first manned flight of Apollo was in 1968.\\r\\nApollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966.\\r\\nGemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.\\r\\nApollo used Saturn family rockets as launch vehicles.\\r\\nApollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\\r\\n\"\"\"\\r\\n\\r\\nsummary=summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)[0]\\r\\n\\r\\nprint(summary[\\'summary_text\\'])\\r\\n\\n\\n\\xa0\\nOutput:\\n\\nThe first manned flight of Apollo ran from 1961 to 1972 . The Apollo program was followed by the two-man ProjectGemini . It was the third mission to land on the Moon .\\r\\n\\n\\n\\xa0\\nEnglish to German translation\\n\\n# English to German\\r\\ntranslator_ger = pipeline(\"translation_en_to_de\")\\r\\nprint(\"German: \",translator_ger(\"Joe Biden became the 46th president of U.S.A.\", max_length=40)[0][\\'translation_text\\'])\\r\\n\\r\\n# English to French\\r\\ntranslator_fr = pipeline(\\'translation_en_to_fr\\')\\r\\nprint(\"French: \",translator_fr(\"Joe Biden became the 46th president of U.S.A\",  max_length=40)[0][\\'translation_text\\'])\\r\\n\\n\\n\\xa0\\nOutput:\\n\\nGerman:  Joe Biden wurde der 46. Präsident der USA.\\r\\nFrench:  Joe Biden est devenu le 46e président des États-Unis\\r\\n\\n\\n\\xa0\\nConversation\\n\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\r\\nimport torch\\r\\n\\r\\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\\r\\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\\r\\n\\r\\n# Let\\'s chat for 5 lines\\r\\nfor step in range(5):\\r\\n   # encode the new user input, add the eos_token and return a tensor in Pytorch\\r\\n   new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors=\\'pt\\')\\r\\n\\r\\n   # append the new user input tokens to the chat history\\r\\n   bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\r\\n\\r\\n   # generated a response while limiting the total chat history to 1000 tokens,\\r\\n   chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\r\\n\\r\\n   # pretty print last output tokens from bot\\r\\n   print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\r\\n\\n\\n\\xa0\\nOutput:\\n\\n>> User:Hi\\r\\nDialoGPT: Hi! :D\\r\\n>> User:How are you doing?\\r\\nDialoGPT: I\\'m doing well! How are you?\\r\\n>> User:I\\'m not that good.\\r\\nDialoGPT: I\\'m sorry.\\r\\n>> User:Thank you\\r\\nDialoGPT: No problem. I\\'m glad you\\'re doing well.\\r\\n>> User:bye\\r\\nDialoGPT: Bye! :D\\r\\n\\n\\n\\xa0\\nNamed Entity Recognition\\n\\nfrom transformers import pipeline, set_seed\\r\\nnlp_token_class = pipeline(\\'ner\\')\\r\\nnlp_token_class(\\'Ronaldo was born in 1985, he plays for Juventus and Portugal. \\')\\r\\n\\n\\n\\xa0\\nOutput:\\n\\n[{\\'word\\': \\'Ronald\\',\\r\\n  \\'score\\': 0.9978647828102112,\\r\\n  \\'entity\\': \\'I-PER\\',\\r\\n  \\'index\\': 1},\\r\\n {\\'word\\': \\'##o\\', \\'score\\': 0.99903804063797, \\'entity\\': \\'I-PER\\', \\'index\\': 2},\\r\\n {\\'word\\': \\'Juventus\\',\\r\\n  \\'score\\': 0.9977495670318604,\\r\\n  \\'entity\\': \\'I-ORG\\',\\r\\n  \\'index\\': 11},\\r\\n {\\'word\\': \\'Portugal\\',\\r\\n  \\'score\\': 0.9991246461868286,\\r\\n  \\'entity\\': \\'I-LOC\\',\\r\\n  \\'index\\': 13}]\\r\\n\\n\\n\\xa0\\nFeatures Extraction\\n\\nimport numpy as np\\r\\nnlp_features = pipeline(\\'feature-extraction\\')\\r\\noutput = nlp_features(\\'output = nlp_features(\\'Deep learning is a branch of Machine learning\\'))\\r\\nnp.array(output).shape # (Samples, Tokens, Vector Size)\\r\\n\\n\\n\\xa0\\nOutput:\\n\\n(1, 10, 768)\\r\\n\\n\\n\\xa0\\nZero-shot Learning\\n\\xa0\\nZero-Shot learning aims to solve a task without receiving any example of that task at the training phase. The task of recognizing an object from a given image where there weren’t any example images of that object during the training phase can be considered as an example of a Zero-Shot Learning task.\\n\\nclassifier_zsl = pipeline(\"zero-shot-classification\")\\r\\n\\r\\nsequence_to_classify = \"Bill gates founded a company called Microsoft in the year 1975\"\\r\\ncandidate_labels = [\"Europe\", \"Sports\",\\'Leadership\\',\\'business\\', \"politics\",\"startup\"]\\r\\nclassifier_zsl(sequence_to_classify, candidate_labels)\\r\\n\\n\\n\\xa0\\nOutput:\\n\\n{\\'sequence\\': \\'Bill gates founded a company called Microsoft in the year 1975\\',\\r\\n \\'labels\\': [\\'business\\',\\r\\n  \\'startup\\',\\r\\n  \\'Leadership\\',\\r\\n  \\'Europe\\',\\r\\n  \\'Sports\\',\\r\\n  \\'politics\\'],\\r\\n \\'scores\\': [0.6144810318946838,\\r\\n  0.1874515861272812,\\r\\n  0.18227894604206085,\\r\\n  0.006684561725705862,\\r\\n  0.0063185556791722775,\\r\\n  0.0027852619532495737]}\\r\\n\\n\\n\\xa0\\nUsing transformers in Widgets\\n\\xa0\\n\\nimport ipywidgets as widgets\\r\\nnlp_qaA = pipeline(\\'question-answering\\')\\r\\n\\r\\ncontext = widgets.Textarea(\\r\\n   value=\\'Einstein is famous for the general theory of relativity\\',\\r\\n   placeholder=\\'Enter something\\',\\r\\n   description=\\'Context:\\',\\r\\n   disabled=False\\r\\n)\\r\\n\\r\\nquery = widgets.Text(\\r\\n   value=\\'Why is Einstein famous for ?\\',\\r\\n   placeholder=\\'Enter something\\',\\r\\n   description=\\'Question:\\',\\r\\n   disabled=False\\r\\n)\\r\\n\\r\\ndef forward(_):\\r\\n   if len(context.value) > 0 and len(query.value) > 0:\\r\\n       output = nlp_qaA(question=query.value, context=context.value)           \\r\\n       print(output)\\r\\n\\r\\nquery.on_submit(forward)\\r\\ndisplay(context, query)\\r\\n\\n\\n\\n\\xa0\\nWant a lighter model?\\n\\xa0\\n“Distillation comes into the picture”\\n\\ncredit\\nOne of the main concerns while using Transformer based models is the computational power they require. All over this article, we are using the BERT model as it can be run on common machines, but that’s not the case for all of the models.\\nFor example, Google released a few months ago T5 an Encoder/Decoder architecture based on Transformer and available in transformers with no more than 11 billion parameters. Microsoft also recently entered the game with Turing-NLG using 17 billion parameters. This kind of model requires tens of gigabytes to store the weights and a tremendous compute infrastructure to run such models, which makes it impracticable for the common man!\\nWith the goal of making Transformer-based NLP accessible to everyone, Hugging Face developed models that take advantage of a training process called Distillation, which allows us to drastically reduce the resources needed to run such models with almost zero drops in performance.\\n\\xa0\\nClassifying text with DistilBERT and Tensorflow\\n\\xa0\\nYou can find Classifying text with DistilBERT and Tensorflow in my Kaggle notebook.\\nYou can also find Hugging Face python notebooks on using transformers for solving various use cases.\\n\\xa0\\nRelated:\\n\\nGetting Started with 5 Essential Natural Language Processing Libraries\\nUnderstanding Transformers, the Data Science Way\\nA Deep Dive Into the Transformer Architecture – The Development of Transformer Models',\n",
       " \"comments\\nBy Steve Shwartz, AI Author, Investor, and Serial Entrepreneur.\\n\\nPhoto: iStockPhoto / NanoStockk\\nConsiderable data science expertise is usually required to create a dataset and build a model for a particular application.\\xa0 But building a good model is usually not enough.\\xa0 In fact, it is not nearly enough.\\xa0 As illustrated below, developing and testing a model is just the first step.\\n\\nMachine Learning Model Lifecycle.\\nMachine Learning Operations (MLOps) is everything else required to make that model useful, including capabilities for an automated development and deployment pipeline, monitoring, lifecycle management, and governance, as illustrated above.\\xa0 Let’s look at each of these.\\n\\xa0\\nAutomation Pipeline\\n\\xa0\\nCreating a production ML system requires multiple steps:\\xa0 First, the data must undergo a series of transformations.\\xa0 Then, the model is trained.\\xa0 Usually, this requires experimentation with different network architectures and hyperparameters.\\xa0 Often, it is necessary to go back to the data and try different features.\\xa0 Next, the model must be validated with unit tests and integration tests.\\xa0 It needs to pass tests for data and model bias and explainability.\\xa0 Finally, it is deployed into a public cloud, an on-premise environment, or a hybrid environment.\\xa0 Additionally, some steps in the process might require an approval workflow.\\nIf each of these steps is performed manually, the development process tends to be slow and brittle.\\xa0 Fortunately, many MLOps tools exist to automate these steps from data transformation to deployment end-to-end.\\xa0 When retraining is necessary, it is an automated, reliable, and reproducible process.\\n\\xa0\\nMonitoring\\n\\xa0\\nML models tend to work well when first deployed and then work less well over time.\\xa0 As Forrester analyst, Dr. Kjell Carlsson said:\\xa0 “AI models are like six-year-olds during quarantine: They need constant attention . . . otherwise, something will break.”\\nIt is critical for deployments to include various types of monitoring so that ML teams can be alerted when this starts to happen.\\xa0 Performance can degrade due to infrastructure issues such as inadequate CPU or memory.\\xa0 Performance can also degrade when the real-world data that constitute the independent variables that are input to the model start to take on different characteristics than the training data, a phenomenon known as data drift.\\nSimilarly, the model may become less applicable because real-world conditions change, a phenomenon known as concept drift.\\xa0 For example, many predictive models of customer and supplier behavior were sent into a tailspin by COVID-19.\\nSome companies also monitor alternative models (e.g., different network architectures or different hyperparameters) to see if any of these “challenger” models starts performing better than the production model.\\nOften, it makes sense to put guardrails around decisions made by the model.\\xa0 These guardrails are simple rules that either trigger an alert, prevent the decision, or put the decision into a workflow for human approval.\\n\\xa0\\nLifecycle Management\\n\\xa0\\nWhen model performance starts to degrade due to data or model drift, model retraining and possibly model re-architecture are required.\\xa0 However, the data science team shouldn’t have to start from scratch.\\xa0 In developing the original model, and perhaps in prior re-architectures, they probably tested many architectures, hyperparameters, and features.\\xa0 It’s critical that all these prior experiments (and results) are recorded so that the data science team doesn’t have to go back to square one.\\xa0 It’s also critical for communication and collaboration between data science team members.\\n\\xa0\\nGovernance\\n\\xa0\\nMachine learning models are being used for many applications that impact people like bank loan decision, medical diagnosis, and hiring/firing decisions.\\xa0 The use of ML models in decision-making has been criticized for two reasons:\\xa0 First, these models are subject to bias, especially if the training data results in models that discriminate based on race, color, ethnicity, national origin, religion, gender, sexual orientation, or other protected classes.\\xa0 Second, these models are often black boxes that don’t explain their decision-making.\\nAs a result, organizations that used ML-based decision-making are under pressure to ensure their models don’t discriminate and are capable of explaining their decisions.\\xa0 Many MLOps vendors are incorporating tools based on academic research (e.g., SHAP and Grad-CAM) that help explain the model decisions and are using a variety of techniques to ensure that the data and models are not biased.\\xa0 Additionally, they are incorporating bias and explainability tests in their monitoring protocols because models can become biased or lose explanatory capability over time.\\nOrganizations also need to build trust and are starting to ensure that on-going performance, lack of bias, and explainability are auditable.\\xa0 This requires model catalogs that not only document all the data, parameter, and architecture decisions but also log each decision and provide traceability so that it can be determined what data, model, and parameters were used for each decision, when the model was retrained or otherwise modified, and who made each change.\\xa0 It is also important for auditors to be able to repeat historical transactions and to test the boundaries of model decision-making with what-if scenarios.\\nSecurity and data privacy are also key concerns for organizations using ML.\\xa0 Care must be taken to ensure the personal information is protected and role-based data access capabilities are essential, especially for regulated industries.\\nGovernments around the world are also moving quickly to regulate ML-based decision-making that affects people.\\xa0 The European Union has led the way with its GDPR and CRD IV regulations.\\xa0 In the US, several regulatory agencies, including the US Federal Reserve Bank and the FDA, have created regulations around ML-based decision-making for financial and medical decisions.\\xa0 A more comprehensive law, the recently proposed Data Accountability and Transparency Act of 2020, is slated for Congressional consideration in 2021.\\xa0 Regulations will likely evolve to the point where CEO’s need to sign off on the explainability of and the lack of bias in their ML models.\\n\\xa0\\nThe MLOps Landscape\\n\\xa0\\nAs we continue in 2021, the market for MLOps is exploding.\\xa0 According to analyst firm Cognilytica, it is expected to be a $4 billion market by 2025.\\nThere are big players and small players in the MLOps space.\\xa0 Major ML platform vendors like Amazon, Google, Microsoft, IBM, Cloudera, Domino, DataRobot, and H2O are incorporating MLOps capabilities into their platforms.\\xa0 According to Crunchbase, there are 35 private companies in the MLOps space who have raised between $1.8M and $1B in financing and who have between 3 and 2800 employees on LinkedIn:\\n\\n\\n\\n\\nFinancing ($millions)\\nNumber of Employees\\n\\nDescription\\n\\n\\nCloudera\\n1000\\n2803\\nCloudera delivers an Enterprise Data Cloud for any data, anywhere, from the Edge to AI.\\n\\n\\nDatabricks\\n897\\n1757\\nDatabricks is a software platform that helps its customers unify their analytics across business, data science, and data engineering.\\n\\n\\nDataRobot\\n750\\n1105\\nDataRobot brings AI technology and ROI enablement services to global enterprises.\\n\\n\\nDataiku\\n246\\n556\\nDataiku operates as an enterprise artificial intelligence and machine-learning platform.\\n\\n\\nAlteryx\\n163\\n1623\\nAlteryx accelerates digital transformation by unifying analytics, data science and automated processes.\\n\\n\\nH2O\\n151\\n257\\nH2O.ai is the open source leader in AI and automatic machine learning with a\\xa0mission to democratize AI for everyone.\\n\\n\\nDomino\\n124\\n232\\nDomino is the world's leading Enterprise Data Science Platform, powering data science at over 20% of the Fortune 100.\\n\\n\\nIguazio\\n72\\n83\\nThe Iguazio Data Science Platform enables you to develop, deploy and manage AI applications at scale and in real-time\\n\\n\\nExplorium.ai\\n50\\n96\\nExplorium offers a data science platform powered by augmented data discovery and feature engineering\\n\\n\\nAlgorithmia\\n38\\n63\\nAlgorithmia is a machine learning model deployment and management solution that automates the MLOps for an organization\\n\\n\\nPaperspace\\n23\\n37\\nPaperspace powers next-generation applications built on GPUs.\\n\\n\\nPachyderm\\n21\\n32\\nPachyderm is an enterprise-grade data science platform that makes explainable, repeatable, and scalable AI/ML a reality.\\n\\n\\nWeights and Biases\\n20\\n58\\nTools for experiment tracking, improved model performance, and results collaboration\\n\\n\\nOctoML\\n19\\n37\\nOctoML is changing how developers optimize and deploy machine learning models for their AI needs.\\n\\n\\nArthur AI\\n18\\n28\\nArthur AI is a platform that monitors the productivity of machine learning models.\\n\\n\\nTruera\\n17\\n26\\nTruera provides a Model Intelligence platform for enterprises to analyze machine learning.\\n\\n\\nSnorkel AI\\n15\\n39\\nSnorkel AI is focused on making AI practical through Snorkel Flow: the data-first platform for enterprise AI\\n\\n\\nSeldon.io\\n14\\n48\\nMachine Learning Deployment Platform\\n\\n\\nFiddler Labs\\n13\\n46\\nFiddler enables users to create AI solutions that are transparent, explainable, and understandable.\\n\\n\\nrun.ai\\n13\\n26\\nRun:AI develops an automated distributed training technology that virtualizes and accelerates deep learning.\\n\\n\\nClearML (Allegro)\\n11\\n29\\nML / DL Experiment Manager and ML-Ops Open-Source Solution End-to-End Product Life-cycle Management Enterprise Solution\\n\\n\\nVerta\\n10\\n15\\nVerta builds software infrastructure to help enterprise data science and machine learning (ML) teams develop and deploy ML models.\\n\\n\\ncnvrg.io\\n8\\n38\\ncnvrg.io is a full stack data science platform that helps teams manage models, and build auto-adaptive machine learning pipelines\\n\\n\\nDatatron\\n8\\n19\\nDatatron provides a single model governance (management) platform for all of your ML, AI, and Data Science models in production\\n\\n\\nComet\\n7\\n19\\nComet.ml is a machine learning platform designed to help AI practitioners and teams build reliable machine learning models.\\n\\n\\nModelOp\\n6\\n39\\nGovern, Monitor and Manage all models across the enterprise\\n\\n\\nWhyLabs\\n4\\n15\\nWhyLabs is the AI observability and monitoring company.\\n\\n\\nArize AI\\n4\\n14\\nArize AI offers a platform that explains and troubleshoots production AI.\\n\\n\\nDarwinAI\\n4\\n31\\nDarwinAI’s Generative Synthesis 'AI building AI' technology enables optimized and explainable deep learning.\\n\\n\\nMona\\n4\\n11\\nMona is a SaaS monitoring platform for Data and AI driven systems\\n\\n\\nValohai\\n2\\n13\\nYour Managed Machine Learning Platform that lets data scientists build, deploy and track machine learning models.\\n\\n\\nModzy\\n0\\n31\\nThe secure ModelOps platform to discover, deploy, manage, and govern machine learning at scale—getting to value faster.\\n\\n\\nAlgomox\\n0\\n17\\nCatalyze Your AI Transformation\\n\\n\\nMonitaur\\n0\\n8\\nMonitaur is a software company that provides auditability, transparency, and governance for companies using machine learning software.\\n\\n\\nHydrosphere.io\\n0\\n3\\nHydrosphere.io is a platform for AI/ML operations automation\\n\\n\\n\\n\\xa0\\nMany of these companies focus on just one segment of MLOps, such as automation pipeline, monitoring, lifecycle management, or governance.\\xa0 Some argue that using multiple, best-of-breed MLOps products are better for data science projects than monolithic platforms.\\xa0 And some companies are building MLOps products for specific verticals.\\xa0 For example, Monitaur positions itself as a best-of-breed governance solution that can work with any platform.\\xa0 Monitaur is also building industry-specific MLOps governance capabilities for regulated industries, starting with insurance.\\xa0 (Full disclosure:\\xa0 I am an investor in Monitaur).\\nThere are also a number of open-source MLOps projects, including:\\n\\nMLFlow manages the ML lifecycle, including experimentation, reproducibility, and deployment, and includes a model registry\\nDVC manages version control for ML projects to make them shareable and reproducible\\nPolyaxon has capabilities for experimentation, lifecycle automation, collaboration, and deployment, and includes a model registry\\nMetaflow is a former Netflix project for managing the automation pipeline and deployment\\nKubeflow has capabilities for workflow automation and deployment in Kubernetes containers\\n\\n2021 promises to be an interesting year for MLOps.\\xa0 We’ll likely see rapid growth, tremendous competition, and most likely, some consolidation.\\n\\xa0\\nBio:\\xa0Steve Shwartz\\xa0(@sshwartz) started his AI career as a postdoc at Yale University many years ago, is a successful serial entrepreneur and investor, and is the author of “Evil Robots, Killer Computers, and Other Myths: The Truth About AI and the Future of Humanity”.\\nRelated:\\n\\nA Machine Learning Model Monitoring Checklist: 7 Things to Track\\nHow to Use MLOps for an Effective AI Strategy\\nMLOps: Model Monitoring 101\",\n",
       " 'Sponsored Post.\\n\\nWith the advent of ‘big data’, data changed from a byproduct of systems to a source of innovation and competitive advantage. Now, following the universal digital transformation and the current rise of AI and ML and cloud data platforms, data is considered a product in and of itself and its use, value, and significance continues to evolve.\\nIf our understanding of data is changing, the role of the data team and DataOps is being modernized and modified faster. A thorough understanding of modern DataOps is necessary to fully grasp the current value and role of data.\\nOur recent survey of over 130 top data engineers, data architects, and executives uncovered details and trends of the current state of data engineering and DataOps. We identified popular trends like adoption of cloud data platforms and gained insight into winning and emerging platforms, data engineering challenges, and how organizations are handling sensitive data.\\xa0\\nRead our survey report to learn more about these trends as well as our predictions for future obstacles and our recommendations for avoiding them.',\n",
       " 'comments\\nBy Stephanie Kirmer, Senior Data Scientist at Saturn Cloud\\nApplying deep learning strategies to computer vision problems has opened up a world of possibilities for data scientists. However, to use these techniques at scale to create business value, substantial computing resources need to be available – and this is just the kind of challenge Saturn Cloud is built to solve!\\nIn this tutorial, you’ll see the steps to conducting image classification inference using the popular Resnet50 deep learning model at scale using NVIDIA GPU clusters on Saturn Cloud. Using the resources Saturn Cloud makes available, we can run the task 40x faster than a non-parallelized approach!\\n\\n\\nWe’ll be classifying dog images today!\\n\\n\\xa0\\nWhat you’ll learn here:\\n\\xa0\\n\\nHow to set up and manage a GPU cluster on Saturn Cloud for deep learning inference tasks\\nHow to run inference tasks with Pytorch on the GPU cluster\\nHow to use batch processing to accelerate your inference tasks with Pytorch on the GPU cluster\\n\\nSetup\\n\\xa0\\nTo begin, we need to ensure that our image dataset is available and that our GPU cluster is running.\\nIn our case, we have stored the data on S3 and use the\\xa0s3fs\\xa0library to work with it, as you’ll see below. \\nIf you would like to use this same dataset, it is the Stanford Dogs dataset, available here:\\xa0http://vision.stanford.edu/aditya86/ImageNetDogs/\\nTo set up our Saturn GPU cluster, the process is very straightforward.\\n\\nimport dask_saturn\\r\\nfrom dask_saturn import SaturnCluster\\r\\n\\r\\ncluster = SaturnCluster(n_workers=4, scheduler_size=\\'g4dnxlarge\\', worker_size=\\'g4dn8xlarge\\')\\r\\nclient = Client(cluster)\\r\\nclient\\n\\n\\xa0\\n\\n[2020-10-15 18:52:56] INFO – dask-saturn | Cluster is ready\\n\\n\\nWe are not explicitly stating it, but we are using 32 threads each on our cluster nodes, making 128 total threads.\\nTip: Individual users may find that you want to adjust the number of threads, reducing it down if your files are very large – too many threads running large tasks simultaneously might require more memory than your workers have available at one time.\\nThis step may take a moment to complete because all the AWS instances that we are requesting need to be spun up. Calling\\xa0client\\xa0at the end, there will monitor the spin-up process and let you know when things are ready to rock!\\n\\xa0\\nGPU Capability\\n\\xa0\\nAt this point, we can confirm that our cluster has GPU capabilities, and make sure we have set everything up correctly.\\nFirst, check that the Jupyter instance has GPU capability.\\n\\ntorch.cuda.is_available() \\r\\n\\n\\n\\nTrue\\n\\n\\nAwesome- now let’s also check each of our four workers.\\n\\nclient.run(lambda: torch.cuda.is_available())\\n\\n\\n\\n{‘tcp://10.0.24.217:45281’: True,\\r\\n‘tcp://10.0.28.232:36099’: True,\\r\\n‘tcp://10.0.3.136:40143’: True,\\r\\n‘tcp://10.0.3.239:40585’: True}\\n\\n\\nHere then we’ll set the “device” to always be cuda, so we can use those GPUs.\\n\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n\\nNote: If you need some help establishing how to run a single image classification, we have an\\xa0expanded code notebook\\xa0available at our github that can give you those instructions as well as the rest of this content.\\n\\n\\xa0\\nInference\\n\\xa0\\nNow, we’re ready to start doing some classification! We’re going to use some custom-written functions to do this efficiently and make sure our jobs can take full advantage of the parallelization of the GPU cluster.\\n\\xa0\\nPreprocessing\\n\\xa0\\nSingle Image Processing\\n\\xa0\\n\\n@dask.delayed\\r\\ndef preprocess(path, fs=__builtins__):\\r\\n    \\'\\'\\'Ingest images directly from S3, apply transformations,\\r\\n    and extract the ground truth and image identifier. Accepts\\r\\n    a filepath. \\'\\'\\'\\r\\n    \\r\\n    transform = transforms.Compose([\\r\\n        transforms.Resize(256), \\r\\n        transforms.CenterCrop(250), \\r\\n        transforms.ToTensor()])\\r\\n\\r\\n    with fs.open(path, \\'rb\\') as f:\\r\\n        img = Image.open(f).convert(\"RGB\")\\r\\n        nvis = transform(img)\\r\\n\\r\\n    truth = re.search(\\'dogs/Images/n[0-9]+-([^/]+)/n[0-9]+_[0-9]+.jpg\\', path).group(1)\\r\\n    name = re.search(\\'dogs/Images/n[0-9]+-[a-zA-Z-_]+/(n[0-9]+_[0-9]+).jpg\\', path).group(1)\\r\\n    \\r\\n    return [name, nvis, truth]\\n\\n\\nThis function allows us to process one image, but of course, we have a lot of images to work with here! We’re going to use some list comprehension strategies to create our batches and get them ready for our inference.\\nFirst, we break the list of images we have from our S3 file path into chunks that will define the batches.\\n\\n3fpath = \\'s3://dask-datasets/dogs/Images/*/*.jpg\\'\\r\\n\\r\\nbatch_breaks = [list(batch) for batch in toolz.partition_all(60, s3.glob(s3fpath))]\\n\\n\\nThen we’ll process each file into nested lists. Then we’ll reformat this list setup slightly and we’re ready to go!\\n\\nimage_batches = [[preprocess(x, fs=s3) for x in y] for y in batch_breaks]\\n\\n\\nNotice that we have used the Dask\\xa0delayed\\xa0decorator on all of this- we don’t want it to actually run yet, but to wait until we are doing work in parallel on the GPU cluster!\\n\\xa0\\nFormat Batches\\n\\xa0\\nThis little step just makes sure that the batches of images are organized in the way that the model will expect them.\\n\\n@dask.delayed\\r\\ndef reformat(batch):\\r\\n    flat_list = [item for item in batch]\\r\\n    tensors = [x[1] for x in flat_list]\\r\\n    names = [x[0] for x in flat_list]\\r\\n    labels = [x[2] for x in flat_list]\\r\\n    return [names, tensors, labels]\\r\\n    \\r\\nimage_batches = [reformat(result) for result in image_batches]\\n\\n\\n\\xa0\\nRun the Model\\n\\xa0\\nNow we are ready to do the inference task! This is going to have a few steps, all of which are contained in functions described below, but we’ll talk through them so everything is clear.\\nOur unit of work at this point is batches of 60 images at a time, which we created in the section above. They are all neatly arranged in lists so that we can work with them effectively.\\nOne thing we need to do with the lists is to “stack” the tensors. We could do this earlier in our process, but because we are using the Dask\\xa0delayed\\xa0decorator on the preprocessing, our functions actually do not know that they are receiving tensors until later in the process. Therefore, we’re delaying the “stacking” as well by putting it inside this function that comes after the preprocessing.\\n\\n@dask.delayed\\r\\ndef run_batch_to_s3(iteritem):\\r\\n    \\'\\'\\' Accepts iterable result of preprocessing, \\r\\n    generates inferences and evaluates. \\'\\'\\'\\r\\n    \\r\\n    with s3.open(\\'s3://dask-datasets/dogs/imagenet1000_clsidx_to_labels.txt\\') as f:\\r\\n        classes = [line.strip() for line in f.readlines()]\\r\\n  \\r\\n    names, images, truelabels = iteritem\\r\\n    \\r\\n    images = torch.stack(images)\\r\\n... \\n\\n\\nSo now we have our tensors stacked so that batches can be passed to the model. We are going to retrieve our model using pretty simple syntax:\\n\\n...\\r\\n    resnet = models.resnet50(pretrained=True)\\r\\n    resnet = resnet.to(device)\\r\\n    resnet.eval()\\r\\n...\\n\\n\\nConveniently, we load the library\\xa0torchvision\\xa0which contains several useful pretrained models and datasets. That’s where we are grabbing Resnet50 from. Calling the method\\xa0.to(device)\\xa0allows us to pass the model object to our workers, giving them the ability to run the inference without having to reach back to the client.\\nNow we are ready to run inference! It is inside the same function, styled this way:\\n\\n...\\r\\n    images = images.to(device)\\r\\n    pred_batch = resnet(images)\\r\\n...\\n\\n\\nWe pass our image stack (just the batch we are working on) to the workers and then run the inference, returning predictions for that batch.\\n\\xa0\\nResult Evaluation\\n\\xa0\\nThe predictions and truth we have so far, however, are not really human-readable or comparable, so we’ll use the functions that follow to fix them up and get us interpretable results.\\n\\ndef evaluate_pred_batch(batch, gtruth, classes):\\r\\n    \\'\\'\\' Accepts batch of images, returns human readable predictions. \\'\\'\\'\\r\\n    _, indices = torch.sort(batch, descending=True)\\r\\n    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\\r\\n    \\r\\n    preds = []\\r\\n    labslist = []\\r\\n    for i in range(len(batch)):\\r\\n        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\\r\\n        preds.append(pred)\\r\\n\\r\\n        labs = gtruth[i]\\r\\n        labslist.append(labs)\\r\\n        \\r\\n    return(preds, labslist)\\n\\n\\nThis takes our results from the model, and a few other elements, to return nice readable predictions and the probabilities the model assigned.\\n\\npreds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)\\n\\n\\nFrom here, we’re nearly done! We want to pass our results back to S3 in a tidy, human-readable way, so the rest of the function handles that. It will iterate over each image because these functionalities are not batch handling.\\xa0is_match\\xa0is one of our custom functions, which you can check out below.\\n\\n...\\r\\n    for j in range(0, len(images)):\\r\\n        predicted = preds[j]\\r\\n        groundtruth = labslist[j]\\r\\n        name = names[j]\\r\\n        match = is_match(groundtruth, predicted)\\r\\n\\r\\n        outcome = {\\'name\\': name, \\'ground_truth\\': groundtruth, \\'prediction\\': predicted, \\'evaluation\\': match}\\r\\n\\r\\n        # Write each result to S3 directly\\r\\n        with s3.open(f\"s3://dask-datasets/dogs/preds/{name}.pkl\", \"wb\") as f:\\r\\n            pickle.dump(outcome, f)\\r\\n...\\n\\n\\n\\xa0\\nPut It All Together\\n\\xa0\\nNow, we aren’t going to patch together all these functions by hand, instead, we have assembled them in one single delayed function that will do the work for us. Importantly, we can then map this across all our batches of images across the cluster!\\n\\ndef evaluate_pred_batch(batch, gtruth, classes):\\r\\n    \\'\\'\\' Accepts batch of images, returns human readable predictions. \\'\\'\\'\\r\\n    _, indices = torch.sort(batch, descending=True)\\r\\n    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\\r\\n    \\r\\n    preds = []\\r\\n    labslist = []\\r\\n    for i in range(len(batch)):\\r\\n        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\\r\\n        preds.append(pred)\\r\\n\\r\\n        labs = gtruth[i]\\r\\n        labslist.append(labs)\\r\\n        \\r\\n    return(preds, labslist)\\r\\n\\r\\ndef is_match(la, ev):\\r\\n    \\'\\'\\' Evaluate human readable prediction against ground truth. \\r\\n    (Used in both methods)\\'\\'\\'\\r\\n    if re.search(la.replace(\\'_\\', \\' \\'), str(ev).replace(\\'_\\', \\' \\')):\\r\\n        match = True\\r\\n    else:\\r\\n        match = False\\r\\n    return(match)    \\r\\n\\r\\n\\r\\n@dask.delayed\\r\\ndef run_batch_to_s3(iteritem):\\r\\n    \\'\\'\\' Accepts iterable result of preprocessing, \\r\\n    generates inferences and evaluates. \\'\\'\\'\\r\\n    \\r\\n    with s3.open(\\'s3://dask-datasets/dogs/imagenet1000_clsidx_to_labels.txt\\') as f:\\r\\n        classes = [line.strip() for line in f.readlines()]\\r\\n  \\r\\n    names, images, truelabels = iteritem\\r\\n    \\r\\n    images = torch.stack(images)\\r\\n    \\r\\n    with torch.no_grad():\\r\\n        # Set up model\\r\\n        resnet = models.resnet50(pretrained=True)\\r\\n        resnet = resnet.to(device)\\r\\n        resnet.eval()\\r\\n\\r\\n        # run model on batch\\r\\n        images = images.to(device)\\r\\n        pred_batch = resnet(images)\\r\\n        \\r\\n        #Evaluate batch\\r\\n        preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)\\r\\n\\r\\n        #Organize prediction results\\r\\n        for j in range(0, len(images)):\\r\\n            predicted = preds[j]\\r\\n            groundtruth = labslist[j]\\r\\n            name = names[j]\\r\\n            match = is_match(groundtruth, predicted)\\r\\n            \\r\\n            outcome = {\\'name\\': name, \\'ground_truth\\': groundtruth, \\'prediction\\': predicted, \\'evaluation\\': match}\\r\\n            \\r\\n            # Write each result to S3 directly\\r\\n            with s3.open(f\"s3://dask-datasets/dogs/preds/{name}.pkl\", \"wb\") as f:\\r\\n                pickle.dump(outcome, f)\\r\\n        \\r\\n        return(names)\\n\\n\\n\\xa0\\nOn the Cluster\\n\\xa0\\nWe have really done all the hard work already and can let our functions take it from here. We’ll be using the\\xa0.map\\xa0method to distribute our tasks efficiently.\\n\\nfutures = client.map(run_batch_to_s3, image_batches) \\r\\nfutures_gathered = client.gather(futures)\\r\\nfutures_computed = client.compute(futures_gathered, sync=False)\\n\\n\\nWith\\xa0map\\xa0we ensure all our batches will get the function applied to them. With\\xa0gather, we can collect all the results simultaneously rather than one by one. With\\xa0compute(sync=False)\\xa0we return all the futures, ready to be calculated when we want them. This may seem arduous, but these steps are required to allow us to iterate over the future.\\nNow we actually run the tasks, and we also have a simple error handling system just in case any of our files are messed up or anything goes haywire.\\n\\nimport logging\\r\\n\\r\\nresults = []\\r\\nerrors = []\\r\\nfor fut in futures:\\r\\n    try:\\r\\n        result = fut.result()\\r\\n    except Exception as e:\\r\\n        errors.append(e)\\r\\n        logging.error(e)\\r\\n    else:\\r\\n        results.extend(result)\\n\\n\\n\\xa0\\nEvaluate\\n\\xa0\\nWe want to make sure we have high-quality results coming out of this model, of course! First, we can peek at a single result.\\n\\nwith s3.open(\\'s3://dask-datasets/dogs/preds/n02086240_1082.pkl\\', \\'rb\\') as data:\\r\\n    old_list = pickle.load(data)\\r\\n    old_list\\n\\n\\n\\n{‘name’: ‘n02086240_1082’,\\r\\n‘ground_truth’: ‘Shih-Tzu’,\\r\\n‘prediction’: [(b”203: ‘West Highland white terrier’,”, 3.0289587812148966e-05)],\\r\\n‘evaluation’: False}\\n\\n\\nWhile we have a wrong prediction here, we have the sort of results we expect! To do a more thorough review, we would download all the results files, then just check to see how many have evaluation: True.\\nNumber of dog photos examined: 20580\\nNumber of dogs classified correctly: 13806\\nThe percent of dogs classified correctly: 67.085%\\nNot perfect, but good looking results overall!\\n\\xa0\\nComparing Performance\\n\\xa0\\nSo, we have managed to classify over 20,000 images in about 5 minutes. That sounds good, but what is the alternative?\\n\\n\\n\\n\\nTechnique\\nRuntime\\n\\n\\n\\n\\nNo Cluster with Batching\\n3 hours, 21 minutes, 13 sec\\n\\n\\nGPU Cluster with Batching\\n5 minutes, 15 sec\\n\\n\\n\\n\\xa0\\nAdding a GPU cluster makes a HUGE difference! If you’d like to see this work for yourself,\\xa0sign up for your free trial of Saturn Cloud today!\\n\\xa0\\nBio: Stephanie Kirmer is a Senior Data Scientist at Saturn Cloud.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nData Science in the Cloud with Dask\\nTop Python Libraries for Deep Learning, Natural Language Processing & Computer Vision\\nHow to Acquire the Most Wanted Data Science Skills',\n",
       " 'Sponsored Post.\\n\\n\\n\\nDate: Thursday, Nov. 19\\nTime: 1 - 1:30 p.m. ET\\nPresenter: Sam Edgemon\\nLocation: Online\\nRegistration: Free\\nImproving our ability to detect disease outbreaks within a given population as early as possible is a never-ending quest - for obvious reasons.\\r\\nWhether the cause is bioterrorism, such as an anthrax attack, or a novel virus, such as COVID-19, early detection enables early public health interventions and possibly the fleeting chance to limit morbidity and mortality.\\r\\nPublic health surveillance has traditionally focused on finding evidence of syndromes, i.e., cases with well-defined sets of symptoms that are classified via standard diagnostic codes. Alerts are issued after the number of cases crosses a specified threshold.\\r\\nIn this 30-minute live webinar, Sam Edgemon will demonstrate how the use of more granular symptoms-level data combined with innovative statistical techniques has the potential to identify disease outbreaks faster while limiting false positives.\\r\\nUsing data from emergency departments, ambulance services, poison control centers and social media, Sam will demonstrate:\\r\\n\\n\\nHow unstructured textual data can be combined with a wide breadth of other symptom-level data to spot previously hidden outbreaks.\\nHow to identify potential \"hot spots\" earlier using statistical techniques not usually associated with biological surveillance.\\nHow these techniques could be used to make a difference during the current pandemic, as well as the next one.\\n\\n\\nRegister here',\n",
       " 'By Stan Pugsley, Data Warehouse and Analytics Consultant.\\ncomments\\nGreat reporting and visualizations should create an action path that will guide users through the analysis process and eventual action to improve the business. The goal is not just story telling, but to help user navigate from data to action.\\nFollowing is a simple infographic for this framework:\\n\\nClick image to enlarge\\n\\xa0\\nLet’s start with the “Current State” batch of KPIs. These are the lead visualizations that you would place on your executive dashboards. They give people a sense for where the key metrics for the business in one place. What are the KPIs that tell the current story of the organization?\\nThe next step is to look for significant signals, changes or events. In our reporting we should use bright colors and attention-grabbing icons to highlight changes in the data.\\nEvery lead KPI and alert needs to be paired with one or more drill-down paths to explore details. These are supplemental visualizations. The path to do drill downs needs to be simple and clearly linked. Ideally each department or business process would have its own drill-down area or folder.\\nNow we’ve got to move beyond just doing a data dump. People want to be led by the hand to recommended routes to resolve issues. Not every change in every chart will have an automatic recommended best route. But many do. If inventory is low in a particular SKU, there are just a few routes to follow, so why not note them right on the report? If costs are increasing faster than planned in one department, do you have standard ways to address the issue? If so, then list out those standard actions right where the information is needed and useful as the information is consumed.\\nAt the end we need to help users to take action by embedding links to the associated applications right in the reports. If inventory is low, add a link below the KPI to the warehouse management software. If sales are dropping in one region, add a link at the bottom of the KPI to Salesforce or your CRM system to follow up.\\nWe can make our reporting tools into navigation tools by using the five steps in the framework.\\n\\xa0\\nBio: Stan Pugsley is a data warehouse and analytics consultant with Eide Bailly Technology Consulting based in Salt Lake City, UT. He is also an adjunct faculty member at the University of Utah Eccles School of Business. You can reach the author via email.\\nRelated:\\n\\nTelling a Great Data Story: A Visualization Decision Tree\\nData Science vs Business Intelligence, Explained\\n5 Concepts Every Data Scientist Should Know',\n",
       " 'Sponsored Post.\\n\\nEvery industry is in a state of transformation. Digital technology is reshaping how organizations manage and act on data, think about innovation and interact with customers.\\nAI has enabled organizations to automate once time-consuming tasks. Advanced in analytics uncovers insights from massive amounts of complex data. And cloud technology has given organizations the freedom and scalability to realize value faster than ever.\\nNow SAS and Microsoft are joining forces to help define the future of these powerful and emerging technologies through a unified analytic platform not seen anywhere else in the market. By bringing analytics, AI and cloud computing together, the two companies will help organizations forge a successful path to digital transformation.\\nAs the leader in analytics, SAS has made it easier to access SAS® Viya®, its cloud-native AI, analytic and data management platform, by optimizing it to run seamlessly on the Microsoft Azure cloud platform.\\nIn addition to providing customers access to powerful SAS analytics, it provides container orchestration managed by Azure Kubernetes, IT administration with MS Tools and seamless access to the Azure Data Estate.\\nThis helps customers democratize analytics throughout their organizations, while seamlessly managing analytic workloads and building SAS into a variety of applications.\\nFrom data scientists, IT departments, business analysts and developers – SAS users will ultimately benefit from an improved customer experience. And enterprises will benefit from being able to make more trusted decisions.\\nWhen SAS and Microsoft bring together what they each do best, the potential for real transformative change for our customers is practically limitless.',\n",
       " \"Sponsored Post.\\n\\nWebinar: Workflow Orchestration with Prefect and Coiled\\nWhen: Jun 30, 2021, 9 am PDT, 12 pm EDT, 17:00 BST.\\nJeremiah Lowin, Founder and CEO of Prefect, along with Kevin Kho, Prefect’s Open Source Community Engineer, will discuss updates about the company and demo a newly released feature called the KV Store.\\nPrefect is an open-source workflow orchestration tool created to handle the modern data stack. Prefect is built on top of Dask, allowing parallel execution of workflows. Coiled helps data scientists use Python for ambitious problems, scaling to the cloud for computing power, ease, and speed—all tuned for the needs of teams and enterprises. This means Coiled and Prefect have a very strong synergy. In this demo example, we’ll also show how to spin up a Coiled cluster to execute Prefect jobs during runtime. The Coiled cluster will provide parallelism for dynamically mapped tasks in Prefect.\\nAfter attending, you’ll leave knowing:\\n\\n How to create a simple Prefect Flow\\n How to use the new Prefect feature, the KV store\\n How to use Coiled and Prefect together\\n More knowledge about Prefect as a company\\n\\nWe're looking forward to seeing you there!\\nRegister here.\",\n",
       " 'By Benjamin Obi Tayo, Ph.D., DataScienceHub.\\ncomments\\n\\nPhoto by\\xa0Mitchell Luo\\xa0on\\xa0Unsplash.\\n“In 2021, professionals in the digital market space must be comfortable with data — period. They must know how to manipulate data, understand how it is collected, and analyze and interpret it. The future of decision making is grounded in data science.” — Wendy Moe, Professor of Marketing, University of Maryland\\nData science skills have become increasingly more important for jobs that once had little to do with statistics, including marketing and business. Adding data science skills to your portfolio will give you an edge in your current role in the market this year.\\nIf you are interested in adding data science to your portfolio, you no doubt might have pondered over these questions:\\n\\nHow long does it take to learn the fundamentals of data science?\\nWhat are some resources for learning data science?\\n\\nThis article discusses some general advice from\\xa0Peter Norvig\\xa0to individuals considering data science.\\n\\xa0\\nBackground about Peter Norvig (Director of Research at Google)\\n\\xa0\\nThe motivation for choosing the above title is based on\\xa0Peter Norvig’s\\xa0idea of the amount of time it takes to become an expert in programming. If you have not read this article:\\xa0“Teach Yourself Programming in 10 Years” by Peter Norvig, I encourage you to do so.\\nThe point here is that you don’t need 10 years to learn the basics of data science, but learning data science in a rush is certainly not helpful. It takes time, effort, energy, patience, and commitment to become a data scientist.\\nPeter Norvig’s\\xa0suggestion is that learning requires time, patience, and commitment. Beware of articles, books, or websites that tell you that you can learn data science in 4 weeks.\\n\\nImage by Benjamin O. Tayo.\\nIf you are interested in learning the fundamentals of data science, be prepared to invest the right amount of time and energy. That way, you can master not just the superficial concepts but the in-depth concepts of data science.\\nIt took me 2 years of in-depth studies to master the basics of data science (through self-study), and I continue to challenge myself to learn new things every day. How long it is going to take you to master the fundamentals of data science would depend on your background. Generally, a solid background in an analytical discipline such as mathematics, statistics, computer science, engineering, or economics is advantageous.\\n\\xa0\\n3 Lessons From Peter Norvig’s “Teach Yourself Programming in Ten Years”\\n\\xa0\\n1) It takes time, effort, energy, patience, and commitment to master the fundamentals of data science.\\nData science is a very multidisciplinary field that requires a solid background in advanced mathematics, statistics, programming, and other related skills in data analysis, data visualization, model building, machine learning, etc. It took me 2 years of dedicated studies to master the fundamentals of data science, and that is because of my solid background in mathematics, physics, and programming. Here are some resources that helped me master the fundamentals of data science.\\n(i)\\xa0Professional Certificate in Data Science (HarvardX, through edX)\\nIncludes the following courses, all taught using R (you can audit courses for free or purchase a verified certificate):\\n\\nData Science: R Basics\\nData Science: Visualization\\nData Science: Probability\\nData Science: Inference and Modeling\\nData Science: Productivity Tools\\nData Science: Wrangling\\nData Science: Linear Regression\\nData Science: Machine Learning\\nData Science: Capstone\\n\\n(ii)\\xa0Analytics: Essential Tools and Methods (Georgia TechX, through edX)\\nIncludes the following courses, all taught using R, Python, and SQL (you can audit for free or purchase a verified certificate):\\n\\nIntroduction to Analytics Modeling\\nIntroduction to Computing for Data Analysis\\nData Analytics for Business\\n\\n(iii)\\xa0Applied Data Science with Python Specialization (the University of Michigan, through Coursera)\\nIncludes the following courses, all taught using Python (you can audit most courses for free, some require the purchase of a verified certificate):\\n\\nIntroduction to Data Science in Python\\nApplied Plotting, Charting & Data Representation in Python\\nApplied Machine Learning in Python\\nApplied Text Mining in Python\\nApplied Social Network Analysis in Python\\n\\n(iv) Data Science Textbooks\\nLearning from a textbook provides a more refined and in-depth knowledge beyond what you get from online courses. This book provides a great introduction to data science and machine learning, with code included:\\xa0“Python Machine Learning” by Sebastian Raschka.\\n\\nThe author explains fundamental concepts in machine learning in a way that is very easy to follow. Also, the code is included, so you can actually use the code provided to practice and build your own models. I have personally found this book to be very useful in my journey as a data scientist. I would recommend this book to any data science aspirant. All that you need is basic linear algebra and programming skills to be able to understand the book.\\nThere are lots of other excellent data science textbooks out there such as “Python for Data Analysis” by Wes McKinney, “Applied Predictive Modeling” by Kuhn & Johnson, and “Data Mining: Practical Machine Learning Tools and Techniques” by Ian H. Witten, Eibe Frank & Mark A. Hall.\\n(v)\\xa0Network with other Data Science Aspirants\\nFrom my personal experience, I have learned a lot from weekly group conversations on various topics in data science and machine learning by teaming up with other data science aspirants. Network with other data science aspirants, share your code on GitHub, showcase your skills on LinkedIn. This will really help you to learn a lot of new concepts and tools within a short period of time. You also get exposed to new ways of doing things, as well as to new algorithms and technologies.\\n2) Understanding the theoretical foundations of data science is as important as hands-on data science skills.\\nData science is heavily math-intensive and requires knowledge in the following:\\n(i) Statistics and Probability\\n(ii) Multi-variable Calculus\\n(iii) Linear Algebra\\n(iv) Optimization and Operational Research\\nFind out more about math topics that you need to focus on from here:\\xa0Essential Math Skills for Machine Learning.\\nEven though packages such as Python’s sci-kit learn and R’s Caret package contain several tools for doing data science and building machine learning models, it is extremely important to understand the theoretical foundations of each method.\\n3) Avoid using machine learning models as blackbox tools.\\nA solid background in data science would enable a data scientist to build reliable predictive models. For example, before building a model, you may ask yourself:\\n(i) What are the predictor variables?\\n(ii) What is the target variable? Is my target variable discrete or continuous?\\n(iii) Should I use classification or regression analysis?\\n(iv) How do I handle missing values in my dataset?\\n(v) Should I use normalization or standardization when bringing variables to the same scale?\\n(vi) Should I use Principal Component Analysis or not?\\n(vii) How do I tune hyperparameters in my model?\\n(viii) How do I evaluate my model to detect biases in the dataset?\\n(ix) Should I use ensemble methods where I train using different models then perform an ensemble average, e.g., using classifiers such as SVM, KNN, Logistic Regression, then average over 3 models?\\n(x) How do I select the final model?\\nWhat makes the difference between a good and a bad machine learning model depends on one’s ability to understand all the details of the model, including knowledge about different hyperparameters and how these parameters can be tuned in order to obtain the model with the best performance. Using any machine learning model as a black box without fully understanding the intricacies of the model will lead to a falsified model.\\nIn summary, data science is one of the hottest fields nowadays. The digital revolution has created tons upon tons of data. Companies, industries, organizations, and the government are producing tons upon tons of data on a daily basis. The demand for high-skilled data scientists will only continue to grow. This is the right time to invest your time to master the fundamentals of data science. In doing so, beware of articles, books, or websites that tell you that you can learn data science in 4 weeks or in a month. Do not be in a rush. Take your time to master the fundamentals of data science.\\nOriginal. Reposted with permission.\\n\\xa0\\nRelated:\\n\\nA checklist to track your Data Science progress\\n10 Mistakes You Should Avoid as a Data Science Beginner\\nDon’t learn Machine Learning in 24 hours',\n",
       " 'comments\\nBy Omer Mahmood, Head of Cloud Customer Engineering, CPG & Travel at Google\\n\\xa0\\nIn my\\xa0last post, I talked about what it means to move machine learning (ML) models into production by introducing the concept of MLOps. This time we’re going to look at the opposite end of the\\xa0data science steps for ML\\xa0— data extraction and integration.\\n\\xa0\\nThe TL;DR\\n\\xa0\\nETL stands for\\xa0Extract-Transform-Load, it usually involves moving data from one or more sources, making some changes, and then loading it into a new single destination.\\n\\nIn most companies\\xa0data tends to be in silos, stored in various formats and is often inaccurate or inconsistent\\nThis situation is far from ideal if we want to be able to easily analyse and get\\xa0insights from that data or\\xa0use it\\xa0for data science\\n\\n\\xa0\\n🚣🏼 How we got here\\n\\xa0\\nMost ML algorithms require large amounts of training data in order to produce models that can make accurate predictions. They also require good quality training data, representative of the problem we are trying to solve.\\nTo reinforce this point there is a great example I came across, analogous to ‘Maslow’s hierarchy of needs’ that highlights the importance of data collection and storage as it relates to data science:\\n\\n\\nFigure 1: The Data Science Hierarchy of Needs Pyramid, SOURCE: “THE AI HIERARCHY OF NEEDS” MONICA ROGATI[1]\\n\\n\\xa0\\nAt the bottom of the pyramid is the basic need to gather the right data, in the right formats and systems, and in the right quantity.\\n\\n\\nAny application of AI and ML will only be as good as the quality of data collected.\\n\\n\\nSo, let’s say you’ve\\xa0framed your problem and determined that it’s a good fit for ML. You know what data you need, at least to start experimenting. But unfortunately it’s sitting in different systems and scattered across your organisation.\\nThe next step is to figure out how to bring that data together, transform it as needed, and then land it somewhere as a single integrated dataset. You can only begin to explore the data, carry out feature engineering, and model training once it is accessible — this is where our friendly acronym ETL comes into play!\\n\\xa0\\n🧪 How does it work?\\n\\xa0\\nTo make it a bit more concrete, let’s use a modern real world ETL example.\\nImagine you are an online retailer that uses a Customer Relationship Management (CRM) system such as SalesForce to keep track of your registered customers.\\nYou also use a payment processor such as Stripe to handle and store details of sales transactions made via your e-commerce website.\\nSuppose your goal is to improve your conversion rate by using data about what your customers purchased historically, to make better product recommendations when they are browsing your website.\\nYou could certainly use an ML model to power a recommendation engine to achieve this goal. But the challenge is that the data you need is sitting in two different systems. The solution in our case is to use an ETL process to extract, transform and combine them into a data warehouse:\\n\\n\\nFigure 2: The process of moving data from different sources to a warehouse using ETL. Illustration by the author.\\n\\n\\xa0\\nLet’s break down what’s happening in the diagram above:\\n1. Extract\\xa0— this part of the process involves retrieving data from our two sources, SalesForce and Stripe. Once the data has been retrieved, the ETL tool will load it into a staging area in preparation for the next step.\\n2. Transform\\xa0— this is a critical step, because it handles the specifics of how our data will be integrated. Any cleansing, reformatting, deduplication, and blending of data happens here before it can move further down the pipeline.\\nIn our case, let’s say in one system a customer record is stored with the name “K. Reeves”, in another system that same customer record is stored against the name “Keanu Reeves”.\\nAssume we know it’s the same customer (based on their shipping address), but the system still needs to reconcile the two, so we don’t end up with duplicate records.\\n➡️ ETL frameworks and tools provide us with the logic needed to automate this sort of transformation, and can cater for many other scenarios too.\\n3. Load\\xa0— involves successfully inserting the incoming data into the target database, data store, or in our case a data warehouse.\\nSo there you have it, we have collected our data, integrated it using an ETL pipeline and loaded it somewhere that is accessible for data science.\\n\\xa0\\n📌 Side note\\xa0📌\\nETL vs. ELT\\nYou might have also come across the term ‘ELT’. Extract, load, and transform (ELT) differs from ETL solely in where the transformation takes place. In the ELT process, the data transformation occurs in the destination data store.\\nThis can simplify the architecture by removing what is sometimes a separate or intermediate staging system that hosts the data transformation. The other advantage is that you can benefit from the additional scale and compute performance usually present in destinations such as cloud data warehouses.\\n📌\\xa0Side note\\xa0📌\\n\\xa0\\n\\xa0\\n🦀 Common challenges\\n\\xa0\\nOK, all this ETL stuff sounds pretty simple, right? Here are some ‘gotchas’ to look out for:\\n\\xa0\\n☄️ Scaling\\n\\xa0\\nThe amount of data businesses produce is only expected to grow — 175 Zettabytes by 2025 according to a report by IDC[2]. So you should ensure that the ETL tool you choose has the ability to scale to not just your current but also future needs. You may move data in batches now, but will that always be the case? How many jobs can you run in parallel?\\nMoving to the cloud is a pretty safe bet if you want to future-proof your ETL processes\\xa0— by having access to theoretically limitless scalability of storage and compute while also reducing your IT capital expenditure.\\n\\xa0\\n🧮 Data Accuracy\\n\\xa0\\nAnother big ETL challenge is ensuring that the data you transform is accurate and complete. Manual coding and changes or failure to plan and test before running an ETL job can sometimes introduce errors, including loading duplicates, missing data, and other issues.\\nAn ETL tool will definitely reduce the need for hand-coding and help cut down on errors. Data accuracy testing can help spot inconsistencies and duplicates, and monitoring features can help identify instances where you are dealing with incompatible data types and other data management issues.\\n\\xa0\\n🍱 Diversity of Data Sources\\n\\xa0\\nData is growing in volume. But more importantly, it’s growing in complexity. One enterprise could be handling diverse data from hundreds — or even thousands — of data sources. These can include structured and semi-structured sources, real-time sources, flat files, CSVs, object buckets, streaming sources, and whatever new comes along.\\nSome of this data is best transformed in batches, while for others, streaming, continuous data transformation works better.\\nHaving a strategy for how you intend to cope with different data sources is key. Some modern ETL tools can offer support for a wide variety, including batch and streaming in one place.\\n\\xa0\\n👷🏾\\u200d♀️ So how do I get started?\\n\\xa0\\nAt this point you should have a good idea why and when you might need to use ETL in your data science workflow. We also covered common challenges to look out for as you begin thinking about your ETL processes.\\nI’ll close with a simple methodology for choosing an ETL tool, and some other useful resources.\\n\\xa0\\n🤷🏽\\u200d♀️ Which ETL tool should I use, and when?\\n\\xa0\\nSo we understand what happens during ETL, but what does it mean in more practical terms?\\nYou will need to design an ETL pipeline that explicitly describes:\\n\\nWhat data sources to extract from and how to connect to them\\nWhat transformations to carry out on the data once you have it, and finally\\nWhere to load the data once the pipeline is complete\\n\\nETL pipelines can be expressed using a code based framework, or a more popular choice these days is to use ETL tools that provide a ‘drag and drop’ user interface that lets you define the steps in your pipeline in a visual way.\\nOnce you’ve implemented your ETL pipeline, it typically needs to run somewhere i.e. using an ETL tool that will execute your pipeline, and an environment that will provide the resources required to temporarily store and transform your data.\\nI have tried to simplify the decision-making steps for you in the diagram below (click to zoom in):\\n\\n\\nFigure 3: Which ETL tool to use and when. Illustration by the author.\\n\\n\\xa0\\nNB. This decision tree is by no means an exhaustive list of either; the decisions you will need to make, frameworks or products available.\\nIndeed for every intermediate ETL step, there are dozens of open source and proprietary offerings. Ranging from orchestration to scheduling — we’re not going to be able to cover everything here.\\nThe aim of this post was to serve as a springboard into the world of ETL! Good luck on your data integration journey! 😀\\n\\xa0\\n💡 Useful resources and further reading\\n\\xa0\\nLinks\\n\\nData Preparation and Feature Engineering for Machine Learning\\nGartner — Data Integration Tools Reviews and Ratings\\n\\nBooks\\n\\nThe Data Warehouse ETL Toolkit: Practical Techniques for Extracting, Cleaning, Conforming, and Delivering Data, Wiley, Authors: Ralph Kimball, Joe Caserta\\nStreaming Systems: The What, Where, When, and How of Large-Scale Data Processing, O’Reilly, Authors: Tyler Akidau, Slava Chernyak, Reuven Lax\\n\\nData Agnostic ETL tools\\n\\nFivetran\\nStitch\\n\\n\\xa0\\n📇 References\\n\\xa0\\n[1] The AI Hierarchy of Needs, Monica Rogati\\nhttps://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007\\n[2] 175 Zettabytes By 2025, Forbes, Tom Coughlin\\nhttps://www.forbes.com/sites/tomcoughlin/2018/11/27/175-zettabytes-by-2025/?sh=6a5d2e7a5459\\n\\xa0\\nBio: Omer Mahmood is Head of Cloud Customer Engineering, CPG & Travel at Google.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nIntroducing dbt, the ETL and ELT Disrupter\\nThe Role of the Data Engineer is Changing\\nWhy the Future of ETL Is Not ELT, But EL(T)',\n",
       " \"By Matthew Mayo, KDnuggets.\\ncomments\\nI readily admit that I'm biased toward Python. This isn't intentional — such is the case with many biases — but coming from a computer science background and having been programming since a very young age, I have naturally tended towards general purpose programming languages (Java, C, C++, Python, etc.). This is the major reason that Python books and resources are at the forefront of my radar, recommendations, and reviews. \\nObviously, however, not all data scientists are in this same position, given that there are innumerable paths to data science. Given that, and since R is powerful and popular programming language for a large swath of data scientists, today let's take a look at a book which uses R as a tool to implement solutions to data science problems.\\nR is designed specifically for statistical computing, in juxtaposition to general purpose languages, the trade-off being that the relative lack of generality means better optimization for specialized scenarios. R's optimization for statistical computing is a big reason why it enjoys such high levels of adoption in data science and analytics.\\nText analytics — like all applications and sub-genres of natural language processing — is continually reaching increasing heights of importance for data science, data scientists, and a variety of industries. As R (and its opinionated collection of packages designed for data science, the tidyverse) is an established environment for statistical computing utilized by data scientists, fully capable of performing text analytics, today we will look at Text Mining for R: A Tidy Approach.\\n\\xa0\\n\\n\\xa0\\nWritten by Julia Silge and David Robinson, this book endeavors to cover the following major topics, taken from the outline in the book's preface:\\n\\n\\nWe start by introducing the tidy text format, and some of the ways dplyr, tidyr, and tidytext allow informative analyses of this structure.\\nText won’t be tidy at all stages of an analysis, and it is important to be able to convert back and forth between tidy and non-tidy formats.\\nWe conclude with several case studies that bring together multiple tidy text mining approaches we’ve learned.\\n\\n\\n\\xa0\\nFor a more fleshed out list of topics treated within, the book's table of contents are as follows:\\n\\nThe tidy text format\\nSentiment analysis with tidy data\\nAnalyzing word and document frequency: tf-idf\\nRelationships between words: n-grams and correlations\\nConverting to and from non-tidy formats\\nTopic modeling\\nCase study: comparing Twitter archives\\nCase study: mining NASA metadata\\nCase study: analyzing usenet text\\nReferences\\n\\n\\n\\xa0\\nText Mining for R: A Tidy Approach is code-heavy and seems to explain concepts well. The focus is on practical implementation, which should be of no surprise given the book's title, and to an R novice it seems to do a very good job. I have not followed along to the entire book, but I did read the first 2 chapters and feel that I got out of it what was intended.\\nThe book is also very transparent as to what it is not:\\n\\nThis book serves as an introduction to the tidy text mining framework along with a collection of examples, but it is far from a complete exploration of natural language processing. The CRAN Task View on Natural Language Processing provides details on other ways to use R for computational linguistics. There are several areas that you may want to explore in more detail according to your needs.\\n\\nClustering, classification, and prediction\\nWord embedding\\nMore complex tokenization\\nLanguages other than English\\n\\n\\xa0\\nAll in all, this seems to strike a good balance. If you aren't familiar with NLP to any degree, regardless as to your familiarity with the tidyverse, jumping into the deep end with complex tokenization and using word embeddings to solve problems probably isn't a good idea. The starting point really should be what this book lays out, and what it lays out well.\\nIt's at this point I should tell you that this is not actually an eBook; Text Mining with R is an online version of the print book. You can read the book online, and you can also buy physical copies from Amazon.\\n\\n\\xa0\\nWhether you are interested in applying text mining to your projects and currently reside in the world of R, or you are looking to venture into using R and need some direction in doing so, check out Text Mining for R: A Tidy Approach. I'm certain you will find it beneficial.\\n\\xa0\\nRelated:\\n\\nStatistics with Julia: The Free eBook\\nCausal Inference: The Free eBook\\nData Mining and Machine Learning: Fundamental Concepts and Algorithms: The Free eBook\",\n",
       " 'comments\\nBy Kaveh Bakhtiyari, PhD Candidate in Artificial Intelligence, Data Scientist at SSENSE\\nThe data science team at SSENSE usually builds very complex tools and dashboards. On the other hand, their maintenance was a challenge for the team. It has been more than a year since the SSENSE data science team has been using\\xa0Streamlit\\xa0actively. Before employing Streamlit, we were using Dash, Flask, R Shiny, etc. to build our tools and make them available to stakeholders within the company. In October 2019, we started to evaluate the potential power of Streamlit for our projects by understanding its benefits and how to integrate it into our data science infrastructure. At the end of 2019, we began some pilot projects on Streamlit instead of Flask and Dash.\\nAfter the evaluation period of Streamlit, we quickly realized that it had a lot of potentials and that it could increase development pace, and decrease the maintenance effort significantly. Besides all the cool features and being easy to work with, Streamlit does not provide the customized behaviors, events, and UI designs that you could get from other web development libraries such as Flask. And eventually, because of the same limitations, it has been much easier to develop clean apps and maintain them easily in the long term. Its uniform UI was also a positive point from my point of view. Firstly, it is clear, clean, and responsive. Secondly, all team members can build tools with uniform designs. But still, how can we provide such custom elements which we had in our Flask applications? Well, the short answer is that it is not quite possible, but we can use some tricks and tips, which can help you to customize more on what you are designing.\\nToday, I am going to talk about a few tips that I learned within more than a year of using Streamlit, that you can also use to unleash your powerful DS/AI/ML (whatever they may be) applications.\\n\\n\\nStreamlit is an active open-source project and the community is providing new updates frequently. I personally have bookmarked their\\xa0Changelog page\\xa0to keep track of new updates and features. Some of what we are discussing today are not natively supported in\\xa0Streamlit (0.82.0), which may not be the case in the future.\\n\\n\\n\\xa0\\nPage Config\\n\\xa0\\nThis feature was initially introduced in the beta version, and it was moved to the Streamlit namespace in\\xa0version 0.70.0. This cool feature allows you to set the page title, favicon, page layout mode, and sidebar state.\\n\\nBy default, Streamlit sets the page title as the original python file name, with Streamlit favicon. Having this line of code, you can customize your page title, which is very beneficial if your users bookmark your apps. Then favicon allows them to differentiate the apps if they have many apps open in multiple browser tabs. Setting the layout and initial state of the sidebar can also run your app in the way you desire.\\nBefore introducing this functionality, some of these features could only be possible by injecting CSS into the page. For example, if you wanted to make a widescreen, you could do the following:\\n\\n\\n\\nThis line must be the first Streamlit command on your page, and it can only be set once. Regardless of what you set for the layout (either centered or wide), users have control over them in the settings.\\n\\n\\n\\xa0\\nEmpty component\\n\\xa0\\nThere are multiple occasions when you want to generate new elements on the page, or you want to replace an existing text or element with another. This is possible using\\xa0st.empty(). This method creates an empty placeholder on your page, and moving forward you can replace it with any object or text that you want.\\n\\nThe above code initially creates a placeholder on your page, then it writes\\xa0“this is a sample text.” in that same place, and after that, it replaces it with an input number object.\\nThis is very useful to have dynamic objects on the page, or simply showing the progress of some calculations such as progress percentage.\\n\\xa0\\nQuery Strings\\n\\xa0\\nSetting and retrieving query strings in your Streamlit apps is an experimental feature at the moment. I hope that it will be moved into the main namespace in the future since I personally love this feature. If you have wondered why we need query strings in Streamlit, you are not alone.\\nWhen you set your customized inputs in query strings, it makes it possible for the users to share the links with the exact same parameters that they had. Otherwise, they have to enter their parameters as well.\\nThe other use-case that I personally use is to share information between different Streamlit apps. In our team, each data scientist may work on different projects, and we may need to redirect users from one app to another. When we provide the link to the user to navigate to the other Streamlit app, we want to make sure that the user’s experience is as seamless as possible. Therefore, we pass the required parameters to the new app so that it loads with the data and analysis they are looking for.\\nFor example, a few of our tools are related to the products that we have on the website (what a surprise). When they are viewing some analysis on Product 1 on App 1, we want to make sure that once they go to App 2 to get more details or different analysis, it automatically shows Product 1, and the user does not need to reenter the information.\\n\\n\\xa0\\nRunning Streamlit in a Subfolder\\n\\xa0\\nThere are scenarios in data science projects that we may need to have our Streamlit apps in a subfolder. In this case, since Streamlit runs the apps from a subfolder, the app does not have access to the libraries in the parent folders. In order to overcome this problem, we may need to either have our Streamlit main app file in the project root or add the root folder into the system path at the beginning of our Streamlit apps.\\n\\n\\xa0\\nSessions\\n\\xa0\\nStreamlit is a session-based application. It means that once a user comes to the app, Streamlit assigns him/her a session ID, and other consecutive actions and data transfers are associated with that session. Because of that when you have a process, it won’t affect the other simultaneous users unless you use caching. We will discuss\\xa0Caching\\xa0later.\\nBy default, you do not have standard access to the Session controls in Streamlit, and it is not documented officially yet, and it is used for internal purposes only. However, you can still access them and make some benefits by using them.\\nStreamlit apps are developed in a script-like format. It means that every interaction with the app will trigger the whole code to re-run from start to bottom. This makes Streamlit extremely easy to work with, but at the same time, very tricky to control consecutive events since there is no event handling capability for the developers.\\nAssume that you have a button (st.button) to start a process, and in the resulting screen, you want to give the user some interactive options to work with, for example, another checkbox, radio button, or simply another button. In this case, when you click on the first button (let’s call it\\xa0button_run) becomes\\xa0True\\xa0when it reruns the whole code. There is nothing wrong, and the app runs smoothly.\\n\\n\\n\\n\\xa0\\nNow, on the resulting page, there is another button (let’s call it\\xa0button_filter) to filter the results. If you now click on the second button (button_filter), its value becomes\\xa0True, and Streamlit runs the whole code again. But the problem is that now the first button (button_run) has become\\xa0False\\xa0because we did not click on that. In this case, when Streamlit reruns the whole code, there is the assumption that\\xa0button_run\\xa0is not clicked, and\\xa0button_filter\\xa0is clicked. And it does not remember that\\xa0button_run\\xa0was previously clicked. Therefore,\\xa0button_filter\\xa0clicked code will never be executed, because\\xa0button_filter\\xa0itself was the result of the first button,\\xa0button_runclick.\\n\\n\\xa0\\nIn such cases, we should register the events, so that Streamlit can remember when a user clicks on the first button, and once the next button is clicked, it can understand that these are two consecutive actions and both buttons should be considered as clicked.\\nYou may think that, well, we can save that information in a DB or temporary text file. It is possible, but how do you differentiate the potential different users?\\nStreamlit has a built-in undocumented Session object that can store some temporary information for every user. In this case, when a user clicks on\\xa0button_run, we store the clicked event in the Session, and once\\xa0button_filter\\xa0is clicked, we can check if\\xa0button_run\\xa0was previously clicked to control the correct flow of data.\\nHere is the session class that you can include in your app:\\n\\nOnce you have the session class added, you can use the session to store and retrieve the information.\\n\\n\\xa0\\nSQLAlchemy\\n\\xa0\\nSQLAlchemy is one of the standard popular libraries to connect to multiple types of databases such as SQLite, MySQL, etc. SQLAlchemy can be used for multiple platforms as desktop apps, web apps, or even mobile apps. If you have used this library before, you have realized that it is pretty simple, but when it comes to web development it may become a bit tricky. The main challenge of using this library for web applications is to control the number of database connections.\\nFor that purpose, we have separate libraries for Flask (sqlalchemy-flask) and Tornado (sqlalchemy-tornado) which developers can use without any worry. But to my knowledge, we do not have any specific library for Streamlit. Since Streamlit is built upon Tornado, maybe we can use the tornado version, but I personally did not test that.\\nAs you remember, Streamlit is session-based, which means that it runs a separate instance for every user. SQLAlchemy here is no exception. If you’re not careful, Streamlit will create a database connection for every user and maybe for every interaction. Depending on your database, your connections may get rejected if there are so many active connections available. As a result, python may end up with some strange error such as “double free or corruption” and crash your application.\\nIn the Streamlit forum, there is a suggestion of caching the connection, which works well on SQLLite, but not very well on MySQL for example. When you cache your database connection, it won’t be open for an unlimited time, so that you may solve that issue with\\xa0ttl. In this case, you can make sure that your connection object has expired before hitting a wall on the database side because the connection was already killed. Theoretically, this works fine if you have a very limited number of simultaneous users.\\nThe main problem with caching the connection starts when two users run the code which caches the object at the same time. And at the end, the cached connection may not be the right one, but the expired one since there were two connections created at the same time, but only one was cached.\\nSQLAlchamy has an object called Session, in which we can create our database connections (engines) and execute our SQL queries. This would check if the new connection is already existing in the pool, and if it is existing, it won’t create a new connection to prevent the database connection saturation issue. In this case, you do not need to use Streamlit caching anymore to store your database connection. The following code snippet will help you understand how to use Session to connect to MySQL.\\n\\nRemember that, prior to using Session in SQLAlchemy, if you were using engine only, you had to return\\xa0conn = engine.connect()\\xa0instead of the session, and you could use\\xa0df = pd.read_sql(query, conn)\\xa0to run the query. However, these methods are not working on SQLAlchemy Sessions.\\n\\xa0\\nCaching\\n\\xa0\\nStreamlit has very thorough, useful documentation on Caching, and honestly, it is one of its most useful features. Not using or misusing it can hugely impact the app performance and load/running time. I do not want to go through the details of caching which is already available in\\xa0Streamlit documentation\\xa0but only mentioning a few tips and findings.\\n\\xa0\\nApp Wide Access\\n\\xa0\\nUnlike Session objects, cached objects are app-wide accessible. It means that once you cache information, it is accessible to all users of the app. So it is important not to cache user-specific settings and data, and instead, we can use Session as we discussed earlier.\\n\\xa0\\nCaching Parameters\\n\\xa0\\nCaching mechanism has few parameters which can control how an object must be cached.\\n\\nttl\\xa0<float, None>:\\xa0This stands for Time-to-Live and sets how long a cached object must be alive. This expiry is set in seconds.\\nmax_entries\\xa0<int, None>:\\xa0Once you start calling a function with different parameters, it starts caching all those variations, and in a short time, it can be a huge amount of cached data. This parameter can set how many variations of a function can be cached, and the old ones will be deleted. This controls and limits the amount of memory consumed.\\npersistent\\xa0<bool>:\\xa0It is a boolean parameter to set if the cached data must be stored in a hard drive or memory. Just remember that, once you set it to True, Streamlit is pickling the object and storing it on the hard drive, and not all objects (such as SQLAlchamy database connection) can be pickled. So you may get an error for some persistent caching functions.\\nallow_output_mutation\\xa0<bool>:\\xa0Once the output of a function is cached, if you change the output (mutate), the results will be stored in the cached object and as I mentioned earlier, this is accessible to all users. So the best practice is to avoid changing the cached object. But still, there are some cases where you need to change the cached object directly. In this case, this parameter would allow Streamlit to mutate the cached object.\\nsuppress_st_warning\\xa0<bool>:\\xa0Sometimes Streamlit raises some warnings to the user/developer so that they are aware of some consequences of caching. Setting this to False will stop those warnings.\\nshow_spinner\\xa0<bool>:\\xa0Each time that Streamlit runs functions that are supposed to be cached, you will see a message on your UI saying “Running function_name”. It may not bother you that much unless you have lots of functions. Then you will see all those kinds of messages on your UI. Setting this parameter to False will prevent showing those messages.\\n\\n\\nThe above code only caches the results for 60 seconds, and it only keeps the last 20 variations of this function. It also does not show any warning, does not show you any message on Streamlit UI when running this function.\\nSince we set\\xa0allow_output_mutation\\xa0to\\xa0False, the following code is not allowed, and we can not update (mutate) the result of the function.\\n\\n\\xa0\\nClearing Cache\\n\\xa0\\nThere are some cases that you may need to clear the cache programmatically. Clearing all cached data is manually possible through the hamburger menu at the top right of the Streamlit apps, but if you want to do it programmatically, you can use the following undocumented method.\\n\\n\\xa0\\nSQLAlchemy Session / Scoped Session\\n\\xa0\\nNow that you could successfully connect to the database using SQLAlchemy Session, Scoped Session, and Pooling, you may need to cache your sessions or the functions that are using the database connection. As discussed earlier, since we are using Pool and Scoped Session, we may not need to cache the connection, but we may still need to cache our functions. Below, we are suggesting two recommendations on caching the functions that are using sessions.\\nThe following example would use\\xa0hash_funcs\\xa0to identify which parameter of Session must be monitored for hashing.\\n\\nIf the above example is not working, for example in the case of using\\xa0scoped_session, you can simply ask Streamlit to ignore hashing session as below:\\n\\n\\xa0\\nUI Hacks\\n\\xa0\\n\\n\\n\\nThe simplicity of Streamlit is because you do not need to deal with UI, and it comes with pre-built-in responsive UI elements which will be placed elegantly on your page. Even though in the recent versions, they have provided new beta updates which enable you to create columns and arrange your elements in them, there is not much customization to do with its UI.\\nWhen I deploy my apps, there is a wide range of users in the company to work with them. I heavily use caching mechanisms to control the performance and speed of my apps. Some of my functions take a few minutes to run, and I use a caching mechanism to make sure that other users won’t wait again for the same request and will have a high-performance experience with the app. But, if a user clicks on that hamburger menu button at the top right, and selects “Clear Cache’’, it can hugely impact the performance of the app for the other users, until the function caches the results again. Or for example, some of my apps are designed to be shown the best in the wide mode, and if a user selects the “center” mode, it can affect how my app looks.\\nBesides all those that can directly affect my app, there are other options in the hamburger menu that a normal user may not need to have access to. For example, access to the Streamlit Github, documentation, etc.\\nThere is a\\xa0proposed idea on Streamlit Github\\xa0to limit those hamburger menu options once the app is deployed, but until today, this issue is still open, and we can not manage them directly. Therefore, I came with my CSS solution to solve this issue.\\n\\n\\n\\nIn my proposed solution, you can remove (hide) the Streamlit footer, and control the items in the hamburger menu. You simply need to inject the following CSS into your application using\\xa0st.markdown\\xa0and allowing “unsafe” HTML codes.\\n\\nThe numbers mentioned above in\\xa0li:nth-of-type(n)\\xa0are referring to the item element in the hamburger menu and their order may change in the future updates of Streamlit.\\n\\n\\n\\nAlso, currently, there is an option in the hamburger menu (3rd item) called “Deploy this app”. This item is shown only if the app is accessed via a loopback local IP address (either\\xa0localhost\\xa0or\\xa0127.0.0.1). If you access your app through your LAN/WAN IP address, this item will not be shown.\\n\\xa0\\nRecord a Screencast\\n\\xa0\\nThis feature was introduced in\\xa0version 0.55.0, and I was personally thrilled by this feature which would allow us to record our apps for training and presentation purposes. Soon, we realized that this feature is not working for the other users accessing our Streamlit apps, and they get the following message upon clicking on that option.\\n\\n\\n\\nBecause of the privacy restrictions implemented and imposed by the browsers, this feature works on the following conditions only:\\n\\nOnly on recent versions of Chrome, Firefox, and Edge\\nAccessing either on\\xa0localhost\\xa0or\\xa0127.0.0.1\\nIf it is not being accessed locally, it must be behind an SSL certificate (https)\\n\\nIf you are serving your apps behind a proxy — such as\\xa0Nginx\\xa0— and you are aiming to use this feature, make sure that it is secured with an SSL certificate. Currently, Streamlit does not natively support SSL, but it can be deployed behind a proxy with an SSL certificate.\\n\\xa0\\nComponents\\n\\xa0\\nSince the introduction of Streamlit components, developers have started building amazing components which can be served on Streamlit apps. If you would like, you can build your own components using Streamlit Component API. Streamlit has also a component gallery that presents some of the useful and interesting components which are publicly available. Among them, I have selected a few of them that I use to build amazing apps in SSENSE.\\n\\xa0\\nACE Editor\\n\\xa0\\nThis editor is providing a color-coded editor for different programming languages. I personally use a lot of JSON data in my apps, and I use this editor to view and edit my JSON content. It is amazing since it can also capture my formatting structures and errors.\\n\\n\\nhttps://github.com/okld/streamlit-ace\\n\\n\\xa0\\nIf you are tired of Streamlit standard multi-line text box, this component can be a very good alternative.\\n\\xa0\\nAg-Grid\\n\\xa0\\nStreamlit can handle data frames, and it can show them in a table-based format either using\\xa0st.write\\xa0or\\xa0st.dataframe. However, by default, Streamlit does not provide customized controllers on the presentation of your data frame except sorting by clicking on the column names.\\nAg-Grid is a grid component that can be imported into Streamlit. Using this component, not only can you present your data frame, but also include links, images, checkboxes, etc into your grid cells as well as filtering the data, searching, aggregate, and grouping them.\\n\\n\\nhttps://github.com/PablocFonseca/streamlit-aggrid\\n\\n\\xa0\\nIf you are dealing with showing data frames a lot, maybe it is time to give Ag-Grid a try to see its huge potential in your applications.\\n\\xa0\\nLottie Animations\\n\\xa0\\nLast, but not least, in my list of components is Lottie Animations. If you check\\xa0lottiefiles.com, you will see thousands of vector-based animations in multiple formats such as JSON, which can be placed in your apps. This component would allow you to serve those Lottie animations by simply giving its JSON file.\\n\\n\\nhttps://lottiefiles.com/968-loading\\n\\n\\xa0\\nI personally use these animations to show beautifully designed spinners while I am loading or calculating stuff. These animations will give a more vibrant and dynamic look to your next data science project.\\n\\xa0\\nFinal Words\\n\\xa0\\nHere, I presented some tips and tricks on how to develop Streamlit applications. Some of these tricks may become natively available in the future versions of Streamlit, so that we may not need to do the hacks, or on the other hand, they may come with some updates to prevent our hacks. Who knows, but we can enjoy them for now, and hope for new amazing features in Streamlit.\\nI would also like to thank the Streamlit community for building such an amazing tool.\\n\\xa0\\nBio: Kaveh Bakhtiyari is a PhD Candidate in Artificial Intelligence and a Data Scientist at SSENSE.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nDeploying Streamlit Apps Using Streamlit Sharing\\nTopic Modeling with Streamlit\\nDeploying Secure and Scalable Streamlit Apps on AWS with Docker Swarm, Traefik and Keycloak',\n",
       " 'comments\\nBy Irfan Alghani Khalid, Computer Science Student\\n\\n\\nPhoto by\\xa0Brian Suh\\xa0on\\xa0Unsplash\\n\\n\\xa0\\nIntroduction\\n\\xa0\\nImage classification is a task where we want to predict which class belongs to an image. This task is difficult because of the image representation. If we flatten the image, it will create a long one-dimensional vector. Also, that representation will lose the neighbor information. Therefore, we need deep learning for extracting features and predict the result.\\nSometimes, Building a deep learning model can become a difficult task. Although we create a base model for image classification, we need to spend lots of time creating the code. We have to prepare code for preparing the data, training the model, testing the model, and deploy it to the server. And that’s where the Flash comes in!\\nFlash is a high-level deep learning framework for fast building, training, and testing the deep learning model. Flash is based on the PyTorch framework. So if you know PyTorch, you will be familiar with Flash easily.\\nIn comparison with PyTorch and Lighting, Flash is easy to use but not so flexible as the previous libraries. If you want to build a more complex model, you can use Lightning or straight to the PyTorch.\\n\\n\\nCreated by the author.\\n\\n\\xa0\\nWith Flash, you can build your deep learning model in few lines of code! So, if you are new to deep learning, don’t be afraid. Flash can help you to build a deep learning model without getting confused because of the code.\\nThis article will show you how to build an image classifier using Flash. Without further, let’s get started!\\n\\xa0\\nImplementation\\n\\xa0\\nInstall the library\\n\\xa0\\nFor installing the library, you can use the pip command like this:\\n\\npip install lightning-flash\\n\\n\\nIf the command doesn’t work, you can install the library by using its GitHub repository. The command looks like this:\\n\\npip install git+https://github.com/PyTorchLightning/lightning-flash.git\\n\\n\\nAfter we can download the package successfully, now let’s load the libraries. We also set the seed with the number 42. Here is the code for doing that:\\n\\n\\n\\n\\xa0\\nDownload the data\\n\\xa0\\nAfter we install the library, now let’s get the data. For demonstration, we will use the dataset called Cat and Dog dataset.\\nThis dataset contains images that are divided into two classes. The classes are cat and dog. To access the dataset, you can find this dataset at Kaggle. You can access the dataset\\xa0here.\\n\\n\\nCaptured by the author.\\n\\n\\xa0\\nLoad the data\\n\\xa0\\nAfter we download the data, now let’s load the dataset into an object. We will use the from_folders method for putting our data into the ImageClassification object. Here is the code for doing that:\\n\\n\\n\\n\\xa0\\nLoad the model\\n\\xa0\\nAfter we load the data, the next step is to load the model. Because we will not create our own architecture from scratch, we will use the pre-trained model based on existing convolutional neural network architecture.\\nWe will use the ResNet-50 model that has already pretrained. Also, We set the number of classes based on the dataset. Here is the code for doing that:\\n\\n\\n\\n\\xa0\\nTrain the model\\n\\xa0\\nAfter we load the model, now let’s train the model. We need to initialize the Trainer object first. We will train the model in 3 epochs. Also, we enable the GPU to train the model. Here is the code for doing that:\\n\\n\\n\\nAfter we initialize the object, now let’s train the model. To train the model, we can use a function called finetune. Inside the function, we set the model and the data. Also, we set the training strategy to freeze, where we don’t want to train the feature extractor. In other words, we train the classifier section only.\\nHere is the code for doing that:\\n\\n\\n\\nAnd here is the evaluation result:\\n\\n\\nCaptured by the author.\\n\\n\\xa0\\nAs you can see from the result, our model has achieved around 97% of accuracy. That’s a good one! Now let’s test the model on several new data.\\n\\xa0\\nTest the model\\n\\xa0\\nWe will use the sample data that have not been trained on the model. Here are the samples that we will test to the model:\\n\\n\\n\\nTo test the model, we can use the predict method from the flash library. Here is the code for doing that:\\n\\n\\n\\nAs you can see from the result above, the model has predicted the samples with correct labels. That’s nice! Now let’s save the model for later use.\\n\\xa0\\nSave the model\\n\\xa0\\nNow we have trained and tested the model. Let’s save the model using the save_checkpoint method. Here is the code for doing that:\\n\\n\\n\\nIf you want to load the model on the other code, you can use the load_from_checkpoint method. Here is the code for doing that:\\n\\n\\n\\n\\xa0\\nFinal Remarks\\n\\xa0\\nWell done! Now you have learned how to build an image classifier using Flash. As I’ve stated from the beginning, it takes only a few lines of code! How cool is that?\\nI hope this article can help you to build your own deep learning model on your own case. And I hope you can take a step to learn PyTorch if you want to implement a more complex model.\\nIf you are interested in my article, you can follow me on\\xa0Medium. I will publish articles related to data science and machine learning. Also, if you have any questions or want to say hi, you can connect with me on\\xa0LinkedIn.\\nThank you for reading my article!\\n\\xa0\\nBio: Irfan Alghani Khalid is a Computer Science Student @ IPB University, interested in Data Science, Machine Learning, and Open Source.\\nOriginal. Reposted with permission.\\nRelated:\\n\\nHigh Performance Deep Learning, Part 1\\nFine-Tuning Transformer Model for Invoice Recognition\\nDeep Learning Is Becoming Overused',\n",
       " 'comments\\nBy Pronojit Saha and Dr. Arnab Bose, Abzooba\\n\\n\\nFig 1: ML Workflow (Image from martinfowler.com, 2019)\\n\\n\\xa0\\nBackground\\n\\xa0\\nML models are driving some of the most important decisions for businesses. As such it is important that these models remain relevant in the context of the most recent data, once deployed into production. A model may go out of context if there is data skew i.e. data distribution may have changed in production from what was used during training. It may also be that a feature becomes unavailable in production data or that the model may no longer be relevant as the real-world environment might have changed (e.g. Covid19) or further and more simply, the user behavior may have changed. Monitoring the changes in model’s behavior and the characteristics of the most recent data used at inference is thus of utmost importance. This ensures that the model remains relevant and/or true to the desired performance as promised during the model training phase.\\nAn instance of such a model monitoring framework is illustrated in Fig 2 below. The objective is to track models on various metrics, the details of which we will get into the next sections. But first, let us understand the motivation of a model monitoring framework.\\n\\n\\xa0\\n\\n\\nFig 2: Model Monitoring Framework Illustrated (Image by author)\\n\\n\\xa0\\n\\xa0\\nMotivation\\n\\xa0\\nFeedback loops play an important role in all aspects of life as well as business. Feedback loops are simple to understand: you produce something, measure information on the production, and use that information to improve production. It’s a constant cycle of monitoring and improvement. Anything that has measurable information and room for improvement can incorporate a feedback loop and ML models can certainly benefit from them.\\nA typical ML workflow includes steps like data ingestion, pre-processing, model building & evaluation, and finally deployment. However, this lacks one key aspect i.e. feedback. The primary motivation of any “model monitoring” framework thus is to create this all-important feedback loop post-deployment back to the model building phase (as depicted in Fig 1). This helps the ML model to constantly improve itself by deciding to either update the model or continue with the existing model. To enable this decision the framework should track & report various model metrics (details in “Metrics” section later) under two possible scenarios described below.\\n\\nScenario I: The training data is available and the framework computes the said model metrics both on training data and production (inference) data post-deployment and compares to make a decision.\\nScenario II: The training data is not available and the framework computes the said model metrics based only on the data that is available post-deployment.\\n\\nThe following table lists the inputs required by the model monitoring framework to generate the said metrics, under the two scenarios.\\n\\n\\xa0\\nBased on which of the two scenarios is applicable, metrics highlighted in the next section are computed to decide if a model in production needs an update or some other interventions.\\n\\xa0\\nMetrics\\n\\xa0\\nA proposed model monitoring metrics stack is given in Fig 3 below. It defines three broad types of metrics based on the dependency of the metric on data and/or ML model. A monitoring framework should ideally consist of one or two metrics from all three categories, but if there are tradeoff then one may build up from the base i.e. starting with operations metrics and then building up with the maturity of the model. Further, operations metrics should be monitored at a more real time level or at-least daily where stability and performance can be at a weekly or even a larger time frame depending on the domain & business scenario.\\n\\n\\nFig 3: Model Monitoring Metrics Stack (Image by author)\\n\\n\\xa0\\n\\xa0\\n1. Stability Metrics\\xa0— These metrics help us to capture two types of data distribution shifts:\\na)\\xa0Prior Probability Shift\\xa0— Captures the distribution shift of the\\xa0predicted outputs and/or dependent variable\\xa0between either the training data and production data (scenario I) or various time frames of the production data (scenario II). Examples of these metrics include Population Stability Index (PSI), Divergence Index (Concept Shift), Error Statistic (details & definition to follow in next article of this series)\\nb)\\xa0Covariate Shift\\xa0— Captures the distribution shift of each\\xa0independent variable\\xa0between either the training data and production data (scenario I) or various time frames of the production data (scenario II), as applicable. Examples of these metrics include Characteristic Stability Index (CSI) & Novelty Index (details & definition to follow in the next article of this series)\\n\\xa0\\n2. Performance Metrics\\xa0— These metrics help us to detect a\\xa0concept shift\\xa0in data i.e. identify whether the relation between independent & dependent variables has changed (e.g. post-COVID the way users purchase during festivals may have changed). They do so by examining how good or bad the existing deployed model is performing viz-a-viz when it was trained (scenario I) or during a previous time frame post-deployment (scenario II). Accordingly, a decision can be taken to re-work the deployed model or not. Examples of these metrics include,\\na)\\xa0Project Metrics\\xa0like RMSE, R-Square, etc for regression and accuracy, AUC-ROC, etc for classification.\\nb)\\xa0Gini and KS -Statistics: A statistical measure of how well the predicted probabilities/classes are separated (only for classification models)\\n\\xa0\\n3. Operations Metrics\\xa0— These metrics help us to determine how the deployed model is performing from a usage point of view. They are as such independent of model type, data & don’t require any inputs as with the above two metrics. Examples of these metrics include,\\na. # of time ML API endpoints called in the past\\nb. Latency when calling ML API endpoints\\nc. IO/Memory/CPU usage when performing prediction\\nd. System uptime\\ne. Disk utilization\\n\\xa0\\nConclusion\\n\\xa0\\nModel monitoring within the realm of MLOps has become a necessity for mature ML systems. It is quintessential to implement such a framework to ensure consistency and robustness of the ML system, as without it ML systems may lose the “trust” of the end-user, which could be fatal. As such including and planning for it in the overall solution architecture of any ML use case implementation is of utmost importance.\\nIn the next blogs of the series, we will get into more details of the two most important model monitoring metric i.e. Stability & Performance metrics and we will see how we can use them to build our model monitoring framework.\\nReferences\\n\\nD. Sato, A. Wider, C. Windheuser,\\xa0Continuous Delivery for Machine Learning\\xa0(2019), martinflower.com\\nM. Stewart,\\xa0Understanding Dataset Shift\\xa0(2019), towardsdatascience.com\\n\\n\\xa0\\nPronojit Saha is an AI practitioner with extensive experience in solving business problems, architecting, and building end-to-end ML driven products & solutions by leading and facilitating cross-functional teams. He is currently the Advanced Analytics Practice Lead at Abzooba, wherein apart from project execution he also engages in leading & growing the Practice by nurturing talent, building thought leadership, and enabling scalable processes. Pronojit has worked in the retail, healthcare, and Industry 4.0 domains. Time series analytics and natural language processing are his expertise and he has applied these along with other AI methodologies for use cases like price optimization, readmission prediction, predictive maintenance, aspect-based sentiment analytics, entity recognition, topic modeling, among others.\\nDr. Arnab Bose is Chief Scientific Officer at Abzooba, a data analytics company and an adjunct faculty at the University of Chicago where he teaches Machine Learning and Predictive Analytics, Machine Learning Operations, Time Series Analysis and Forecasting, and Health Analytics in the Master of Science in Analytics program. He is a 20-year predictive analytics industry veteran who enjoys using unstructured and structured data to forecast and influence behavioral outcomes in healthcare, retail, finance, and transportation. His current focus areas include health risk stratification and chronic disease management using machine learning, and production deployment and monitoring of machine learning models.\\nRelated:\\n\\nMLOps – “Why is it required?” and “What it is”?\\nModel Experiments, Tracking and Registration using MLflow on Databricks\\nData Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes',\n",
       " 'By Jesus Rodriguez, Intotheblock.\\ncomments\\n\\n\\nSource:\\xa0https://distill.pub/2018/building-blocks/\\n\\n\\xa0\\n\\nI recently started a new newsletter focus on AI education. TheSequence is a no-BS (meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:\\n\\n\\xa0\\n\\n\\xa0\\nOne of the challenging elements of any deep learning solution is to understand the knowledge and decisions made by deep neural networks. While the interpretation of decisions made by a neural networks has always been difficult, the issue has become a nightmare with the raise of deep learning and the proliferation of large scale neural networks that operate with multi-dimensional datasets. Not surprisingly, the interpretation of neural networks has become one of the most active areas of research in the deep learning ecosystem.\\nTry to imagine a large neural network with hundreds of millions of neurons that is performing a deep learning task such as image recognition. Typically, you would like to understand how the network arrives to specific decisions. Most of the current research has focused on detecting what neurons in the network have been activated. Knowing that neuron-12345 fired five times is relevant but not incredibly useful in the scale of the entire network. The research about understanding decisions in neural networks has focused on three main areas: feature visualization, attribution and dimensionality reduction. Google, in particular, has done a lot of work in the feature visualization space publishing some\\xa0remarkable research and tools. Over a year ago, Google researchers published a paper titled\\xa0“The Building Blocks of Interpretability”\\xa0that became a seminal paper in the are of machine learning interpretability. The paper proposes some new ideas to understand how deep neural networks make decisions.\\nThe main insight of Google’s research is to not see the different interpretability techniques in isolation but as composable building blocks of larger models that help understand the behavior of neural networks. For instance, feature visualization is a very effective technique to understand the information processed by individual neurons but fails to correlate that insight with the overall decision made by the neural network. Attribution is a more solid technique to explain the relationship between different neurons but not so much when comes to understand the decision made by individual neurons. Combining those building blocks, Google has created an interpretability models that does not only\\xa0explains what\\xa0a neural network detects, but it does answer\\xa0how\\xa0the network assembles these individual pieces to arrive at later decisions, and\\xa0why\\xa0these decisions were made.\\nHow does the new Google model for interpretability works specifically? Well, the main innovation, in my opinion, is that it analyzes the decisions made by different components of a neural network at different levels: individual neurons, connected groups of neurons and complete layers. Google also uses a novel research technique called matrix factorization to analyze the impact that arbitrary groups of neurons can have in the final decision.\\n\\n\\nSource:\\xa0https://distill.pub/2018/building-blocks/\\n\\n\\xa0\\nA good way to think about Google’s blocks of interpretability is as a model that detects insights about the decisions of a neural network at different levels of abstraction from the basic computation graph to the final decision.\\n\\n\\nSource:\\xa0https://distill.pub/2018/building-blocks/\\n\\n\\xa0\\nGoogle research of deep neural network interpretability is not only a theoretical exercise. The research group accompanied the paper with the release of\\xa0Lucid, a neural network visualization library that allow developers to make the sort lucid feature visualizations that illustrate the decisions made by individual segments of a neural network. Google also released\\xa0colab notebooks. These notebooks make it extremely easy to use Lucid to create Lucid visualization in an interactive environment.\\n\\xa0\\nOriginal. Reposted with permission.\\nRelated:\\n\\nLearning by Forgetting: Deep Neural Networks and the Jennifer Aniston Neuron\\nUber’s Ludwig is an Open Source Framework for Low-Code Machine Learning\\nGoogle Unveils TAPAS, a BERT-Based Neural Network for Querying Tables Using Natural Language']"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "url_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://www.kdnuggets.com/2021/08/visplore-data-understanding-interactive-exploration.html',\n",
       " 'https://www.kdnuggets.com/2020/07/pytorch-deep-learning-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2021/06/interactive-plots-directly-pandas.html',\n",
       " 'https://www.kdnuggets.com/2021/01/sas-viya-faster-trusted-decisions-cloud.html',\n",
       " 'https://www.kdnuggets.com/2021/07/building-machine-learning-pipelines-snowflake-dask.html',\n",
       " 'https://www.kdnuggets.com/2021/04/microsoft-research-trains-neural-networks-understand-read.html',\n",
       " 'https://www.kdnuggets.com/2021/06/create-deploy-sentiment-analysis-app-api.html',\n",
       " 'https://www.kdnuggets.com/2020/12/informs-machine-learning-roots.html',\n",
       " 'https://www.kdnuggets.com/2020/09/solving-linear-regression.html',\n",
       " 'https://www.kdnuggets.com/2020/12/crack-sql-interviews.html',\n",
       " 'https://www.kdnuggets.com/2021/04/automated-text-classification-evalml.html',\n",
       " 'https://www.kdnuggets.com/2021/06/high-performance-deep-learning-part2.html',\n",
       " 'https://www.kdnuggets.com/2021/03/top-youtube-machine-learning-channels.html',\n",
       " 'https://www.kdnuggets.com/2020/12/data-science-machine-learning-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2021/06/analytics-engineering-everywhere.html',\n",
       " 'https://www.kdnuggets.com/2021/07/python-data-structures-compared.html',\n",
       " 'https://www.kdnuggets.com/2021/07/roidna-aws-webinar-data-driven-esg-sustainability-decisions.html',\n",
       " 'https://www.kdnuggets.com/2021/06/nomad-data-matters.html',\n",
       " 'https://www.kdnuggets.com/2021/02/6-data-science-certificates.html',\n",
       " 'https://www.kdnuggets.com/2021/04/covid-do-all-our-models.html',\n",
       " 'https://www.kdnuggets.com/2021/04/automated-anomaly-detection-pycaret.html',\n",
       " 'https://www.kdnuggets.com/2021/04/e-commerce-data-analysis-sales-strategy-python.html',\n",
       " 'https://www.kdnuggets.com/2020/10/ai-learn-human-values.html',\n",
       " 'https://www.kdnuggets.com/2020/10/getting-data-science-job-harder.html',\n",
       " 'https://www.kdnuggets.com/2021/05/generate-meaningful-sentences-t5-transformer.html',\n",
       " 'https://www.kdnuggets.com/2021/06/applied-language-technology.html',\n",
       " 'https://www.kdnuggets.com/2020/12/essential-math-data-science-probability-density-probability-mass-functions.html',\n",
       " 'https://www.kdnuggets.com/2020/07/monitoring-apache-spark-better-ui.html',\n",
       " 'https://www.kdnuggets.com/2021/05/soda-io-managing-data-quality-sql-scale.html',\n",
       " 'https://www.kdnuggets.com/2021/05/streamsets-dataops-summit-2021-cfp.html',\n",
       " 'https://www.kdnuggets.com/2020/09/data-scientist-not-just-tiny-hands.html',\n",
       " 'https://www.kdnuggets.com/2021/09/antifragility-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/01/data-science-learning-journey.html',\n",
       " 'https://www.kdnuggets.com/2021/07/top-stories-2021-jun.html',\n",
       " 'https://www.kdnuggets.com/2021/08/querying-granular-demographic-dataset.html',\n",
       " 'https://www.kdnuggets.com/2020/10/understanding-transformers-data-science-way.html',\n",
       " 'https://www.kdnuggets.com/2020/07/better-blog-post-analysis-google-analytics-r.html',\n",
       " 'https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html',\n",
       " 'https://www.kdnuggets.com/2021/03/begin-nlp-journey.html',\n",
       " 'https://www.kdnuggets.com/2021/08/common-data-science-interview-questions-answers.html',\n",
       " 'https://www.kdnuggets.com/2020/10/deploying-streamlit-apps-streamlit-sharing.html',\n",
       " 'https://www.kdnuggets.com/2021/08/5-data-science-career-mistakes-avoid.html',\n",
       " 'https://www.kdnuggets.com/2021/07/brief-introduction-concept-data.html',\n",
       " 'https://www.kdnuggets.com/2021/09/top-18-low-code-no-code-machine-learning-platforms.html',\n",
       " 'https://www.kdnuggets.com/2021/03/simple-way-time-code-python.html',\n",
       " 'https://www.kdnuggets.com/2021/04/models-data-science-teams-chess-checkers.html',\n",
       " 'https://www.kdnuggets.com/2021/08/expert-nlp-insights-music.html',\n",
       " 'https://www.kdnuggets.com/2021/07/full-cross-validation-learning-curves-time-series.html',\n",
       " 'https://www.kdnuggets.com/2020/10/fastcore-underrated-python-library.html',\n",
       " 'https://www.kdnuggets.com/2021/03/worlddata-ai-kdnuggets-partner.html',\n",
       " 'https://www.kdnuggets.com/2021/09/roidna-aws-webinar-consumer-insights-data.html',\n",
       " 'https://www.kdnuggets.com/2021/05/6-side-hustles-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/03/11-essential-code-blocks-exploratory-data-analysis.html',\n",
       " 'https://www.kdnuggets.com/2020/08/samsung-semiconductor-innovation-prevent-pandemic.html',\n",
       " 'https://www.kdnuggets.com/2020/12/sqream-massive-data-video-challenge.html',\n",
       " 'https://www.kdnuggets.com/2021/03/evaluating-object-detection-models-using-mean-average-precision.html',\n",
       " 'https://www.kdnuggets.com/2021/07/overview-albumentations-open-source-library-advanced-image-augmentations.html',\n",
       " 'https://www.kdnuggets.com/2021/01/mastering-tensorflow-variables-5-easy-steps.html',\n",
       " 'https://www.kdnuggets.com/2021/06/bigquery-snowflake-comparison-data-warehouse-giants.html',\n",
       " 'https://www.kdnuggets.com/2021/04/time-series-forecasting-predict-weather.html',\n",
       " 'https://www.kdnuggets.com/2020/12/facebook-open-sources-rebel-new-reinforcement-learning-agent.html',\n",
       " 'https://www.kdnuggets.com/2021/01/deep-learning-pioneer-geoff-hinton-research-future-ai.html',\n",
       " 'https://www.kdnuggets.com/2020/11/5-things-doing-wrong-pycaret.html',\n",
       " 'https://www.kdnuggets.com/2021/07/10-machine-learning-model-training-mistakes.html',\n",
       " 'https://www.kdnuggets.com/2021/07/high-performance-deep-learning-part3.html',\n",
       " 'https://www.kdnuggets.com/2021/05/essential-math-data-science-basis-change-basis.html',\n",
       " 'https://www.kdnuggets.com/2021/04/dijkstra-principle-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/imerit2-bias-variance-unpredictability.html',\n",
       " 'https://www.kdnuggets.com/2021/04/secret-analysing-large-complex-datasets-constraint.html',\n",
       " 'https://www.kdnuggets.com/2021/08/top-stories-2021-jul.html',\n",
       " 'https://www.kdnuggets.com/2021/09/speeding-neural-network-training-multiple-gpus-dask.html',\n",
       " 'https://www.kdnuggets.com/2021/03/10-amazing-machine-learning-projects-2020.html',\n",
       " 'https://www.kdnuggets.com/2021/06/10-mistakes-avoid-data-science-beginner.html',\n",
       " 'https://www.kdnuggets.com/2021/08/train-bert-model-scratch.html',\n",
       " 'https://www.kdnuggets.com/2021/01/google-trillion-parameter-switch-transformer-model.html',\n",
       " 'https://www.kdnuggets.com/2020/09/geographical-plots-python.html',\n",
       " 'https://www.kdnuggets.com/2020/12/predictions-ai-machine-learning-data-science-research.html',\n",
       " 'https://www.kdnuggets.com/2021/04/cogitotech-6-mistakes-avoid-training-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/04/best-machine-learning-frameworks-extensions-tensorflow.html',\n",
       " 'https://www.kdnuggets.com/2020/11/dalex-explain-tensorflow-model.html',\n",
       " 'https://www.kdnuggets.com/2021/08/ai21-jurassic1-language-models.html',\n",
       " 'https://www.kdnuggets.com/2021/03/learning-from-machine-learning-mistakes.html',\n",
       " 'https://www.kdnuggets.com/2021/06/determined-ai-speed-up-deep-learning-language-model.html',\n",
       " 'https://www.kdnuggets.com/2020/10/data-science-cloud-dask.html',\n",
       " 'https://www.kdnuggets.com/2020/09/autograd-best-machine-learning-library-not-using.html',\n",
       " 'https://www.kdnuggets.com/2021/06/nij-recidivism-forecasting-challenge.html',\n",
       " 'https://www.kdnuggets.com/2021/04/build-impressive-data-science-resume.html',\n",
       " 'https://www.kdnuggets.com/2021/03/top-10-python-libraries-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/09/springboard-difference-data-engineers-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/08/demystifying-ai-prejudices.html',\n",
       " 'https://www.kdnuggets.com/2021/05/vaex-pandas-1000x-faster.html',\n",
       " 'https://www.kdnuggets.com/2021/08/learned-women-data-science-conferences.html',\n",
       " 'https://www.kdnuggets.com/2021/06/shortage-data-science-jobs-5-years.html',\n",
       " 'https://www.kdnuggets.com/2021/04/awesome-tricks-best-practices-kaggle.html',\n",
       " 'https://www.kdnuggets.com/2021/07/best-sota-nlp-course-free.html',\n",
       " 'https://www.kdnuggets.com/2020/12/industry-2021-predictions-ai-data-science-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2021/05/vc-pitch-deck-open-source-elt-platform.html',\n",
       " 'https://www.kdnuggets.com/2020/11/data-professionals-add-variation-resumes.html',\n",
       " 'https://www.kdnuggets.com/2020/12/mlops-why-required-what-is.html',\n",
       " 'https://www.kdnuggets.com/2021/06/machine-learning-model-interpretation.html',\n",
       " 'https://www.kdnuggets.com/2021/04/easy-automl-python.html',\n",
       " 'https://www.kdnuggets.com/2020/08/nlp-model-forge.html',\n",
       " 'https://www.kdnuggets.com/2021/07/wht-simpler-fast-fourier-transform-fft.html',\n",
       " 'https://www.kdnuggets.com/2021/06/fine-tune-bert-transformer-spacy.html',\n",
       " 'https://www.kdnuggets.com/2020/09/data-scientist-data-problem-wrong.html',\n",
       " 'https://www.kdnuggets.com/2021/01/data-engineering-troublesome.html',\n",
       " 'https://www.kdnuggets.com/2021/07/abstraction-data-science-not-great-combination.html',\n",
       " 'https://www.kdnuggets.com/2020/10/ethics-ai-qa-farzindar.html',\n",
       " 'https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html',\n",
       " 'https://www.kdnuggets.com/2021/03/pandas-big-data-better-options.html',\n",
       " 'https://www.kdnuggets.com/2021/05/dont-need-data-engineers-need-better-tools-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/03/top-stories-2021-feb.html',\n",
       " 'https://www.kdnuggets.com/2021/03/8-women-ai-striving-humanize-world.html',\n",
       " 'https://www.kdnuggets.com/2021/04/how-organize-your-data-science-project-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/04/time-series-using-sql.html',\n",
       " 'https://www.kdnuggets.com/2021/09/a-breakdown-deep-learning-frameworks.html',\n",
       " 'https://www.kdnuggets.com/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html',\n",
       " 'https://www.kdnuggets.com/2021/06/pycaret-101-introduction-beginners.html',\n",
       " 'https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/04/top-3-statistical-paradoxes-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/07/github-copilot-open-source-alternatives-code-generation.html',\n",
       " 'https://www.kdnuggets.com/2020/11/microsoft-google-open-sourced-frameworks-scaling-deep-learning-training.html',\n",
       " 'https://www.kdnuggets.com/2021/01/deepmind-muzero-important-deep-learning-systems.html',\n",
       " 'https://www.kdnuggets.com/2021/07/11-important-probability-distributions-explained.html',\n",
       " 'https://www.kdnuggets.com/2021/09/8-deep-learning-project-ideas-beginners.html',\n",
       " 'https://www.kdnuggets.com/2021/07/python-tips-snippets-data-processing.html',\n",
       " 'https://www.kdnuggets.com/2020/07/ethical-social-issues-natural-language-processing.html',\n",
       " 'https://www.kdnuggets.com/2021/09/data-scientists-compete-global-job-market.html',\n",
       " 'https://www.kdnuggets.com/2020/12/14-data-science-projects-improve-skills.html',\n",
       " 'https://www.kdnuggets.com/2020/09/performance-machine-learning-model.html',\n",
       " 'https://www.kdnuggets.com/2021/02/deploy-flask-api-kubernetes-connect-micro-services.html',\n",
       " 'https://www.kdnuggets.com/2021/03/cmu-ms-business-analytics.html',\n",
       " 'https://www.kdnuggets.com/2020/12/manning-deep-learning-design-patterns.html',\n",
       " 'https://www.kdnuggets.com/2021/04/data-science-101-normalization-standardization-regularization.html',\n",
       " 'https://www.kdnuggets.com/2020/11/mastering-tensorflow-tensors-5-easy-steps.html',\n",
       " 'https://www.kdnuggets.com/2021/02/multidimensional-multi-sensor-time-series-data-analysis-framework.html',\n",
       " 'https://www.kdnuggets.com/2021/09/sas-popular-certifications-data-analytics-skills.html',\n",
       " 'https://www.kdnuggets.com/2021/04/data-science-predict-prevent-real-world-problems.html',\n",
       " 'https://www.kdnuggets.com/2020/12/resampling-imbalanced-data-limits.html',\n",
       " 'https://www.kdnuggets.com/2020/11/essential-math-data-science-integrals-area-under-curve.html',\n",
       " 'https://www.kdnuggets.com/2021/07/7-open-source-libraries-deep-learning-graphs.html',\n",
       " 'https://www.kdnuggets.com/2020/10/imbalanced-data-machine-learning.html',\n",
       " 'https://www.kdnuggets.com/2020/09/international-alternatives-kaggle-data-science-competitions.html',\n",
       " 'https://www.kdnuggets.com/2021/03/kdnuggets-survey-data-community-job-satisfaction.html',\n",
       " 'https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html',\n",
       " 'https://www.kdnuggets.com/2021/06/7-data-security-best-practices-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-visualization-feature-selection.html',\n",
       " 'https://www.kdnuggets.com/2021/06/pandas-vs-sql.html',\n",
       " 'https://www.kdnuggets.com/2020/12/5-free-books-learn-statistics-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/most-demand-skills-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/02/dannet-triggers-deep-cnn-revolution.html',\n",
       " 'https://www.kdnuggets.com/2021/07/kafka-open-source-data-pipeline-processing-real-time-data.html',\n",
       " 'https://www.kdnuggets.com/2021/02/gilbert-people-skills-analytical-thinkers.html',\n",
       " 'https://www.kdnuggets.com/2020/09/implementing-deep-learning-library-scratch-python.html',\n",
       " 'https://www.kdnuggets.com/2020/11/top-stories-2020-oct.html',\n",
       " 'https://www.kdnuggets.com/2021/07/sas-building-tech-skills.html',\n",
       " 'https://www.kdnuggets.com/2021/05/ai-books-read-2021.html',\n",
       " 'https://www.kdnuggets.com/2020/11/best-data-science-certification-never-heard.html',\n",
       " 'https://www.kdnuggets.com/2020/09/potential-predictive-analytics-labor-industries.html',\n",
       " 'https://www.kdnuggets.com/2021/04/gradient-boosted-trees-conceptual-explanation.html',\n",
       " 'https://www.kdnuggets.com/2020/08/data-science-skills-superpower.html',\n",
       " 'https://www.kdnuggets.com/2020/08/microsoft-dowhy-framework-causal-inference.html',\n",
       " 'https://www.kdnuggets.com/2021/03/informs-virtual-career-fair.html',\n",
       " 'https://www.kdnuggets.com/2021/04/essential-math-data-science-linear-transformation-matrices.html',\n",
       " 'https://www.kdnuggets.com/2021/07/accept-null-hypothesis-wrong-intuitive-explanation.html',\n",
       " 'https://www.kdnuggets.com/2020/08/accelerated-computer-vision-free-course-amazon.html',\n",
       " 'https://www.kdnuggets.com/2020/10/data-protection-techniques-guarantee-privacy.html',\n",
       " 'https://www.kdnuggets.com/2021/04/8-most-common-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/06/data-storytelling.html',\n",
       " 'https://www.kdnuggets.com/2021/01/graph-theory-why-care.html',\n",
       " 'https://www.kdnuggets.com/2021/09/openai-codex-challenges.html',\n",
       " 'https://www.kdnuggets.com/2021/05/dataops-5-things-need-know.html',\n",
       " 'https://www.kdnuggets.com/2021/01/cloud-data-warehouse-future-data-storage.html',\n",
       " 'https://www.kdnuggets.com/2021/07/top-blogs-rewards-jun.html',\n",
       " 'https://www.kdnuggets.com/2020/08/data-science-machine-learning-capability-python.html',\n",
       " 'https://www.kdnuggets.com/2021/07/deep-learning-gpu-accelerate-data-science-data-analytics.html',\n",
       " 'https://www.kdnuggets.com/2021/01/unsupervised-learning-predictive-maintenance-auto-encoders.html',\n",
       " 'https://www.kdnuggets.com/2021/09/datacated-expo-oct-5.html',\n",
       " 'https://www.kdnuggets.com/2021/06/ai-with-feature-store.html',\n",
       " 'https://www.kdnuggets.com/2021/01/best-python-ide-code-editors.html',\n",
       " 'https://www.kdnuggets.com/2020/10/feature-ranking-recursive-feature-elimination-scikit-learn.html',\n",
       " 'https://www.kdnuggets.com/2020/10/5-must-read-data-science-papers.html',\n",
       " 'https://www.kdnuggets.com/2020/10/advice-aspiring-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/06/train-joint-entities-relation-extraction-classifier-bert-spacy.html',\n",
       " 'https://www.kdnuggets.com/2021/01/greatlearning-data-science-analytics-career-trends-2021.html',\n",
       " 'https://www.kdnuggets.com/2021/04/ab-testing-7-common-questions-answers-data-science-interviews-2.html',\n",
       " 'https://www.kdnuggets.com/2021/01/comprehensive-guide-normal-distribution.html',\n",
       " 'https://www.kdnuggets.com/2021/02/northwestern-ms-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/06/managing-reusable-python-code-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/06/generate-automated-pdf-documents-python.html',\n",
       " 'https://www.kdnuggets.com/2021/08/bemyapp-florida-hacks-ibm.html',\n",
       " 'https://www.kdnuggets.com/2020/07/immuta-scale-sensitive-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/consider-being-data-engineer-instead-data-scientist.html',\n",
       " 'https://www.kdnuggets.com/2021/05/okera-airside-security-data-governance.html',\n",
       " 'https://www.kdnuggets.com/2021/07/5-mistakes-data-science-career.html',\n",
       " 'https://www.kdnuggets.com/2021/03/bayesian-hyperparameter-optimization-tune-sklearn-pycaret.html',\n",
       " 'https://www.kdnuggets.com/2021/04/coursera-career-growing-field-google-data-analytics-certificate.html',\n",
       " 'https://www.kdnuggets.com/2021/02/hugging-face-transformer-basics.html',\n",
       " 'https://www.kdnuggets.com/2021/03/overview-mlops.html',\n",
       " 'https://www.kdnuggets.com/2020/12/immuta-future-cloud-now.html',\n",
       " 'https://www.kdnuggets.com/2020/11/computer-vision-scale-dask-pytorch.html',\n",
       " 'https://www.kdnuggets.com/2020/11/jmp-effective-disease-outbreak-alert-system.html',\n",
       " 'https://www.kdnuggets.com/2021/03/forget-telling-stories-help-people-navigate.html',\n",
       " 'https://www.kdnuggets.com/2021/04/sas-viya-cloud-microsoft.html',\n",
       " 'https://www.kdnuggets.com/2021/06/coiled-workflow-orchestration-prefect.html',\n",
       " 'https://www.kdnuggets.com/2021/07/google-advice-learning-data-science.html',\n",
       " 'https://www.kdnuggets.com/2021/04/whats-etl.html',\n",
       " 'https://www.kdnuggets.com/2020/10/text-mining-r-free-ebook.html',\n",
       " 'https://www.kdnuggets.com/2021/07/streamlit-tips-tricks-hacks-data-scientists.html',\n",
       " 'https://www.kdnuggets.com/2021/07/build-image-classifier-in-few-lines-of-code-with-flash.html',\n",
       " 'https://www.kdnuggets.com/2021/01/mlops-model-monitoring-101.html',\n",
       " 'https://www.kdnuggets.com/2020/07/understanding-neural-networks-think.html']"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "df_kdnuggets_data = pd.DataFrame()\n",
    "df_kdnuggets_data['link'] = url_list\n",
    "df_kdnuggets_data['title'] = title_list\n",
    "df_kdnuggets_data['summary'] = summary_list\n",
    "df_kdnuggets_data['content'] = content_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "df_kdnuggets_data.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 212 entries, 0 to 211\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   link     212 non-null    object\n",
      " 1   title    212 non-null    object\n",
      " 2   summary  212 non-null    object\n",
      " 3   content  212 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 6.8+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "df_kdnuggets_data.to_csv(data_path + '/content_KDnuggets.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "with open(os.path.join(data_path, 'KDnuggets_valid_urls.pkl'), 'wb') as f:\n",
    "    pickle.dump(valid_urls, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e8ae99d9c20b736ed1ebdc5d0d4557145eaffa6a1bc0849b3635b3af9b7f0aff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}