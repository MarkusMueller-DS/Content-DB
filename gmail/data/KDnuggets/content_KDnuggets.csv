link,title,summary,content
https://www.kdnuggets.com/2021/08/visplore-data-understanding-interactive-exploration.html,Speeding up data understanding by interactive exploration,A key success factor of data science projects is to understand the data well. This blog explains why coding can be inefficient for this and how you can improve.,"Sponsored Post.
By Visplore. 
Understanding the data is fundamental for all steps of a data science project. However, gaining that understanding can be challenging and time-consuming. It involves interpreting data patterns with deep knowledge from the project domain to understand, for example, which parts of the data are useful and which correlations actually matter.
Oftentimes, data scientists don't have that deep domain expertise. A key to project success is thus the communication to domain experts.
Traditionally, this communication works as follows: data scientists prepare a presentation by coding charts in languages such as Python. Then, they discuss the charts with domain experts. This typically triggers further questions such as ""What are these clusters?"" or ""Which other variables show this trend?"". Even great coders usually can’t spontaneously answer all these questions so that they get postponed to a follow-up meeting. Altogether, it may require several tedious feedback loops, significantly increasing the duration and costs of the project. Worse even, cutting this phase short may risk project success!

Traditional data communication vs. joint exploration
 
How can you improve this? Reconsider the use of coding for understanding the data! Coding is great for building data pipelines, models, monitoring apps, and much more. However, it is generally too static for answering ad-hoc questions from domain experts during a meeting.
Visplore is a graphical tool which is optimized for agile exploration of massive multi-variate data, in particular (but not limited to) time series such as sensor data.
Explore millions of raw values as fast as never before - even while sitting together with domain experts. This turns data exploration into a vivid dialogue and makes understanding the data an exciting step of a project.
Simply load data directly from Python, Matlab, R or other sources. Pre-configured analysis cockpits provide deep insights within seconds, with hardly any configuration. Built-in analytics answers complex questions on-the-fly, and powerful interaction tools support for selecting, cleaning and labeling data.
For shorter project duration, less risk, and more satisfaction of all stakeholders.
 
Learn more about interactive data exploration"
https://www.kdnuggets.com/2020/07/pytorch-deep-learning-free-ebook.html,PyTorch for Deep Learning: The Free eBook,"For this week's free eBook, check out the newly released Deep Learning with PyTorch from Manning, made freely available via PyTorch's website for a limited time. Grab it now!","By Matthew Mayo, KDnuggets.
comments
What better option for this week's free eBook than the brand new Manning published Deep Learning with PyTorch, made freely available via PyTorch's website for a limited time (we don't know how limited, so grab it now).
Written by Eli Stevens, Luca Antiga, and Thomas Viehmann, 3 people with serious PyTorch bona fides, Soumith Chintala, co-creator of PyTorch, writes the following in the foreword:

With the publication of Deep Learning with PyTorch, we finally have a definitive treatise on PyTorch. It covers the basics and abstractions in great detail, tearing apart the underpinnings of data structures like tensors and neural networks and making sure you understand their implementation. Additionally, it covers advanced subjects such as JIT and deployment to production (an aspect of PyTorch that no other book currently covers).


 
Lots of organizations have made the move to PyTorch, and it doesn't seem to be a trend that will stop anytime soon. The project has a large community, and numerous recent APIs such as PyTorch Lightning, fastai, and torchlayers make the library even more flexible and easy to use than ever. A robust ecosystem centered on PyTorch has evolved and rivals that of any other neural network framework out there.
Why PyTorch? From the first chapter of the book:

As Python does for programming, PyTorch provides an excellent introduction to deep learning. At the same time, PyTorch has been proven to be fully qualified for use in professional contexts for real-world, high-profile work. We believe that PyTorch’s clear syntax, streamlined API, and easy debugging make it an excellent choice for introducing deep learning. We highly recommend studying PyTorch for your first deep learning library. Whether it ought to be the last deep learning library you learn is a decision we leave up to you.

If you head over to the PyTorch website you can grab your own PDF copy by filling out the simple form — which only asks what your role is and what it is you are going to build with PyTorch (no email == no spam) — a seemingly reasonable trade-off to get your hands on the book. Once you do, you can see what is covered in the table of contents:

Introduction to Deep Learning and the PyTorch Library
Pre-trained Networks
It Starts with a Tensor
Real-World Data Representation Using Tensors
The Mechanics of Learning
Using a Neural Network to Fit the Data
Telling Birds from Airplanes: Learning from Images
Using Convolutions to Generalize
Using PyTorch to Fight Cancer
Ready, Dataset, Go!
Training a Classification Model to Detect Suspected Tumors
Monitoring Metrics: Precision, Recall, and Pretty Pictures
Using Segmentation to Find Suspected Nodules
End-to-End Nodule Analysis, and Where to Got Next
Deploying to Production

Manning highlights these main points on their website as to what you will find in the book:

Training deep neural networks
Implementing modules and loss functions
Utilizing pretrained models from PyTorch Hub
Exploring code samples in Jupyter Notebooks

I, for one, am excited to get into this book, and am appreciative of PyTorch's move to make it freely available for a limited time before it is officially released. A great public relations move, but also one which benefits the community of PyTorch researchers and students just the same.
 
Related:

Deep Learning: The Free eBook
Dive Into Deep Learning: The Free eBook
Mathematics for Machine Learning: The Free eBook"
https://www.kdnuggets.com/2021/06/interactive-plots-directly-pandas.html,Get Interactive Plots Directly With Pandas,"Telling a story with data is a core function for any Data Scientist, and creating data visualizations that are simultaneously illuminating and appealing can be challenging. This tutorial reviews how to create Plotly and Bokeh plots directly through Pandas plotting syntax, which will help you convert static visualizations into interactive counterparts -- and take your analysis to the next level.","comments
By Parul Pandey, Data Science at H2O.ai | Editor @wicds.

Infographic vector created by macrovector — www.freepik.com.
Data exploration is by far one of the most important aspects of any data analysis task. The initial probing and preliminary checks that we perform, using the vast catalog of visualization tools, give us actionable insights into the nature of data. However, the choice of visualization tool at times is more complicated than the task itself. On the one hand, we have libraries that are easier to use but are not so helpful in showing complex relationships in data. Then there are others that render interactivity but have a considerable learning curve. Fortunately, some open-source libraries have been created that try to address this pain point effectively.
In this article, we’ll look at two such libraries, namely pandas_bokeh and cufflinks. We’ll learn how to create plotly and bokeh charts with the basic pandas plotting syntax, which we all are comfortable with. Since the article's emphasis is on the syntax rather than the types of plots, we’ll limit ourselves to the five basic charts, i.e., line charts, bar charts, histograms, scatter plots, and pie charts. We’ll create each of these charts first with pandas plotting library and then recreate them in plotly and bokeh, albeit with a twist.
 
Import the Dataset
 
We’ll work with the NIFTY-50 dataset. The NIFTY 50 index is the National Stock Exchange of India’s benchmark for the Indian equity market. The dataset is openly available on Kaggle, but we’ll be using a subset of the data containing the stock value of only four sectors, i.e., bank, pharma, IT, and FMCG.
You can download the sample dataset from here.
Let’s import the necessary libraries and dataset required for the visualization purpose:

# Importing required modules
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# Reading in the data
nifty_data = pd.read_csv('NIFTY_data_2020.csv',parse_dates=[""Date""],index_col='Date')
nifty_data.head()



 

Combined dataframe consisting of NIFTY indices of the bank, pharma, IT, and FMCG sectors.
We can also resample/aggregate the data by month-end. The pandas’ library has a resample() function, which resamples the time series data.

nifty_data_resample = nifty_data.resample(rule = 'M').mean()
nifty_data_resample



 

Now that we have our dataframes ready, it is time to visualize them via different plots.
 
Plotting with Pandas directly
 
Let’s begin with the most straightforward plotting technique — pandas’ plotting functions. To plot a graph using pandas, we’ll call the .plot() method on the dataframe.
Syntax: dataframe.plot()
The plot method is just a simple wrapper around matplotlib’s plt.plot(). We can also specify some additional parameters like the ones mentioned below:

Some of the important Parameters
--------------------------------

x : label or position, default None
    Only used if data is a DataFrame.
y : label, position or list of label, positions, default None

title: title to be used for the plot

X and y label: Name to use for the label on the x-axis and y-axis.

figsize : specifies the size of the figure object.    
kind : str
    The kind of plot to produce:

    - 'line' : line plot (default)
    - 'bar' : vertical bar plot
    - 'barh' : horizontal bar plot
    - 'hist' : histogram
    - 'box' : boxplot
    - 'kde' : Kernel Density Estimation plot
    - 'density' : same as 'kde'
    - 'area' : area plot
    - 'pie' : pie plot
    - 'scatter' : scatter plot
    - 'hexbin' : hexbin plot.



 
For a complete list of the parameters and their usage, please refer to the documentation. Let’s now look at ways to create different plots. In this article, we’ll not go into detail explaining each plot. We’ll only focus on the syntax, which is self-explanatory if you have some experience in pandas. For a detailed understanding of the pandas’ plots, this article will be helpful.

Line Plot


nifty_data.plot(title='Nifty Index values in 2020', 
                xlabel = 'Values',
                figsize=(10,6);



 

Line plot with pandas plotting.

Scatter Plot


nifty_data.plot(kind='scatter',
        x='NIFTY FMCG index', 
        y='NIFTY Bank index',
        title = 'Scatter Plot for NIFTY Index values in 2020',
        figsize=(10,6));



 

Scatter plot with pandas plotting.

Histograms


nifty_data[['NIFTY FMCG index','NIFTY Bank index']].plot(kind='hist',figsize=(9,6), bins=30);



 

Histogram with pandas plotting.

Bar plots


nifty_data_resample.plot(kind='bar',figsize=(10,6));



 

Bar plot with pandas plotting.

Stacked bar plots


nifty_data_resample.plot(kind='barh',figsize=(10,6));



 

Stacked Bar plot with pandas plotting.

Pie Charts


nifty_data_resample.index=['Jan','Feb','March','Apr','May','June','July']
nifty_data_resample['NIFTY Bank index'].plot.pie(legend=False, figsize=(10,6),autopct='%.1f');



 

Pie Chart with pandas plotting.
These were some of the charts that can be directly created with pandas’ dataframes. However, these charts lack interactivity and capabilities like zoom and pan. Let’s now change these existing charts in syntax into their fully interactive counterparts with just a slight change in the syntax.
 
Bokeh Backend for Pandas — plotting with Pandas-Bokeh.
 

Image by Author.
The bokeh library clearly stands out when it comes to data visualizations. The Pandas-Bokeh provides a bokeh plotting backend for Pandas, GeoPandas, and Pyspark DataFrames. This backend adds a plot_bokeh() method to the DataFrames and Series.
Installation
Pandas-Bokeh can be installed from PyPI via pip or conda:

pip install pandas-bokeh

or

conda install -c patrikhlobil pandas-bokeh



 
Usage
The Pandas-Bokeh library should be imported after Pandas, GeoPandas, or Pyspark.

import pandas as pd
import pandas_bokeh



 
Then one needs to define the plotting output, which can be either of the two:

pandas_bokeh.output_notebook() # for embedding plots in Jupyter Notebooks.
pandas_bokeh.output_file(filename) # for exporting plots as HTML.



 
Syntax
Now, the plotting API is accessible for a Pandas DataFrame via the dataframe.plot_bokeh().
For more details about the plotting outputs, see the reference here or the Bokeh documentation. Let’s now plot all the five kinds of plots as plotted in the above section. We’ll be using the same datasets as used above.

import pandas as pd
import pandas_bokeh
pandas_bokeh.output_notebook()



 


Line Plot


nifty_data.plot_bokeh(kind='line') #equivalent to nifty_data.plot_bokeh.line()



 

Line plot with pandas_bokeh.

Scatter Plot


nifty_data.plot_bokeh.scatter(x='NIFTY FMCG index', y='NIFTY Bank index');



 

Scatter plot with pandas_bokeh.

Histograms


nifty_data[['NIFTY FMCG index','NIFTY Bank index']].plot_bokeh(kind='hist', bins=30);



 

Histogram with pandas_bokeh.

Bar plots


nifty_data_resample.plot_bokeh(kind='bar',figsize=(10,6));



 

Bar plot with pandas_bokeh.

Stacked bar plots


nifty_data_resample.plot_bokeh(kind='barh',stacked=True);



 

Stacked bar plot with pandas_bokeh.

Pie Charts


nifty_data_resample.index=['Jan','Feb','March','Apr','May','June','July']
nifty_data_resample.plot_bokeh.pie(y ='NIFTY Bank index')



 

Pie chart with pandas_bokeh.
Additionally, you can also create multiple nested pie plots within the same plot:

nifty_data_resample.plot_bokeh.pie()



 

Nested pie chart with pandas_bokeh.
This section saw how we could seamlessly create bokeh plots without any significant change in the pandas plotting syntax. Now we can have the best of both worlds without having to learn any new format.
 
Plotly Backend for Pandas — plotting with Cufflinks.
 

Image by Author.
Another commonly used library for data visualization is Plotly. With plotly, you can make interactive charts in Python, R, and JavaScript. As of version 4.8, Plotly came out with a Plotly Express-powered backend for Pandas plotting, which meant that one even does not need to import plotly to create plotly like visualizations.
However, the library I want to mention here is not plotly express, but an independent third-party wrapper library around Plotly called Cufflinks. The beauty of cufflinks is that it is more versatile, has more functionalities, and has an API similar to pandas plotting. This means you only need to add a .iplot() method to Pandas dataframes for plotting graphs.
Installation
Make sure you have plotly installed before installing cufflinks. Read this guide for instructions.

pip install cufflinks



 
Usage
The repository has a lot of useful examples and notebooks to get started.

import pandas as pd
import cufflinks as cf
from IPython.display import display,HTML
#making all charts public and setting a global theme
cf.set_config_file(sharing='public',theme='white',offline=True)



 
That is all. We can now create visualizations with the power of plotly but with the ease of pandas. The only change in the syntax is dataframe.iplot().

Line Plot


nifty_data.iplot(kind='line')



 

Line plot with cufflinks.

Scatter Plot

You need to mention the plotting mode for scatter trace while creating a scatterplot. The mode could be lines, markers, text, or a combination of either of them.

nifty_data.iplot(kind='scatter',x='NIFTY FMCG index', y='NIFTY Bank index',mode='markers');



 

Scatter plot with cufflinks.

Histograms


nifty_data[['NIFTY FMCG index','NIFTY Bank index']].iplot(kind='hist', bins=30);



 

Histogram with cufflinks.

Bar plots


nifty_data_resample.iplot(kind='bar');



 

Barplots with cufflinks.

Stacked bar plots


nifty_data_resample.iplot(kind='barh',barmode = 'stack');



 

Stacked bar plots with cufflinks.

Pie Charts


nifty_data_resample.index=['Jan','Feb','March','Apr','May','June','July']
nifty_data_resample.reset_index().iplot(kind='pie',labels='index',values='NIFTY Bank index')



 

Pie charts with cufflinks.
The Cufflinks library provides an easy way to get the power of plotly within plotly. The similarity in syntax is another point of advantage.
 
Conclusion
 
The Bokeh or a Plotly plot is self-sufficient in conveying the entire information. Based on your choice and preference, you can choose both or either of them while keeping a focus on the primary purpose of making visualizations more intuitive and interactive at the same time.
 
Original. Reposted with permission.
 
Related:

Animated Bar Chart Races in Python
How to Make Sure Your Analysis Actually Gets Used
Know your data much faster with the new Sweetviz Python library"
https://www.kdnuggets.com/2021/01/sas-viya-faster-trusted-decisions-cloud.html,"Travel to faster, trusted decisions in the cloud","Join technology experts, partners and analysts in the industry to see what is taking off in AI, cloud computing and putting models into production for better outcomes and trusted results. Register today!","Sponsored Post.

For most enterprises, the cloud presents an opportunity to rethink not only how to use technology, but how to manage your business. The scalability and flexibility of a cloud environment offers the possibility of lower costs, greater agility and more accurate decisions.
But not all cloud environments are created equal. And there’s no single way to implement. While enterprises could simply “lift and shift” their current systems to the cloud, most will embark on an approach to steadily transform their operations. No matter the approach you take, SAS® Viya® can help.
SAS Viya takes full advantage of the cloud's scalability, providing a solution that delivers the latest analytics and AI capabilities.
With Viya, you can balance costs and agility with a built-in automation that enables you to strengthen your operations.​ Massively parallel processing delivers results in seconds, not hours. Plus, built-in governance makes your decisions repeatable, explainable, transparent and trustworthy – no matter the scale of your data, workloads or users.
Yet perhaps the biggest benefit of Viya is its ability to democratize data and analytics. Everyone – from data scientists to business analysts to the CEO – can access the power of analytics to make faster, better decisions.
All of these capabilities are offered on a cloud-native platform that unifies disparate technologies, skill sets and processes with end-to-end capabilities powered by automated AI.​
Join technology experts, partners and analysts in the industry for this live virtual event to see how SAS Viya can help you make the most of AI, analytics and the cloud for faster decisions and trusted results."
https://www.kdnuggets.com/2021/07/building-machine-learning-pipelines-snowflake-dask.html,Building Machine Learning Pipelines using Snowflake and Dask,"In this post, I want to share some of the tools that I have been exploring recently and show you how I use them and how they helped improve the efficiency of my workflow. The two I will talk about in particular are Snowflake and Dask. Two very different tools but ones that complement each other well especially as part of the ML Lifecycle.","comments
By Daniel Foley, Data Scientist

 
Introduction
 

Recently I have been trying to find better ways to improve my workflow as a data scientist. I tend to spend a decent chunk of my time modelling and building ETLs in my job. This has meant that more and more I need to rely on tools to reliably and efficiently handle large datasets. I quickly realised that using pandas for manipulating these datasets is not always a good approach and this prompted me to look into other alternatives.
In this post, I want to share some of the tools that I have been exploring recently and show you how I use them and how they helped improve the efficiency of my workflow. The two I will talk about in particular are Snowflake and Dask. Two very different tools but ones that complement each other well especially as part of the ML Lifecycle. My hope is that after reading this post you will have a good understanding of what Snowflake and Dask are, how they can be used effectively and be able to get up and running with your own use cases.
More specifically, I want to show you how you can build an ETL pipeline using Snowflake and Python to generate training data for a machine learning task. I then want to introduce Dask and Saturn Cloud and show you how you can take advantage of parallel processing in the cloud to really speed up the ML training process so you can increase your productivity as a data scientist.
 
Building ETLs in Snowflake and Python
 

Before we jump into coding I better briefly explain what Snowflake is. This is a question I recently asked when my team decided to start using it. At a high level, it is a data warehouse in the cloud. After playing around with it for a while I realised how powerful it was. I think for me, one of the most useful features is the virtual warehouses that you can use. A virtual warehouse gives you access to the same data but is completely independent of other virtual warehouses so compute resources are not shared across teams. This has proven very useful as it removes any potential for performance issues caused by other users executing queries throughout the day. This has resulted in less frustration and time wasted waiting for queries to run.
Since we are going to be using Snowflake I will briefly outline how you can set it up and start experimenting with it yourself. We need to do the following:

Get a Snowflake account set up
Get our data into Snowflake
Write and test our queries using SQL and the Snowflake UI
Write a Python class that can execute our queries to generate our final dataset for modelling

Setting up an account is as easy as signing up for a free trial on their website. Once you have done that you can download the snowsql CLI here. This will make it straightforward to add data to Snowflake. After following these steps we can try and connect to Snowflake using our credentials and the command line.

snowsql -a <account_name> -u <user_name>

 
You can find your account name in the URL when you log in to the Snowflake UI. It should look something like this: xxxxx.europe-west2.gcp. Ok, let’s move onto the next step and get our data into Snowflake. There are a few steps we need to follow here namely:

Create our virtual warehouse
Create a database
Define and Create our tables
Create a staging table for our CSV files
Copying the data into our tables

Luckily this isn’t too difficult and we can do this entirely using the snowsql CLI. For this project, I will be using a smaller dataset than I would like but unfortunately, I cannot use any of my company’s data and it can be pretty difficult to find large suitable datasets online. I did however find some transaction data from Dunnhumby which is freely available on Kaggle. Just for kicks though I create a much larger synthetic dataset using this data to test how well Dask handles the challenge compared to sklearn.
First of all, we need to set up a virtual warehouse and a database using the following commands in the Snowflake UI.
create or replace warehouse analytics_wh with
warehouse_size=”X-SMALL”
auto_suspend=180
auto_resume=true
initially_suspended=true;
create or replace database dunnhumby;
Our data consists of 6 CSVs which we will convert into 6 tables. I won’t spend too much time going over the dataset as this post is more about using Snowflake and Dask rather than interpreting data.
Below are the commands we can use to create our tables. All you will need to know in advance is what columns and data types you will be working with.

create or replace table campaign_desc ( 
description string, 
campaign number,
start_day number,
end_day number );

create or replace table campaign_table ( 
description string, 
Household_key number, 
campaign number );

create or replace table coupon ( 
COUPON_UPC number, 
product_id number, 
campaign number );

create or replace table coupon_redempt ( 
household_key number, 
day number, 
coupon_upc number, 
campaign number );

create or replace table transactions ( 
household_key number, 
BASKET_ID number, 
day number, 
product_id number, 
quantity number, 
sales_value number, 
store_id number, 
retail_disc decimal, 
trans_time number, 
week_no number, 
coupon_disc decimal, 
coupon_match_disc decimal );

create or replace table demographic_data ( 
age_dec string, 
marital_status_code string, 
income_desc string, 
homeowner_desc string, 
hh_comp_desc string, 
household_size_desc string, 
kid_category_desc string, 
Household_key number);

 
Now that we have our tables created we can start thinking about how to get data into them. For this, we will need to stage our CSV files. This is basically just an intermediary step so Snowflake can directly load the files from our stage into our tables. We can use the PUT command to put local files in our stage and then the COPY INTO command to instruct Snowflake where to put this data.

use database dunnhumby;

create or replace stage dunnhumby_stage;

PUT file://campaigns_table.csv @dunnhumby.public.dunnhumby_stage;

PUT file://campaigns_desc.csv @dunnhumby.public.dunnhumby_stage;

PUT file://coupon.csv @dunnhumby.public.dunnhumby_stage;

PUT file://coupon_d=redempt.csv @dunnhumby.public.dunnhumby_stage; 
PUT file://transaction_data.csv @dunnhumby.public.dunnhumby_stage; 
PUT file://demographics.csv @dunnhumby.public.dunnhumby_stage;

 
As a quick check, you can run this command to check what is in the staging area.

ls @dunnhumby.public.dunnhumby_stage;

 
Now we just need to copy the data into our tables using the queries below. You can execute these either in the Snowflake UI or in the command line after logging into Snowflake.

copy into campaign_table 
from @dunnhumby.public.dunnhumby_stage/campaigns_table.csv.gz 
file_format = ( type = csv
skip_header=1 
error_on_column_count_mismatch = false 
field_optionally_enclosed_by=’”’);

copy into campaign_desc 
from @dunnhumby.public.dunnhumby_stage/campaign_desc.csv.gz 
file_format = ( type = csv
skip_header=1 
error_on_column_count_mismatch = false 
field_optionally_enclosed_by=’”’);

copy into coupon 
from @dunnhumby.public.dunnhumby_stage/coupon.csv.gz 
file_format = ( type = csv
skip_header=1 
error_on_column_count_mismatch = false 
field_optionally_enclosed_by=’”’);

copy into coupon_redempt 
from @dunnhumby.public.dunnhumby_stage/coupon_redempt.csv.gz 
file_format = ( type = csv
skip_header=1 
error_on_column_count_mismatch = false 
field_optionally_enclosed_by=’”’);

copy into transactions 
from @dunnhumby.public.dunnhumby_stage/transaction_data.csv.gz 
file_format = ( type = csv
skip_header=1 
error_on_column_count_mismatch = false 
field_optionally_enclosed_by=’”’);

copy into demographic_data 
from @dunnhumby.public.dunnhumby_stage/demographics.csv.gz 
file_format = ( type = csv skip_header=1 
error_on_column_count_mismatch = false 
field_optionally_enclosed_by=’”’);

 
Ok great, with any luck we have our data in our tables first try. Oh, if only it was that simple, this whole process took me a few tries to get right (beware of spelling things wrong). Hopefully, you can follow along with this and be good to go. We are getting closer to the interesting stuff but the steps above are a vital part of the process so make sure you understand each of these steps.
 
Writing our Pipeline in SQL
 
In this next step, we will be writing the queries to generate our target, our features and then finally produce a training data set. One approach to creating a dataset for modelling is to read this data into memory and use pandas to create new features and join all the data frames together. This is typically the approach you see on Kaggle and in other online tutorials. The issue with this is that it is not very efficient, particularly when you are working with any reasonably sized datasets. For this reason, it is a much better idea to outsource the heavy lifting to something like Snowflake which handles massive datasets extremely well and will likely save you a huge amount of time. I won’t be spending much time diving into the specifics of our dataset here as it isn’t really vital for what I am trying to show. In general, though, you would want to spend a considerable amount of time exploring and understanding your data before you start modelling. The goal of these queries will be to preprocess the data and create some simple features which we can later use in our models.
 
Target Definition
 
Obviously, a vital component of supervised machine learning is defining an appropriate target to predict. For our use case, we will be predicting churn by calculating whether or not a user makes another visit within two weeks after a cutoff week. The choice of 2 weeks is pretty arbitrary and will depend on the specific problem we are trying to solve but let’s just assume that it is fine for this project. In general, you would want to carefully analyse your customers to understand the distribution in gaps between visits to arrive at a suitable definition of churn.
The main idea here is that for each table we want to have one row per household_key containing values for each of our features.

 
Campaign Features
 

 
Transaction Features
 
Below we create some simple metrics based on aggregate statistics such as the average, the max and standard deviation.

 
Demographic Features
 
This dataset has lots of missing data so I decided to use imputation here. There are plenty of techniques out there for missing data from dropping the missing data, to advanced imputation methods. I have just made life easy for myself here and replaced missing values with the mode. I wouldn’t necessarily recommend taking this approach in general as understanding why this data is missing is really important in deciding how to deal with it but for the purposes of this example, I will go ahead and take the easy approach. We first compute the mode for each of our features and then use coalesce to replace each row with the mode if data is missing.

 
Training Data
 
Finally, we build a query for our training data by joining our main tables together and end up with a table containing our target, our campaign, transactions and demographic features which we can use to build a model.
As a brief aside, for those interested in learning more about the features and nuances of Snowflake I would recommend the following book: Snowflake Cookbook. I started reading this book and it is full of really helpful information on how to use Snowflake and goes into far more detail than I do here.

 
Python Code for ETL
 
The final piece we require for this ETL is to write a script to execute it. Now, this is only really required if you plan on running an ETL like this regularly but this is good practice and makes it much easier to run the ETL as and when needed.
Let’s briefly discuss the main components of our EtlTraining class. Our class takes one input which is the cutoff week. This is due to the way data is defined in our dataset but ordinarily, this would be in a date format that corresponds to the cutoff date we want to choose for generating training data.
We initialise a list of our queries so we can easily loop through these and execute them. We also create a dictionary containing our parameters which we pass to our Snowflake connection. Here we use environment variables that we set up in Saturn Cloud. Here is a guide on how to do this. It is not too difficult to connect to Snowflake, all we need to do is use the Snowflake connector and pass in our dictionary of credentials. We implement this in the Snowflake connect method and return this connection as an attribute.
To make these queries a little bit easier to run I save each query as a python string variable in the ml_query_pipeline.py file. The execute_etl method does exactly what it says on the tin. We loop through each query, format it, execute it and finish off by closing the Snowflake connection.

To run this ETL we can simply type the commands below into the terminal. (where ml_pipeline is the name of the script above.)

python -m ml_pipeline -w 102 -j ‘train’

 
As a brief aside, you will probably want to run an ETL like this at regular intervals. For example, if you want to make daily predictions then you will need to generate a dataset like this each day to pass to your model so you can identify which of your customers are likely to churn. I won’t go into this in detail here but in my job, we use Airflow to orchestrate our ETLs so I would recommend checking it out if you are interested. In fact, I recently bought a book ‘Data Pipelines with Apache Airflow’ which I think is great and really gives some solid examples and advice on how to use airflow.
 
Dask and Modeling
 

Now that we have our data pipeline built, we can begin to think about modelling. The other main goal I have for this post is to highlight the advantages of using Dask as part of the ML development process and show you guys how easy it is to use.
For this part of the project, I also used Saturn Cloud which is a really nice tool I came across recently that allows us to harness the power of Dask across a cluster of computers in the cloud. The main advantages of using Saturn for me are that it is really easy to share your work, super simple to scale up your compute as and when you need it and it has a free tier option. Model development in general is a really good use case for Dask as we usually want to train a bunch of different models and see what works best. The faster we can do this the better as we have more time to focus on other important aspects of model development. Similar to Snowflake you just need to sign up here and you can very quickly spin up an instance of Jupyter lab and start experimenting with it yourself.
Now, I realise at this point I have mentioned Dask a few times but have never really explained what it is. So let me take a moment to give you a very high-level overview of Dask and why I think it is awesome. Very simply, Dask is a python library that takes advantage of parallel computing to allow you to process and perform operations on very large datasets. And, the best part is, if you are already familiar with Python, then Dask should be very straightforward as the syntax is very similar.
The graph below highlights the main components of Dask.


Source: Dask Documentation

Collections allow us to create a graph of tasks which can then be executed across multiple computers. Some of these data structures probably sound pretty familiar such as arrays and data frames and they are similar to what you would find in python but with some important differences. For example, you can think of a Dask data frame as a bunch of pandas data frames built in such a way that allows us to perform operations in parallel.
Moving on from collections we have the scheduler. Once we create the task graph the scheduler handles the rest for us. It manages the workflow and sends these tasks to either a single machine or distributes them across a cluster. Hopefully, that gives you a very brief overview of how Dask works. For more info, I suggest checking out the documentation or this book. Both are very good resources to dig deeper into this topic.
 
Python Code for Modelling
 
When modelling, I tend to have a small number of go-to algorithms that I will always try out first. This will generally give me a good idea of what might be suited to the specific problem I have. These models are Logistic Regression, Random Forest and GradientBoosting. In my experience, when working with tabular data these algorithms will usually give you pretty good results. Below we build a sklearn modelling pipeline using these 3 models. The exact models we use here are not really important as the pipeline should work for any sklearn classification model, this is just my preference.
Without further ado, let’s dive into the code. Luckily we outsourced most of our preprocessing to Snowflake so we don’t have to mess around with our training data too much here but we will add a few additional steps using sklearn pipelines.
The first code snippet below shows the pipeline when using sklearn. Notice our dataset is a plain old pandas data frame and our preprocessing steps are all carried out using sklearn methods. There is nothing particularly out of the ordinary going on here. We are reading in our data from the table produced by our Snowflake ETL and passing this into a sklearn pipeline. The usual modelling steps apply here. We split the dataset into train and test and do some preprocessing, namely impute missing values using the median, scale the data and one-hot encode our categorical data. I am a big fan of sklearn pipelines and basically use them whenever I develop models nowadays, they really facilitate clean and concise code.
How does this pipeline perform on a dataset with about 2 million rows? Well, running this model without any hyperparameter tuning takes about 34 minutes. Ouch, kinda slow. You can imagine how prohibitively long this would take if we wanted to do any type of hyperparameter tuning. Ok, so not ideal but let’s see how Dask handles the challenge.

 
Dask ML Python Code
 
Our goal here is to see if we can beat the sklearn pipeline above, spoiler alert, we definitely can. The cool thing about Dask is that the barrier to entry when you are already familiar with python is pretty low. We can get this pipeline up and running in Dask with only a few changes.
The first change you probably will notice is that we have some different imports. One of the key differences between this pipeline and the previous one is that we will be using a Dask data frame instead of a pandas data frame to train our model. You can think of a Dask data frame as a bunch of pandas data frames where we can perform computations on each one at the same time. This is the core of Dask’s parallelism and is what is going to reduce the training time for this pipeline.
Notice we use @dask.delayed as a decorator to our load_training_data function. This instructs Dask to parallelise this function for us.
We are also going to import some preprocessing and pipeline methods from Dask and most importantly, we will need to import SaturnCluster which will allow us to create a cluster for training our models. Another key difference with this code is that we use dask.persist after our train test split. Before this point, none of our functions has actually been computed due to Dask’s lazy evaluation. Once we use the persist method though we are telling Dask to send our data to the workers and execute the tasks we have created up until this point and leave these objects on the cluster.
Finally, we train our models using the delayed method. Again, this enables us to create our pipeline in a lazy way. The pipeline is not executed until we reach this code:

fit_pipelines = dask.compute(*pipelines_)

 

This time it only took us around 10 minutes to run this pipeline on the exact same dataset. That is a speedup by a factor of 3.4, not too shabby. Now, if we wanted to, we could speed this up even more by scaling up our compute resources at the touch of a button in Saturn.
 
Deploying our Pipeline
 

I mentioned earlier that you will probably want to run a pipeline like this quite regularly using something like airflow. It just so happens that if you don’t want the initial hassle of setting everything up for airflow Saturn Cloud offers a simple alternative with Jobs. Jobs allow us to package up our code and run it at regular intervals or as needed. All you need to do is go to an existing project and click on create a job. Once we do that, it should look like the following:


Source: Saturn

 
From here, all we need to do is make sure our python files above are in the directory in the image and we can enter our python command above

python -m ml_pipeline -w 102 -j 'train'

 
We can also set up a schedule using cron syntax to run the ETL on a daily basis if we like. For those interested, here is a Tutorial that goes into all the nitty-gritty.
 
Conclusions and Takeaways
 

Well, we have reached the end of our project at this point. Now obviously I have left out some key parts of the ML development cycle such as hyperparameter tuning and deploying our model but perhaps I will leave that for another day. Do I think you should try Dask? I am no expert by any means but from what I have seen so far it certainly seems really useful and I am super excited to experiment more with it and find more opportunities to incorporate it into my daily work as a data scientist. Hopefully, you found this useful and you too can see some of the advantages of Snowflake and Dask and you will start experimenting with them on your own.
 
Resources
 

Data Pipelines with Apache Airflow
Snowflake Cookbook
Data Science at Scale with Python and Dask
Coursera: SQL for Data Science

 
Some of my other posts you may find interesting
 
Let’s Build a Streaming Data Pipeline
 
Gaussian Mixture Modelling (GMM)
 
A Bayesian Approach to Time Series Forecasting
 
Note: Some of the links in this post are affiliate links.
 
Bio: Daniel Foley is a former Economist turned Data Scientist working in the mobile gaming industry.
Original. Reposted with permission.
Related:

BigQuery vs Snowflake: A Comparison of Data Warehouse Giants
Pandas not enough? Here are a few good alternatives to processing larger and faster data in Python
Are You Still Using Pandas to Process Big Data in 2021? Here are two better options"
https://www.kdnuggets.com/2021/04/microsoft-research-trains-neural-networks-understand-read.html,Microsoft Research Trains Neural Networks to Understand What They Read,The new models make inroads in a new areas of deep learning known as machine reading comprehension.,"By Jesus Rodriguez, Intotheblock.
comments


Source: https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017/

 

I recently started a new newsletter focus on AI education and already has over 50,000 subscribers. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:


 
Machine reading comprehension(MRC) is an emergent discipline in the field of deep learning. From a conceptual standpoint, MRC focuses on deep learning models that can answer intelligent questions about specific text documents. For humans, reading comprehension is a native cognitive skill developed since the early days of school or even before. At we are reading a text, we are instinctively extracting the key ideas that will allow us to answer future questions about that subject. In the case of artificial intelligence(AI) models, that skill is still largely underdeveloped.
The first widely adopted generation of natural language understanding(NLU) techniques has focused mostly on detecting the intentions and concepts associated with a specific sentence. We can think about these models as a first tier of knowledge to enable reading comprehension. However, full machine reading comprehension needs additional building blocks that can extrapolate and correlate questions to specific sections of a text and build knowledge from specific sections of a document.
One of the biggest challenges in the MRC domain is that most models are based on supervised training with datasets that contain not only the documents but potential questions and answers. As you can imagine, this approach is not only very difficult to scale but practically impossible to implement in some domains in which the data is simply not available. Recently, researchers from Microsoft proposed an interesting approach to deal with this challenge in MRC algorithms.
In a paper titled “Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension”, Microsoft’s Research introduced a technique called two stage synthesis networks or SynNet that applies transfer learning to reduce the effort to train a MRC model. SynNet can be seen as a two phase approach to build knowledge related to a specific text. In the first phase, SynNet learns a general pattern of identifying potential “interestingness” in a text document. These are key knowledge points, named entities, or semantic concepts that are usually answers that people may ask for. Then, in the second stage, the model learns to form natural language questions around these potential answers, within the context of the article.
The fascinating thing about SynNet is that, once trained, a model can be applied to a new domain, read the documents in the new domain and then generate pseudo questions and answers against these documents. Then, it forms the necessary training data to train an MRC system for that new domain, which could be a new disease, an employee handbook of a new company, or a new product manual.
Many people erroneously associate MRC technique with the more developed field of machine translation. In the case of MRC models such as SynNet, the challenge is that they need to synthesize both questions and answers for a document. While the question is a syntactically ﬂuent natural language sentence, the answer is mostly a salient semantic concept in the paragraph, such as a named entity, an action, or a number. Since the answer has a different linguistic structure than the question, it may be more appropriate to view answers and questions as two different types of data. SynNet materializes in that theory by decomposing the process of generating question-answer pairs into two fundamental steps: The answer generation conditioned on the paragraph and the question generation conditioned on the paragraph and the answer.


Image Credit: Microsoft Research

 
You can think about SynNet as a teacher that is very good at generating questions from documents based on its experience. As it learn about the relevant questions in one domain, it can apply the same patterns to documents in a new domain. Microsoft researchers have applied the principles of SynNet to different MRC models including the recently published ReasoNet which have shown a lot of promise towards making machine reading comprehension a reality in the near future.
 
Original. Reposted with permission.
Related:

Explainable Visual Reasoning: How MIT Builds Neural Networks that can Explain Themselves
Teaching AI to See Like a Human
How Reading Papers Helps You Be a More Effective Data Scientist"
https://www.kdnuggets.com/2021/06/create-deploy-sentiment-analysis-app-api.html,How to Create and Deploy a Simple Sentiment Analysis App via API,"In this article we will create a simple sentiment analysis app using the HuggingFace Transformers library, and deploy it using FastAPI.","By Matthew Mayo, KDnuggets.
comments

Image source: Reputation X 
Let's say you've built an NLP model for some specific task, whether it be text classification, question answering, translation, or what have you. You've tested it out locally and it performs well. You've had others test it out as well, and it continues to perform well. Now you want to roll it out to a larger audience, be that audience a team of developers you work with, a specific group of end users, or even the general public. You have decided that you want to do so using a REST API, as you find this to be your best option. What now?
FastAPI might be able to help. FastAPI is FastAPI is a web framework for building APIs with Python. We will use FastAPI in this article to build a REST API to service an NLP model which can be queried via GET request and can dole out responses to those queries.
For this example, we will skip the building of our own model, and instead leverage the Pipeline class of the HuggingFace Transformers library. Transformers is full of SOTA NLP models which can be used out of the box as-is, as well as fine-tuned for specific uses and high performance. The library's pipelines can be summed up as:
The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.
 
Using the Transformers library, FastAPI, and astonishingly little code, we are going to create and deploy a very simple sentiment analysis app. We will also see how extending this same approach to a more complex app would be quite straightforward.
 
Getting Started
As outlined above, we will be using Transformers and FastAPI to build this app, which means you will need these installed on your system. You will also require installation of Uvicorn, an ASGI server that FastAPI relies on as part of its backend. I easily installed them all on my *buntu system using pip:

pip install transformers
pip install fastapi
pip install uvicorn

 
That's it. Moving on...
 
Analyzing Sentiment
Since we will be using Transformers for our NLP pipeline, let's first see how we would get this working standalone. Doing so is remarkably uncomplicated, and requires very few basic parameters be passed to the pipeline object in order to get started. Specifically, what we will need to define are a task — what it is we want to do — and a model — what it is we will use to perform our task. And that's really all there is to it. We can optionally provide additional parameters or fine-tune the pipeline to our specific task and data, but for our purposes using a model out of the box should work just fine.
For us, the task is sentiment-analysis and the model is nlptown/bert-base-multilingual-uncased-sentiment. This is a BERT model trained for multilingual sentiment analysis, and which has been contributed to the HuggingFace model repository by NLP Town. Note that the first time you run this script the sizable model will be downloaded to your system, so ensure that you have the available free space to do so.
Here is the code to setup the standalone sentiment analysis app:

from transformers import pipeline

text = 'i love this movie!!! :)'

# Instantiate a pipeline object with our task and model passed as parameters
nlp = pipeline(task='sentiment-analysis', 
               model='nlptown/bert-base-multilingual-uncased-sentiment')

# Pass the text to our pipeline and print the results
print(f'{nlp(text)}')

 
What is returned by the call to the pipeline object is a predicted label and its corresponding probability. In our case, the combination of task and model that we are using results in labels between 1 (negative) and 5 (positive), along with the prediction probability. Let's give our script a run to see how it does.

python test_model.py

>>> [{'label': '5 stars', 'score': 0.923753023147583}]

 
And that's all there is to getting some functionally basic NLP task-specific out of the box model up and running using the Transformers library and its Pipeline class.
If you want to test this out a bit more before we move on to deploying it via a REST API, give this modified script a shot, which allows us to pass text from the command line and responds with a more nicely formatted reply:

import sys
from transformers import pipeline

if len(sys.argv) != 2:
    print('Usage: python model_test.py <input_string>')
    sys.exit(1)

text = sys.argv[1]

# Instantiate a pipeline object with our task and model passed as parameters
nlp = pipeline(task='sentiment-analysis',
               model='nlptown/bert-base-multilingual-uncased-sentiment')

# Get and process result
result = nlp(text)

sent = ''
if (result[0]['label'] == '1 star'):
    sent = 'very negative'
elif (result[0]['label'] == '2 star'):
    sent = 'negative'
elif (result[0]['label'] == '3 stars'):
    sent = 'neutral'
elif (result[0]['label'] == '4 stars'):
    sent = 'positive'
else:
    sent = 'very positive'

prob = result[0]['score']

# Format and print results
print(f""{{'sentiment': '{sent}', 'probability': '{prob}'}}"")

 
Let's run this a few times to see how it performs:

python model_test.py 'the sky is blue'

>>> {'sentiment': 'neutral', 'probability': '0.2726307213306427'}

python model_test.py 'i really hate this restaurant!'

>>> {'sentiment': 'very negative', 'probability': '0.9228281378746033'}

python model_test.py 'i love this movie!!! :)'

>>> {'sentiment': 'very positive', 'probability': '0.923753023147583'}

 
This is better functionality, since we don't have to hard code the text we want analyzed into our program, and we have also made the results a bit more user-friendly. Let's extend this more useful version and deploy it as a REST API.
 
Creating The API
Time to deploy this ridiculously simple sentiment analysis app via REST API built using FastAPI. If you want to learn more about FastAPI and how to format your code using the framework, check out its documentation. What you absolutely need to know here is that we will create an instance of FastAPI (app), and then must define get requests, attach them to URLs, and assign responses for these requests via functions.
Below, we will define 2 such get requests; one for the root URL ('/'), which displays a welcome message, and another for accepting strings for performing sentiment analysis on ('/sentiment_analysis/'). The code for both is quite simple; you should recognize much of what is contained in the analyze_sentiment() function that the '/sentiment_analysis/' request calls from our standalone app.

from transformers import pipeline
from fastapi import FastAPI

nlp = pipeline(task='sentiment-analysis',
               model='nlptown/bert-base-multilingual-uncased-sentiment')

app = FastAPI()


@app.get('/')
def get_root():
    return {'message': 'This is the sentiment analysis app'}


@app.get('/sentiment_analysis/')
async def query_sentiment_analysis(text: str):
    return analyze_sentiment(text)


def analyze_sentiment(text):
    """"""Get and process result""""""

    result = nlp(text)

    sent = ''
    if (result[0]['label'] == '1 star'):
        sent = 'very negative'
    elif (result[0]['label'] == '2 star'):
        sent = 'negative'
    elif (result[0]['label'] == '3 stars'):
        sent = 'neutral'
    elif (result[0]['label'] == '4 stars'):
        sent = 'positive'
    else:
        sent = 'very positive'

    prob = result[0]['score']

    # Format and return results
    return {'sentiment': sent, 'probability': prob}

 
And now we have a REST API capable of accepting get requests and performing sentiment analysis. Before we try it out, we have to run the Uvicorn web server which will provide the necessary back end functionality. In order to do so, and assuming you saved the above code in a file called main.py and left the name of the FastAPI instance as app, run this:

 uvicorn main:app --reload

 
You should then see something like this:

INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [18271] using statreload
INFO:     Started server process [18273]
INFO:     Waiting for application startup.
INFO:     Application startup complete.

 
Open your browser as indicated to http://127.0.0.1:8000 and you should see:

If you see the welcome message, congrats, it works! We can try using the browser address bar to make some requests, pasting what is in the quotation marks below after the request URL and query string (http://127.0.0.1:8000/sentiment_analysis/?text=):
""welcome to my home!""

""i don't like your cat""

""that movie was just okay""

Great, we get results! Now what if we want to treat this like an API and access it accordingly? In Python, we could use the requests library. Make sure it's installed using:

pip install requests

 
Then give a script like this a try:

import requests

query = {'text':'i love the fettucine alfredo and would definitely recommend this restaurant to my friends!'}
response = requests.get('http://127.0.0.1:8000/sentiment_analysis/', params=query)
print(response.json())

 
After saving, execute the script and you should get a result like this:

python rest_request.py

>>> {'sentiment': 'very positive', 'probability': 0.8293750882148743}


 
This worked as well. Excellent!
You can read more about the requests library here.
 
Conclusions
There is obviously much more we could have done here. Preprocessing the data would have been useful. Performing error checking would have been advisable. As confirmation of this, try using some crazy characters or not wrapping a lengthy request with punctuation in quotes and see what happens.
Next time I promise we will expand upon what we have done here today and will make something more robust and far more interesting. I'm already working on it.
Related:

Getting Started with 5 Essential Natural Language Processing Libraries
Learn Neural Networks for Natural Language Processing Now
Machine Learning Pipeline Optimization with TPOT"
https://www.kdnuggets.com/2020/12/informs-machine-learning-roots.html,Machine Learning: Cutting Edge Tech with Deep Roots in Other Fields,"Join INFORMS community of data, analytics, operations research, and statistics professionals and tackle the future together. With nearly 13,000 members around the world, INFORMS is the largest international association for data science professionals.","Sponsored Post.

As you probably know, many breakthroughs in machine learning and related disciplines can be attributed to the rapid advances in computing power and data capacity made in the last several decades. However, there are many fields outside of computer engineering and programming that have played and continue to play a role in the development of machine learning. And even within computing, there are bleeding edge topics with significance for AI and machine learning just starting to gain steam. Quantum computing, for instance, can potentially help address a major issue with current machine learning models – namely, the data going into them. Quantum computing moves beyond classical computing’s binary data, which provides opportunities to use multidimensional datasets, which should yield higher-fidelity models.
If data quality is paramount, data science is then, naturally, another component field of machine learning. Simon Lee, Chief Analytics Officer at Waitr Inc., argues that, contrary to the public’s conception of their role, data science professionals spend most of their time gathering and cleaning data and finding data artifacts. “Farmers don’t just spend their time harvesting the crops and selling them. Most of their time is spent in the dirt,” he says.
Lee’s colorful metaphor recalls yet another related field: linguistics. Writes Joseph Byrum, chief data scientist at Principal Financial Group, “Linguist Noam Chomsky once held out the possibility of a universal grammar, which, if properly analyzed, could bring human and computer language closer. The idea was that there were innate properties of communication shared between the thousands of different languages spoken around the world [14].” Natural language processing remains one of the buzziest sub-disciplines of machine learning, and linguistics experts continue to be consulted in experimental efforts.
There are many more fields and subfields with ties to machine learning and INFORMS is proud that the expertise of its members spans much of this incredibly diverse space. If you’re excited about saving lives, saving money, and solving problems, join our community of likeminded data and analytics professionals, programmers, statisticians, and operations researchers. Let’s tackle the future together.
 
With nearly 13,000 members around the world, INFORMS is the largest international association for data science professionals. INFORMS provides unique and valuable opportunities for individual professionals and organizations to better use a wide variety of big data, analytics, and operations research methods to drive strategic visions and achieve better outcomes."
https://www.kdnuggets.com/2020/09/solving-linear-regression.html,Which methods should be used for solving linear regression?,"As a foundational set of algorithms in any machine learning toolbox, linear regression can be solved with a variety of approaches. Here, we discuss. with with code examples, four methods and demonstrate how they should be used.","comments
By Ahmad Bin Shafiq, Machine Learning Student.
Linear Regression is a supervised machine learning algorithm. It predicts a linear relationship between an independent variable (y), based on the given dependant variables (x), such that the independent variable (y) has the lowest cost.
 
Different approaches to solve linear regression models
 
There are many different methods that we can apply to our linear regression model in order to make it more efficient. But we will discuss the most common of them here.

Gradient Descent
Least Square Method / Normal Equation Method
Adams Method
Singular Value Decomposition (SVD)

Okay, so let’s begin…
 
Gradient Descent
 
One of the most common and easiest methods for beginners to solve linear regression problems is gradient descent.
How Gradient Descent works
Now, let's suppose we have our data plotted out in the form of a scatter graph, and when we apply a cost function to it, our model will make a prediction. Now this prediction can be very good, or it can be far away from our ideal prediction (meaning its cost will be high). So, in order to minimize that cost (error), we apply gradient descent to it.
Now, gradient descent will slowly converge our hypothesis towards a global minimum, where the cost would be lowest. In doing so, we have to manually set the value of alpha, and the slope of the hypothesis changes with respect to our alpha’s value. If the value of alpha is large, then it will take big steps. Otherwise, in the case of small alpha, our hypothesis would converge slowly and through small baby steps.

Hypothesis converging towards a global minimum. Image from Medium.
The Equation for Gradient Descent is

Source: Ruder.io.
Implementing Gradient Descent in Python

import numpy as np
from matplotlib import pyplot

#creating our data
X = np.random.rand(10,1)
y = np.random.rand(10,1)
m = len(y)
theta = np.ones(1)

#applying gradient descent
a = 0.0005
cost_list = []
for i in range(len(y)):
    
    theta = theta - a*(1/m)*np.transpose(X)@(X@theta - y)
           
    cost_val = (1/m)*np.transpose(X)@(X@theta - y)
    cost_list.append(cost_val)

#Predicting our Hypothesis
b = theta
yhat = X.dot(b)

#Plotting our results
pyplot.scatter(X, y, color='red')
pyplot.plot(X, yhat, color='blue')
pyplot.show()



 

Model after Gradient Descent.
Here first, we have created our dataset, and then we looped over all our training examples in order to minimize our cost of hypothesis.
Pros:
Important advantages of Gradient Descent are

Less Computational Cost as compared to SVD or ADAM
Running time is O(kn²)
Works well with more number of features

Cons:
Important cons of Gradient Descent are

Need to choose some learning rate α
Needs many iterations to converge
Can be stuck in Local Minima
If not proper Learning Rate α, then it might not converge.

 
Least Square Method
 
The least-square method, also known as the normal equation, is also one of the most common approaches to solving linear regression models easily. But, this one needs to have some basic knowledge of linear algebra.
How the least square method works
In normal LSM, we solve directly for the value of our coefficient. In short, in one step, we reach our optical minimum point, or we can say only in one step we fit our hypothesis to our data with the lowest cost possible.

Before and after applying LSM to our dataset. Image from Medium.
The equation for LSM is

Implementing LSM in Python

import numpy as np
from matplotlib import pyplot

#creating our data
X = np.random.rand(10,1)
y = np.random.rand(10,1)

#Computing coefficient
b = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

#Predicting our Hypothesis
yhat = X.dot(b)
#Plotting our results
pyplot.scatter(X, y, color='red')
pyplot.plot(X, yhat, color='blue')
pyplot.show()



 

Here first we have created our dataset and then minimized the cost of our hypothesis using the
b = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
code, which is equivalent to our equation.
Pros:
Important advantages of LSM are:

No Learning Rate
No Iterations
Feature Scaling Not Necessary
Works really well when the Number of Features is less.

Cons:
Important cons are:

Is computationally expensive when the dataset is big.
Slow when Number of Features is more
Running Time is O(n³)
Sometimes, your X transpose X is non-invertible, i.e., a singular matrix with no inverse. You can use np.linalg.pinv instead of np.linalg.inv to overcome this problem.

 
Adam’s Method
 
ADAM, which stands for Adaptive Moment Estimation, is an optimization algorithm that is widely used in Deep Learning.
It is an iterative algorithm that works well on noisy data.
It is the combination of RMSProp and Mini-batch Gradient Descent algorithms.
In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.
We compute the decaying averages of past and past squared gradients respectively as follows:

Credit: Ruder.io.
As mt and vt are initialized as vectors of 0’s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e., β1β1 and β2β2 are close to 1).
They counteract these biases by computing bias-corrected first and second-moment estimates:

Credit: Ruder.io.
They then update the parameters with:

Credit: Ruder.io.
You can learn the theory behind Adam here or here.
Pseudocode for Adam is

Source: Arxiv Adam.
Let’s see it’s code in Pure Python.

#Creating the Dummy Data set and importing libraries
import math
import seaborn as sns
import numpy as np 
from scipy import stats
from matplotlib import pyplot
x = np.random.normal(0,1,size=(100,1))
y = np.random.random(size=(100,1))



Now Let’s find the actual graph of Linear Regression and values for slope and intercept for our dataset.

print(""Intercept is "" ,stats.mstats.linregress(x,y).intercept)
print(""Slope is "", stats.mstats.linregress(x,y).slope)



 

Now let us see the Linear Regression line using the Seaborn regplot function.

pyplot.figure(figsize=(15,8))
sns.regplot(x,y)
pyplot.show()



 

Let us code Adam Optimizer now in pure Python.

h = lambda theta_0, theta_1, x: theta_0 + np.dot(x,theta_1) #equation of straight lines

# the cost function (for the whole batch. for comparison later)
def J(x, y, theta_0, theta_1):
    m = len(x)
    returnValue = 0
    for i in range(m):
        returnValue += (h(theta_0, theta_1, x[i]) - y[i])**2
    returnValue = returnValue/(2*m)
    return returnValue

# finding the gradient per each training example
def grad_J(x, y, theta_0, theta_1):
    returnValue = np.array([0., 0.])
    returnValue[0] += (h(theta_0, theta_1, x) - y)
    returnValue[1] += (h(theta_0, theta_1, x) - y)*x
    return returnValue

class AdamOptimizer:
    def __init__(self, weights, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.alpha = alpha
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = 0
        self.v = 0
        self.t = 0
        self.theta = weights
        
    def backward_pass(self, gradient):
        self.t = self.t + 1
        self.m = self.beta1*self.m + (1 - self.beta1)*gradient
        self.v = self.beta2*self.v + (1 - self.beta2)*(gradient**2)
        m_hat = self.m/(1 - self.beta1**self.t)
        v_hat = self.v/(1 - self.beta2**self.t)
        self.theta = self.theta - self.alpha*(m_hat/(np.sqrt(v_hat) - self.epsilon))
        return self.theta



Here, we have implemented all the equations mentioned in the pseudocode above using an object-oriented approach and some helper functions.
Let us now set the hyperparameters for our model.

epochs = 1500
print_interval = 100
m = len(x)
initial_theta = np.array([0., 0.]) # initial value of theta, before gradient descent
initial_cost = J(x, y, initial_theta[0], initial_theta[1])

theta = initial_theta
adam_optimizer = AdamOptimizer(theta, alpha=0.001)
adam_history = [] # to plot out path of descent
adam_history.append(dict({'theta': theta, 'cost': initial_cost})#to check theta and cost function



And finally, the training process.

for j in range(epochs):
    for i in range(m):
        gradients = grad_J(x[i], y[i], theta[0], theta[1])
        theta = adam_optimizer.backward_pass(gradients)
    
    if ((j+1)%print_interval == 0 or j==0):
        cost = J(x, y, theta[0], theta[1])
        print ('After {} epochs, Cost = {}, theta = {}'.format(j+1, cost, theta))
        adam_history.append(dict({'theta': theta, 'cost': cost}))
        
print ('\nFinal theta = {}'.format(theta))



 

Now, if we compare the Final theta values to the slope and intercept values, calculated earlier using scipy.stats.mstat.linregress, they are almost 99% equal and can be 100% equal by adjusting the hyperparameters.
Finally, let us plot it.

b = theta
yhat = b[0] + x.dot(b[1])
pyplot.figure(figsize=(15,8))
pyplot.scatter(x, y, color='red')
pyplot.plot(x, yhat, color='blue')
pyplot.show()



 

And we can see that our plot is similar to plot obtained using sns.regplot.
Pros:

Straightforward to implement.
Computationally efficient.
Little memory requirements.
Invariant to diagonal rescale of the gradients.
Well suited for problems that are large in terms of data and/or parameters.
Appropriate for non-stationary objectives.
Appropriate for problems with very noisy/or sparse gradients.
Hyper-parameters have intuitive interpretation and typically require little tuning.

Cons:

Adam and RMSProp are highly sensitive to certain values of the learning rate (and, sometimes, other hyper-parameters like the batch size), and they can catastrophically fail to converge if e.g., the learning rate is too high. (Source: stackexchange)

 
Singular Value Decomposition
 
Singular value decomposition shortened as SVD is one of the famous and most widely used dimensionality reduction methods in linear regression.
SVD is used (amongst other uses) as a preprocessing step to reduce the number of dimensions for our learning algorithm. SVD decomposes a matrix into a product of three other matrices (U, S, V).

Once our matrix has been decomposed, the coefficients for our hypothesis can be found by calculating the pseudoinverse of the input matrix X and multiplying that by the output vector y. After that, we fit our hypothesis to our data, and that gives us the lowest cost.
Implementing SVD in Python

import numpy as np
from matplotlib import pyplot

#Creating our data
X = np.random.rand(10,1)
y = np.random.rand(10,1)

#Computing coefficient
b = np.linalg.pinv(X).dot(y)

#Predicting our Hypothesis
yhat = X.dot(b)

#Plotting our results
pyplot.scatter(X, y, color='red')
pyplot.plot(X, yhat, color='blue')
pyplot.show()




Though it is not converged very well, it is still pretty good.
Here first, we have created our dataset and then minimized the cost of our hypothesis using b = np.linalg.pinv(X).dot(y), which is the equation for SVD.
Pros:

Works better with higher dimensional data
Good for gaussian type distributed data
Really stable and efficient for a small dataset
While solving linear equations for linear regression, it is more stable and the preferred approach.

Cons:

Running time is O(n³)
Multiple risk factors
Really sensitive to outliers
May get unstable with a very large dataset

 
Learning Outcome
 
As of now, we have learned and implemented gradient descent, LSM, ADAM, and SVD. And now, we have a very good understanding of all of these algorithms, and we also know what are the pros and cons.
One thing we noticed was that the ADAM optimization algorithm was the most accurate, and according to the actual ADAM research paper, ADAM outperforms almost all other optimization algorithms.
 
Related:

Linear to Logistic Regression, Explained Step by Step
A Beginner’s Guide to Linear Regression in Python with Scikit-Learn
Linear Regression In Real Life"
https://www.kdnuggets.com/2020/12/crack-sql-interviews.html,Crack SQL Interviews,"SQL is an essential programming language for data analysis and processing. So, SQL questions are always part of the interview process for data science-related jobs, including data analysts, data scientists, and data engineers. Become familiar with these common patterns seen in SQL interview questions and follow our tips on how to neatly handle each with SQL queries.","comments
By Xinran Waibel, Data Engineer at Netflix.

Photo by Green Chameleon on Unsplash.
SQL is one of the most essential programming languages for data analysis and data processing, and so SQL questions are always part of the interview process for data science-related jobs, such as data analysts, data scientists, and data engineers. SQL interviews are meant to evaluate candidates’ technical and problem-solving skills. Therefore, it is critical to write not only correct queries based on sample data but also consider various scenarios and edge cases as if working with real-world datasets.
I’ve helped design and conduct SQL interview questions for data science candidates and have undergone many SQL interviews for jobs in giant technology companies and startups myself. In this blog post, I will explain the common patterns seen in SQL interview questions and provide tips on how to neatly handle them in SQL queries.
 
Ask Questions
 
To nail an SQL interview, the most important thing is to make sure that you have all the details of the given task and data sample by asking as many questions as you need. Understanding the requirements will save you time from iterating on problems later and enable you to handle edge cases well.
I noticed many candidates tend to jump right into the solution without having a good understanding of the SQL questions or the dataset. Later on, they had to repeatedly modify their queries after I pointed out problems in their solution. In the end, they wasted a lot of interview time in iteration and may not have even arrived at the right solution.
I recommend treating SQL interviews as if you are working with a business partner at work. You would want to gather all the requirements on the data request before you provide a solution.
Example
Find the top 3 employees who have the highest salary.
 
The sample employee_salary table.
You should ask the interviewer(s) to clarify the “top 3”. Should I include exactly 3 employees in my results? How do you want me to handle ties? In addition, carefully review the sample employee data. What is the data type of the salary field? Do I need to clean the data before calculate?
 
Which JOIN
 

Source: dofactory.
In SQL, JOIN is frequently used to combine information from multiple tables. There are four different types of JOIN, but in most cases, we only use INNER, LEFT, and FULL JOIN because the RIGHT JOIN is not very intuitive and can be easily rewritten using LEFT JOIN. In an SQL interview, you need to choose the right JOIN to use based on the specific requirement of the given question.
Example
Find the total number of classes taken by each student. (Provide student id, name, and number of classes taken.)
 
The sample student and class_history tables.
As you might have noticed, not all students appearing in the class_history table are present in the student table, which might be because those students are no longer enrolled. (This is actually very typical in transactional databases, as records are often deleted once inactive.) Depending on whether the interviewer wants inactive students in the results, we need to use either LEFT JOIN or INNER JOIN to combine two tables:

WITH class_count AS (
    SELECT student_id, COUNT(*) AS num_of_class
    FROM class_history
    GROUP BY student_id
)
SELECT
    c.student_id,
    s.student_name,
    c.num_of_class
FROM class_count c
-- CASE 1: include only active students
JOIN student s ON c.student_id = s.student_id
-- CASE 2: include all students
-- LEFT JOIN student s ON c.student_id = s.student_id



 

Photo by petr sidorov on Unsplash.
 
GROUP BY
 
GROUP BY is the most essential function in SQL since it is widely used for data aggregation. If you see keywords such as sum, average, minimum, or maximum in a SQL question, it is a big hint that you should probably use GROUP BY in your query. A common pitfall is mixing WHERE and HAVING when filtering data along with GROUP BY — I have seen many people make this mistake.
Example
Calculate the average required course GPA in each school year for each student and find students who are qualified for the Dean’s List (GPA ≥ 3.5) in each semester.
 
The sample GPA history table.
Since we consider only the required courses in our GPA calculation, we need to exclude optional courses using WHERE is_required = TRUE. We need the average GPA per student per year, so we will GROUP BY both the student_id and the school_year columns and take the average of the gpa column. Lastly, we only keep rows where the student has an average GPA higher than 3.5, which can be implemented using HAVING. Let’s put everything together:

SELECT
    student_id,
    school_year,
    AVG(gpa) AS avg_gpa
FROM gpa_history
WHERE is_required = TRUE
GROUP BY student_id, school_year
HAVING AVG(gpa) >= 3.5



 
Keep in mind that whenever GROUP BY is used in a query, you can only select group-by columns and aggregated columns because the row-level information in other columns has already been discarded.
Some people might wonder what’s the difference between WHERE and HAVING, or why we don’t just write HAVING avg_gpa >= 3.5 instead of specifying the function. I will explain more in the next section.
 
SQL query execution order
 
Most people write SQL queries from top to bottom starting from SELECT, but do you know that SELECT is one of the very last functions executed by the SQL engine? Below is the execution order of a SQL query:

FROM, JOIN
WHERE
GROUP BY
HAVING
SELECT
DISTINCT
ORDER BY
LIMIT, OFFSET

Consider the previous example again. Because we want to filter out optional courses before computing average GPAs, I used WHERE is_required = TRUE instead of HAVING because WHERE is executed before GROUP BY and HAVING. The reason I can’t write HAVING avg_gpa >= 3.5 is that avg_gpa is defined as part of SELECT, so it cannot be referred to in steps executed before SELECT.
I recommend following the execution order when writing queries, which is helpful if you struggle with writing complicated queries.

Photo by Stefano Ghezzi on Unsplash.
 
Window functions
 
Window functions frequently appear in SQL interviews as well. There are five common window functions:

RANK/DENSE_RANK /ROW_NUMBER: these assign a rank to each row by ordering specific columns. If any partition columns are given, rows are ranked within a partition group that it belongs to.
LAG/LEAD: it retrieves column values from a preceding or following row based on a specified order and partition group.

In SQL interviews, it is important to understand the differences between ranking functions and know when to use LAG/LEAD.
Example
Find the top 3 employees who have the highest salary in each department.

Another sample employee_salary table.
When an SQL question asks for “TOP N”, we can use either ORDER BY or ranking functions to answer the question. However, in this example, it asks to calculate “TOP N X in each Y”, which is a strong hint that we should use ranking functions because we need to rank rows within each partition group.
The query below finds exactly 3 highest-payed employees regardless of ties:

WITH T AS (
SELECT
    *,
    ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY employee_salary DESC) AS rank_in_dep
FROM employee_salary)
SELECT * FROM T
WHERE rank_in_dep <= 3
-- Note: When using ROW_NUMBER, each row will have a unique rank number and ranks for tied records are assigned randomly. For exmaple, Rimsha and Tiah may be rank 2 or 3 in different query runs.



 
 
Moreover, based on how ties should be handled, we could pick a different ranking function. Again, details matter!

A comparison of the results of ROW_NUMBER, RANK, and DENSE_RANK functions.

Photo by Héctor J. Rivas on Unsplash.
 
Duplicates
 
Another common pitfall in SQL interviews is ignoring data duplicates. Although some columns seem to have distinct values in the sample data, candidates are expected to consider all possibilities as if they are working with a real-world dataset. For example, in the employee_salary table from the previous example, it is possible to have employees sharing the same name.
One easy way to avoid potential problems caused by duplicates is to always use ID columns to uniquely identify distinct records.
Example
Find the total salary from all departments for each employee using the employee_salary table.
The right solution is to GROUP BY employee_id and calculate the total salary using SUM(employee_salary). If employee names are needed, join with an employee table at the end to retrieve employee name information.
The wrong approach is to GROUP BY employee_name.
 
NULL
 
In SQL, any predicates can result in one of the three values: true, false, and NULL, a reserved keyword for unknown or missing data values. Handling NULL datasets can be unexpectedly tricky. In an SQL interview, the interviewer might pay extra attention to whether your solution has handled NULL values. Sometimes it is obvious if a column is not nullable (ID columns, for instance), but for most other columns, it is very likely there will be NULL values.
I suggest confirming whether key columns in the sample data are nullable and, if so, utilize functions such as IS (NOT) NULL, IFNULL, and COALESCE to cover those edge cases.
(Want to learn more about how to deal with NULL values? Check out my guide on working with NULL in SQL.)
 
Communication
 
Last but not least — keep the communication going during SQL interviews.
I interviewed many candidates who barely talked except when they had questions, which would be okay if they came up with the perfect solution at the end. However, it is generally a good idea to keep up communication during technical interviews. For example, you can talk about your understanding of the question and data, how you plan to approach the problem, why you use some functions versus other alternatives, and what edge cases you are considering.
TL;DR:

Always ask questions to gather the required details first.
Carefully choose between INNER, LEFT, and FULL JOIN.
Use GROUP BY to aggregate data and properly use WHERE and HAVING.
Understand the differences between the three ranking functions.
Know when to use LAG/LEAD window functions.
If you struggle with creating complicated queries, try following the SQL execution order.
Consider potential data problems, such as duplicates and NULL values.
Communicate your thought process with the interviewers.

To help you understand how to use these strategies in an actual SQL interview, I will walk you through a sample SQL interview question from end to end in the video below:

Original. Reposted with permission.
 
Bio: Xinran Waibel is an experienced Data Engineer in the San Francisco Bay Area, currently working at Netflix. She is also a technical writer for Towards Data Science, Google Cloud, and The Startup on Medium.
Related:

The Ultimate Guide to Data Engineer Interviews
How to Rock a Virtual Data Interview
The Data Science Interview Study Guide"
https://www.kdnuggets.com/2021/04/automated-text-classification-evalml.html,Automated Text Classification with EvalML,"Learn how EvalML leverages Woodwork, Featuretools and the nlp-primitives library to process text data and create a machine learning model that can detect spam text messages.","comments
By Angela Lin, EvalML Software Engineer

Text can be a rich and informative type of data. It can be used in a variety of tasks, including sentiment analysis, topic extraction, and spam detection. However, raw text cannot be fed directly to machine learning algorithms, because most models can only understand numeric values. Thus, to utilize text as data in machine learning, it must first be processed and transformed to numeric values.
In this post, we will learn how we can use EvalML to detect spam text messages by framing it as a binary classification problem using text data. EvalML is an AutoML library written in Python that uses Woodwork to detect and specify how data should be treated, and the nlp-primitives library to create meaningful numeric features from raw text data.
 
Spam Dataset
 
The dataset we will be using in this demo consists of SMS text messages in English, some of which are tagged as legitimate (“ham”), and others which are tagged as spam. For this demo, we have modified the original dataset from Kaggle by joining all of the input text columns and downsampling the majority class (“ham”) so that the “ham” to “spam” ratio is 3:1. The following references to the data we will be inspecting will always refer to our modified and smaller dataset.
Let’s load in our data and display a few rows to understand what our text messages look like:

from urllib.request import urlopen
import pandas as pd

input_data = urlopen('https://featurelabs-static.s3.amazonaws.com/spam_text_messages_modified.csv')
data = pd.read_csv(input_data)
X = data.drop(['Category'], axis=1)
y = data['Category']display(X.head())


A sample of our input data

 
We can plot the frequency of our target values to verify that the ratio of “ham” to “spam” in our modified dataset is approximately 3:1.

y.value_counts().plot.pie(figsize=(10,10))



The ratio of ""ham"" to ""spam"" is approximately 3:1

 
Because the ratio of “ham” to “spam” is 3:1, we can create a trivial model that always classifies a message as the majority “ham” class to obtain a model that has a 75% accuracy. This model would also have a recall score of 0%, since it is unable to classify any of the minority “spam” class samples correctly, and a balanced accuracy score of 50%. This means that a machine learning model should have an accuracy score greater than 75%, a recall score greater than 0%, and a balanced accuracy score greater than 50% to be useful.



Baseline model (always guesses majority class)


Accuracy
75%


Balanced Accuracy
50%


Recall
0%



Let’s generate a model using EvalML and see if we can do better than this trivial model!
 
Introducing Woodwork
 
Before feeding our data into EvalML, we have a more fundamental issue to address: How can we specify that our data should be treated as text data? Using pandas alone, we can't distinguish between text data and non-text data (such as categorical data) because pandas uses the same object data type to store both. How we can make sure that our models correctly treat our text messages as text data, and not as hundreds of different unique categories?
pandas treats “Message” as an “object” data type by default

 
EvalML utilizes the open-source Woodwork library to detect and specify how each feature should be treated, independent of its underlying physical data type. This means we can treat columns with the same physical data type differently. For example, we can specify that we want some columns that contain text to be treated as categorical columns, while we treat other columns with text as natural language columns, even if these columns have the same underlying object datatype. This differentiation allows us to clear up the ambiguity between features that may have the same underlying datatype in pandas, but ultimately represent different types of data.
Here, we initialize a Woodwork DataTable with our feature. Our single Message feature is automatically detected as a natural language or text column.

import woodwork as ww
X = ww.DataTable(X)

# Note: We could have also manually set the Message column to 
# natural language if Woodwork had not automatically detected
from evalml.utils import infer_feature_types
X = infer_feature_types(X, {'Message': 'NaturalLanguage'})


Our ""Message"" feature is automatically detected as a natural language (text) column

 
We can also initialize a Woodwork DataColumn for our target.

y = ww.DataColumn(y)


Our target is automatically detected as a categorical column. This makes sense, since we have a binary classification problem with two categories of text messages: spam and ham.
Our target (""y"") is automatically detected as a categorical column

 
Running AutoMLSearch
 
Now, let’s feed our data to AutoMLSearch to see if we can produce a nontrivial machine learning model to detect spam. AutoML is the process of automating the construction, training, and evaluation of machine learning models. AutoMLSearch is EvalML’s interface for AutoML.
First, we will split our data into training and test data sets. We will use the training data set to train and find the best model, and then validate our model’s performance on the test data.
EvalML offers a utility method that makes this easy. All we need to do is specify that we have a binary classification problem, and that we want to reserve 20% of our data as test data.

from evalml.preprocessing import split_data

X_train, X_holdout, y_train, y_holdout = split_data(X, y, problem_type='binary', test_size=0.2)


Next, we can set up AutoMLSearch by specifying the problem type and passing in our training data. Again, we have a binary classification problem because we are trying to classify our messages as one of two categories: ham or spam.

automl = AutoMLSearch(X_train=X_train, y_train=y_train, problem_type='binary')


Calling the constructor initializes an AutoMLSearch object that is configured for our data. Now, we can call automl.search() to start the AutoML process. This will automatically generate pipelines for our data, and then train a collection of various models.

automl.search()


EvalML's AutoML search has trained and evaluated nine different models.

 
To understand the type of pipelines AutoMLSearch has built, we can grab the best performing pipeline and examine it in greater detail. We can call automl.describe_pipeline(id) to see detailed information about the pipeline’s components and performance, or automl.graph(pipeline) to see a visual representation of our pipeline as a flow of components.

# rankings are ordered from best to worst, 
# so 0th index is the best pipeline

best_pipeline_id = automl.rankings.iloc[0][""id""])
automl.describe_pipeline(best_pipeline_id)


Description of our best pipeline

 

# We can also grab the best performing pipeline like this
automl.best_pipeline
automl.graph(automl.best_pipeline)


Graphical representation of our best pipeline

 
By examining the best performing pipeline, we can better understand what AutoMLSearch is doing, and what pipelines it built with our text data. The best pipeline consists of an Imputer, a Text Featurization Component and a Random Forest Classifier component. Let’s break this down and understand how this pipeline was constructed:

AutoMLSearch always adds an Imputer to each generated pipeline to handle missing values. By default, the Imputer will fill the missing values in numeric columns with the mean of each column, and fill the missing values in categorical columns with the most frequent category of each column. Because we don’t have any categorical or numeric columns in our input, the Imputer does not transform our data.
Since AutoMLSearch identified a text column (our Message feature), it appended a Text Featurization Component to each pipeline. This component first cleans the text input by removing all non-alphanumerical characters (except spaces) and converting the text input to lowercase. The component then processes the cleaned text features by replacing each text feature with representative numeric features using LSA and the nlp-primitives package. This component is necessary if we want to handle text features in machine learning, because most machine learning models are not able to handle text data natively. Thus, we need this component to help extract useful information from the raw text input and convert it to numeric values that the models can understand.
Finally, each pipeline has an estimator (a model) which is fitted on our transformed training data and is used to make predictions. Our best pipeline has a Random Forest classifier. If we took a look at some other pipelines, we would also see other pipelines constructed with a LightGBM classifier, Decision Tree classifier, XGBoost classifier, etc.

 
Best Pipeline Performance
 
Now, let’s see how well our best pipeline performed on various metrics and if we could beat the baseline trivial model by scoring the pipeline on test data.

>>> scores = best_pipeline.score(X_holdout, y_holdout,  objectives=evalml.objectives.get_core_objectives('binary') + ['recall'])
>>> scores
OrderedDict([('MCC Binary', 0.9278003804626707),
			 ('Log Loss Binary', 0.1137465525638786),
			 ('AUC', 0.9823022077397945),
             ('Precision', 0.9716312056737588),
             ('F1', 0.9448275862068964),
             ('Balanced Accuracy Binary', 0.9552772006397513),
             ('Accuracy Binary', 0.9732441471571907),
             ('Recall', 0.9194630872483222)])

Our best pipeline performs much better than the baseline
 



Baseline model (always guesses majority class)
Pipeline with Text Featurization Component


Accuracy
75%
97.32%


Balanced Accuracy
50%
95.53%


Recall
0%
91.95%



We have significantly outperformed the baseline model in the three metrics (accuracy, balanced accuracy, and recall) we were focused on! With EvalML, we were able to build a model that is able to detect spam fairly well with just a few lines of code, and even before doing any tuning of the binary classification decision threshold.
 
The Importance of Text
 
We previously discussed that Woodwork had automatically detected that our Messages column was a natural language feature. We now understand that AutoMLSearch was able to create a Text Featurization Component because it identified this natural language column. To explain why this was useful, we can manually set our Messages feature as a categorical feature, run the same steps, and compare our scores.

from evalml.utils import infer_feature_types

# manually set ""Message"" feature as categorical 
X = infer_feature_types(X, {'Message': 'Categorical'}) 

X_train, X_holdout, y_train, y_holdout = split_data(X, y, problem_type='binary', test_size=0.2, random_seed=0)

automl_no_text = AutoMLSearch(X_train=X_train, y_train=y_train,                              problem_type='binary')
automl_no_text.search()


If we score the best pipeline found this time, we get an accuracy score of 75.2%, a balanced accuracy score of 50.3%, and a recall score of 0.6%. These scores are only marginally better than the scores for our baseline model!

>>> best_pipeline_no_text = automl_no_text.best_pipeline
>>> scores = best_pipeline_no_text.score(X_holdout, y_holdout,
objectives=evalml.objectives.get_core_objectives('binary') + ['recall'])
>>> scores

OrderedDict([('MCC Binary', 0.0710465299061946),
			 ('Log Loss Binary', 0.5576891229036224),
			 ('AUC', 0.5066740407467751),
             ('Precision', 1.0),
             ('F1', 0.013333333333333332),
             ('Balanced Accuracy Binary', 0.5033557046979866),
             ('Accuracy Binary', 0.7525083612040134),
             ('Recall', 0.006711409395973154)])


The scores for our best pipeline here are not much better than our baseline scores
 



Baseline model (always guesses majority class)
Pipeline with Text Featurization Component
Pipeline without Text Featurization Component


Accuracy
75%
97.32%
75.25%


Balanced Accuracy
50%
95.53%
50.34%


Recall
0%
91.95%
0.67%



This means that unlike the previous best model found, this model is not much better than the trivial baseline model, and is no better than always guessing the majority “ham” class. By observing the components that make up this pipeline, we can better understand why.

automl_no_text.graph(best_pipeline_no_text)


Graph of our best pipeline if we treat ""Message"" as a categorical feature

 
Because AutoMLSearch was told to treat “Message” as a categorical feature this time, each pipeline included a one-hot encoder (rather than a text featurization component). The one-hot encoder encoded the top 10 most frequent “categories” of these texts; however, because each text is unique, this means that 10 unique text messages were encoded while the rest of the messages were dropped. Doing this removed almost all of the information from our data, so our best pipeline could not do much better than our trivial baseline model.
 
What’s Next?
 
In this post, we covered how EvalML can be used to classify text messages as spam or ham (non-spam), and we learned how EvalML can detect and automatically handle text features with the help of Woodwork and the nlp-primitives library. You can learn more about Woodwork and nlp-primitives through their documentation, linked in the resources below. Finally, be sure to check out a blog post our former intern Clara Duffy wrote to learn more about nlp-primitives.
Special thanks to Becca McBrayer for writing the demo which this blog post is based on!
 
More Resources
 

Using Text Data in EvalML demo
Blog post about nlp-primitives
nlp-primitives GitHub repo
Woodwork documentation

 
Bio: Angela Lin is a software engineer on the team building the open-source EvalML automated machine learning package in Python.
Original. Reposted with permission.
Related:

Getting Started with 5 Essential Natural Language Processing Libraries
Natural Language Processing Pipelines, Explained
How to Clean Text Data at the Command Line"
https://www.kdnuggets.com/2021/06/high-performance-deep-learning-part2.html,"High-Performance Deep Learning: How to train smaller, faster, and better models – Part 2","As your organization begins to consider building advanced deep learning models with efficiency in mind to improve the power delivered through your solutions, the software and hardware tools required for these implementations are foundational to achieving high-performance.","By Gaurav Menghani, Software Engineer at Google AI.
comments

In Part 1, we discussed why efficiency is important for deep learning models to achieve high-performance models that are pareto-optimal. Let us further dive deeper into the tools and techniques for achieving efficiency.
 
Focus Areas of Efficiency in Deep Learning
 
We can think of the work on efficiency to be categorized in roughly four pillars of modelling techniques and a foundation of infrastructure and hardware.

Focus Areas of Efficient Deep Learning.

Compression Techniques: These are general techniques and algorithms that look at optimizing the architecture itself, typically by compressing its layers. Often, these approaches are generic enough to be used across architectures. A classic example is quantization [1,2], which tries to compress the weight matrix of a layer by reducing its precision (e.g., from 32-bit floating-point values to 8-bit unsigned integers). Quantization can generally be applied to any network which has a weight matrix.
Learning Techniques: These are algorithms that focus on training the model differently so as to make fewer prediction errors. Improved accuracy can then be exchanged for a smaller footprint or a more efficient model by trimming the number of parameters if needed. An example of a learning technique is Distillation [3], which, as mentioned earlier, helps a smaller model learn from a larger, more accurate model.
Automation: These are tools for automatically improving the core metrics of the given model using automation. An example is a hyper-parameter search [4], where the model architecture remains the same, but optimizing the hyper-parameters helps increase the accuracy, which could then be exchanged for a model with fewer parameters. Similarly, architecture search [5,6] falls in this category, where the architecture itself is tuned, and the search helps find a model that optimizes both the loss/accuracy and some other objective function. An example of a secondary objective function could be the model latency/size, etc.
Efficient Model Architectures & Layers: These form the crux of efficiency in deep learning and are the fundamental blocks that were designed from scratch (Convolutional Layers, Attention, etc.), which are a significant leap over the baseline methods used before them. As an example, convolutional layers introduce parameter sharing and filters for use in image models, which avoids having to learn separate weights for each input pixel. This clearly saves the number of parameters when you compare it to a standard multi-layer perceptron (MLP) network. Avoiding over-parameterization further helps in making the networks more robust. In this pillar, we would look at layers and architectures that have been designed specifically with efficiency in mind.

 
Infrastructure & Hardware
 
Finally, we also need a foundation of infrastructure and tools that help us build and leverage efficient models. This includes the model training framework, such as Tensorflow, PyTorch, etc., as introduced earlier. Often these frameworks will be paired with the tools required specifically for deploying efficient models. For example, Tensorflow has tight integration with Tensorflow Lite (TFLite) [7] and related libraries, which allow exporting and running models on mobile devices. Similarly, TFLite Micro [8] helps in running these models on DSPs. Just like Tensorflow, PyTorch also offers PyTorch Mobile for quantizing and exporting models for inference on mobile and embedded devices.
We often depend on this infrastructure and tooling to leverage the gains from efficient models. For example, for obtaining both size and latency improvements with quantized models, we need the inference platform to support common neural net layers in quantized mode. TFLite supports quantized models by allowing the export of models with 8-bit unsigned int weights and having integration with libraries like GEMMLOWP [8] and XNNPACK [9] for fast inference. Similarly, PyTorch uses QNNPACK [10] to support quantized operations.
On the hardware front, we rely on devices like CPUs, GPUs, and Tensor Processing Units (TPUs) [11] to allow us to train and deploy these models. On the mobile and embedded front, we have ARM-based processors, mobile GPUs, and other accelerators [12] that let us leverage efficiency gains for deployment (inference).
In our next part, we will go over examples of tools and techniques that fit in each of these pillars. Also, feel free to go over our survey paper that explores this topic in detail.
 
References
[1] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.
[2] Raghuraman Krishnamoorthi. 2018. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv (Jun 2018). arXiv:1806.08342
[3] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015)
[4] Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley. 2017. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. 1487–1495.
[5] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016).
[6] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2820–2828.
[7] ML for Mobile and Edge Devices. https://www.tensorflow.org/lite
[8] GEMMLOWP. - https://github.com/google/gemmlowp
[9] XNNPACK. - https://github.com/google/XNNPACK
[10] Marat Dukhan, Yiming Wu Wu, and Hao Lu. 2020. QNNPACK: Open source library for optimized mobile deep learning - Facebook Engineering. https://engineering.fb.com/2018/10/29/ml-applications/qnnpack
[11] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture. 1–12.
[12] EdgeTPU. https://cloud.google.com/edge-tpu
 
Bio: Gaurav Menghani (@GauravML) is a Staff Software Engineer at Google Research, where he leads research projects geared towards optimizing large machine learning models for efficient training and inference on devices ranging from tiny microcontrollers to Tensor Processing Unit (TPU)-based servers. His work has positively impacted > 1 Billion active users across YouTube, Cloud, Ads, Chrome, etc. He is also an author of an upcoming book with Manning Publication on Efficient Machine Learning. Before Google, Gaurav worked at Facebook for 4.5 years and has contributed significantly to Facebook’s Search system and large-scale distributed databases. He has an M.S. in Computer Science from Stony Brook University.
Related:

High Performance Deep Learning, Part 1
5 Challenges to Scaling Machine Learning Models
Scaling a Massive State-of-the-art Deep Learning Model in Production"
https://www.kdnuggets.com/2021/03/top-youtube-machine-learning-channels.html,Top YouTube Machine Learning Channels,"These are the top 15 YouTube channels for machine learning as determined by our stated criteria, along with some additional data on the channels to help you decide if they may have some content useful for you.","By Matthew Mayo, KDnuggets.
comments
KDnuggets recently brought you the Top YouTube Channels for Data Science, employing a qualitative approach to identifying those channels of value on the platform. As the endeavour seemed to be be useful to some of our readers, we have repeated the exercise, this time bringing you the top machine learning channels that YouTube has to offer.
For this iteration we changed up our metric for determining the ""top"" channels. We have maintained our quantitative approach, but tweaked the specifics. (Also, we fully recognize that the act of creating a criteria is a form of subjectivity, but realistically some decisions need to be made.) This time around, determining which channels ended up on our list began with this YouTube search criteria:

search term: ""machine learning""
search type: channel
sort by: relevance

The results to this search were gathered on March 21, 2021, and appeared at this URL at the time.
The top 100 results were scraped. After this, the following bit of data processing was applied:

remove channels with <100K views
remove channels without updates for past 12 months
sort channels by videos/views

We had planned to take the top X form of the resulting list to include in the post. However, one channel was removed in an act of subjectivity given some of the channel creator's recent controversies. While we wish the individual well, we could not in good faith include their content in our recommendations list.
Finally, we cut the results off at top 15, which is what are listed below, and what are included in the following picture-painting visualization.

Figure 1. Top YouTube Channels for Machine Learning
Plotted by number of views and number of subscribers; relative size by number of videos;
color intensity by number of views / number of subscribers
 
And so here are the top 15 YouTube channels for machine learning by number of views / number of videos (or views per video), in order, with short descriptions directly from the channels themselves (where available).
 
1. sentdex
Views/Video: 76 K Subscribers: 1020 K, Videos: 1212, Views: 92 M, Views/Subscriber: 90

“Python Programming tutorials, going further than just the basics. Learn about machine learning, finance, data analysis, robotics, web development, game development and more.”

 
2. codebasics
Views/Video: 44 K, Subscribers: 271 K, Videos: 365, Views: 16 M, Views/Subscriber: 59

“The goal of this channel is to fulfill this vision by teaching the programming in most simplest and intuitive manner. I teach simple programming, data science, data analytics, artificial intelligence, machine learning, data structures, software architecture etc on my channel.”

 
3. DeepLearningAI
Views/Video: 42 K, Subscribers: 130 K, Videos: 205, Views: 8.7 M, Views/Subscriber: 67

“Welcome to the official DeepLearning.AI YouTube channel! Here you can find the videos from our Coursera programs on machine learning as well as recorded events. DeepLearning.AI has created high-quality AI programs on Coursera that have gained an extensive global following. By providing a platform for education and fostering a tight-knit community, DeepLearning.AI has become the pathway for anyone looking to build an AI career.”

 
4. deeplizard
Views/Video: 25 K, Subscribers: 93.8 K, Videos: 289, Views: 7.1 M, Views/Subscriber: 76

“Building collective intelligence.”

 
5. Krish Naik
Views/Video: 24 K, Subscribers: 334 K, Videos: 921, Views: 22 M, Views/Subscriber: 65

“This is my YouTube channel where I explain various topics on machine learning, deep learning, and AI with many real-world problem scenarios. I have delivered over 30 tech talks on data science, machine learning, and AI at various meet-ups, technical institutions, and community-arranged forums. My main aim is to make everyone familiar of ML and AI”

 
6. Kilian Weinberger
Views/Video: 17 K, Subscribers: 11.9 K, Videos: 39, Views: .65 M, Views/Subscriber: 54

“Kilian has absolutely no channel description, but the content consists of machine leanring lectures from Cornell University, where Kilian is an Associate Professor of Computer Science.”

 
7. Machine Learning
Views/Video: 14 K, Subscribers: 1.39 K, Videos: 20, Views: .28 M, Views/Subscriber: 199

“Watch Industry Experts thoughts, Earn Free Cloud Credits, Session on Operating Systems, Modern day Technology Tutorials, Valuable sessions on IAAS, PAAS, SAAS, Hybrid cloud strategy, Get replays and Live feeds from Meetups, Conferences and much more.“

 
8. Daniel Bourke
Views/Video: 14 K, Subscribers: 79.4 K, Videos: 270, Views: 3.7 M, Views/Subscriber: 46

“I'm a machine learning engineer who plays at the intersection of technology and health. My videos will help you learn better and live healthier.”

 
9. Hsuan-Tien Lin
Views/Video: 12 K, Subscribers: 21.8 K, Videos: 195, Views: 2.3 M, Views/Subscriber: 105

“Hsuan-Tien Lin has no channel description, but has videos on Machine Learning for Modern Artificial Intelligence, Data Structures and Algorithms, Machine Learning Foundations/Techniques, and more. His channel has only been posting videos for ~2 months at the time of this article’s publication, and they are recorded in a mix of English and Mandarin (I believe).”

 
10. Python Engineer
Views/Video: 11 K, Subscribers: 28 K, Videos: 121, Views: 1.3 M, Views/Subscriber: 46

“Hi, I'm Patrick. I’m a passionate Software Engineer who loves Machine Learning, Computer Vision, and Data Science. I create free content in order to help more people get into those fields. If you have any questions, feedback, or comments, just shoot me a message! I am happy to talk to you :)”

 
11. Data Science Courses
Views/Video: 10 K, Subscribers: 14.5 K, Videos: 81, Views: .85 M, Views/Subscriber: 59

“No description, but Ali Ghodsi is a professor at Waterloo, and also a member of the university’s Artificial Intelligence Research Group. The channel includes lecture videos.”

 
12. Abhishek Thakur
Views/Video: 9.4 K, Subscribers: 42.8 K, Videos: 85, Views: .8 M, Views/Subscriber: 19

“I make videos about applied machine learning, deep learning and data science. I am the world's first 4x grandmaster on Kaggle.”

 
13. Jeff Heaton
Views/Video: 9.1 K, Subscribers: 48.3 K, Videos: 411, Views: 3.7 M, Views/Subscriber: 78

“Would you like to learn about deep neural networks and other areas of my machine learning research that has allowed me to score in the top 7-10% of some Kaggle competitions?  If so, please subscribe to my channel!  My name is Jeff Heaton, Ph.D.  I am a VP of data science for a Fortune 300 company and I teach a deep learning course as an adjunct instructor for a top university.”

 
14. Subalalitha C N
Views/Video: 8.8 K, Subscribers: 2.87 K, Videos: 40, Views: .35 M, Views/Subscriber: 123

“I am Dr.Subalalitha C.N, working as Associate Professor in SRM Institute of Science and Technology, India.   In this channel, you can find my lectures on Machine Learning, Natural Language Processing and Design and Analysis of Algorithms.”

 
15. Machine Learning TV
Views/Video: 8.0 K, Subscribers: 23.1 K, Videos: 126, Views: 1.0 M, Views/Subscriber: 43

“This channel is all about machine learning (ML). It contains all the useful resources which help ML lovers and computer science students gain a better understanding of the concepts of this successful branch of Artificial Intelligence.”

 
And these are the top 15 YouTube channels for machine learning by views per video, along with some additional data to help you decide whether these channels may have video content which may be of interest to you. Good luck with your viewing!
 
Related:

Top YouTube Channels for Data Science
Best Machine Learning Youtube Videos Under 10 Minutes
Top Python Libraries for Data Science, Data Visualization & Machine Learning"
https://www.kdnuggets.com/2020/12/data-science-machine-learning-free-ebook.html,Data Science and Machine Learning: The Free eBook,"Check out the newest addition to our free eBook collection, Data Science and Machine Learning: Mathematical and Statistical Methods, and start building your statistical learning foundation today.","By Matthew Mayo, KDnuggets.
comments
It's been a while since we shared a free eBook with our readers, but this week we have come across another worthy entrant in the series, and wanted to share it in time for the holiday learning season (which is most definitely a thing).
Today we share Data Science and Machine Learning: Mathematical and Statistical Methods, by D.P. Kroese, Z.I. Botev, T. Taimre & R. Vaisman. The book was published last year, and aside from being freely-available as a PDF can also be purchased in print form (and Kindle).

 
Data Science and Machine Learning: Mathematical and Statistical Methods is a practically-oriented text, with a focus on doing data science and implementing machine learning models using Python. It does a good job of explaining relevant theory and introducing the necessary math as needed, which results in very nice pacing for a practical book.
The book's raison d'être, according to its website, is actually somewhat at odds with my take:

The purpose of this book is to provide an accessible, yet comprehensive textbook intended for students interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science.

 
I believe this is the opposite side of the same coin: where I see this book's strength as teaching the practical and reinforcing it with the necessary theory and underlying math, the argument can clearly be made that it focuses on the theory and underlying math and reinforces this with practical implementation.
Even money, I'd say.
Regardless of the approach you endorse, the book's table of contents are as follows:

Importing, Summarizing, and Visualizing Data
Statistical Learning
Monte Carlo Methods
Unsupervised Learning
Regression
Regularization and Kernel Methods
Classification
Decision Trees and Ensemble Methods
Deep Learning

Lots of relevant topics are covered here, and in logical succession. I particularly like the transition from Monte Carlo methods to unsupervised learning, and how that happens prior to the introduction of supervised concepts. Classification, though likely more useful in the long run (at least seemingly so at present) seemed far less impactful than did clustering when I first encountered machine learning, and so in my view its introduction prior may prove equally captivating for other new learners.
To ensure the book is self-contained for even the newest of data science and machine learning students, the book includes the adequate and useful appendices of:

Linear Algebra and Functional Analysis
Multivariate Differentiation and Optimization
Probability and Statistics
Python Primer

You won't become a complete data science expert by reading this book, but that's not its goal. By working through Data Science and Machine Learning: Mathematical and Statistical Methods you will get a solid foundation in the basics of the field, upon which more cutting edge methods and algorithms can be added.
One of my favorite machine learning books that I used as my first foray into learning the subject matter was Data Mining: Practical Machine Learning Tools and Techniques, also known as the Weka book. I really liked as a newcomer how it mixed practical and theoretical, introducing and explaining the math as needed in order to learn the practical implementation being presented at the time. I find that this book is reminiscent of that format, with the advantage of using Python instead of the Weka toolkit which, at least today, is a much more relevant implementation pathway. 
I recommend this book to anyone learning the basics of data science and machine learning, and looking to do so in the presentation format described.
 
Related:

Understanding Machine Learning: The Free eBook
An Introduction to Statistical Learning: The Free eBook
Data Mining and Machine Learning: Fundamental Concepts and Algorithms: The Free eBook"
https://www.kdnuggets.com/2021/06/analytics-engineering-everywhere.html,Analytics Engineering Everywhere,"Many new roles have appeared in the data world ever since the rise of the Data Scientist took the spotlight several years ago. Now, there is a new core player ready to take center stage, and we may see in five years, nearly every organization will have an Analytics Engineering team.","comments
By Jason Ganz, Special Adviser, Data for Progress.

 
Analytics Engineering — An Introduction
 
There’s a quiet revolution happening in the world of data. For years we have been blasted with nonstop articles about “The Sexiest Job of the 21st Century” — a data scientist. A data scientist, we have been taught, is a figure of almost otherworldly intelligence who uses quasi-mystical arts to perform feats of data wizardly. But these days, if you talk to the people who watch the data space most closely — there’s a different data role that has them even more excited.
To be clear, there are some very real and very cool applications of data science that can allow organizations to do things with data that can completely transform how their organization operates. But for many orgs, particularly smaller organizations without millions of dollars to invest, data science initiatives tend to fall flat because of the lack of a solid data infrastructure to support them.
While everyone was focused on the rise of data science, another discipline has been quietly taking shape, driven not by glitzy articles in Harvard Business Review but by the people working in the trenches in data-intensive roles. They call it the analytics engineer.
An analytics engineer is someone who brings together the data-savvy and domain knowledge of an analyst with software engineering tooling and best practices. Day to day, that means spending in a suite of tools that is becoming known as “The Modern Data Stack” and particularly dbt. These tools allow analytics engineers to centralize data and then model it for analysis in a way that is remarkably cheap and easy compared to how the ETL of traditional Business Intelligence teams of the past operated.
While data scientists are seen by some as wizardry, the attitude of the analytics engineer is a little different. You’ll hear them refer to themselves as everything from “humble data plumbers’’ to “just a pissed off data analyst.” The work of an Analytics Engineer seems easy to understand, almost banal. They combine data sources, apply logic, make sure there are clean and well-modeled materializations to analyze.
It turns out analytics engineering is a goddamn superpower. Anyone that has worked in, well, basically any organization knows that a tremendous amount of effort goes into standardizing data points that feel like they should be a no-brainer to pull, while more complex questions just sit unanswered for years. Analytics Engineering allows you to have data systems that just work.
A good analytics engineer is hugely impactful for an org, with each analytics engineer being able to help build a truly data-driven culture in ways that would be challenging for a team of people using legacy tools. While in the past there was tremendous repetitive work to do any simple analysis, Analytics Engineers can build complex data models using tools like dbt and have analysis-ready data tables built on any schedule. While before it was impossible to get anyone to agree on standard definitions of metrics, Analytics Engineers can simply build them into their codebase. And in the past, people struggled with incomplete and messy data, and Analytics Engineers… still struggle with incomplete and messy data. But at least we can have a suite of tests on our analytics systems to know when something has gone wrong!
 
The Rise of Analytics Engineering
 
You might think that this development would be scary for people working in data — if one analytics engineer is substantially more impactful than a data analyst, won’t our jobs be at risk? Could an org replace five data analysts with one Analytics Engineer and come out ahead?
But the fact of the matter is that no data analyst, anywhere, has ever come close to performing all of the analysis they think could be impactful at their organization — the opposite is far more likely to be the problem. Most data orgs are begging for additional headcount.
As analytics engineers increase the amount of insight organizations can find from data, it actually becomes more likely that these orgs will want to hire additional data talent (both analytics engineers and analysts). In his fantastic post The Reorganization of the Factory, Erik Bernhardsson makes the case that as the toolsets for software engineers has become ever more efficient, the demand for software engineers has counterintuitively grown — as there are more and more use cases where it now makes sense to build software rather than a manual process. This point not only holds for data, but I think it actually is more true for data.
While every organization needs software, not every organization needs software engineers. But every organization needs to learn from their data, and since the ways in which the data needs to be understood will be unique at every organization, they will all need analytics engineers. Software is commonly said to be eating the world — analytics engineering will be embedded in the world. As the incremental value of each data hire rises, there are substantial new areas where data insights and learnings could be applied that they aren’t today. And even if you aren’t interested in becoming an analytics engineer, having well modeled and accurate data makes data analysts and data scientists more effective. It’s a win across the board.
That does not necessarily mean that every analytics engineering role will be doing good for the world. Having more powerful data operations allows you to question, seek insights, and look for new strategies. It can also allow an organization new ways to monitor their employees, surveil, or discriminate. One needs only look at the myriad of public issues in the tech and data science industries right now to see the ways that powerful tech can be misused. It is important to recognize the potential dangers as well as the new opportunities.
If it feels like we’re at a real inflection point for Analytics Engineering — it’s because we are. What was very recently the domain of a few adventurous data teams is quickly becoming industry standard for tech organizations — and there’s every reason to think that other types of organizations will be following along shortly. The impact is just too high.
We’re about to see a huge expansion in the number of and types of places where you can find employment as an analytics engineer. The coming boom in opportunities for analytics engineers will take place across three rough domains, with each having different challenges and opportunities.

More and more large enterprises, both tech and non-tech organizations, are going to adapt to the modern data stack. As analytics engineering is brought into the most complex legacy data systems, we’ll begin to see what patterns develop to support analytics engineering at scale. If you are interested in really figuring out what the large-scale data systems of the future look like, this will be the place to go.
Just about every new company is going to be searching for an analytics engineer to lead their data initiatives. This will give them a step up against any competition that isn’t investing in their core data. Being an early analytics engineer at a fast-growing company is tremendously fun and exciting, as you are able to build up a data organization from scratch and see firsthand how analytics engineering can change the trajectory of an organization.
Finally, many organizations outside the tech business world are going to begin seeing the impact that analytics engineering can bring. You might not have quite the same tech budget, and you might have to learn to advocate for yourself a little more but it might be the area where analytics engineering has the most potential to do good for the world. City governments will use analytics engineering to monitor programs and ensure that government resources are being used effectively. Academic institutions will use analytics engineering to create datasets, many of them public, that will aid in scientific and technological development. The possibility space is wide open.

Analytics engineering is fundamentally a discipline that’s about making sense of the world around us. It’s about allowing everyone in an organization to see a little bit further in their impact on the org and how their work connects to it. Right now, analytics engineering is still a new discipline — pretty soon, it will be everywhere.
 
Original. Reposted with permission.
 
Related:

Why You Should Consider Being a Data Engineer Instead of a Data Scientist
9 Skills You Need to Become a Data Engineer
Data Engineering — the Cousin of Data Science, is Troublesome"
https://www.kdnuggets.com/2021/07/python-data-structures-compared.html,Python Data Structures Compared,"Let's take a look at 5 different Python data structures and see how they could be used to store data we might be processing in our everyday tasks, as well as the relative memory they use for storage and time they take to create and access.","By Matthew Mayo, KDnuggets.
comments

Photo by Hitesh Choudhary on Unsplash 
Choosing a structure for storing your data is an important part of solving programming tasks and implementing solutions, yet it is often not given the attention a choice of such potential weight deserves. Unfortunately, in Python I often see the list used as a catch-all data structure. The list has its advantages, of course, but also its drawbacks. There are lots of other data structure options as well.
Let's take a look at 5 different Python data structures and see how they could be used to store data we might be processing in our everyday tasks, as well as the relative memory they use for storage and time they take to create and access.
 
Types of Data Structures
First, let's lay out the 5 data structures we will consider herein, and provide some preliminary insight.
Class
In this case we are talking about vanilla classes (as opposed to data classes below), which are described at a high level in Python documentation as follows:
Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state.
 
The advantages of using classes is that they are conventional, and are well-used and -understood. Whether or not they are overkill in terms of relative required memory or time is something to look at.
Data Class
Added in Python 3.7, the data class is a special class meant for mainly holding data, which comes with some freebie methods out of the box for typical functionality like instantiating and printing instance contents. Creating a data class is accomplished using the @dataclass decorator.
Although they use a very different mechanism, Data Classes can be thought of as ""mutable namedtuples with defaults"". Because Data Classes use normal class definition syntax, you are free to use inheritance, metaclasses, docstrings, user-defined methods, class factories, and other Python class features. Such a class is called a Data Class, but there's really nothing special about the class: the decorator adds generated methods to the class and returns the same class it was given.
 
As you can see, the automatically generated methods and the related time-savings are the main reason to consider data classes.
Named Tuple
Named tuples are an elegant implementation of a useful data structure, essentially tuple subclasses with named fields.
Named tuples assign meaning to each position in a tuple and allow for more readable, self-documenting code. They can be used wherever regular tuples are used, and they add the ability to access fields by name instead of position index.
 
At first look, named tuples appear to be the closest thing to simple C-like struct types natively available in Python, making them naively attractive to many.
Dictionary
The Python dictionary is a collection of key-value pairs.
Python Dictionaries are mutable unordered collections (they do not record element position or order of insertion) of key-value pairs. Keys within the dictionary must be unique and must be hashable. That includes types like numbers, strings and tuples. Lists and dicts can not be used as keys since they are mutable.
 
The advantage of dictionaries is that they are simple, the data within are easily accessible, and they are well-used and -understood.
List
Here it is, the one-size-fits-all Python data superstructure, or so lots of code would have you believe. Here is what the list really is:
Lists are mutable ordered and indexed collections of objects. The items of a list are arbitrary Python objects. Lists are formed by placing a comma-separated list of expressions in square brackets.
 
Why the widespread use of the list? It's very simple to understand and implement, and is usually the first structure one learns when picking up Python. Are there disadvantages related to speed and memory usage? Let's take a look.
 
Implementations
First off, let's have a look at the creation process of each of these structures and how they compare to one another.
The reason we might be using any of these data structures to store our data would vary widely, but for the unimaginative, let's imagine we are extracting data from a SQL database and need to store each record in one such structure in order to perform some processing prior to moving the data further along our pipeline.
With that in mind, here is instantiation code for creating each of the five structures.

"""""" class """"""
class CustomerClass:
    def __init__(self, cust_num:str, f_name:str, l_name:str, address:str,
                       city:str, state:str, phone:str, age:int):
        self.cust_num = cust_num
        self.f_name = f_name
        self.l_name = l_name
        self.address = address
        self.city = city
        self.state = state
        self.phone = phone
        self.age = age

    def to_string(self):
        return(f'{self.cust_num}, {self.f_name}, {self.l_name}, {self.age}, 
                 {self.address}, {self.city}, {self.state}, {self.phone}'stgrcutures)

"""""" data class """"""
from dataclasses import dataclass
@dataclass
class CustomerDataClass:
    cust_num: int
    f_name: str
    l_name: str
    address: str
    city: str
    state: str
    phone: str
    age: int

"""""" named tuple """"""
from collections import namedtuple
CustomerNamedTuple = namedtuple('CustomerNamedTuple', 
                                'cust_num f_name l_name address city state phone age')

"""""" dict """"""
def make_customer_dict(cust_num: int, f_name: str, l_name: str, address: str, 
                       city: str, state: str, phone: str, age: int):
    return {'cust_num': cust_num,
            'f_name': f_name,
            'l_name': l_name,
            'address': address,
            'city': city,
            'state': state,
            'phone': phone,
            'age': age}

"""""" list """"""
def make_customer_list(cust_num: int, f_name: str, l_name: str, address: str, 
                       city: str, state: str, phone: str, age: int):
    return [cust_num, f_name, l_name, address,
            city, state, phone, age]

 
Note the following:

The creation of an instance of the built-in types dictionary and list have been place inside functions
The difference between the class and the data class implementations, in light of the discussion above
The (clearly subjective) elegance and simplicity of the named tuple

Let's have a look at instantiation of these structures, and a comparison of the resources required to do so.
 
Testing and Results
We will create a single instance of each of the 5 structures, each housing a single data record. We will repeat this process using the same data fields for each structure 1,000,000 times to get a better sense of average time, performing this process on my modest Dell notebook, using an Ubuntu-derived operating system.

Compare the code between the 5 structure instantiations below.

"""""" instantiating structures """"""

from sys import getsizeof
import time

# class
customer_1 = CustomerClass('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
print(f'Data: {customer_1.to_string()}')
print(f'Type: {type(customer_1)}')
print(f'Size: {getsizeof(customer_1)} bytes')

t0 = time.time()
for i in range(1000000):
    customer = CustomerClass('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
t1 = time.time()
print('Time: {:.3f}s\n'.format(t1-t0))

# data class
customer_2 = CustomerDataClass('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
print(f'Data: {customer_2}')
print(f'Type: {type(customer_2)}')
print(f'Size: {getsizeof(customer_2)} bytes')

t0 = time.time()
for i in range(1000000):
    customer = CustomerDataClass('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
t1 = time.time()
print('Time: {:.3f}s\n'.format(t1-t0))

# named tuple
customer_3 = CustomerNamedTuple('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
print(f'Data: {customer_3}')
print(f'Type: {type(customer_3)}')
print(f'Size: {getsizeof(customer_3)} bytes')

t0 = time.time()
for i in range(1000000):
    customer = CustomerNamedTuple('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
t1 = time.time()
print('Time: {:.3f}s\n'.format(t1-t0))

# dict
customer_4 = make_customer_dict('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
print(f'Data: {customer_4}')
print(f'Type: {type(customer_4)}')
print(f'Size: {getsizeof(customer_4)} bytes')

t0 = time.time()
for i in range(1000000):
    customer = make_customer_dict('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
t1 = time.time()
print('Time: {:.3f}s\n'.format(t1-t0))

# list
customer_5 = make_customer_list('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
print(f'Data: {customer_5}')
print(f'Type: {type(customer_5)}')
print(f'Size: {getsizeof(customer_5)} bytes')

t0 = time.time()
for i in range(1000000):
    customer = make_customer_list('EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56)
t1 = time.time()
print('Time: {:.3f}s\n'.format(t1-t0))

 
And here is the output of the above:

Data: EP90210, Edward, Perez, 56, 123 Fake Street, Cityville, TX, 888-456-1234
Type: <class '__main__.CustomerClass'>
Size: 56 bytes
Time: 0.657s

Data: CustomerDataClass(cust_num='EP90210', f_name='Edward', l_name='Perez', address='123 Fake Street', city='Cityville', state='TX', phone='888-456-1234', age=56)
Type: <class '__main__.CustomerDataClass'>
Size: 56 bytes
Time: 0.630s

Data: CustomerNamedTuple(cust_num='EP90210', f_name='Edward', l_name='Perez', address='123 Fake Street', city='Cityville', state='TX', phone='888-456-1234', age=56)
Type: <class '__main__.CustomerNamedTuple'>
Size: 112 bytes
Time: 0.447s

Data: {'cust_num': 'EP90210', 'f_name': 'Edward', 'l_name': 'Perez', 'address': '123 Fake Street', 'city': 'Cityville', 'state': 'TX', 'phone': '888-456-1234', 'age': 56}
Type: <class 'dict'>
Size: 368 bytes
Time: 0.318s

Data: ['EP90210', 'Edward', 'Perez', '123 Fake Street', 'Cityville', 'TX', '888-456-1234', 56]
Type: <class 'list'>
Size: 128 bytes
Time: 0.184s

 
Finally, another useful piece of data would be to know the relative access times of values stored within our structures (in the case below, the address). The same retrieval will be repeated 1,000,000 times, and the average time reported below.

"""""" accessing an element """"""

# class
t0 = time.time()
for i in range(1000000):
    address = customer_1.address
t1 = time.time()
print(f'Type: {type(customer_1)}')
print('Time: {:.3f}s\n'.format(t1-t0))

# data class
t0 = time.time()
for i in range(1000000):
    address = customer_2.address
t1 = time.time()
print(f'Type: {type(customer_2)}')
print('Time: {:.3f}s\n'.format(t1-t0))

# named tuple
t0 = time.time()
for i in range(1000000):
    address = customer_3.address
t1 = time.time()
print(f'Type: {type(customer_3)}')
print('Time: {:.3f}s\n'.format(t1-t0))

# dictionary
t0 = time.time()
for i in range(1000000):
    address = customer_4['address']
t1 = time.time()
print(f'Type: {type(customer_4)}')
print('Time: {:.3f}s\n'.format(t1-t0))

# list
t0 = time.time()
for i in range(1000000):
    address = customer_5[3]
t1 = time.time()
print(f'Type: {type(customer_5)}')
print('Time: {:.3f}s\n'.format(t1-t0))

 
And the output:

Type: <class '__main__.CustomerClass'>
Time: 0.098s

Type: <class '__main__.CustomerDataClass'>
Time: 0.092s

Type: <class '__main__.CustomerNamedTuple'>
Time: 0.134s

Type: <class 'dict'>
Time: 0.095s

Type: <class 'list'>
Time: 0.117s


 
The intent of this article is not to make a recommendation one way or another as to which data structure to use, nor is it to suggest that there is a universal best structure for every case. Instead, we wanted to have a look at some different options and their relative strength and weakness. As with all things, there are trade-offs to be made, and less quantitative considerations such as understandability, ease of use, etc. are to be taken into account when making these types of decisions.
That said, a few things do stand out from the above analysis:

The dictionary uses the greatest amount of storage of all the structures, in our case almost 3 times as much as the next greatest — though we should be careful about generalizing until we look at the effects of scaling and internal field data types
Unsurprisingly, the list is the fastest to instantiate, yet not the fastest from which to retrieve an element (it's almost the slowest)
In our case, the named tuple is the slowest structure from which to retrieve an element, yet is middle of the pack for storage space
Both classes take relatively longer to instantiate (expected), but element retrieval and space used, in both cases, are very competitive with the other structures

So not only are we not looking to recommend a single structure in every case, there is no clear winner that could be recommended in every case. Even taking the caution to generalize based on our small experiment, it is clear that priorities will need to be taken into account for making a decision as to which structure you use for a particular job. At the very least, this limited experimentation has provided some small window of insight into the performance of data structures available in Python.
Related:

Managing Your Reusable Python Code as a Data Scientist
5 Python Data Processing Tips & Code Snippets
Date Processing and Feature Engineering in Python"
https://www.kdnuggets.com/2021/07/roidna-aws-webinar-data-driven-esg-sustainability-decisions.html,AWS Webinar: How are data-driven companies using ESG and sustainability data to make actionable decisions?,"In this virtual session, on Jul 29 @ 11AM PT, 2PM ET, our panel of experts will uncover how companies across several verticals use ESG data to move beyond the reporting benchmark, deepen business insights, and create competitive differentiation.","Sponsored Post.









.















			Using ESG and sustainability data in the cloud to drive business value
		      






REGISTER NOW







.
























					  You're invited!
					










						T﻿hursday, J﻿uly 2﻿9
					      







						1﻿1A﻿M P﻿T | 2﻿ P﻿M E﻿T
					      







						60 MIN SESSION
					      












REGISTER NOW















			      Join this webinar to discover how forward-thinking organizations are transforming environmental, social, and governance (ESG) data into actionable business intelligence.
			    












			In this webinar:
		      



			In this virtual session, our panel of experts will uncover how companies across several verticals use ESG data to move beyond the reporting benchmark, deepen business insights, and create competitive 
			differentiation.
		      









			Key takeaways include:
		      
















			      Leveraging third-party data and visualization tools to identify 
			      assets with environmental violations and climate risk exposures
			    













			      Uncovering how financial institutions use ESG data to align with 
			      new integration and disclosure requirements
			    













			      Modeling investment forecasts and implementing visualization 
			      services to meet sustainability targets and distill key business 
			      insights from diverse data sets
			    













			      Supporting data discovery and democratization by utilizing AWS 
			      Data Exchange
			    










		  Moderator:
		






















						Kelci Zile
					      



						Global Lead, Sustainability, AWS Data Exchange
					      














Kelci Zile leads Sustainability for AWS Data Exchange. She is responsible for connecting AWS customers across industries with the data they need to drive sustainability-related research, innovation, and decision-making. She primarily focuses on environmental, social, and governance data applications within the financial services, consulting, and insurance space. She has been at Amazon for over five years, previously running international marketing for Prime Video Direct, and driving discovery of emerging video content. Kelci is also the Co-Chair for Inside Sustainability on the Amazon Global Sustainability Ambassadors Board, 
				    and in her free time she enjoys upcycling clothing and home goods.
				  









			Presenters:
		      



















						Daniel Klier
					      



						Partner, CEO of Arabesque S-Ray and President of Arabesque
						Holding, Arabesque S-Ray
					      














Dr. Daniel leads the global growth strategy and expansion of S-Ray's ESG services worldwide. Prior to joining Arabesque, Daniel was Global Head of Sustainable Finance for HSBC. During his tenure, he developed the HSBC's global climate strategy and led the development of sustainable finance business across the company's corporate, retail, and asset management business. He joined HSBC in 2013 as Group Head of Strategy in London, following nine years at McKinsey & Company, as a Partner. Daniel has also chaired the Bank of England Climate Risk Working Group and the Sustainable Finance Working Group at the Institute of International Finance. He is a member of the Board of Sustainable Energy for All.
				  

























						Mukesh Jain
					      



						Chief Technology and Innovation Officer, Vice President and Head - Insights & Data Technology, Capgemini
					      














Mukesh Jain leads technology strategy, innovation, architecture advisory, and R&D solutions development in the areas of analytics, machine learning, and artificial intelligence across key sectors like finance, retail, automotive, education, manufacturing, and life sciences. He is a veteran in the data, analytics, and artificial intelligence space with 25-plus years of experience building large-scale products at Capgemini, Microsoft, Jio, and NICE Systems. He is a known figure in the industry and often speaks at internal conferences on these topics. He is active in teaching at several universities and is the author of two books.
				  






















					  Colin Marden
					



					  Solutions Architect, AWS
					














Colin Marden is a Solutions Architect in the financial services industry supporting AWS Enterprise Greenfield customers in their journey to modernize, transform, and migrate on-premises workloads to the AWS Cloud. Colin is a champion and specialist for Amazon QuickSight and AWS Data Exchange, regularly speaking at AWS and partner events on subjects like data engineering and visualization.
			    



















REGISTER NOW














		  © 2021 AWS Marketplace."
https://www.kdnuggets.com/2021/06/nomad-data-matters.html,The Data Matters: Choosing the right data to analyze can make or break your analysis,We started Nomad Data to help data scientists and business analysts quickly find the right commercial datasets to match their specific use case. We catalog use cases of data and use machine learning and AI to match analysis goals with datasets.,"Sponsored Post.

Photo by Jackson Simmer on Unsplash
We are living in a world where microchips and sensors are being added to devices from televisions to keychains. To say the amount of data being created is exploding doesn’t do justice to the sheer acceleration under way. This data revolution, however, is a double edged sword. Although the data that would best serve the analysis you’re trying to do probably exists, the difficulty in finding it is significant and increasing. Choosing the wrong data can doom your analysis from the start.
We started Nomad Data to help data scientists and business analysts quickly find the right commercial datasets to match their specific use case. We catalog use cases of data and use machine learning and AI to match analysis goals with datasets.
Imagine you’re working as a data scientist for an auto loan company in late 2017, a few months after Hurricane Maria struck Puerto Rico, devastating the island. Your boss in the risk department asks you to come up with an analysis on whether people are fleeing the island permanently, therefore increasing the risk that the firm’s auto loans will go into default. The first step in tackling the problem is deciding what data is a good proxy for population migration. Using Nomad Data you would quickly be connected to:

Consumer Transaction Data - This allows you to see near real-time consumer spend at stores on the island.
Geolocation Data - Using signals from anonymous consumer cell phones you can see the number of active phones on the island and how that number has changed since before the hurricane.
Consumer Credit Data - Using data from the three credit bureaus you can see if the volume of residents paying their bills has changed since before the hurricane.
Airline Manifest Data - Data from online travel agencies allows visibility into the volume of airline tickets being purchased to/from the island.

Most data scientists don’t know the different types of external data sources that exist as there are thousands and new ones are being created daily. Even if they do know, being sure that a given dataset can answer a very particular question requires investigation which means spending time and money. If you went through and tested the above sources, you would learn that there is only one unbiased by infrastructure challenges that the island was facing. By searching with Nomad Data you save valuable time by testing only the most relevant data."
https://www.kdnuggets.com/2021/02/6-data-science-certificates.html,6 Data Science Certificates To Level Up Your Career,Anyone looking to obtain a data science certificate to prove their ability in the field will find a range of options exist. We review several valuable certificates to consider that will definitely pump up your resume and portfolio to get you closer to your dream job.,"By Sara A. Metwalli, Associate Editor at Towards Data Science.
comments

Photo by Lewis Keegan on Unsplash.
Because of the appeal of the field of data science and the premise of high incomes, more and more people decide to join the field every day. Some may come from a technical background, while others just join in due to curiosity; regardless of the reason you decide to join the field, your no. 1 goal will probably be to have a strong, solid portfolio that can help you land the job you want.

So, how can you increase the appeal of your portfolio?

Although getting into data science doesn’t necessarily require any degrees or certificates, sometimes having some could help make you stand out in the applicants' pool when applying for a job.
What makes a good data science portfolio is collecting projects that show your skills, prove your knowledge, and demonstrate your ability to build solid data science projects. That’s the core of a good portfolio, but you can also add some certificates to prove that you put in the time, effort, and money to hone your skills and become a more qualified data scientist.
Luckily, not all certificates you can get require you to go to a testing center. In fact, most of the desirable data science certificates can be taken from the comfort of your couch.
This article presents you with 6 highly desirable certificates that you can obtain to increase your chances of landing an internship or your dream job.
 
Microsoft Certified: Azure Data Scientist Associate
 
Microsoft is one of the leading names of technology and software; they offer a certificate that aims to measure your ability to run experiments, train machine learning models, optimize your model's performance, and deploy it using the Azure Machine Learning workspace.
To obtain this certificate, you will need to pass one exam, and you can prepare for this exam in one of two ways. Microsoft offers free online materials that you can self-study to prepare for the exam. If you prefer having an instructor, they also offer a paid option where an Azure machine learning instructor can tutor you.
This exam will cost around $165. The price varies based on the country you will proctor the test from.
 
IBM Data Science Professional Certificate
 
This certificate comes from IBM and is offered at the end of a course series that takes you from being a complete data science beginner to a professional data scientist online and at your own pace.
IBM Data science professional certificate is offered on both Coursera and edX. On either platform, you have to complete a set of courses covering all the core knowledge of data science to get the certificate and an IBM badge once you’re done.
To get the certificate from Coursera, you will need to pay a fee of $39 per month, so the sooner you can finish the series, the less you will need to pay. On the other hand, edX requires $793 for the full course experience regardless of how long you will talk to complete it.
 
Google Professional Data Engineer Certification
 
Google’s professional data engineer certification is aimed to examine the skills you need to be qualified as a data engineer. A data engineer can make data-driven decisions, build reliable models, train, test and optimize them.
You can get this certificate by applying directly through the official Google certificate page, or you can take a course series and the certificate on Coursera. The courses will teach you all you need to know about machine learning and AI fundamentals and build efficient data pipelines and analytics.
To access the course series on Coursera, you will need to have Coursera Plus or pay a fee of $49 per month for as long as you need to finish the series and obtain your certificate.
 
Cloudera Certified Professional (CCP) Data Engineer
 
Cloudera targets open-source developers and offers the CCP Data Engineer certificate for developers to test their ability to collect, process, and analyze data efficiently on the Cloudera CDH environment.
To pass this exam, you will be given 5-10 data science problems, each with its own large dataset and CDH cluster. Your task will be to find a high-precision solution for each of these problems and implement it correctly.
To take this exam, you will need to score at least 70% in the exam. The exam will be 4 hours long and will cost you $400. You can take this exam anywhere online.
 
SAS Certified AI & Machine Learning Professional
 
Unlike the certificates we discussed so far, the SAS AI & Machine Learning Professional certificate is acquired by passing three exams that test three different skill sets. The three exams you will need to pass to get the certificate are:

Machine learning exam where your skills to build, train, test performance and optimize supervised machine learning models will be tested.
Forecast and optimization test. In this test, your ability to handle, visualize data, build data pipelines and solve optimization problems will be tested.
NLP and computer vision test.

SAS offers a free 30 days worth of preparation materials that will get you ready to take and pass each of these three exams.
 
TensorFlow Developer Certificate
 
TensorFlow is one of the widely used packages for machine learning, AI, and deep learning applications. The TensorFlow Developer Certificate is given to a developer to demonstrate their ability to use TensorFlow to develop solutions for machine learning and deep learning problems.
You can prepare for this certificate by finishing the DeepLearning.AI TensorFlow Developer Professional Certificate Coursera course series. Once you have earned this certificate, your name and picture will be added to the Google Developers webpage.
The TensorFlow Developer Certificate is valid for 3 years. Afterward, you will need to retake the test to keep your skill level synced with the TensorFlow package's recent updates.
 
Takeaways
 
If you ask any data scientist whether they needed their degree or certificate to land their job roles, most will tell you that they got into data science from a non-technical background with a curious mind that only wanted to learn more.
And even though you can become a data scientist and get a good job by self-studying the core concepts of data science and building real-life-sized projects or projects that can be applied easily to real-life data, sometimes having a certificate can help make your portfolio stand out and attract the eyes of recruiters to you.
Because data science is one of the popular fields today, you will find a redundant amount of tutorials and guides online on what you need to do to become a “good data scientist” or “how to land your dream data science role.” Not to mention the tons of certificates that you can get and free courses you can take to improve your skills. I have been where you are, overwhelmed by the amount of information out there about data science and how to get into the field. But, I always appreciated simple, straightforward articles that get to the point without dragging the topic too long.
Original. Reposted with permission.
Related:

Data science certification – why it is important and where to get it?
10 resources for data science self-study
7 Most Recommended Skills to Learn to be a Data Scientist"
https://www.kdnuggets.com/2021/04/covid-do-all-our-models.html,What did COVID do to all our models?,"An interview with Dean Abbott and John Elder about change management, complexity, interpretability, and the risk of AI taking over humanity.","comments
By Heather Fyson, KNIME

After the KNIME Fall Summit, the dinosaurs went back home… well, switched off their laptops. Dean Abbott and John Elder, longstanding data science experts, were invited to the Fall Summit by Michael to join him in a discussion of The Future of Data Science: A Fireside Chat with Industry Dinosaurs. The result was a sparkling conversation about data science challenges and new trends. Since switching off the studio lights, Rosaria has distilled and expanded some of the highlights about change management, complexity, interpretability, and more in the data science world. Let’s see where it brought us.
 
What is your experience with change management in AI, when reality changes and models have to be updated? What did COVID do to all our models?
 
[Dean] Machine Learning (ML) algorithms assume consistency between past and future. When things change, the models fail. COVID has changed our habits, and therefore our data. Pre-COVID models struggle to deal with the new situation.
[John] A simple example would be the Traffic layer on Google Maps. After lockdowns hit country after country in 2020, Google Maps traffic estimates were very inaccurate for a while. It had been built on fairly stable training data but now that system was thrown completely out of whack.
 
How do you figure out when the world has changed and the models don't work anymore?
 
[Dean] Here’s a little trick I use: I partition my data by time and label records as “before” and “after”. I then build a classification model to discriminate the “after” vs. the “before” from the same inputs the model uses. If the discrimination is possible, then the “after” is different from the “before”, the world has changed, the data has changed, and the models must be retrained.
 
How complicated is it to retrain models in projects, especially after years of customization?
 
[John] Training models is usually the easiest step of all! The vast majority of otherwise successful projects die in the implementation phase. The greatest time is spent in the data cleansing and preparation phase. And the most problems are missed or made in the business understanding / project definition phase. So if you understand what the flaw is and can obtain new data and have the implementation framework in place, creating a new model is, by comparison, very straightforward.
 
Based on your decades-long experience, how complex is it to put together a really functioning Data Science application?
 
[John] It can vary of course, by complexity. Most of our projects get functioning prototypes at least in a few months. But for all, I cannot stress enough the importance of feedback: You have to talk to people much more often than you want to. And listen! We learn new things about the business problem, the data, or constraints, each time. Not all us quantitative people are skilled at speaking with humans, so it often takes a team. But the whole team of stakeholders has to learn to speak the same language.
[Dean] It is important to talk to our business counterpart. People fear change and don’t want to change the current status. One key problem really is psychological. The analysts are often seen as an annoyance. So, we have to build the trust between the business counterpart and the analytics geeks. The start of a project should always include the following step: Sync up domain experts / project managers, the analysts, and the IT and infrastructure (DevOps) team so everyone is clear on the objectives of the project and how it will be executed. Analysts are number 11 on the top 10 list of people they have to see every day! Let’s avoid embodying data scientist arrogance: “The business can’t understand us/our techniques, but we know what works best”. What we don’t understand, however, are the domains experts are actually experts in the domain we are working in! Translation of data science assumptions and approaches into language that is understood by the domain experts is key!
 
The latest trend now is deep learning, apparently it can solve everything. I got a question from a student lately, asking “why do we need to learn other ML algorithms if deep learning is the state of the art to solve data science problems”?
 
[Dean] Deep learning sucked a lot of the oxygen out of the room. It feels so much like the early 1990s when neural networks ascended with similar optimism! Deep Learning is a set of powerful techniques for sure, but they are hard to implement and optimize. XGBoost, Ensembles of trees, are also powerful but currently more mainstream. The vast majority of problems we need to solve using advanced analytics really don’t require complex solutions, so start simple; deep learning is overkill in these situations. It is best to use the Occam’s razor principle: if two models perform the same, adopt the simplest.
 
About complexity. The other trend, opposite to deep learning, is ML interpretability. Here, you greatly (excessively?) simplify the model in order to be able to explain it. Is interpretability that important?
 
[John] I often find myself fighting interpretability. It is nice, sure, but often comes at too high a cost of the most important model property: reliable accuracy. But many stakeholders believe interpretability is essential, so it becomes a barrier for acceptance. Thus, it is essential to discover what kind of interpretability is needed. Perhaps it is just knowing what the most important variables are? That’s doable with many nonlinear models. Maybe, as with explaining to credit applicants why they were turned down, one just needs to interpret outputs for one case at a time? We can build a linear approximation for a given point. Or, we can generate data from our black box model and build an “interpretable” model of any complexity to fit that data.
Lastly, research has shown that if users have the chance to play with a model – that is, to poke it with trial values of inputs and see its outputs, and perhaps visualize it – they get the same warm feelings of interpretability. Overall, trust – in the people and technology behind the model – is necessary for acceptance, and this is enhanced by regular communication and by including the eventual users of the model in the build phases and decisions of the modeling process.
[Dean] By the way KNIME Analytics Platform has a great feature to quantify the importance of the input variables in a Random Forest! The Random Forest Learner node outputs the statistics of candidate and splitting variables. Remember that, when you use the Random Forest Learner node.
 
There is an increase in requests for explanations of what a model does. For example, for some security classes, the European Union is demanding verification that the model doesn’t do what it’s not supposed to do. If we have to explain it all, then maybe Machine Learning is not the way to go. No more Machine Learning?
 
[Dean]  Maybe full explainability is too hard to obtain, but we can achieve progress by performing a grid search on model inputs to create something like a score card describing what the model does. This is something like regression testing in hardware and software QA. If a formal proof what models are doing is not possible, then let’s test and test and test! Input Shuffling and Target Shuffling can help to achieve a rough representation of the model behavior.
[John] Talking about understanding what a model does, I would like to raise the problem of reproducibility in science. A huge proportion of journal articles in all fields -- 65 to 90% -- is believed to be unreplicable. This is a true crisis in science. Medical papers try to tell you how to reproduce their results. ML papers don’t yet seem to care about reproducibility. A recent study showed that only 15% of AI papers share their code.
 
Let’s talk about Machine Learning Bias. Is it possible to build models that don’t discriminate?
 
[John] (To be a nerd for a second, that word is unfortunately overloaded. To “discriminate” in the ML world word is your very goal: to make a distinction between two classes.) But to your real question, it depends on the data (and on whether the analyst is clever enough to adjust for weaknesses in the data): The models will pull out of the data the information reflected therein. The computer knows nothing about the world except for what’s in the data in front of it. So the analyst has to curate the data -- take responsibility for those cases reflecting reality. If certain types of people, for example, are under-represented then the model will pay less attention to them and won’t be as accurate on them going forward. I ask, “What did the data have to go through to get here?” (to get in this dataset) to think of how other cases might have dropped out along the way through the process (that is survivor bias). A skilled data scientist can look for such problems and think of ways to adjust/correct for them.
[Dean] The bias is not in the algorithms. The bias is in the data. If the data is biased, we’re working with a biased view of the world. Math is just math, it is not biased.
 
Will AI take over humanity?!
 
[John] I believe AI is just good engineering. Will AI exceed human intelligence? In my experience anyone under 40 believes yes, this is inevitable, and most over 40 (like me, obviously): no! AI models are fast, loyal, and obedient. Like a good German Shepherd dog, an AI model will go and get that ball, but it knows nothing about the world other than the data it has been shown. It has no common sense. It is a great assistant for specific tasks, but actually quite dimwitted.
[Dean] On that note, I would like to report two quotes made by Marvin Minsky in 1961 and 1970, from the dawn of AI, that I think describe well the future of AI.
“Within our lifetime some machines may surpass us in general intelligence” (1961)
“In three to eight years we’ll have a machine with the intelligence of a human being” (1970)
These ideas have been around for a long time. Here is one reason why AI will not solve all the problems: We’re judging its behavior based on one number, one number only! (Model error.) For example, predictions of stock prices over the next five years, predicted by building models using root mean square error as the error metric, cannot possibly paint the full picture of what the data are actually doing and severely hampers the model and its ability to flexibly uncover the patterns. We all know that RMSE is too coarse of a measure. Deep Learning algorithms will continue to get better, but we also need to get better at judging how good a model really is. So, no! I do not think that AI will take over humanity.
 
We have reached the end of this interview. We would like to thank Dean and John for their time and their pills of knowledge. Let’s hope we meet again soon!
 
About Dean Abbott and John Elder
 




Dean Abbott is Co-Founder and Chief Data Scientist at SmarterHQ. He is an internationally recognized expert and innovator in data science and predictive analytics, with three decades of experience solving problems in omnichannel customer analytics, fraud detection, risk modeling, text mining & survey analysis. Included frequently in lists of pioneering data scientists and data scientists, he is a popular keynote speaker and workshop instructor at conferences worldwide, also serving on Advisory Boards for the UC/Irvine Predictive Analytics and UCSD Data Science Certificate programs. He is the author of Applied Predictive Analytics (Wiley, 2014) and co-author of The IBM SPSS Modeler Cookbook (Packt Publishing, 2013).








John Elder founded Elder Research, America’s largest and most experienced data science consultancy in 1995. With offices in Charlottesville VA, Baltimore MD, Raleigh, NC, Washington DC, and London, they’ve solved hundreds of challenges for commercial and government clients by extracting actionable knowledge from all types of data. Dr. Elder co-authored three books — on practical data mining, ensembles, and text mining — two of which won “book of the year” awards. John has created data mining tools, was a discoverer of ensemble methods, chairs international conferences, and is a popular workshop and keynote speaker.



 
Bio: Heather Fyson is the blog editor at KNIME. Initially on the Event Team, her background is actually in translation & proofreading, so by moving to the blog in 2019 she has returned to her real passion of working with texts. P.S. She is always interested to hear your ideas for new articles.
Original. Reposted with permission.
Related:

Mastering Time Series Analysis with Help From the Experts
How to break a model in 20 days — a tutorial on production model analytics
Main 2020 Developments and Key 2021 Trends in AI, Data Science, Machine Learning Technology"
https://www.kdnuggets.com/2021/04/automated-anomaly-detection-pycaret.html,Automated Anomaly Detection Using PyCaret,Learn to automate anomaly detection using the open source machine learning library PyCaret.,"comments
By Ekta Sharma, Data Science Enthusiast


Photo Credit — Unsplash

 
PyCaret is an open-source library that provides a variety of machine learning functions through various modules one of which is anomaly detection.
PyCaret’s anomaly detection module is an unsupervised machine learning module that is used for identifying extreme values present in the data that can sometimes indicate suspicious activity or an abnormal instance.
PyCaret’s anomaly detection module provides twelve different anomaly detection techniques to choose from depending on the problem you are working on. It also lets us perform feature engineering tasks through a function called “setup” by using various parameter values passed to this function.
In this article, we are going to apply three of the anomaly detection techniques provided by PyCaret on one of the datasets provided by PyCaret itself. The three techniques covered in this article are — Isolation Forest, K Nearest Neighbors, and Clustering.
Before we implement any of these techniques, let’s take a look at the steps that we will need to follow in a specific order to identify anomalies in the data by using the following functions. These steps are common to all techniques provided by PyCaret for anomaly detection.

get_data() — This function is used to access the PyCaret dataset. This is an optional step.
setup() — This function initializes the environment and performs the preprocessing tasks needed before anomaly detection. The only required parameter that it takes is a Dataframe in the “data” parameter but below is an example of various preprocessing tasks that can be achieved through the setup function.


setup(data, categorical_features = None, categorical_imputation = ‘constant’, ordinal_features = None, high_cardinality_features = None, numeric_features = None, numeric_imputation = ‘mean’, date_features = None, ignore_features = None, normalize = False, normalize_method = ‘zscore’, transformation = False, transformation_method = ‘yeo-johnson’, handle_unknown_categorical = True, unknown_categorical_method = ‘least_frequent’, pca = False, pca_method = ‘linear’, pca_components = None, ignore_low_variance = False, combine_rare_levels = False, rare_level_threshold = 0.10, bin_numeric_features = None, remove_multicollinearity = False, multicollinearity_threshold = 0.9, group_features = None, group_names = None, supervised = False, supervised_target = None, session_id = None, profile = False, verbose=True)



create_model() — This function creates the model and trains it on the dataset passed as the parameter during the setup stage. Hence, this function requires setup() function to be called before it is used.


df = pd.read_csv(path_to_csv) # to access your own dataset
or
df = get_data(“anomaly”) # to access PyCaret’s anomaly datasetsetup_data = setup(data=df)
sample_model = create_model(“iforest”)



plot_model() — This function takes the trained model created during create_model() function and plots the data passed during the setup() function. Hence, this method requires both setup() and create_model() function to be called before it is called. The returned plot cleary shows anomaly data in a different color.


plot_model(sample_model)



predict_model() — This function takes the trained model and uses it to make predictions on the new data. The new data must be in the form of a Pandas Dataframe. The output of this function is a Dataframe with predictions called “Label” and the associated decision Score.



Label = 0 means normal data or inlier
Label = 1 means an anomaly or outlier


Now, when we have a basic understanding of how PyCaret Anomaly Detection Functions work, let’s dive into the actual implementation.

# Importing PyCaret dependencies.
from pycaret.datasets import get_data
anomaly = get_data(“anomaly”)# Importing anomaly detection module.
from pycaret.anomaly import *# Initializing the setup function used for pre-processing.
setup_anomaly_data = setup(anomaly)


 
Isolation Forest Implementation
 

# Instantiating Isolation Forest model.
iforest = create_model(“iforest”)# Plotting the data using Isolation Forest model.
plot_model(iforest)# Generating the predictions using Isolation Forest trained model.
iforest_predictions = predict_model(iforest, data = anomaly)
print(iforest_predictions)# Checking anomaly rows. Label = 1 is the anomaly data.
iforest_anomaly_rows = iforest_predictions[iforest_predictions[“Label”] == 1]
print(iforest_anomaly_rows.head())# Checking the number of anomaly rows returned by Isolaton Forest.
print(iforest_anomaly_rows.shape) # returned 50 rows





Top 5 Anomaly Rows (Label 1)



Anomaly Plots created using Isolation Forest (Anomaly highlighted in Yellow color)


Isolation Forest Based Anomaly Plot

 
 
K Nearest Neighbors (KNN) Implementation
 

# Instantiating KNN model.
knn = create_model(“knn”)# Plotting the data using KNN model.
plot_model(knn)# Generating the predictions using KNN trained model.
knn_predictions = predict_model(knn, data = anomaly)
print(knn_predictions)# Checking KNN anomaly rows. Predictions with Label = 1 are anomalies.
knn_anomaly_rows = knn_predictions[knn_predictions[“Label”] == 1]
knn_anomaly_rows.head()# Checking the number of anomaly rows returned by KNN model.
knn_anomaly_rows.shape # returned 46 rows





Top 5 Anomaly Rows (Label 1)



Anomaly Plot created using K Nearest Neighbors (Anomaly highlighted in Yellow color)


KNN Based Anomaly Plot

 
 
Clustering Implementation
 

# Instantiating Cluster model.
cluster = create_model(“cluster”)# Plotting the data using Cluster model.
plot_model(cluster)# Generating the predictions using Cluster trained model.
cluster_predictions = predict_model(cluster, data = anomaly)
print(cluster_predictions)# Checking cluster anomaly rows. Predictions with Label = 1 are anomalies.
cluster_anomaly_rows = cluster_predictions[cluster_predictions[“Label”] == 1]
print(cluster_anomaly_rows.head())# Checking the number of anomaly rows returned by Cluster model.
cluster_anomaly_rows.shape # returned 50 rows





Top 5 Anomaly Rows (Label 1)



Anomaly Plot created using Clustering (Anomaly highlighted in Yellow color)


Clustering Based Anomaly Plot

 
References

https://pycaret.org/

 
Bio: Ekta Sharma is a Data Science Enthusiast.
Original. Reposted with permission.
Related:

How to use Machine Learning for Anomaly Detection and Conditional Monitoring
Bayesian Hyperparameter Optimization with tune-sklearn in PyCaret
Introducing MIDAS: A New Baseline for Anomaly Detection in Graphs"
https://www.kdnuggets.com/2021/04/e-commerce-data-analysis-sales-strategy-python.html,E-commerce Data Analysis for Sales Strategy Using Python,Check out this informative and concise case study applying data analysis using Python to a well-defined e-commerce scenario.,"comments
By Juhi Sharma, Product Analyst

Source — https://www.wvgazettemail.com/
 
Introduction
 
Kmart is a leading online retailer in the US and as part of their annual sales review meeting, they need to decide on their sales strategy for the year 2020 based on the insights from the sales data in 2019.
Data is related to sales for each month of 2019 and the task is to generate key insights which will help the sales team of Kmart to take some key business decisions towards Fine-tuning their sales strategy.
 
Data Understanding
 

Data Belongs to Kmart -a leading online retailer in the US.
Time Period — January 2019 — December 2019
Unique Product — 19
Total Orders — 178437
Cities — 9
KPI’s — Total Sales, Total Products Sold



Source — By Author

 
Business Problem Statements
 

What was the best month for sales? How much was earned that month?
Which city had the highest number of sales?
Recommend the most appropriate time to display advertising to maximize the likelihood of customers buying the products?
What products sold the most? Why do you think it sold the most?

 
Data Analysis Using Python
 

Loaded Data of each month and made data frame using pandas
Concatenated Dataset to make one Dataset for 2019 sales.
Treating Null Values and Junk Data.
Made a Filtered Dataset after preprocessing data
Analysis and answers to business problems. (visualizations using matplot and seaborn library)

 
1. Importing Libraries
 

import pandas as pd


 
2. Loading Dataset and making Dataframes
 

df1=pd.read_csv(""Sales_January_2019.csv"")
df1[""month""]=""Jan""
df2=pd.read_csv(""Sales_February_2019.csv"")
df2[""month""]=""feb""
df3=pd.read_csv(""Sales_March_2019.csv"")
df3[""month""]=""mar""
df4=pd.read_csv(""Sales_April_2019.csv"")
df4[""month""]=""apr""
df5=pd.read_csv(""Sales_May_2019.csv"")
df5[""month""]=""may""
df6=pd.read_csv(""Sales_June_2019.csv"")
df6[""month""]=""june""
df7=pd.read_csv(""Sales_July_2019.csv"")
df7[""month""]=""july""
df8=pd.read_csv(""Sales_August_2019.csv"")
df8[""month""]=""aug""
df9=pd.read_csv(""Sales_September_2019.csv"")
df9[""month""]=""sep""
df10=pd.read_csv(""Sales_October_2019.csv"")
df10[""month""]=""oct""
df11=pd.read_csv(""Sales_November_2019.csv"")
df11[""month""]=""nov""
df12=pd.read_csv(""Sales_December_2019.csv"")
df12[""month""]=""dec""list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12]


 
3. The shape of each month’s dataset
 

for i in list:
    print(i.shape)




Source- By Author

 
 
4. Concatenating dataset
 

frame=pd.concat(list)




Source- By Author

 
 
5. Columns of Final Dataset
 

frame.columns




Source-By Author

 
 
6. Dataframe Information
 

frame.info()




Source-By Author

 
 
7. Null values in the dataset
 

frame.isnull().sum() # there are 545 null values in each column except month




Source-By Author

 

(frame.isnull().sum().sum())/len(frame)*100  # we have 1.75 percent null values , so we can drop them




Source-By Author

 
 
8. Dropping Null Values
 

frame=frame.dropna()
frame.isnull().sum()




Source-By Author

 
 
9. Removing Junk Data
 
we observed that there are 355 columns in which values in rows are the same as the header. so making a new data frame where these values will be excluded.

frame[frame['Quantity Ordered'] == ""Quantity Ordered""]






df_filtered = frame[frame['Quantity Ordered'] != ""Quantity Ordered""] 
df_filtered.head(15) 
df_filtered.shape




Source-By Author

 
 
Solutions to Business Problems
 
Q 1. What was the best month for sales? How much was earned that month?

df_filtered[""Quantity Ordered""]=df_filtered[""Quantity Ordered""].astype(""float"")
df_filtered[""Price Each""]=df_filtered[""Price Each""].astype(""float"")# Creating Sales Column By multiplying Quantity Ordered and Price of Each Productdf_filtered[""sales""]=df_filtered[""Quantity Ordered""]*df_filtered[""Price Each""]




Source-By Author

 

month=[""dec"",""oct"",""apr"",""nov"",""may"",""mar"",""july"",""june"",""aug"",'feb',""sep"",""jan""] 
df[""month""]=monthfrom matplotlib import pyplot as plt
a4_dims = (11.7, 8.27)
fig, ax = pyplot.subplots(figsize=a4_dims)
import seaborn as sns
sns.barplot(x = ""sales"",
            y = ""month"",
            data = df)
plt.title(""Month wise Sale"")
plt.show()




Source-By Author

 
The best Month for sales was DECEMBER.
Total sales in December is $ 4619297.
 
Q 2. Which city had the highest number of sales?

dftemp = df_filtered
list_city = []
for i in dftemp['Purchase Address']:
    list_city.append(i.split("","")[1])
dftemp['City'] = list_city
dftemp.head()




Source-By Author

 

df_city=df_filtered.groupby([""City""])['sales'].sum().sort_values(ascending=False)
df_city=df_city.to_frame()
df_city




Source-By Author

 

city=[""San Francisco"",""Los Angeles"",""New York City"",""Boston"",""Atlanta"",""Dallas"",""Seattle"",""Portland"",""Austin""]
df_city[""city""]=cityfrom matplotlib import pyplot
a4_dims = (11.7, 8.27)
fig, ax = pyplot.subplots(figsize=a4_dims)
sns.barplot(x = ""sales"",
            y = ""city"",
            data = df_city)
plt.title(""City wise Sales"")
plt.show()




Source-By Author

 
San Francisco has the highest sales f around $8262204.
 
Q 3 What products sold the most?

print(df_filtered[""Product""].unique())
print(df_filtered[""Product""].nunique())




source- By Author

 

df_p=df_filtered.groupby(['Product'])['Quantity Ordered'].sum().sort_values(ascending=False).head()
df_p=df_p.to_frame()
df_p




Source-By Author

 

product=[""AAA Batteries (4-pack)"",""AA Batteries (4-pack)"",""USB-C Charging Cable"",""Lightning Charging Cable"",""Wired Headphones""]
df_p[""Product""]=productfrom matplotlib import pyplot
a4_dims = (11.7, 8.27)
fig, ax = pyplot.subplots(figsize=a4_dims)
sns.barplot(x = ""Quantity Ordered"",
            y = ""Product"",
            data = df_p)
plt.title(""Prouct and Quantity Ordered"")
plt.show()




Source-By Author

 
31017.0 quantity of AAA Batteries (4-pack) is sold in a year. It is sold maximum because it is the cheapest product.
 
Q 4 Recommend the most appropriate time to display advertising to maximize the likelihood of customers buying the products?

dftime = df_filtered
list_time = []
for i in dftime['Order Date']:
    list_time.append(i.split("" "")[1])
dftime['Time'] = list_time
dftime.head()




Source-By Author

 

df_t=df_filtered.groupby(['Time'])['sales'].sum().sort_values(ascending=False).head()
df_t=df_t.to_frame()
df_t




Source-By Author

 

df_t.columns




Source -By Author

 
 
Before You Go
 
Thanks for reading! If you want to get in touch with me, feel free to reach me at jsc1534@gmail.com or my LinkedIn Profile. Also, you can find the code for this article and some really useful data science projects on my GitHub account.
 
Bio: Juhi Sharma (Medium, GitHub) has 2+ years of work experience as an Analyst with the role of Project Management, Business Analysis, and Client Handling. Currently, Juhi is working as a Data Analyst for a Product Company. Juhi has hands-on experience in analyzing datasets, creating machine learning and deep learning models. Juhi is passionate about solving business problems with data-driven approaches.
Original. Reposted with permission.
Related:

Pandas Profiling: One-Line Magical Code for EDA
The question that makes your data project more valuable
How to frame the right questions to be answered using data"
https://www.kdnuggets.com/2020/10/ai-learn-human-values.html,Can AI Learn Human Values?,OpenAI believes that the path to safe AI requires social sciences.,"By Jesus Rodriguez, Intotheblock.
comments


Source: https://www.accenture.com/nl-en/blogs/insights/responsible-ai-with-opportunity-comes-responsibility

 

I recently started a new newsletter focus on AI education. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:


 
Ensuring fairness and safety in artificial intelligence(AI) applications is considered by many the biggest challenge in the space. As AI systems match or surpass human intelligence in many areas, it is essential that we establish a guideline to align this new form of intelligence with human values. The challenge is that, as humans, we understand very little about how our values are represented in the brain or we can’t even formulate specific rules to describe a specific value. While AI operates in a data universe, human values are a byproduct of our evolution as social beings. We don’t describe human values like fairness or justice using neuroscientific terms but using arguments from social sciences like psychology, ethics or sociology. Last year, researchers from OpenAI published a paper describing the importance of social sciences to improve the safety and fairness or AI algorithms in processes that require human intervention.
We often hear that we need to avoid bias in AI algorithms by using fair and balanced training datasets. While that’s true in many scenarios, there are many instances in which fairness can’t be described using simple data rules. A simple question such as “do you prefer A to B” can have many answers depending on the specific context, human rationality or emotion. Imagine the task of inferring a pattern of “happiness”, “responsibility” or “loyalty” given a specific dataset. Can we describe those values simply using data? Extrapolating that lesson to AI systems tells us that in order to align with human values we need help from the disciplines that better understand human behavior.
 
AI Value Alignment: Learning by Asking the Right Questions
 
In their research paper, the OpenAI team introduced the notion of AI value alignment as “the task of ensuring that artificial intelligence systems reliably do what humans want”. AI value alignment requires a level of understanding of human values in a given context. However, many times, we can’t simply explain the reasoning for a specific value-judgment in a data-rule. In those scenarios, the OpenAI team believes that the best way to understand human values is by simply asking questions.
Imagine a scenario in which we are trying to train a machine learning classifier in whether the outcome of a specific event is “better” or “worse”. Is an “increase in taxes better or worse?”, maybe is better for government social programs and worse for your economic plans. “Would it be better or worse if it rains today?”, maybe it would be better for the farmers and worse for the folks that were planning a biking trip. Questions about human values can have different subjective answers depending on a specific context. From that perspective, if we can get AI systems to ask specific questions maybe we can learn to imitate human judgement in specific scenarios.
Asking the right question is an effective method for achieving AI value alignment. Unfortunately, this type of learning method is vulnerable to three well-known limitations of human value judgment:

Reflective Equilibrium: In many cases, humans can’t arrive to the right answer to a question related to value judgement. Cognitive or ethical biases, lack of domain knowledge or fuzzy definition of “correctness” are factors that might introduce ambiguity in the answers. However, if we remove many of the contextual limitations of the question, a person might arrive to the “right answer”. In philosophy this is known as the “reflective equilibrium” as is one of the mechanism that any AI algorithm that tries to learn about human values should try to imitate.
Uncertainly: Even if we can achieve a reflective equilibrium for a given question, there might be many circumstances in which uncertainly or disagreement prevent humans for arriving to the right answer. Any activities related to future planning often entail uncertainty.
Deception: Humans have a unique ability to provide plausible answers to a question but that might wrong in some non-obvious way. Intentionally or unintentionally, deceptive or misleading behavior often results in a misalignment between the outcome of a given event and the values of the parties involved. Recognizing deceptive behavior is a non-trivial challenge that needs to be solved to achieve AI value alignment.

 
Learning Human Values by Debating
 
So far we have two main arguments to the thesis of AI value alignment:

AI systems can learn human values by asking questions.
Questions are often vulnerable to challenges like uncertainty, deception or the absence of a reflective equilibrium.

Bringing these two ideas together, the OpenAI team decided to induce AI agents to learn human values by relying on one of the purest question-answering dynamics: debates. Conceptually, debating is a form of discussion that breaks down a complex argument into an iterative set of simpler questions in order to formulate a reasoning path towards a specific answer. In applying debate techniques to achieve AI value alignment, the OpenAI team relied on an operating hypothesis:
“Optimal play in the debate game (giving the argument most convincing to a human) results in true, useful answers to questions.”
With that hypothesis as the foundation, OpenAI created a game in which two AI agents engaged in debate, trying to convince a human judge. The debaters are trained only to win the game, and are not motivated by truth separate from the human’s judgments. On the human side, the objective is to understand whether people are strong enough as judges in debate to make this scheme work, or how to modify debate to fix it if it doesn’t. Using AI debaters in the OpenAI debate is an ideal setting but the technology hasn’t really caught up to that point. Most real debates leverage sophisticated natural language patterns that are beyond the capabilities of AI systems today. Certainly, efforts like IBM Project Debater are rapidly closing this gap.
To avoid the limitations of AI debaters, OpenAI uses a scheme with two human debaters and a human judge. The outcome of this debate game are used to train the AI-AI-Human setting.


Source: https://openai.com/blog/ai-safety-needs-social-scientists/

 
To test the idea of training AI systems using this debate model, the OpenAI team created a prototype website where two debaters argue over the contents of an image. The games chooses an image of a cat or dog, and show the image to the two debaters but not the judge. One debater is honest and argues for the true contents of the image; the other debater lies. The debaters can talk to the judge and illustrate their points by drawing rectangles on the image, but the judge sees only the rectangles. At the end of the debate, each debater is allowed to reveal a single pixel to the judge, which is the only part of the debate which cannot be a lie. The outputs of the debate are used to train sophisticated image classifiers.


Source: https://openai.com/blog/ai-safety-needs-social-scientists/

 
Using debates as the underlying technique, can help to answer important questions about the relationship between humans and AI agents.
The idea of applying social sciences to AI is not a new one but the OpenAI efforts are some of the first pragmatic steps in this area. While social sciences focus on understanding human behavior in the real world, AI sorts of takes the best version of human behavior as a starting point. From that perspective, the intersection of social sciences and AI can lead to a more fairer and safer machine intelligence.
 
Original. Reposted with permission.
Related:

Free From Stanford: Ethical and Social Issues in Natural Language Processing
The Ethics of AI
Algorithms of Social Manipulation"
https://www.kdnuggets.com/2020/10/getting-data-science-job-harder.html,Getting A Data Science Job is Harder Than Ever – How to turn that to your advantage,"Although many aspiring Data Scientists are finding it is becoming more difficult to land a job than it was in previous years, understanding what has changed in the hiring landscape can be used to to your advantage in matching with the best organization for your goals and interests.","comments
By Kurtis Pykes, AI Writer.

Photo by Martin Péchy on Unsplash.
As of writing this post, I am currently on the job hunt for a new role as a Data Scientist due to difficulties in correspondence to Covid-19 at my previous company.
This time around, I’ve noticed things seem to be much harder than they were when I was last was on the market, but instead of using these challenges to prolong our dreams of becoming a Data Scientist or end them in a worst case scenario, I’ve endeavored to better understand these challenges so I could come up with some solutions to make them work best in my favor, and now yours!
 
Outlandish Job Requirements
 
This seems to be a thread in most discussions I’ve had with Data Science job seekers —

Nobody feels qualified anymore.

Many Data Science Job descriptions do not communicate the actual requirements of the role being advertised.
One major effect of this is that aspiring Data Scientists who prioritize their personal and technical skills based on job descriptions would be mislead regarding the requirements to fulfill a role. Another issue with this is that recruiters would get plenty of applications that do not meet the requirements.
According to a fabulous post by Jeremie Harris titled The Problem With Data Science Job Postings, there are many reasons why a Job description may seem incomprehensible, and it’s down to you to distinguish what category the one you’re on falls in:

Many ways to solve problems, so it is hard to narrow it down for a job description.
New Data Science teams may encourage people to be a jack of all trades, which translates into the job description.
The company lacks the experience to know what problems they have and what capabilities the person who can solve them should have.
Written by recruiters.

Solution
Though it requires some discernment on your part, it is important to identify what the potential reason for an outlandish job description may be as some scenarios may be harmful to your growth as a Data Scientist, such as being a jack of all trades.
A great way to overcome this challenge is to acknowledge that Job Descriptions are merely a wish-list from the company, and they want to hire someone they believe has the ability to solve a real problem they actually have.
On that note, definitely go about displaying your ability in a way that allows others (the companies) to perceive you have what it takes to tackle their challenges. Additionally, if you meet at least 50% of requirements on any job description, then you’re probably qualified and should definitely attempt to go for the role — if you meet the job description 100%, you’re probably overqualified.
 
Data Science is becoming more Productionized
 
Being able to spin up a Jupyter Notebook and do some visualizations then build a model has worked in the past, but it’s no longer enough to get you noticed, in my opinion.
Jupyter Notebooks are great for conducting experiments, but when you get into the real world, there’s a point we move past the experimental phase. I believe it should be expected of every Data Scientist to know how to spin up a Jupyter Notebook, but as Data Science is becoming more productionized, bonus points are given to the Data Scientist that can write production-level code because you’ll be saving cost and time.
Here are 3 reasons every Data Scientist should know how to write production-level code:

Things can get lost in translation from Data Scientist to Engineer.
Eradicates the delay in the process.
Killing 2 birds with 1 stone since one person can do 2 people’s job — makes you more valuable.

Solution
Although controversial, I believe the skills of a Data Scientist and a Software engineer are converging when it comes to product facing Data Science applications, so more Data Scientist should be learning about software engineering best practices.
Given you already know how to write production-level code, you may want to check out Schaun Wheeler post titled What does it mean to “Productionize” Data Science?, which exceptionally summarizes the focus of employment of systems beyond the code level best practices for Data Science productionization — a really interesting read to say the least.

“For something to be ‘in production’ means it is part of the pipeline from the business to its customers. […] In data science, if something is in production, it’s on the path to putting information in a place where it is consumed.”

 
Competition
 
Data Science is among the fastest growing and emerging technologies on the planet, and tons of people are flocking to update their skills to have a shot a making a career as a Data Scientist. And, just in-case you don’t believe me, over 3.5 million people have enrolled in Andrew Ng Machine Learning course (which is an important part of Data Science) on Coursera since its inception.

It’s the sexiest job of the 21st Century for a reason.
These days more and more people are trying to break into the field, so the jobs have huge competition. However, this doesn’t have to be a reason to decide not to look for jobs!
Solution
Yes, we ought to do more to stand out. However, according to a recent poll I took on my LinkedIn profile, this doesn’t necessarily mean having the most fancy looking CV.

Source: Kurtis Pykes LinkedIn.
Of course, having a great portfolio is a great way to stand out, yet what seems to be undefeated in increasing your chances of being handed an opportunity is reaching out to hiring managers or Data Scientist in senior roles in the places that you are applying for.
LinkedIn has made it so easy to find people who work at a particular company, and it should be made part of the job application process when applying for jobs.
 
Conclusion
 
The fact that it’s difficult to get a job in Data Science should never be the reason you don’t have one. There are many challenges you’ll face at any job in itself, and getting the job is just the qualification phase to see if the employers believe you are capable of facing the challenges and whether you believe the employers are whom you would like to be on your team. Always seek to improve yourself, don’t wait to be ready to apply because you may never feel ready, and don’t be afraid to be rejected or to reject companies that don’t align with where you are going.
Original. Reposted with permission.
 
Bio: Kurtis Pykes is passionate about harnessing the power of machine learning and data science to help people become more productive and effective.
Related:

How I Levelled Up My Data Science Skills In 8 Months
Unpopular Opinion – Data Scientists Should Be More End-to-End
9 Developing Data Science & Analytics Job Trends"
https://www.kdnuggets.com/2021/05/generate-meaningful-sentences-t5-transformer.html,How To Generate Meaningful Sentences Using a T5 Transformer,Read this article to see how to develop a text generation API using the T5 transformer.,"comments
By Vatsal Saglani, Machine Learning Engineer at Quinnox


Photo by Tech Daily on Unsplash

 
In the blog, Generating storylines using a T5 Transformer we saw how we can fine-tune a Sequence2Sequence (Text-To-Text) Transformer (T5) to generate storylines/plots by providing inputs like genre, director, cast, and ethnicity. In this blog, we will check out how we can use that trained T5 Model for inference. Later, we will also see how can we deploy it using gunicorn and flask.
 
How to do Model Inference?
 

Let’s set up the script with the imports


import os
import re
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook, tnrange
from sklearn.utils import shuffle
import pickle
import math
import torch
import torch.nn.functional as F
from transformers import T5Tokenizer, T5ForConditionalGeneration



Set the SEED value and load the model and tokenizer


torch.manual_seed(3007)model = T5ForConditionalGeneration.from_pretrained('./outputs/model_files')
tokenizer = T5Tokenizer.from_pretrained('./outputs/model_files')



Use the model.generate function to generate sequences


text = ""generate plot for genre: horror""
input_ids = tokenizer.encode(text, return_tensors=""pt"")
greedyOp = model.generate(input_ids, max_length=100)
tokenizer.decode(greedyOp[0], skip_special_tokens=True)


Note: Read this amazing Hugging Face blog regarding how you to use different decoding strategies for Text Generation using Transformers

Let’s put this in a function


def generateStoryLine(text, seq_len, seq_num):
				'''
				args:
					text: input text eg. generate plot for: {genre} or generate plot for: {director}
					seq_len: Max sequence length for the generated text
					seq_num: Number of sequences to generate
				'''
        outputDict = dict()
        outputDict[""plots""] = {}
        input_ids = tokenizer.encode(text, return_tensors = ""pt"")
        beamOp = model.generate(
            input_ids,
            max_length = seq_len,
            do_sample = True,
            top_k = 100,
            top_p = 0.95,
            num_return_sequences = seq_num
        )        for ix, sample_op in enumerate(beamOp):
            outputDict[""plots""][ix] = self.tokenizer.decode(sample_op, skip_special_tokens = True)
            
        
        return outputDict


 
How to deploy this with Flask?
 
There are multiple ways a user can provide the inputs and the model might need to generate the plots. The user can provide only the genre, or they can provide genre and cast or they can even provide all the four i.e. genre, director, cast and ethnicity. But for the purpose of this implementation, I have kept it mandatory to provide a genre at the least.
You can check out the link below to see how the API will work.
Movie Plot Generator
I generate vague movie plots on the web (but sometimes they are good). But I can assure you that it will always be…
 
Let’s develop a backend to achieve the API call used for the link above
 
Install the requirements
 

pip install flask flask_cors tqdm rich gunicorn


 
Create an app.py file
 

Imports


# app.pyfrom flask import Flask, request, jsonify
import json
from flask_cors import CORS
import uuidfrom predict import PredictionModelObjectapp = Flask(__name__)
CORS(app)print(""Loading Model Object"")
predictionObject = PredictionModelObject()
print(""Loaded Model Object"")



Add an API route


@app.route('/api/generatePlot', methods=['POST'])
def gen_plot():    req = request.get_json()
    genre = req['genre']
    director = req['director'] if 'director' in req else None
    cast = req['cast'] if 'cast' in req else None
    ethnicity = req['ethnicity'] if 'ethnicity' in req else None
    num_plots = req['num_plots'] if 'num_plots' in req else 1
    seq_len = req['seq_len'] if 'seq_len' in req else 200    if not isinstance(num_plots, int) or not isinstance(seq_len, int):
        return jsonify({
            ""message"": ""Number of words in plot and Number of plots must be integers"",
            ""status"": ""Fail""
        })
    
    try:
        plot, status = predictionObject.returnPlot(
            genre = genre, 
            director = director,
            cast = cast,
            ethnicity = ethnicity,
            seq_len = seq_len,
            seq_num = num_plots
        )        if status == 'Pass':
            
            plot[""message""] = ""Success!""
            plot[""status""] = ""Pass""
            return jsonify(plot)
        
        else:            return jsonify({""message"": plot, ""status"": status})
    
    except Exception as e:        return jsonify({""message"": ""Error getting plot for the given input"", ""status"": ""Fail""})



The main block to run the flask app


if __name__ == ""__main__"":
    app.run(debug=True, port = 5000)


This script won’t work yet. You might receive an ImportError when executing the script at this point as we haven’t yet created the predict.py script with the PredictionModelObject
 
Create the PredictionModelObject
 

Create an predict.py file and import the following


# predict.py
import os
import re
import random
import torch
import torch.nn as nn
from rich.console import Console
from transformers import T5Tokenizer, T5ForConditionalGeneration
from collections import defaultdictconsole = Console(record = True)torch.cuda.manual_seed(3007)
torch.manual_seed(3007)



Create the PredictionModelObject class


# predict.py
class PredictionModelObject(object):    def __init__(self):console.log(""Model Loading"")
        self.model = T5ForConditionalGeneration.from_pretrained('./outputs/model_files')
        self.tokenizer = T5Tokenizer.from_pretrained('./outputs/model_files')
        console.log(""Model Loaded"")
    
    def beamSearch(self, text, seq_len, seq_num):        outputDict = dict()
        outputDict[""plots""] = {}
        input_ids = self.tokenizer.encode(text, return_tensors = ""pt"")
        beamOp = self.model.generate(
            input_ids,
            max_length = seq_len,
            do_sample = True,
            top_k = 100,
            top_p = 0.95,
            num_return_sequences = seq_num
        )        for ix, sample_op in enumerate(beamOp):
            outputDict[""plots""][ix] = self.tokenizer.decode(sample_op, skip_special_tokens = True)
            
        
        return outputDict    def genreToPlot(self, genre, seq_len, seq_num):        text = f""generate plot for genre: {genre}""        return self.beamSearch(text, seq_len, seq_num)    def genreDirectorToPlot(self, genre, director, seq_len, seq_num):        text = f""generate plot for genre: {genre} and director: {director}""
        
        return self.beamSearch(text, seq_len, seq_num)    def genreDirectorCastToPlot(self, genre, director, cast, seq_len, seq_num):        text = f""generate plot for genre: {genre} director: {director} cast: {cast}""        return self.beamSearch(text, seq_len, seq_num)    def genreDirectorCastEthnicityToPlot(self, genre, director, cast, ethnicity, seq_len, seq_num):        text = f""generate plot for genre: {genre} director: {director} cast: {cast} and ethnicity: {ethnicity}""        return self.beamSearch(text, seq_len, seq_num)
    
    def genreCastToPlot(self, genre, cast, seq_len, seq_num):        text = f""genreate plot for genre: {genre} and cast: {cast}""        return self.beamSearch(text, seq_len, seq_num)    def genreEthnicityToPlot(self, genre, ethnicity, seq_len, seq_num):        text = f""generate plot for genre: {genre} and ethnicity: {ethnicity}""        return self.beamSearch(text, seq_len, seq_num)    def returnPlot(self, genre, director, cast, ethnicity, seq_len, seq_num):
        console.log('Got genre: ', genre, 'director: ', director, 'cast: ', cast, 'seq_len: ', seq_len, 'seq_num: ', seq_num, 'ethnicity: ',ethnicity)
        
        seq_len = 200 if not seq_len else int(seq_len)
        
        seq_num = 1 if not seq_num else int(seq_num)
        
        if not director and not cast and not ethnicity:            return self.genreToPlot(genre, seq_len, seq_num), ""Pass""
        
        elif genre and director and not cast and not ethnicity:            return self.genreDirectorToPlot(genre, director, seq_len, seq_num), ""Pass""        elif genre and director and cast and not ethnicity:            return self.genreDirectorCastToPlot(genre, director, cast, seq_len, seq_num), ""Pass""        elif genre and director and cast and ethnicity:            return self.genreDirectorCastEthnicityToPlot(genre, director, cast, ethnicity, seq_len, seq_num), ""Pass""        elif genre and cast and not director and not ethnicity:            return self.genreCastToPlot(genre, cast, seq_len, seq_num), ""Pass""
        
        elif genre and ethnicity and not director and not cast:            return self.genreEthnicityToPlot(genre, ethnicity, seq_len, seq_num), ""Pass""        else:            return ""Genre cannot be empty"", ""Fail""


Save the predict.py file and then run the app.py file in debug mode using,

python app.py


 
Test your API
 

Create a test_api.py file and execute


# test_api.py
import requests
import osurl = ""<http://localhost:5000/api/generatePlot>""
json = {
    ""genre"": str(input(""Genre: "")),
    ""director"": str(input(""Director: "")),
    ""cast"": str(input(""Cast: "")),
    ""ethnicity"": str(input(""Ethnicity: "")),
    ""num_plots"": int(input(""Num Plots: "")),
    ""seq_len"": int(input(""Sequence Length: "")),
}r = requests.post(url, json = json)
print(r.json())


 
How to run with gunicorn ?
 
Using gunicorn with flask is very easy. While installing the requirements at the start we have installed the gunicorn command and now we need to go to the folder where the app.py file is located via. the terminal and run the following command

gunicorn -k gthread -w 2 -t 40000 --threads 3 -b:5000 app:app


The format and flags we use above represent the following

k: kind (type of workers)- gthread, gevent, etc...
w: number of workers
t: timeout time
threads: number of threads per worker
b: bind port number

If your filename is server.py or flask_app.py the app:app part will change to server:app or flask_app:app
 
In Summary
 
In this blog, we saw how can we use our previously trained T5 transformer to generate storylines and deploy it using flask and gunicorn. This blog is made to be followed pretty easily so you don't waste time going around different platforms to check out the issues. Hope you have fun reading and implementing this.
 
Bio: Vatsal Saglani (@saglanivatsal) is a Machine Learning Engineer at Quinnox.
Original. Reposted with permission.
Related:

Hugging Face Transformers Package – What Is It and How To Use It
Multilingual CLIP with Huggingface + PyTorch Lightning
GPT-2 vs GPT-3: The OpenAI Showdown"
https://www.kdnuggets.com/2021/06/applied-language-technology.html,Applied Language Technology: A No-Nonsense Approach,Here is a free entry-level applied natural language processing course that can fit into any beginner's roadmap to understanding NLP. Check it out.,"By Matthew Mayo, KDnuggets.
comments

 
Dr. Tuomo Hiippala, Assistant Professor in English Language and Digital Humanities in the Department of Languages at the University of Helsinki, has shared his videos and other learning materials for a pair of courses that he teaches, all in a single website for those looking to learn Applied Language Technology.
While it appears that some of the material is not available to users beyond the University, specifically at least one hosted instance of the course code notebooks, besides the course website, the videos are all available in a single playlist as well. 
Here is a high-level overview of what you can expect from these courses:

Together, these two courses provide an introduction to applied language technology for audiences who are unfamiliar with language technology and programming. The learning materials assume no previous knowledge of the Python programming language.

 

 
The material is broken down into 3 major top-level learning components:

Part I: A Minimal Introduction to Python
Part II: Working with Text in Python
Part III: Natural Language Processing for Linguists

The material has a language focus and emerges from the field of linguistics — in contrast to a technology focus, emerging from the field of computer science — as evidenced by the following, which is a different perspective from most natural language processing courses and learning materials I have done my best to highlight in the past:

Instead of treating text simply as data and a source of some information to be extracted, these learning materials emphasise text as the product of linguistic processes, which are inextricably related to language use in society. If you are already familiar with language technology, these materials may hopefully broaden your perspectives on language.

 
But don't let that fool you; you will be delving into the technologies available via existing tools in the Python ecosystem to work on applied language tasks. What this means, notably, is that you will use existing libraries to accomplish these goals, as opposed to writing your own implementations of NLP algorithms, a fact which which should not be surprising given the applied nature of this course.
Also, you should not be intimidated by anything covered. Hiippala starts slowly with basic Python, moves on to how to think about text in relation to technology, and then on to more advanced applied NLP, so there is a lot of hand-holding and explanation along the way. This course assume no knowledge of either the technological or linguistic sides of this equation, and so is a great fit for any beginner level learner.
Coming out on the other side of this course with a knowledge of Python, text processing, and general NLP usefulness and application, you would be well positioned to then take on more advanced NLP learning materials and state of the art approaches to applied NLP, numerous courses of which exist. I could recommend the recently-released Hugging Face course on this very subject.
Did I mention this is all freely-available? Thanks goes out to Dr. Tuomo Hiippala at the University of Helsinki for such great material.
 
Related:

The Best Way to Learn Practical NLP?
How to Create and Deploy a Simple Sentiment Analysis App via API
The Essential Guide to Transformers, the Key to Modern SOTA AI"
https://www.kdnuggets.com/2020/12/essential-math-data-science-probability-density-probability-mass-functions.html,Essential Math for Data Science: Probability Density and Probability Mass Functions,"In this article, we’ll cover probability mass and probability density function in this sample. You’ll see how to understand and represent these distribution functions and their link with histograms.","By Hadrien Jean, Machine Learning Scientist.
comments

 
In the chapter 02 of Essential Math for Data Science, you can learn about basic descriptive statistics and probability theory. We’ll cover probability mass and probability density function in this sample. You’ll see how to understand and represent these distribution functions and their link with histograms.
Deterministic processes give the same results when they are repeated multiple times. This is not the case for random variables, which describe stochastic events, in which randomness characterizes the process.
This means that random variables can take various values. How can you describe and compare these values? One good way is to use the probability that each outcome will occur. The probability distribution of a random variable is a function that takes the sample space as input and returns probabilities: in other words, it maps possible outcomes to their probabilities.
In this section, you’ll learn about probability distributions for discrete and continuous variables.
 
Probability Mass Functions
 
Probability functions of discrete random variables are called probability mass functions (or PMF). For instance, let’s say that you’re running a dice-rolling experiment. You call X the random variable corresponding to this experiment. Assuming that the die is fair, each outcome is equiprobable: if you run the experiment a large number of times, you will get each outcome approximately the same number of times. Here, there are six possible outcomes, so you have one chance over six to draw each number.
Thus, the probability mass function describing X returns 1616 for each possible outcome and 0 otherwise (because you can’t get something different than 1, 2, 3, 4, 5 or 6).
You can write ,  and so on.
Properties of Probability Mass Functions
Not every function can be considered as a probability mass function. A probability mass function must satisfy the following two conditions:

The function must return values between 0 and 1 for each possible outcome:



The sum of probabilities corresponding to all the possible outcomes must be equal to 1:


The value of x can be any real number because values outside of the sample space are associated with a probability of 0. Mathematically, for any value x not in the sample space S, P(x)=0.
Simulation of the Dice Experiment
Let’s simulate a die experiment using the function np.random.randint(low, high, size) from Numpy which draw n (size) random integers between low and high (excluded). Let’s simulate 20 die rolls:


rolls = np.random.randint(1, 7, 20)
rolls




array([6, 3, 5, ..., 6, 5, 1])



This array contains the 20 outcomes of the experiment. Let’s call X the discrete random variable corresponding to the die rolling experiment. The probability mass function of X is defined only for the possible outcomes and gives you the probability for each of them.
Assuming the die is fair, you should have an uniform distribution, that is, equiprobable outcomes.
Let’s visualize the quantity of each outcome you got in the random experiment. You can divide by the number of trials to get the probability. Let’s use plt.stem() from Matplotlib to visualize these probabilities:

val, counts = np.unique(rolls, return_counts=True)
plt.stem(val, counts/len(rolls), basefmt=""C2-"", use_line_collection=True)





Figure 1: Probability mass function of the random variable X corresponding to a die rolling a six-sided die estimated from 20 rolls.
With a uniform distribution, the plot would have the same height for each outcome (since the height corresponds to the probability, which is the same for each outcome of a die throw). However, the distribution shown in Figure 1 doesn’t look uniform. That’s because you didn’t repeat the experiment enough: the probabilities will stand when you repeat the experiment a large number of times (in theory, an infinite number of times).
Let’s increase the number of trials:

throws = np.random.randint(1, 7, 100000)
val, counts = np.unique(throws, return_counts=True)
plt.stem(val, counts/len(throws), basefmt=""C2-"", use_line_collection=True)



Figure 2: Probability mass function of the random variable X corresponding to a die rolling experiment estimated from 100,000 rolls.
With enough trials, the probability mass function showed in Figure 2 looks uniform. This underline the importance of the number of trials from a frequentist probability point of view.
 
Probability Density Functions
 
With continuous variables, there is an infinite number of possible outcomes (limited by the number of decimals you use). For instance, if you were drawing a number between 0 and 1 you might get an outcome of, for example, 0.413949834. The probability of drawing each number tends towards zero: if you divide something by a very large number (the number of possible outcomes), the result will be very small, close to zero. This is not very helpful in describing random variables.
It is better to consider the probability of getting a specific number within a range of values. The y-axis of probability density functions is not a probability. It is called a probability density or just density. Thus, probability distributions for continuous variables are called probability density functions (or PDF).
The integral of the probability density function over a particular interval gives you the probability that a random variable takes a value in this interval. This probability is thus given by the area under the curve in this interval (as you can see in Essential Math for Data Science).
Notation
Here, I’ll denote probability density functions using a lowercase p. For instance, the function p(x) gives you the density corresponding to the value x.
Example
Let’s inspect an example of probability density function. You can randomly draw data from a normal distribution using the Numpy function np.random.normal (you’ll find more details about the normal distribution in Essential Math for Data Science).
You can choose the parameters of the normal distribution (the mean and the standard deviation) and the number of samples. Let’s create a variable data with 1,000 values drawn randomly from a normal distribution with a mean of 0.3 and a standard deviation of 0.1.

np.random.seed(123)
data = np.random.normal(0.3, 0.1, 1000)



Let’s look at the shape of the distribution using an histogram. The function plt.hist() returns the exact values for the x- and y-coordinates of the histogram. Let’s store this in a variable called hist for latter use:

hist = plt.hist(data, bins=13, range=(-0.3, 1))




Figure 3: Histogram of the data generated from a normal distribution. The x-axis is the value of the element in the vector and the y-axis the number of elements (count) that are in the corresponding range.
Histograms
Histograms show how values are distributed. It is a way to model a probability distribution using a finite number of values from the distribution. Since we're dealing with continuous distributions, this histogram corresponds to the number of values for specific intervals (the intervals depends on the parameter bins in the function hist()).
For instance, Figure 3 shows that there are around 347 elements in the interval (0.2, 0.3). Each bin corresponds to a width of 0.1, since we used 13 bins to represent data in the range -0.3 to 1.
Let’s have a closer look at the distribution with more bins. You can use the parameter density to make the y-axis correspond to the probability density instead of the count of values in each bin:

hist = plt.hist(data, bins=24, range=(-0.2, 1), density=True)



Figure 4: Histogram using 30 bins and density instead of counts.
You can see in Figure 4 that there are more bins in this histogram (24 instead of 13). This means that each bin has now a smaller width. The y-axis is also on a different scale: it corresponds to the density, not the counter of values as before.
To calculate the probability to draw a value in a certain range from the density, you need to use the area under the curve. In the case of histograms, this is the area of the bars.
Let’s take an example with the bar ranging from 0.2 to 0.25, associated with the following density:

print(f""Density: {hist[0][8].round(4)}"")
print(f""Range x: from {hist[1][8].round(4)} to {hist[1][9].round(4)}"")



Density: 2.8
Range x: from 0.2 to 0.25


Since there are 24 bins and the range of possible outcomes is from -0.2 to 1, each bar corresponds to a range of . In our example, the height of the bar (the one from 0.2 to 0.25) is around 2.8, so the area of this bar is . This means that the probability of getting a value between 0.2 and 0.25 is around 0.14, or 14%.
You saw that the sum of the probabilities must be equal to one, so the sum of the bar’s areas should be equal to one. Let’s check that: you can take the vector containing the densities (hist[0]) and multiply it by the bar width (0.05):

(hist[0] * 0.05).sum().round(4)



1.0


All good: the sum of the probabilities is equal to one.
From Histograms to Continuous Probability Density Functions
Histograms represent a binned version of the probability density function. Figure 5 shows a representation of the true probability density function. The blue shaded area in the figure corresponds to the probability of getting a number between 0 and 0.2 (the area under the curve between 0 and 0.2).

Figure 5: The probability to draw a number between 0 and 0.2 is the highlighted area under the curve.
Properties of Probability Density Functions
Like probability mass functions, probability density functions must satisfy some requirements. The first is that it must return only non negative values. Mathematically written:

The second requirement is that the total area under the curve of the probability density function must be equal to 1:

In this part on probability distributions, you saw that probability mass functions are for discrete variables and probability density functions for continuous variables. Keep in mind that the value on the y axis of probability mass functions are probabilities, which is not the case for probability density functions. Look at the density values (for instance in Figure 4): they can be larger than one, which shows that they are not probabilities.
 
Bio: Hadrien Jean is a machine learning scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog (hadrienj.github.io).
Original. Reposted with permission.
Related:

Essential Math for Data Science: Integrals And Area Under The Curve
Boost your data science skills. Learn linear algebra.
Essential Math for Data Science:  ‘Why’ and ‘How’"
https://www.kdnuggets.com/2020/07/monitoring-apache-spark-better-ui.html,Monitoring Apache Spark – We’re building a better Spark UI,"Data Mechanics is developing a free monitoring UI tool for Apache Spark to replace the Spark UI with a better UX, new metrics, and automated performance recommendations. Preview these high-level feedback features, and consider trying it out to support its first release.","comments
By Jean-Yves Stephan, Data Mechanics.

The Spark UI is the open source monitoring tool shipped with Apache Spark, the #1 big data engine. It generates a lot of frustration among Apache Spark users, beginners and experts alike.

“It’s hard to understand what’s going on.”
“Even if there’s a critical information, it’s buried behind a lot of noisy information that only experts know how to navigate.”
“There’s a lot of tribal knowledge involved.”
“The Spark history server is a pain to setup.”

Data Mechanics is a YCombinator startup building a serverless platform for Apache Spark — a Databricks, AWS EMR, Google Dataproc, or Azure HDinsight alternative — that makes Apache Spark more easy-to-use and performant.
In this article, we present our ambition to replace the Spark UI and Spark History Server with a free and cross-platform monitoring tool for Spark called the Data Mechanics UI. The project is at the prototype phase, but we'd love your feedback before we push it to production.
 
What’s wrong with the Spark UI?
 

The familiar Spark UI (jobs page).
It’s hard to get the bird’s eye view of what is going on.

Which jobs/stages took most of the time?
How do they match with my code?
Is there a critical stability or performance issue?
What is the bottleneck of my app (I/O bound, CPU bound, memory bound)?

The Spark UI lacks essential node metrics (CPU, Memory, and I/O usage).

Without them, any significant infrastructure change is a dangerous leap of faith.
To enable them, you need to set up a separate metrics monitoring system (such as Ganglia, or Prometheus + Grafana), and then constantly jump back and further between this system and the Spark UI, trying to match the timestamps.

The Spark History Server (rendering the Spark UI for terminated Spark apps) is hard to setup.

You need to persist Spark event logs to long-term storage and often deploy it yourself.
It can take forever to load, and it often crashes.

 
What does the Data Mechanics UI look like?
 

This GIF shows our prototype Data Mechanics UI in action!
What is new about it? Let's go over the main sections.
Summary statistics

TheData Mechanics UI - Summary Statistics.
The section shows the duration of the app, the total amount of resources (CPU uptime), the total duration of all the Spark tasks (should be close to your CPU uptime if your app is well parallelised). This information - surprisingly hard to get! - is critical if you care about your infrastructure costs.
Recommendations

The Data Mechanics UI – Recommendations.
This section builds upon the Data Mechanics platform auto-tuning feature where infrastructure parameters and Spark configurations are continuously optimized to boost performance and stability based on the history of the past runs of a given application.
This section gives high-level actionable feedback to developers, such as:

“Job 4 suffers from an input data skew. Consider repartitioning your data or salting the partition key”.
“The default number of tasks (200) is too small compared to the number of CPU cores (400) available. Increase spark.sql.shuffle.partitions to 1200.”

Executors CPU Usage

The Data Mechanics UI - Executors CPU Usage.
This screen lets you visually align system metrics on CPU utilization with the different Spark phases of your app. In a couple of seconds, you should see if your app is spent on an expensive shuffle operation, if a lot of resources are wasted due to inefficient parallelism, or if it is bottlenecked by I/O operations or CPU-intensive operations.
So this information is critical to understand your application performance and make smarter choices. You can then click on a specific job or stage to dive deeper into the problematic phase.
Executors Peak Memory Usage

The Data Mechanics UI - Executors Peak Memory Usage.
This screen shows you the memory usage breakdown for each executor when the total memory consumption was at its peak. Again, you'll immediately see if you're flirting with your container memory limits (maybe hitting OutOfMemory issues) or, on the contrary, if your memory is largely overprovisioned.
Memory issues are the most common sources of crashes for Apache Spark. OutOfMemory comes in two flavors:

The JVM can run out of memory. The JVM heap grew to its maximum size, and despite a full GC, it couldn't allocate more space. This can happen due to skewed shuffles, high concurrency, improper use of caching, or simply too small heap size settings.
The cluster manager (like YARN or, our favorite, Kubernetes) can kill a container because it exceeded its memory limit. This happens a lot when using PySpark, as a Spark executor will spawn one python process per running task, and these processes memory usage can quickly add up. This is very hard to troubleshoot for PySpark users, as almost no monitoring tool reports the memory usage of python processes, even though PySpark makes up the larger portion of the Spark community.

So this screen should give you critical information to make and keep your Spark applications stable.
 
How does the Data Mechanics UI work? How can I use it?
 
Due to technical reasons, the Data Mechanics UI will not be implemented in Spark open-source. But it will work on top of any Spark platform, entirely free of charge.
To use it, you’ll need to install an agent - a single jar attached to Spark. The code for the agent will be open-sourced, and we'll provide init scripts to install it automatically for each major Spark platform. Once this is done, you're done! The agent will send the Spark event logs to the Data Mechanics backend infrastructure, which will serve the Data Mechanics UI in your web browser!
Initially, it will only be available for terminated apps (a few minutes after they've run), so it will be more of Spark History Server than a live Spark UI replacement. We hope it'll be useful to you nonetheless!
 
Conclusion: We need YOU to make this happen
 
Data Mechanics is a managed platform for Apache Spark - like Amazon EMR, Google Dataproc, Databricks, and others. Our serverless features make Spark more easy-to-use and performant. It is deployed inside our customer's cloud account on a Kubernetes cluster that we manage for them, and it is available on AWS, GCP, and Azure.
The Data Mechanics UI will be a great complement to this platform — it would give Spark developers the high-level feedback about their code that they need to develop, scale, and maintain stable and performant Spark applications.
UPDATE November 2020. A first milestone of Data Mechanics Delight has been released. For now it only consists of a free hosted Spark History Server, but the team is planning a next release with new screens and metrics by January. Check out our website to sign up.
 
Bio: Jean-Yves Stephan, a former software engineer and Spark infrastructure lead at Databricks, is now the Co-Founder and CEO at Data Mechanics, a serverless platform making Apache Spark easy to use and performant.
Related:

The Benefits & Examples of Using Apache Spark with PySpark
Nitpicking Machine Learning Technical Debt
Practical Apache Spark in 10 Minutes"
https://www.kdnuggets.com/2021/05/soda-io-managing-data-quality-sql-scale.html,How to get started managing data quality with SQL and scale,"Silent data quality issues are the biggest problem facing data teams today, who are flying blind with no systems or processes in place to monitor and detect bad data before it has a downstream impact.","By Tom Baeyens, CTO and Co-Founder, Soda.io
 
Why data management?
 
In the last three years I’ve transitioned from being a software engineer to a data engineer. I fell into the area of data management when Maarten Masschelein, my fellow co-founder at Soda, and I started working together to solve the problem of data issues that are silent and undetected. Coming from a software engineering background, writing unit tests and monitoring applications in production is a given but in data, it’s quite different. Whilst most organizations are aware they should test, there is no strategy in place and they just don’t know how to start addressing the problem which leaves their systems exposed and can result in serious downstream issues for the data products they are building.
With more and more products being built using data as the core input, it’s never been more important to test and monitor the quality of data being used. And so we set about building a data observability platform that enables organizations to discover, prioritize and resolve data issues.
 
Define good data quality
 
We started with Soda SQL, made available in February 2021. It’s our first open source data testing, monitoring and profiling tool for data-intensive environments. It works with your existing data engineering workflows to create a quick and easy way to define what good quality data means to your business. This enables data engineers to define tests and protect against the silent data issues that go undetected in datasets, data lakes, and data warehouses.
 
Open source to the rescue
 
Soda SQL is an open source tool with simple Command Line Interface (CLI) and Python library to test your data through metric collection. It utilizes YAML config files as input to prepare SQL queries that run tests on tables in a database to compute a wide range of metrics and tests. It's super easy to find invalid, missing, or unexpected data. Because Soda SQL leverages --you guessed it-- SQL, the data can stay where it is and existing compute engines can be leveraged.
If tests fail, Soda SQL allows you to stop the pipeline and prevent bad data from causing damage. As metrics are computed, diagnostic information is captured as well to help with the analysis if a data issue is detected. Steps can then be taken to prioritize and collaboratively resolve issues as one data team. Soda SQL can be used manually on its own or integrated with a data orchestration tool to schedule scans and automate actions based on scan results.
You can check out the 5-minute tutorial on how to get started but here’s a quick example:

Simple metrics and tests can be configured in scan YAML configuration files. An example of the contents of such a file is as follows:



Based on these configuration files, Soda SQL will scan your data each time new data arrived like this:


 
 
Bring everyone closer to the data
 
We have just released Soda Cloud, which is a web application where the Soda SQL metrics and test results can be monitored over time. Soda Cloud creates transparency from engineers to other people in the data team. With this collaboration data teams get ahead of the silent data issues. Soda Cloud extends Soda SQL and the two work together seamlessly.
First of all Soda Cloud extends Soda SQL with a metrics database so that measurements and test results can be visualized over time. This enables monitoring change over time and anomaly detection on all of the metrics.
These visualizations and data profiles already create transparency between different people in the larger data team. All people in the data team get to see what data is actually present, what tests are performed.
But the Soda Cloud goes one step further. It enables non-technical people to build and maintain their own monitors in a simple UI with a 3-step wizard. This is important because it removes the bottleneck to monitoring the domain knowledge that Subject Matter Experts have. If they don't need to involve data engineers to get their domain logic tested, that means a lot more of that domain knowledge will be used to define what good data looks like. And as a result, a lot more bad data will be captured preventing various kinds of damages.
Soda Cloud prescriptively solves the problem of discovering the silent data issues, by giving data teams a central platform to track and score the health of data across core quality dimensions.
Data and analytics engineers are equipped with a way to test data each and every time it transforms to ensure data pipelines are reliable. Via Soda SQL, data production can be stopped and quarantined. Soda Cloud visualizes the health of data sets and acts as a communication hub for data issues.

 

 
Data consumers and producers can now easily align on what’s important, what’s expected, and what to measure so that data remains fit for purpose. We’ve also built integrations with email and Slack to ensure the right people are alerted, at the right time to diagnose, prioritize and resolve the data issues.

 

 
We’re on a mission to bring everyone closer to the data, as we believe that data quality is a team sport. Everyone who has a stake in the data (and we think that’s everyone in the business nowadays), needs to understand it, trust it, and stay on top of it.
My main responsibility at Soda is to ensure data engineers love using our products and help them solve real problems quickly. We help solve the problem with a combination of a cloud platform and a set of open source developer tools, that give data teams the configurability they need to create end-to-end observability.
Good quality data is for everyone. Access Soda SQL on GitHub and Soda Starter, our free trial, on Soda.io (extended to June 30, 2021). Our Slack Community and Docs contain best practices and helpful resources.
Get ahead of the silent data issues. Good luck!"
https://www.kdnuggets.com/2021/05/streamsets-dataops-summit-2021-cfp.html,DataOps Summit 2021 CFP Is Now Open!,Calling all Conductors of Chaos: Tell Us How You Tamed your Data at DataOps Summit 2021 CFP is open through May 31st,"Sponsored Post.

 
 
Speaking at DataOps Summit showcases your leadership in the DataOps movement. We are looking for Data Rock Stars versed in building, deploying, and operationalizing DataOps practices and procedures. The Call for Papers is open until May 31st so submit your abstract now!
 
Submit at https://www.dataopssummit-sf.com/about 
 
You may be asking, What is DataOps Summit 2021?
 
DataOps Summit 2021 is the premier professional conference dedicated to best practices, thought leadership, and technical education for the emerging domain of DataOps. To be held September 28-30, virtually, the DataOps Summit brings together enterprise data professionals, industry experts, and key technology companies in the DataOps ecosystem. 
 
The three-day conference will feature keynotes, three speaking tracks with over 30 sessions, hands-on training, and a Learning Zone, where customers, partners, and industry peers will gather to learn, educate each other, and network with other professionals amid the emerging trends in DataOps. 
 
Submit for CFP and we hope to see you there!"
https://www.kdnuggets.com/2020/09/data-scientist-not-just-tiny-hands.html,"I’m a Data Scientist, Not Just The Tiny Hands that Crunch your Data","Not everyone ""gets"" the role of the Data Scientist -- including management. While there can be frustrating aspects of being a data scientist, there are effective ways to go about fixing them.","comments
By Ahmed Besbes, AI Engineer // Blogger // Runner.

source
This comes out as a personal observation, but I’m sure that many of you will share the same feeling upon reading this post.
I’m a data scientist, and I like my job because I think it covers various interdependent domains that make it rich and stimulating. However, I sometimes have to deal with people who don’t exactly understand this role in the organization nor the field in general. This, quite frankly, is what makes things a little bit frustrating for me and also for a lot of people I know.
Before you keep reading, I should mention that I don’t aim to discourage anyone from aspiring to this role. I’m only stating some negative aspects that occur in the industry in general and the possible solutions for avoiding them.
 
Some people don’t exactly understand what you do … and don’t even bother to explain!
 

imgflip
In principle, this is fine. I don’t understand what most other people do either. What I do not get, however, is the total lack of interest and curiosity of some parties in learning about what you do while helping them. I don’t mean they should get every small algorithmic detail of your neural network, for example, but at least, they should get to know your approach, your way of solving the problem. Sometimes, it’s as if you were commissioned with the painful, dirty task that no one cares about.
Some project managers take zero interest in what you’re doing unless you’re done doing it. I think these fellas bring management to a whole new level.

Oh! You’re a data scientist? You must be really good with the numbers. Why don’t you have a look at my files and crunch the data? I hear your “python” thing can pop out the magic real quick. Here, go play with my files and come see me when you’re done.

— What to do?
To make everybody on the same page, one solution is to provide training and awareness to the teams who have no technical background. This goes through internal workshops, certifications, or MOOC subscriptions in broad technical topics such as introductory lectures to machine learning, deep learning, or NLP. When building knowledge in these areas, teammates become proactive and more engaged in the building process. Project managers also become aware of the challenges.
 
Data scientists are still considered marketing tools to pimp proposals
 

imgflip
Well, this worked quite well ten years ago when the field started to emerge, and the words Hadoop and Spark were all over the place. You could stack all the buzzwords you know and hope for a big check (and it worked!).
This isn’t 2010 anymore. Companies now pay close attention to what you’re willing to sell. They know the market, the competitors, and the challenges. They’ve scanned nearly everything thoroughly. They also know what’s feasible and what’s not. If you don’t stand out of the crowd and are not clear enough about your value proposition and the technical expertise that your data science team can bring, you’re most likely to lose the deal.
Of course, despite all of this, there is always some ballsy guy in a suit to make this kind of inspirational statement:

Let’s throw a little bit of data science here and there to beef up our pitch and make the client pay a buttload of money!

Isn’t that beautiful?
— What to do?
Don’t act as if a data scientist would completely change and disrupt your organization. The market starts to know what the limitations are. Be in line with the market.
 
You shouldn’t be the tiny hand that doesn’t take enough credit for its work.
 

imgflip
We all know this feeling, and it sucks. You bust your a** working hard, and some other guy presents your results and takes all the credit. This is common everywhere and happens even more when you work in a data science team in collaboration with business partners.
If you’re valuable to the team, your colleagues should naturally let you shine in front of the stakeholders. Your voice is then heard and engaged in the decision process.
If you’re feeling, however, that you’re treated like an interchangeable resource or put aside working in the shadow and producing numbers for those who speak, maybe it’s time to rethink your position.
— What to do?
Everyone is important when building a data product. This should not only be a statement that we tell ourselves. It must materialize in our meetings, presentations, and daily relationships.
 
Data scientists cannot produce insights upon request
 

Source dilbert.com.
Well, as tempting as it sounds, this is not as easy as we think. Just because we’re equipped with the tools doesn’t necessarily mean that you can expect immediate actionable results. This requires building knowledge about the business, forging the right intuitions, and the assumptions. This takes time, and it’s a learning process.

Let’s crunch the data and make it speak.

— What to do?
Accept the fact that a data scientist has to spend a substantial amount of time learning about the business and building his own intuitions about it. This goes through interviewing different actors in the organization, running all sorts of analyses on the data, experimenting, failing, and getting continuous constructive feedback.
If you also want to provide the best conditions for your data science teams, make sure you have, at least, clean data pipelines with clear descriptions.
 
A data scientist cannot be the go-to person for every data related issue
 

Source: medium.
There’s still a strong misconception about the role of the data scientist. Not only non-technical executives but other colleagues in tech believe that data scientists know their way around Spark, Hadoop, SQL, TensorFlow, NLP, AWS, production-level applications, docker, and more. It’s great to master these tools, but this process takes several years and a lot of experience.
If you’re a data scientist and you’re applying for a company that mentions all of these techy words in one application, double-check the company. It’s possible that it hasn’t a clear vision of its data strategy nor a clear definition of the role it’s hiring for.

We need to fix our data problems. Let’s hire a data scientist.

— What to do?
A data scientist is not always the ultimate solution to your data problems—double-check before hiring. Maybe what you need is a data analyst or a back-end developer. A data scientist is not a ninja that masters everything.
 
Pro-tip for those who want to build a strong data team
 
If you want your team to succeed in building whatever you intend to build, make sure you surround yourself with complementary skills.
At the delivery level:

Data scientiststo build sophisticated machine learning models, draw complex analyses, and formulate business needs in terms of metrics.
Data Engineers to build, among other things, robust data pipelines so that data is clean and accessible for the data science team at any time
ML / AI Engineers: this is a new role emerging in the field. I see it as a hybrid profile between a data scientist and a data engineer. In practice, it’s a data scientist who goes beyond modeling and thinks about deployment aspects. Questions he solves, for example, are: how do I make the model scalable? How do I dockerize my application properly? How do I ensure low latency at inference time? etc.
Front and back-end devs to build web applications that integrate and package the machine learning logic. They deal with code quality, robustness, security, design, stability, building APIs, etc.
A data scientist can find his way around building small web applications but remember that this not his expertise. If you want a professional mobile or web application, hire a team of developers.

At the management level:

Data science managers: these are the most technical profiles within the management team. They supervise the data science teams and make sure they take the right (modeling) decisions.
Project managers: They make sure things are on track in terms of deadlines. They spot the blocking issues and interact directly with the business or the client.
Chier Data Officer (CDO): This is the top management role. His goal is to infuse the culture within the organization, to look for the projects, and build the business.

This is based on a compilation of discussions and several feedbacks coming from friends and colleagues.
Original. Reposted with permission.
 
Bio: Ahmed Besbes is a data scientist working across many industries, such as financial services, media, and the public sector. Part of Ahmed's work includes crafting, building, and deploying AI applications to answer business issues.
Related:

If I had to start learning Data Science again, how would I do it?
What every Data Scientist needs to learn from Business Leaders
Software engineering fundamentals for Data Scientists"
https://www.kdnuggets.com/2021/09/antifragility-machine-learning.html,Antifragility and Machine Learning,"Our intuition for most products, processes, and even some models might be that they either will get worse over time, or if they fail, they will experience an cascade of more failure. But, what if we could intentionally design systems and models to only get better, even as the world around them gets worse?","comments
By Prad Upadrashta, SVP & Chief Data Science Officer (AI solutions) at Mastech InfoTrellis

A client recently asked if our entity matching algorithms are “antifragile.” This got me thinking. It is a really interesting question. Bear with me as I take you on a mind trip to explore this question and its implications. First, let’s start with a common understanding of “antifragile.” We’ve all read the google definition: When a system gains from stressors, shocks, volatility, noise, disorder, mistakes, faults, attacks, or failures, it is termed “antifragile.”
So, what does that look like? Well, at first pass, we might think it would look something like this:

Figure. A linear relationship between Entropy and Gain.
The line shows clearly that the system is gaining from disorder, i.e., +1 unit of gain for every +1 unit of disorder (for instance), but this is where things become weird. The fact is, when you are dealing with antifragile systems, they also exhibit a peculiar response function to each incremental increase in the level of disorder, where the previous gains feedback into the system in such a way as to compound the effect – this gives it a curved or convex appearance. This convexity is a critical feature of antifragile systems.
Simply, antifragility is defined as a convex response to a stressor or source of harm (for some range of variation), leading to a positive sensitivity to increase in volatility (or variability, stress, dispersion of outcomes, or uncertainty, what is grouped under the designation ""disorder cluster""). Likewise, fragility is defined as a concave sensitivity to stressors, leading to a negative sensitivity to an increase in volatility. The relation between fragility, convexity, and sensitivity to disorder is mathematical, obtained by theorem, not derived from empirical data mining or some historical narrative. It is a priori.
— Taleb, N. N., Philosophy: 'Antifragility' as a mathematical idea. Nature, 2013 Feb 28; 494 (7438), 430.
So, the line should really be curved upward as follows:

Figure. Convexity of antifragility where the accelerating curve is due to feedback.
Compound interest is a well-known concept and serves here as a useful illustration of how the simple act of re-investing returns can lead to the convexity of the return stream. When you take your prior gains (%s) and re-invest them into a consistently producing process, your prior gains also see gains, and so on ad infinitum. As these gains cumulate, we see a snowball effect over time. This results in a curved (accelerating) line, not a straight line. This curvature is convexity. Convex systems are nonlinear. I’m not suggesting here that compound interest is antifragile – rather, it is one practical example of a process in which positive feedback (reinvestment) drives the slope of the function to increase nonlinearly
In his books, Nassim Taleb gives three examples of antifragility: Airlines, Restaurants, and Silicon Valley. All three of these become stronger every time something goes wrong. If an airplane goes down, every manufacturer will take pains to make sure that the next generation of airplanes will never experience the same problem. Silicon Valley is especially interesting because they see every failure or inefficiency in the market as an opportunity, and that leads to value creation through the formation of companies that address the problem. It is important to point out that these systems were not engineered to be antifragile. This realization seems to be ex post facto. 
So, one open question is whether we can purposefully engineer systems and/or processes to make them antifragile?
Within the subset of natural systems, those that benefit from feedback turn out to be antifragile; for instance, the body’s immune system is strengthened in response to external stressors (viruses, bacteria, etc.). This, in fact, is the basis of all vaccines – the introduction of a weak stressor that triggers the immune system to adapt by learning to recognize the surface proteins that make up the viral shell – so that antibodies can be produced to attack the full-strength virus when it enters the system.
Most man-made systems and/or processes do not exhibit this sort of antifragile behavior, though at a process level, they turn out to be antifragile because humans have a natural inclination towards process improvement by studying past failures and adapting themselves accordingly. So, we impose our own antifragile tendencies on the systems we design because we simply can’t leave things alone. The vast majority of the natural physical world exhibits a tendency to decay over time. This is particularly true of the failure modes observed in complex systems (where system failures rise exponentially over time). So, most real systems exhibit fragility, which looks more like this:

Figure. A failure curve that points downward. As one thing fails, it creates the conditions for something else to fail, leading to negative convexity.
In fragile systems, each failure leads to successively more failures. So, failures compounding on top of failures leads to a rapidly decaying system; again, we note that explicitly that it is not a linearly decaying scenario – intuitively, we know that once something breaks, it sets in motion the tendency for other things to break (especially where there are strong direct or mechanical dependencies). Every failure makes the next failure more likely because failures are not independent of one another – in mechanical systems, in particular, failures tend to be highly dependent. This gives rise to the “right edge” of the so-called “bathtub curve” that is famous in the reliability world. One of the most frequently asked questions in the reliability world is: How likely is it that component B will fail, given that component A has already failed?

Figure. A “Bathtub Curve” model is used for modeling things like machine component failures. On the far right, you see an accelerating curve that represents the accelerating failure rate of a machine that is in its “wear out” phase.
We can develop an intuition for the behavior of “antifragile” systems by studying and contrasting these extreme cases, i.e., the edges of the “known,” if you will. Studying the extremes of any problem can help you identify the “boundaries” of a problem. By bounding a problem between two lines, we can start to understand the characteristic behaviors we are likely to expect in-between, all the while implicitly making a few mathematical assumptions about continuity and regularity, which we won’t go into here. The point is, we can interpolate between the extremes to understand the characteristics of the system under a variety of different operating parameters.
Original. Reposted with permission.
 
Bio: Prad Upadrashta has over 20 years of experience, culminating in the role of Chief Data Science Officer at Mastech InfoTrellis. His focus areas are Artificial Intelligence, Machine/Deep Learning, Blockchain, IIoT/IoT, and Industry 4.0.
Related:

What is Noise?
10 Must-Know Statistical Concepts for Data Scientists
How To Overcome The Fear of Math and Learn Math For Data Science"
https://www.kdnuggets.com/2021/01/data-science-learning-journey.html,My Data Science Learning Journey So Far,"These are some obstacles the author faced in their data science learning journey in the past year, including how much time it took to overcome each obstacle and what it has taught the author.","comments
By Arnuld on Data, freelance Data Scientist
Eric Weber (yes, that nice-looking guy with a lovely dog) wrote a post on LinkedIn recently about 10 things he wished he had done less when he started his data science career. This post is my journey through those 10 points. First, you should go ahead and read his post. Here is a screenshot:


Original Post on LinkedIn by Eric Weber

 
First things first, this is not going to be a “content” post.
There are so many articles and blog-posts on that already, so check them out. Here we will talk about your focus and direction when it comes to your desire to become a data scientist and get noticed by the industry.
 
1) Thinking I needed to learn everything
 
Yeah, this one takes lots of your time and energy. This obstacle is one you should deal with right away. I struggled with it in the beginning but in a few months, it died down. I attribute this breakthrough to my daily reading habit.
I keep on reading LinkedIn posts (especially from Eric Weber himself). Also, I read a lot on Towards Data Science, Medium, KDnuggets, and individual blogs from different data scientists and machine learning engineers for an hour or two or more every single day. This has taught me the importance of data science when it comes to industrial work: how much value you are adding to an organization with your skill set. You define value by building something you have an interest in or by building something to solve a problem. You choose what to learn by answering this question and it will give you an idea of what to learn and what not to.
It took me several months to realize this (I guess 6 months). I will add these months together as we progress point by point to see how much time we could have saved.


Image by Andrew Martin from Pixabay

 
2) Prepping for interview trivia.
 
Yes, this is another struggle, primarily because of several reasons:

There is no one agreeable definition of what a data scientist is. Only a vague idea on his job responsibilities and how those responsibilities are different from a data analyst or a machine learning engineer?
Then there are confusing job descriptions. Since there is no agreeable definition of data scientist, you will see descriptions who want you to be a master of everything: machine learning, software engineering, Python, R, years of Statistics, Calculus, Linear Algebra, Big-O, and whatnot. Looking at the job description, you feel like you need to be 50+ to apply for the jobs.

Don’t fall for this. Don’t take a job description to your heart. Mostly “interview trivia” is a combination of this newness of data science along with a poor communication channel between talent acquisition, data science, and software engineering teams in an organization. Rather than feeling overwhelmed at this, you need to focus on how to crack it.
One way to crack this is by looking at reality. If you know any real-life data scientists, data analysts, and machine learning engineers (offline, in the physical world), it will be a great idea to talk to them about their work. If you don’t know anyone then you can always check blogs and articles.
I don’t know any professional in this field offline. So I learned by reading blogs and articles. What I learned is companies get many people for interviews, all of the kind who “know” stuff but very few who have “built” stuff. So focus on building stuff than mere learning and education (e.g. deployment and production are two major things). It took me 5–6 months to realize this.
6 + 6 = 12 months so far
 
3) Trying to emulate someone else’s path
 
Aha, this is my favorite :-) because this is where I had wasted most of my time:

Tetiana Ivanova landed a job in 6 months
Kelly Peng landed a job in one year after she quit her data analyst job
Natassha Selvaraj landed a job and she is studying in college
Mikko Koskinen did not even plan to become a data scientist
Thomas Hepner felt lost at at anything other than Titanic dataset and a year later he landed as a data scientist in the industry



Photo by Edward Jenner from Pexels

 
Look at my profile, I have 4.5 years of experience in software development (C language) and been doing data science for 8 months now and still nowhere near answering this question:

What’s your favorite machine learning algorithm and why?

 
Yes, I agree my case looks like the worst case of Big-O: O(n^n)
I have read hundreds and thousands (no, I am not exaggerating) of blog-posts and articles of the people who have landed data science jobs and changed industries. I traced and emulated their data science journeys into my life, from their thinking-patterns to the choice of their courses, even their choice of certain chapters in certain books, like a perfect carbon-copy. And I still failed at answering the question above because I don’t even know why I will like one machine learning algorithm over the other. After all, I am just mindlessly chewing all the models in the name of “becoming like them”.
Two days ago I gave it up and decided to follow what I think I should do. (Surprisingly, I came across Eric’s post today. It is as if Universe is trying to tell me I am on the right path, a path that belongs to me)
I think each of us have to personalize our journeys. Our environment, our talent, our experience, our attitude, our work ethic, our backgrounds, and our learning capacities, all are different and unique. That is why maybe tracing someone else’s path never works.
So I decided I will experiment and carve my own path to become a data scientist. This is not to say that I will stop reading other people’s journeys, I will still read but instead of following them blindly and trying to copy it into my life, I will use them as a compass, as a guiding mechanism. This has cost me 8 months. Better late than never though.
6 + 6 + 8 = 20 months


Image by Gerd Altmann from Pixabay

 
4) Focusing on perfect solutions.
 
My computer programming experience took care of this. I spent half of a decade doing programming in the industry, writing code to generate money for my employers, that already taught me “done” is better than “perfect”. Finding a problem someone is facing and building a solution is actually the only important thing that matters. Mere learning and education don’t.
6 + 6 + 8 + 0 = 20 months
 
5) Learning advanced stats I rarely used
 
Back in 2018, I spent a lot of time learning Mathematics and Statistics for data science. I spent 4 months studying:

Algebra I and II at Khan Academy
College Level Algebra and Problem Solving from Arizona State University at edX
MIT Big Picture Calculus from YouTube
Calculus Made Easy by Silvanus P. Thompson. Available for free from Project Gutenberg
Calculus 1A: Differentiation from MIT at edX.
Limits and Integral Calculus from Calculus-1 at Khan Academy.
Reading different books on Statistics to get a statistical mindset

What a mistake it was :-( . From what I know today, all I needed was this:

Basics of Statistics. Not Statistics per se but only the topics specifically necessary for Machine Learning and Data Analysis
Basics of Bayes Theorem
Basics of Linear Algebra (only a few small things like matrix multiplication and transposing etc )
Basics of Big-O Notation (Check out Interview Cake’s Explanation)

Yes, nothing fancy but only the basics. All the fancy stuff you can do after you land a job. Till then you use Python or R Libraries. Instead of trying to learn Mathematical formulas just like in school or college, try to learn how to use it using Library calls in Python e.g. calculating t-test using Scipy, and learn the math needed to understand it:
3.1. Statistics in Python - Scipy lecture notes
A simple linear regression Given two set of observations, x and y, we want to test the hypothesis that y is a linear...
 
Well, there went 8–10 months:
6 + 6 + 8 + 0 + 10 = 30 months


Photo by Vlad Dediu from Pexels

 
6) Thinking the R vs. Python debate required picking just 1.
 
I struggled with this one:

Started with R for Data Science by Hadley Wickham. Read a few chapters and then gave up because I read Python is gaining ground in the industrial world.
I started with Python and tried a few books and then I came back to R because ggplot looked better than matplotlib.
Then I went back to Python because it had a more software Engineering feel to it.
Went back to R because tidyverse as a package looked much more mature at data analysis and visualization than Python’s tools.

This problem went away when I got a take-home assignment from a company who approached me for R related work. After using both R and Python for take-home assignment work, I never wanted to touch R again. From my experience Python suits better for software engineering practices and software engineering practices are definitely needed when it comes to writing data science code for real-life industrial work. It is almost the same as when you are doing software development. I went fully Python after that. Personally, If I ever have to use another language, I will use Julia instead. Around 4–6 months on this.
6 + 6 + 8 + 0 + 10 + 4 = 34 months
 
7) Spending lots of time thinking about unstructured data
 
This mistake I did after the “the math mistake”. I spent months contemplating SQL vs NoSQL. We look at something and we think of it from our viewpoint and think this is what it means. E.g we all know this is the age of data and millions and millions of megabytes of data is being generated each day. Most of it is unstructured. I guessed I should learn NoSQL. But then almost all of the job descriptions mention only SQL. Then I will think of doing SQL.


Photo by Mika Baumeister on Unsplash

 
I learned neither SQL nor NoSQL. This is how being two-minds about a thing kills months of your time.
Instead of interpreting things in my way, I started looking at the people who landed data science jobs and what they learned. All of them had listed SQL as a skill. So I switched to SQL. A good place to start is SQLBolt.
I won’t consider any time wastage here because even though I did not learn anything, I used that time to learn other stuff. So, the equation so far is:
6 + 6 + 8 + 0 + 10 + 4 + 0 = 34 months
 
8) Thinking about the tech, not the business
 
This is one area where you need a serious change in mindset and I needed such change too. My computer programming background makes me a 100% tech guy who really does not know how to be more than a team-worker. Contributing to the team is where my social and my communication skills ended.
I never knew this in beginning but thanks to my reading habit, I came across so many characteristics of data science that put it at odds with other tech jobs. One way I overcome this is by talking about Big Data with people I know or I meet. By explaining data science, machine learning concepts to my friends and other people. But because my freelance work and data science learning require me to spend a lot of time in front of my computer, I don’t get the opportunity to exercise this method much.


Image by Lorenzo Cafaro from Pixabay

 
Data science is not just programming, data science is not just web-development, it is not just about analyzing data and building models. This is half of the story. Another half of data science is being able to communicate to not so tech-savvy people. Business stakeholders, decision-makers in management, and clients are three different types of non-tech people you are going to deal with. So collaborating with people is going to be a big pain if we think of it as “another tech job”. There is an excellent book on communicating data insights titled “Storytelling With Data” by Cole Nussbaumer Knaflic. It is kind of a must-read.
There is another side to this. Business Problems. The model you build, the comparisons you did, and the accuracy you achieved, how it is benefiting the business? You see, a data scientist’s job has no meaning if he can’t bring some profit or benefit or some value addition to the business. This is a hard thing to get hold of and become good at if you come from a tech-background like mine. What the tech-mentality does, in this case, is to make your mind focus only on building the model and analyzing data because it is what we do. We do not have a business context.
I don’t have a great solution for this because never had any personal experience with it. So take my advice with a grain of salt here. Search yourself too. I could only read blogs, posts and articles to understand what to do. I don’t know any product manager either (I have met one or two managers in IT service but I don’t know if that qualifies). The only method I have come across to solve this is two-fold:

Read about case-studies, product case-studies. This is what a product manager does. So if you know any product manager (or even a project manager) you should talk to them about how their product/project brought value to the company.
Read books like Cracking the PM Interview by Gayle Laakmann McDowell and Jackie (Bodine) Bavaro

Not understanding this makes you work on your tech skills long and hard if you are a programmer or a software developer. Wastage of 6 months:
6 + 6 + 8 + 0 + 10 + 4 + 0 + 6 = 40 months
 
9) Trying to keep up with all the papers
 
Another pitfall you need to avoid. I got stuck in this for a while. I want to implement a paper or two myself but now the first focus of mine is always on “building something”. Learn as less as you need to start working on to build something.
Yes, all those papers look really, really impressive, and beautiful. And papers are mostly about academics. You are trying to land a job in the industry. Academics and industry do not match, with two possible exceptions:

You are looking for a research position within the industry. In this case, your portfolio will be limited to only 10–20% of the employers.
You want to work for the big 4 a.k.a Facebook, Amazon, Google, and Microsoft.

Except for the above, I don’t see any point in drifting from my focus of landing a data scientist position at a good tier I or II company. Don’t take me wrong, I love to do research. In fact, back in college, I wanted to do a Ph.D. in microkernel research. Research work takes a hell of lot of time and energy. I think a better way to live is to find balance in your career: a balance between your interests and the market/industry needs. Avoid falling on either side.


Photo by Furkan from Pexels

 
Instead of keeping up with all the papers, a better way to balance your learning is:

Learn the basics of data cleaning using Pandas (Kaggle datasets have done the 90% of work for you. In real-life, you gotta do all the cleaning. Learn to scrape some data and clean it)
Learn the basics of machine learning modeling and why we choose one model over the other. What kinds of model fit what kind of domain problems e.g. healthcare vs finances
Learn how to deploy a model into production (you will know a bit of how the real work feels like when you will use Strealmlit, Heroku, and Voila. I have implemented the bear-detection model using Voila here. )

6 + 6 + 8 + 0 + 10 + 4 + 0 + 6 + 10= 50 months
 
10) Believing there was only one way to do something
 
This one is a biggie. I think I am struggling with this for life. Some people have it and some people don’t. I am inclined to say that maybe smart people don’t have this problem (the smart ones I have met or read about, they don’t). People like me spend a lifetime trying to beat it. It is a jail, trust me. It is quite frustrating to live with a mindset of “only one way to do something”. Ideas don’t have any limits if you look at real-life stories.


Photo by Timo Volz from Pexels

 
This is more of a personal-development obstacle than a technical one because no matter which field you will work in, this one will show up there, it absolutely has nothing to do with the tech. I am still trying to work on it. A solution I have found so far is when I can’t find my way around a problem then I will get off the machine and go for a walk if it is evening or read a completely unrelated book if it is not evening (some non-fiction e.g.) or go on a motorcycle ride and completely forget about the problem. Then I will come back later and try to learn the same thing from a different article or blog post while not referring to the original point where I was stuck. Just a fresh new perspective on the same problem from someone else.
I can’t put any time-limit on this. I have struggled this for all of my life:
6 + 6 + 8 + 0 + 10 + 4 + 0 + 6 + 10 + Life = 50 + Life
So, I wasted almost 50 months?
Not really.
All of these points overlap with each other when it comes to where I wasted time. It is actually 12 months. Dec 2019 to Nov 2020. For a few months, in the beginning, I did not even know what I needed to do. Things started making sense only in March 2020 this year. I think I could have saved 4–6 months if things were clearer to me but this is just a wild guess, some really smart people have told me: it takes whatever time it takes to break down the obstacles. Let me re-iterate:
Each of us has a personal data science journey. Our environment, our talent, our experience, our attitude, our work ethic, our backgrounds, and our learning capacities, all are different and unique. That is why maybe tracing someone else’s path never works. That is why you need to keep on pushing yourself to learn what you can, to keep yourself informed of what is going on in the industry and keep on correcting your path (just like apps like maps on our smartphones keep on correcting us and showing the way)
 
BONUS — Your Mental Outlook
 
I was trying to learn neural networks before I could comprehend what kind of problems logistic regression fits better than linear regression. I was doing deep learning before machine learning made any sense. In my case it was because of:

Media-hype about AI and deep learning
My focus on building something great and truly impressive
The assumption that everyone is doing it and I need to do better than them if I want to land a job. After all, the market is so competitive.
Focusing on the big 4
I have an interest in healthcare data and Practical Deep Learning for Coders has chapters on medical imaging diagnosis. You can see one example here.

Deep learning and AI are in media everywhere. We tend to think we need to be better than everyone else and others are already writing highly mathematical blog posts with their flashy formulas along with lots of code. Don’t believe me? Check this out then. Who will approach us when such people have already mastered deep learning and data science?
Yeah, it is so common that they got a name for it. It is called “Imposter Syndrome”. Go read about it a bit. I thought I was the only one suffering from it. But then I realized it is so common. Yes, the market is competitive and because of the current pandemic, many have lost jobs. I have seen posts on LinkedIn where several data scientists and machine learning engineers have lost jobs. I have seen them even literally begging to “like and share” that they are looking for a job. It is heartbreaking to see that. Everyone deserves a good life.


Photo by Engin Akyurt from Pexels

 
Let’s look at the positive side, this pandemic has disrupted the world, it has brought many businesses to a halt while some businesses have their client number shot sky-high (podcast and video conferencing services for one). In such disruptive times, we need to be more resilient to pain and suffering and find ways to strengthen our resolve. I believe it is not by chance that we were born in a certain year and that is how we got in the middle of this pandemic. I think we were supposed to learn from it, we are supposed to make a better life out of these times. I wish you good luck in your data science learning journey and I hope we keep on learning from each other to make ourselves better.
 
Bio: Arnuld is an industrial software developer with 5 years of experience working in C, C++, Linux, and UNIX. After transitioning to Data Science and working as data science content writer for over a year, Arnuld currently works as a freelance data scientist.
Original. Reposted with permission.
Related:

My Data Science Online Learning Journey on Coursera
Learn Data Science for free in 2021
A Journey from Software to Machine Learning Engineer"
https://www.kdnuggets.com/2021/07/top-stories-2021-jun.html,Top June Stories: 5 Tasks To Automate With Python; Data Scientists Will be Extinct in 10 Years,5 Tasks To Automate With Python; Data Scientists Will be Extinct in 10 Years: How to Generate Automated PDF Documents with Python; How I Doubled My Income with Data Science and Machine Learning.,"By Gregory Piatetsky, KDnuggets.  

Here are the most popular June 2021 stories on KDnuggets. As you see, a lot of attention was on several blogs on the topic of demand for Data Scientists in 10 years, which was also a topic for KDnuggets poll in June: Relax! Data Scientists will not go extinct in 10 years.

Stories in green font are also the winners of KDnuggets Top Blog Rewards for June.

Most Viewed - Platinum Badge (>32,000 UPV)

 5 Tasks To Automate With Python, by Dylan Roy (*)
 Data Scientists Will be Extinct in 10 Years, by Mikhail Mew (*)
 How to Generate Automated PDF Documents with Python, by Mohammad Khorasani (*)


Most Viewed - Gold Badge (>16,000 UPV)

 How I Doubled My Income with Data Science and Machine Learning, by Terence Shin
 Top 10 Data Science Projects for Beginners, by Natassha Selvaraj (*)
 Pandas vs SQL: When Data Scientists Should Use Each Tool, by Matthew Przybyla (*)
 Five types of thinking for a high performing data scientist, by Anand Rao
 Will There Be a Shortage of Data Science Jobs in the Next 5 Years?, by Pranjal Saxena (*)
 What will the demand for Data Scientists be in 10 years? Will Data Scientists be extinct?, by Matthew Mayo (*)


Most Viewed - Silver Badge (> 8,000 UPV)

 Get Interactive Plots Directly With Pandas, by Parul Pandey
 How to Make Python Code Run Incredibly Fast, by Pralabh Saxena (*)
 How to Land a Data Analytics Job in 6 Months, by Natassha Selvaraj (*)
 Add A New Dimension To Your Photos Using Python, by Dylan Roy (*)
 Managing Your Reusable Python Code as a Data Scientist, by Matthew Mayo




Most Shared - Gold Badge (>500 shares)

 Data Scientists Will be Extinct in 10 Years, by Mikhail Mew
 Five types of thinking for a high performing data scientist, by Anand Rao
 5 Tasks To Automate With Python, by Dylan Roy
 Analytics Engineering Everywhere, by Jason Ganz


Most Shared - Silver Badge (>300 shares)

 How I Doubled My Income with Data Science and Machine Learning, by Terence Shin
 Pandas vs SQL: When Data Scientists Should Use Each Tool, by Matthew Przybyla
 Top 10 Data Science Projects for Beginners, by Natassha Selvaraj
 How to Generate Automated PDF Documents with Python, by Mohammad Khorasani


(*) indicates that badge added or upgraded based on these monthly results.

Most Shareable (Viral) Blogs
Among the top blogs, here are the blogs with the highest ratio of shares/unique views, which suggests that people who read it really liked it. 

 10 Mistakes You Should Avoid as a Data Science Beginner, by Isabelle Flueckiger
 Analytics Engineering Everywhere, by Jason Ganz
  The Essential Guide to Transformers, the Key to Modern SOTA AI, by Matthew Mayo
 This Data Visualization is the First Step for Effective Feature Selection, by Benjamin Obi Tayo
  Major changes: Where Analytics, Data Science, Machine Learning were applied in 2020/21, by Gregory Piatetsky"
https://www.kdnuggets.com/2021/08/querying-granular-demographic-dataset.html,Querying the Most Granular Demographics Dataset,"Having access to broad and detailed population data can potentially offer enormous value to any organization looking to interact with specific demographics. However, access alone is not sufficient without being able to leverage advanced techniques to explore and visualize the data.","comments
By Matti Grotheer, startup enthusiast and Co-Founder of Kuwala.
There are a plethora of use cases that require detailed population data. For example, having a detailed breakdown of the demographic structure is a significant factor in predicting real estate prices or finding the perfect retail outlet location. Also, humanitarian projects such as vaccination campaigns or rural electrification plans highly depend on good population data.
It is very challenging to find high-quality and up-to-date data on a global scale for these use cases. Usually, census data is published every four years, which makes those datasets outdated quickly. Arguably the best datasets out there for population densities and demographics are published by Facebook under their Data for Good initiative. They combine official census data with their internal data and leverage machine learning algorithms for image recognition to determine buildings' location and type.

Facebook Data for Good and Kuwala (2021).
Using those different sources can give a detailed statistical breakdown of demographic groups in 1-arcsecond blocks, a resolution of approximately 30 meters. Each square contains statistical values for the following demographic groups:

Total
Female
Male
Children under 5
Youth 15 - 24
Elderly 60 plus
Women of reproductive age 15 – 49

Facebook delivers for each country a file per demographic group, either as a GeoTIFF or CSV. The CSV contains the latitude and longitude of the cell and the respective population value.

The files are stored per country and key metric on 1-arcsecond blocks. This results in gigabytes and millions of rows of data for a single country. If you want to prototype or visualize the data for a single city, you need to browse through the endless files and parse the information.
That is why we created an open-source wrapper that exposes the data through a package. You can directly download the data for entire countries over a CLI. We preprocess the data to make it easily queryable. For that, we are leveraging the power of Uber's H3 spatial indexing.
Thanks to the H3 indexing, it is easy to build queries on top of the database. Using either H3 cells or coordinate pairs, you can retrieve the population based on a point, a given radius, or polygon. Furthermore, it is straightforward to aggregate the population on a zip code level, for example.

Uber H3 and Kuwala (2021).
The data integration follows a sequential process. The CSV files for countries and demographic characteristics are automatically loaded and linked by Spark. The data is efficiently stored in a Parquet file. The Parquet file is then automatically loaded to a Neo4j database (graph database). Then, using Cypher, queries can be made for specific polygons, points with a given radius, and different aggregations using H3. For a medium-sized country like Germany (approximately 7-8 GB), the data is processed locally in less than 30 minutes and ready for your spatial analysis.
Neo4j was chosen as the database because it can intuitively connect other pipelines of the Kuwala ecosystem. In a similar process, POI information from OpenStreetMap can be loaded and directly related to the demographics data. Many more geo-related sources, such as Google Trends, location-based urban events, or social media data, will follow as connectors to enable you with fast and holistic queries on comparable worldwide datasets.

For quick data exploration and visualization, you can directly create datasets compatible with Kepler.gl or Unfolded.ai to make beautiful maps. We published an example map for Malta. It is directly visible where the highly populated regions are and where the heart of the city is.

By having Facebook's population data now directly queryable, it is much faster to create predictive models or visualizations so data teams can spend time on the value-adding tasks. That is also the main reason why we are building an open-source community for third-party data integration with Kuwala. So if you want to get your hands on more connectors like these, star us on GitHub and join our Slack community.
But our open-source project does not stop here. Our big goal is to facilitate access to external data sources, ensure data quality, and help data scientists quickly develop features that they can incorporate into their modeling. For example, we are planning a Jupyter notebook that can be used to manipulate and observe the data swiftly. So stay tuned for that!
 
Related:

The secret to analysing large, complex datasets quickly and productively?
3 Key Data Science Questions to Ask Your Big Data
How Visualization is Transforming Exploratory Data Analysis"
https://www.kdnuggets.com/2020/10/understanding-transformers-data-science-way.html,"Understanding Transformers, the Data Science Way","Read this accessible and conversational article about understanding transformers, the data science way — by asking a lot of questions that is.","By Rahul Agarwal, MLE @ FB | Ex-Walmart DS | MLWhiz.
comments
Transformers have become the defacto standard for any NLP tasks nowadays. Not only that, but they are now also being used in Computer Vision and to generate music. I am sure you would all have heard about the GPT3 Transformer and its applications thereof. But all these things aside, they are still hard to understand as ever.
It has taken me multiple readings through the Google research paper that first introduced transformers along with just so many blog posts to really understand how a transformer works.
So, I thought of putting the whole idea down in as simple words as possible along with some very basic Math and some puns as I am a proponent of having some fun while learning. I will try to keep both the jargon and the technicality to a minimum, yet it is such a topic that I could only do so much. And my goal is to make the reader understand even the goriest details of Transformer by the end of this post.
Also, this is officially my longest post both in terms of time taken to write it as well as the length of the post. Hence, I will advise you to Grab A Coffee. ☕️
So, here goes — This post will be a highly conversational one and it is about “Decoding The Transformer”.
Q: So, Why should I even understand Transformer?
In the past, the LSTM and GRU architecture(as explained here in my past post on NLP) along with attention mechanism used to be the State of the Art Approach for Language modeling problems (put very simply, predict the next word) and Translation systems. But, the main problem with these architectures is that they are recurrent in nature, and the runtime increases as the sequence length increases. That is, these architectures take a sentence and process each word in a sequential way, and hence with the increase in sentence length the whole runtime increases.
Transformer, a model architecture first explained in the paper Attention is all you need, lets go of this recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. And that makes it FAST.


Source

 
This is the picture of the full transformer as taken from the paper. And, it surely is intimidating. So, I will aim to demystify it in this post by going through each individual piece. So read ahead.
 
The Big Picture
 
Q: That sounds interesting. So, what does a transformer do exactly?
Essentially, a transformer can perform almost any NLP task. It can be used for language modeling, Translation, or Classification as required, and it does it fast by removing the sequential nature of the problem. So, the transformer in a machine translation application would convert one language to another, or for a classification problem will provide the class probability using an appropriate output layer.
It all will depend on the final outputs layer for the network but, the Transformer basic structure will remain quite the same for any task. For this particular post, I will be continuing with the machine translation example.
So from a very high place, this is how the transformer looks for a translation task. It takes as input an English sentence and returns a German sentence.


Transformer for Translation (Image by author)

 
The Building Blocks
 
Q: That was too basic. 😎 Can you expand on it?
Okay, just remember in the end, you asked for it. Let’s go a little deeper and try to understand what a transformer is composed of.
So, a transformer is essentially composed of a stack of encoder and decoder layers. The role of an encoder layer is to encode the English sentence into a numerical form using the attention mechanism, while the decoder aims to use the encoded information from the encoder layers to give the German translation for the particular English sentence.
In the figure below, the transformer is given as input an English sentence, which gets encoded using 6 encoder layers. The output from the final encoder layer then goes to each decoder layer to translate English to German.


Data Flow in a Transformer (Image by author)

 
1. Encoder Architecture
 
Q: That’s alright but, how does an encoder stack encode an English sentence exactly?
Patience, I am getting to it. So, as I said the encoder stack contains six encoder layers on top of each other(As given in the paper, but the future versions of transformers use even more layers). And each encoder in the stack has essentially two main layers:

a multi-head self-attention Layer, and
a position-wise fully connected feed-forward network



Very basic encoder Layer (Image by author)

 
They are a mouthful. Right? Don’t lose me yet as I will explain both of them in the coming sections. Right now, just remember that the encoder layer incorporates attention and a position-wise feed-forward network.
Q: But, how does this layer expect its inputs to be?
This layer expects its inputs to be of the shape SxD (as shown in the figure below) where S is the source sentence(English Sentence) length, and D is the dimension of the embedding whose weights can be trained with the network. In this post, we will be using D as 512 by default throughout. While S will be the maximum length of sentence in a batch. So it normally changes with batches.


Encoder — Input and Output shapes are the same (Image by author)

 
And what about the outputs of this layer? Remember that the encoder layers are stacked on top of each other. So, we want to be able to have an output of the same dimension as the input so that the output can flow easily into the next encoder. So the output is also of the shape, SxD.
Q: Enough about the sizes talk, I understand what goes in and what goes out but what actually happens in the Encoder layer?
Okay, let’s go through the attention layer and the feedforward layer one by one:
 
A) Self-attention layer
 


How Self-Attention Works (Image by author)

 
The above figure must look daunting but it is easy to understand. So just stay with me here.
Deep Learning is essentially nothing but a lot of matrix calculations and what we are essentially doing in this layer is a lot of matrix calculations intelligently. The self-attention layer initializes with 3 weight matrices — Query(W_q), Key(W_k), and Value(W_v). Each of these matrices has a size of (Dxd) where d is taken as 64 in the paper. The weights for these matrices will be trained when we train the model.
In the first calculation(Calc 1 in the figure), we create matrices Q, K, and V by multiplying the input with the respective Query, Key, and Value matrix.
Till now it is trivial and shouldn’t make any sense, but it is at the second calculation where it gets interesting. Let’s try to understand the output of the softmax function. We start by multiplying the Q and Kᵀ matrix to get a matrix of size (SxS) and divide it by the scalar √d. We then take a softmax to make the rows sum to one.
Intuitively, we can think of the resultant SxS matrix as the contribution of each word in another word. For example, it might look like this:


Softmax(QxKt/sqrt(d)) (Image by author)

 
As you can see the diagonal entries are big. This is because the word contribution to itself is high. That is reasonable. But we can see here that the word “quick” devolves into “quick” and “fox” and the word “brown” also devolves into “brown” and “fox”. That intuitively helps us to say that both the words — “quick” and “brown” each refers to the “fox”.
Once we have this SxS matrix with contributions we multiply this matrix by the Value matrix(Sxd) of the sentence and it gives us back a matrix of shape Sxd(4x64). So, what the operation actually does is that it replaces the embedding vector of a word like “quick” with say .75 x (quick embedding) and .2x(fox embedding) and thus now the resultant output for the word “quick” has attention embedded in itself.
Note that the output of this layer has the dimension (Sxd) and before we get done with the whole encoder we need to change it back to D=512 as we need the output of this encoder as the input of another encoder.
Q: But, you called this layer Multi-head self-attention Layer. What is the multi-head?
Okay, my bad but in my defense, I was just getting to that.
It’s called a multi-head because we use many such self-attention layers in parallel. That is, we have many self-attention layers stacked on top of each other. The number of attention layers,h, is kept as 8 in the paper. So the input X goes through many self-attention layers parallelly, each of which gives a z matrix of shape (Sxd) = 4x64. We concatenate these 8(h) matrices and again apply a final output linear layer, Wo, of size DxD.
What size do we get? For the concatenate operation we get a size of SxD(4x(64x8) = 4x512). And multiplying this output by Wo, we get the final output Z with the shape of SxD(4x512) as desired.
Also, note the relation between h,d, and D i.e. h x d = D


The Full multi-headed self-attention Layer (Image by author)

 
Thus, we finally get the output Z of shape 4x512 as intended. But before it goes into another encoder we pass it through a Feed-Forward Network.
 
B) Position-wise feed-forward network
 
Once we understand the multi-headed attention layer, the Feed-forward network is actually pretty easy to understand. It is just a combination of various linear and dropout layers on the output Z. Consequentially, it is again just a lot of Matrix multiplication here.


Each word goes into the feed-forward network. (Image by author)

 
The feed-forward network applies itself to each position in the output Z parallelly(Each position can be thought of as a word) and hence the name Position-wise feed-forward network. The feed-forward network also shares weight, so that the length of the source sentence doesn’t matter(Also, if it didn’t share weights, we would have to initialize a lot of such networks based on max source sentence length and that is not feasible)


It is actually just a linear layer that gets applied to each position(or word) (Image by author)

 
With this, we near an okayish understanding of the encoder part of the Transformer.
Q: Hey, I was just going through the picture in the paper, and the encoder stack has something called “positional encoding” and “Add & Norm” also. What are these?


I am back again here so you don’t have to scroll Source

 
Okay, These two concepts are pretty essential to this particular architecture. And I am glad you asked this one. So, we will discuss these steps before moving further to the decoder stack.
 
C. Positional Encodings
 
Since, our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of both the encoder and decoder stacks(as we will see later). The positional encodings need to have the same dimension, D as the embeddings have so that the two can be summed.


Add a static positional pattern to X (Image by author)

 
In the paper, the authors used sine and cosine functions to create positional embeddings for different positions.


Source

 
This particular mathematical thing actually generates a 2d matrix which is added to the embedding vector that goes into the first encoder step.
Put simply, it’s just a constant matrix that we add to the sentence so that the network could get the position of the word.
  

Positional encoding matrix for the first 300 and 3000 positions (Image by author)

 
Above is the heatmap of the position encoding matrix that we will add to the input that is to be given to the first encoder. I am showing the heatmap for the first 300 positions and the first 3000 positions. We can see that there is a distinct pattern that we provide to our Transformer to understand the position of each word. And since we are using a function comprised of sin and cos, we are able to embed positional embeddings for very high positions also pretty well as we can see in the second picture.
Interesting Fact: The authors also let the Transformer learn these encodings too and didn’t see any difference in performance as such. So, they went with the above idea as it doesn’t depend on sentence length and so even if the test sentence is bigger than train samples, we would be fine.
 
D. Add and Normalize
 
Another thing, that I didn’t mention for the sake of simplicity while explaining the encoder is that the encoder(the decoder architecture too) architecture has skip level residual connections(something akin to resnet50) also. So, the exact encoder architecture in the paper looks like below. Simply put, it helps traverse information for a much greater length in a Deep Neural Network. This can be thought of as akin(intuitively) to information passing in an organization where you have access to your manager as well as to your manager’s manager.


The Skip level connections help information flow in the network (Image by author)

 
2. Decoder Architecture
 
Q: Okay, so till now we have learned that an encoder takes an input sentence and encodes its information in a matrix of size SxD(4x512). That’s all great but how does it help the decoder decode it to German?
Good things come to those who wait. So, before understanding how the decoder does that, let us understand the decoder stack.
The decoder stack contains 6 decoder layers in a stack (As given in the paper again) and each decoder in the stack is comprised of these main three layers:

Masked multi-head self-attention Layer
multi-head self-attention Layer, and
a position-wise fully connected feed-forward network

It also has the same positional encoding as well as the skip level connection as well. We already know how the multi-head attention and feed-forward network layers work, so we will get straight into what is different in the decoder as compared to the encoder.


Decoder Architecture (Image by author)

 
Q: Wait, but do I see the output we need flowing into the decoder as input? What? Why? 😖
I am noticing that you are getting pretty good at asking questions. And that is a great question, something I even though myself a lot of times, and something that I hope will get much clearer by the time you reach the end of this post.
But to give an intuition, we can think of a transformer as a conditional language model in this case. A model that predicts the next word given an input word and an English sentence on which to condition upon or base its prediction on.
Such models are inherently sequential as in how would you train such a model? You start by giving the start token(<s>) and the model predicts the first word conditioned on the English sentence. You change the weights based on if the prediction is right or wrong. Then you give the start token and the first word (<s> der) and the model predicts the second word. You change weights again. And so on.
The transformer decoder learns just like that but the beauty is that it doesn’t do that in a sequential manner. It uses masking to do this calculation and thus takes the whole output sentence (although shifted right by adding a <s> token to the front) while training. Also, please note that at prediction time we won’t give the output to the network
Q: But, how does this masking exactly work?
 
A) Masked Multi-Head Self Attention Layer
 
It works, as usual, you wear it I mean 😷. Kidding aside, as you can see that this time we have a Masked Multi-Head attention Layer in our decoder. This means that we will mask our shifted output (that is the input to the decoder) in a way that the network is never able to see the subsequent words since otherwise, it can easily copy that word while training.
So, how does the mask exactly work in the masked attention layer? If you remember, in the attention layer we multiplied the query(Q) and keys(K) and divided them by sqrt(d) before taking the softmax.
In a masked attention layer, though, we add the resultant matrix before the softmax(which will be of shape (TxT)) to a masking matrix.
So, In a masked layer, the function changes from:


(Image by author)

 
Q: I still don’t get it, what happens if we do that?
That’s understandable actually. Let me break it in steps. So, our resultant matrix(QxK/sqrt(d)) of shape (TxT) might look something like below:(The numbers can be big as softmax not applied yet)


Schnelle currently attends to both Braune and Fuchs (Image by author)

 
The word Schnelle will now be composed of both Braune and Fuchs if we take the above matrix’s softmax and multiply it with the value matrix V. But we don’t want that, so we add the mask matrix to it to give:


The mask operation applied to the matrix. (Image by author)

 
And, now what will happen after we do the softmax step?


Schnelle never attends to any word after Schnelle. (Image by author)

 
Since e^{-inf} = 0, all positions subsequent to Schnelle have been converted to 0. Now, if we multiply this matrix with the value matrix V, the vector corresponding to Schnelle’s position in the Z vector passing through the decoder would not contain any information of the subsequent words Braune and Fuchs just like we wanted.
And that is how the transformer takes the whole shifted output sentence at once and doesn’t learn in a sequential manner. Pretty neat I must say.
Q: Are you kidding me? That’s actually awesome.
So glad that you are still with me and you appreciate it. Now, coming back to the decoder. The next layer in the decoder is:
 
B) Multi-Headed Attention Layer
 
As you can see in the decoder architecture, a Z vector(Output of encoder) flows from the encoder to the multi-head attention layer in the Decoder. This Z output from the last encoder has a special name and is often called as memory. The attention layer takes as input both the encoder output and data flowing from below(shifted outputs) and uses attention. The Query vector Q is created from the data flowing in the decoder, while the Key(K) and value(V) vectors come from the encoder output.
Q: Isn’t there any mask here?
No, there is no mask here. The output coming from below is already masked and this allows every position in the decoder to attend over all the positions in the Value vector. So for every word position to be generated the decoder has access to the whole English sentence.
Here is a single attention layer(which will be part of a multi-head just like before):


(Image by author)

 
Q: But won’t the shapes of Q, K, and V be different this time?
You can look at the figure where I have done all the weights calculation. I would also ask you to see the shapes of the resultant Z vector and how our weight matrices until now never used the target or source sentence length in any of their dimensions. Normally, the shape cancels away in all our matrix calculations. For example, see how the S dimension cancels away in calculation 2 above. That is why while selecting the batches during training the authors talk about tight batches. That is in a batch all source sentences have similar lengths. And different batches could have different source lengths.
I will now talk about the skip level connections and the feed-forward layer. They are actually the same as in ….
Q: Ok, I get it. We have the skip level connections and the FF layer and get a matrix of shape TxD after this whole decode operation. But where is the German translation?
 
3. Output Head
 
We are actually very much there now friend. Once, we are done with the transformer, the next thing is to add a task-specific output head on the top of the decoder output. This can be done by adding some linear layers and softmax on top to get the probability across all the words in the german vocab. We can do something like this:


(Image by author)

 
As you can see we are able to generate probabilities. So far we know how to do a forward pass through this Transformer architecture. Let us see how we do the training of such a Neural Net Architecture.
 
Training:
 
Till now, if we take a bird-eye view of the structure we have something like:


(Image by author)

 
We can give an English sentence and shifted output sentence and do a forward pass and get the probabilities over the German vocabulary. And thus we should be able to use a loss function like cross-entropy where the target could be the German word we want, and train the neural network using the Adam Optimizer. Just like any classification example. So, there is your German.
In the paper though, the authors use slight variations of optimizers and loss. You can choose to skip the below 2 sections on KL Divergence Loss and Learning rate schedule with Adam if you want as it is done only to churn out more performance out of the model and not an inherent part of the Transformer architecture as such.
Q: I have been here for such a long time and have I complained? 😒
Okay. Okay. I get you. Let’s do it then.
 
A) KL Divergence with Label Smoothing:
 
KL Divergence is the information loss that happens when the distribution P is approximated by the distribution Q. When we use the KL Divergence loss, we try to estimate the target distribution(P) using the probabilities(Q) we generate from the model. And we try to minimize this information loss in the training.


(Image by author)

 
If you notice, in this form(without label smoothing which we will discuss) this is exactly the same as cross-entropy. Given two distributions like below.
  

Target distribution and probability distribution for a word(token) (Image by author)

 
The KL Divergence formula just plain gives -logq(oder) and that is the cross-entropy loss.
In the paper, though the authors used label smoothing with α = 0.1 and so the KL Divergence loss is not cross-entropy. What that means is that in the target distribution the output value is substituted by (1-α) and the remaining 0.1 is distributed across all the words. The authors say that this is so that the model is not too confident.
  

(Image by author)

 
Q: But, why do we make our models not confident? It seems absurd.
Yes, it does but intuitively, you can think of it as when we give the target as 1 to our loss function, we have no doubts that the true label is True and others are not. But vocabulary is inherently a non-standardized target. For example, who is to say that you cannot use good in place of great? So we add some confusion in our labels so our model is not too rigid.
 
B) A particular Learning Rate schedule with Adam
 
The authors use a learning rate scheduler to increase the learning rate until warmup steps and then decrease it using the below function. And they used the Adam optimizer with β¹ = 0.9, β² = 0.98. Nothing too interesting here just some learning choices.


Source: Paper

 
Q: But wait I just remembered that we won’t have the shifted output at the prediction time, would we? How do we do predictions then?
If you realize what we have at this point is a generative model and we will have to do the predictions in a generative way as we won’t know the output target vector when doing prediction. So predictions are still sequential.
 
Prediction Time
 


Predicting with a greedy search using the Transformer (Image by author)

 
This model does piece-wise predictions. In the original paper, they use the Beam Search to do prediction. But a greedy search would work fine as well for the purpose of explaining it. In the above example, I have shown how a greedy search would work exactly. The greedy search would start with:

Passing the whole English sentence as encoder input and just the start token <st> as shifted output(input to the decoder) to the model and doing the forward pass.
The model will predict the next word — der
Then, we pass the whole English sentence as encoder input and add the last predicted word to the shifted output(input to the decoder = <st> der) and do the forward pass.
The model will predict the next word — schnelle
Passing the whole English sentence as encoder input and <st> der schnelle as shifted output(input to the decoder) to the model and doing the forward pass.
and so on, until the model predicts the end token </s> or we generate some maximum number of tokens(something we can define) so the translation doesn’t run for an infinite duration in any case it breaks.

 
Beam Search:
 
Q: Now I am greedy, Tell me about beam search as well.
Okay, the beam search idea is inherently very similar to the above idea. In beam search, we don’t just look at the highest probability word generated but the top two words.
So, For example, when we gave the whole English sentence as encoder input and just the start token as shifted output, we get two best words as i(p=0.6) and der(p=0.3). We will now generate the output model for both output sequences,<s> i and <s> der and look at the probability of the next top word generated. For example, if <s> i gave a probability of (p=0.05) for the next word and <s> der> gave (p=0.5) for the next predicted word, we discard the sequence <s> iand go with <s> der instead, as the sum of probability of sentence is maximized(<s> der next_word_to_der p = 0.3+0.5 compared to <s> i next_word_to_i p = 0.6+0.05). We then repeat this process to get the sentence with the highest probability.
Since we used the top 2 words, the beam size is 2 for this Beam Search. In the paper, they used beam search of size 4.
PS: I showed that the English sentence is passed at every step for brevity, but in practice, the output of the encoder is saved and only the shifted output passes through the decoder at each time step.
Q: Anything else you forgot to tell me? I will let you have your moment.
Yes. Since you asked. Here it is:
 
BPE, Weight Sharing and Checkpointing
 
In the paper, the authors used Byte pair encoding to create a common English German vocabulary. They then used shared weights across both the English and german embedding and pre-softmax linear transformation as the embedding weight matrix shape would work (Vocab Length X D).
Also, the authors average the last k checkpoints to create an ensembling effect to reach the performance. This is a pretty known technique where we average the weights in the last few epochs of the model to create a new model which is sort of an ensemble.
Q: Can you show me some code?
This post has already been so long, so I will do that in the next post. Stay tuned.
Now, finally, my turn to ask the question: Did you get how a transformer works? Yes, or No, you can answer in the comments. :)
 
References

Attention Is All You Need: The Paper which started it all.
The Annotated Transformer: This one has all the code. Although I will write a simple transformer in the next post too.
The Illustrated Transformer: This is one of the best posts on transformers.

In this post, I covered how the Transformer architecture works from a detail-oriented, intuitive perspective.
If you want to learn more about NLP, I would like to call out an excellent course on Natural Language Processing from the Advanced Machine Learning Specialization. Do check it out.
I am going to be writing more of such posts in the future too. Let me know what you think about them. Should I write on heavily technical topics or more beginner level articles? The comment section is your friend. Use it. Also, follow me up at Medium or Subscribe to my blog.
And, finally a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
This story was first published here.
 
Bio: Rahul Agarwal is Senior Statistical Analyst at WalmartLabs. Follow him on Twitter @mlwhiz.
Original. Reposted with permission.
Related:

A Deep Dive Into the Transformer Architecture – The Development of Transformer Models
The Hitchhiker’s Guide to Feature Extraction
Deep Learning’s Most Important Ideas"
https://www.kdnuggets.com/2020/07/better-blog-post-analysis-google-analytics-r.html,Better Blog Post Analysis with googleAnalyticsR,"In this post, we'll walk through using googleAnalyticsR for better blog post analysis, so you can do my better blog post analysis for yourself!","comments
By Loryn Cole, Dataquest
In my previous role as a marketing data analyst for a blogging company, one of my most important tasks was to track how blog posts performed.
On the surface, it’s a fairly straightforward goal. With Google Analytics, you can quickly get just about any metric you need for your blog posts, for any date range.
But when it comes to comparing blog post performance, things get a bit trickier.
For example, let’s say we want to compare the performance of the blog posts we published on the Dataquest blog in June (using the month of June as our date range).

But wait… two blog posts with more than 1,000 pageviews were published earlier in the month, And the two with fewer than 500 pageviews were published at the end of the month. That’s hardly a fair comparison!
My first solution to this problem was to look up each post individually, so that I could make an even comparison of how each post performed in their first day, first week, first month, etc.
However, that required a lot of manual copy-and-paste work, which was extremely tedious if I wanted to compare more than a few posts, date ranges, or metrics at a time.
But then, I learned R, and realized that there was a much better way.
In this post, we'll walk through how it's done, so you can do my better blog post analysis for yourself!
 
What we'll need
 
To complete this tutorial, you’ll need basic knowledge of R syntax and the tidyverse, and access to a Google Analytics account.
Not yet familiar with the basics of R? We can help with that! Our interactive online courses teach you R from scratch, with no prior programming experience required. Sign up and start today!

 
You’ll also need the dyplr, lubridate, and stringr packages installed — which, as a reminder, you can do with the install.packages() command.
Finally, you will need a CSV of the blog posts you want to analyze. Here’s what’s in my dataset:
post_url: the page path of the blog post
post_date: the date the post was published (formatted m/d/yy)
category: the blog category the post was published in (optional)
title: the title of the blog post (optional)
Depending on your content management system, there may be a way for you to automate gathering this data — but that’s out of the scope of this tutorial!
For this tutorial, we’ll use a manually-gathered dataset of the past ten Dataquest blog posts.
 
Setting up the googleAnalyticsR package
 
To access data from the Google Analytics API, we’ll use the excellent googleAnalyticsR package by Mark Edmonson.
As described in the documentation, there are two ""modes"" to the googleAnalyticsR package. The first mode, which we’ll use here, is a “Try it out” mode, which uses a shared Google Project to authorize your Google Analytics account.
If you want to make this report a recurring tool for your blog or client, be sure to create your own Google Project, which will help keep the traffic on the shared Project to a minimum. To find out how to set this up, head over to the package setup documentation.
For now, though, we’ll stick with “Try it out” mode.
First, we'll install the package using this code:
install.packages('googleAnalyticsR', dependencies = TRUE)
This installs the package, as well as the required dependencies.
Next, we'll load the library, and authorize it with a Google Analytics account using the ga_auth() function.

library(googleAnalyticsR)
ga_auth()


When you run this code the first time, it will open a browser window and prompt you to log in to your Google account. Then, it will give you a code to paste into your R console. After that, it will save an authorization token so you only have to do this once!
Once you’ve completed the Google Analytics authorization, we’re ready to set up the rest of the libraries and load in our blog posts. We’ll also use dplyr::mutate() to change the post_date to a Date class while we’re at it!

library(dplyr)
library(lubridate)
library(stringr)
library(readr)

blog_posts <- read.csv(""articles.csv"") %>%
  mutate(
    post_date = as.Date(post_date, ""%m/%d/%y"") # changes the post_date column to a Date
  )


Here’s what the blog post data frame looks like:

Finally, to get data from your Google Analytics account, you will need the ID of the Google Analytics view you want to access. ga_account_list() will return a list of your available accounts.

accounts <- ga_account_list()

# select the view ID by view and property name, and store it for ease of use
view_id <- accounts$viewId[which(accounts$viewName == ""All Web Site Data"" & accounts$webPropertyName == ""Dataquest"")]
# be sure to change this out with your own view and/or property name!


Now, we’re ready to do our first Google Analytics API requests!
 
Accessing blog post data with googleAnalyticsR
 
In this tutorial, our goal is to gather data for the first week each post was active, and compile it in a dataframe for analysis. To do this, we’ll create a function that runs a for loop and requests this data for each post in our blog_posts dataframe.
So, let’s take a look at how to send a request to the Google Analytics API using googleAnalyticsR.

google_analytics(view_id,
                  date_range = c(as.Date(""2020-06-01""), as.Date(""2020-06-30"")),
                  metrics = c(""pageviews""),
                  dimensions = c(""pagePath"")
)


This request has a few components. First, enter the view_id, which we already stored from our ga_accounts() dataframe.
Next, specify the date range, which needs to be passed in as a list of dates.
Then, we input the metrics (like pageviews, landing page sessions, or time on page) and dimensions (like page path, channel, or device). We can use any dimension or metric that’s available in the Google Analytics UI — here’s a useful reference for finding the API name of any UI metric or dimension.
So, the request above will return a dataframe of all pageviews in June, by page path (by default googleAnalyticsR will only return the first 1,000 results).
But, in our case, we only want to retrieve pageviews for a specific page – so we need to filter on the pagePath dimension using a dimension filter, which looks like this:

page_filter <- dim_filter(dimension = ""pagePath"",
                          operator = ""REGEXP"",
                          expressions = ""^www.dataquest.io/blog/r-markdown-guide-cheatsheet/$"")


To use this filter in our request, googleAnalyticsR wants us to create a filter clause – which is how you would combine filters if you wanted to use multiple dimension filters. But in our case, we just need the one:

page_filter_clause <- filter_clause_ga4(list(page_filter))


Now, let’s try sending a response with this filter:

google_analytics(view_id,
              date_range = c(as.Date(""2020-07-01""), Sys.Date()),
              metrics = c(""pageviews""),
              dimensions = c(""pagePath""),
              dim_filters = page_filter_clause)



The result is a dataframe with the pageviews for the R Markdown post!
 
Creating the for loop
 
Now that we can gather data and filter it by dimension, we are ready to build out our function to run our for loop! The steps to the function are:

Set up a data frame to hold the results
Begin the loop based on the number of rows in the data frame
Access the post URL and post date for each post
Create a page filter based on the post URL
Send a request to Google Analytics using the post_date as the start date, and date the week later as the end date
Add the post URL and pageview data to the final data frame

I also have added a print() command to let us know how far along the loop is (because it can take awhile) and a Sys.Sleep() command to keep us from hitting the Google Analytics API rate limit.
Here’s what that looks like all put together!

get_pageviews <- function(posts) {

  # set up dataframe to be returned, using the same variable names as our original dataframe
  final <- tibble(pageviews = numeric(),
                      post_url = character())

  # begin the loop for each row in the posts dataframe
  for (i in seq(1:nrow(posts))) {

    # select the post URL and post date for this loop — also using the same variable names as our original dataframe
    post_url <- posts$post_url[i]
    post_date <- posts$post_date[i]

    # set up the page filter and page filter clause with the current post URL
    page_filter <- dim_filter(dimension = ""pagePath"",
                              operator = ""REGEXP"",
                              expressions = post_url)

    page_filter_clause <- filter_clause_ga4(list(page_filter))

    # send the request, and set the date range to the week following the date the post was shared
    page_data <- google_analytics(view_id,
                                    date_range = c(post_date, post_date %m+% weeks(1)),
                                    metrics = c(""pageviews""),
                                    dim_filters = page_filter_clause)

    # add the post url to the returned dataframe
    page_data$post_url <- post_url

    # add the returned data to the data frame we created outside the loop
    final <- rbind(final, page_data)

    # print loop status
    print(paste(""Completed row"", nrow(final), ""of"", nrow(posts)))

    # wait two seconds
    Sys.sleep(2)

  }

  return(final)

}


We could potentially speed this up with a “functional” in R, such as purrr::map(). The map() function takes a function as an input and returns a vector as output. Check out Dataquest's interactive online lesson on the map function if you'd like to deepen your knowledge!
For this tutorial, though, we'll use a for loop because it's a bit less abstract.
Now, we’ll run the loop on our blog_posts dataframe, and merge the results to our blog_posts data.

recent_posts_first_week <- get_pageviews(blog_posts)
recent_posts_first_week <- merge(blog_posts, recent_posts_first_week)

recent_posts_first_week



And that’s it! Now, we can get on to the good stuff — analyzing and visualizing the data.
 
Blog post data, visualized!
 
For demonstration, here's a ggplot bar chart that shows how many pageviews each of our most recent 10 blog posts got in the first week after they were published:

library(ggplot2)
library(scales)

recent_posts_first_week %>%
  arrange(
    post_date
  ) %>%
  mutate(
    pretty_title = str_c(str_extract(title, ""^(\\S+\\s+\\n?){1,5}""), ""...""),
    pretty_title = factor(pretty_title, levels = pretty_title[order(post_date)])
  ) %>%
  ggplot(aes(pretty_title, pageviews)) +
  geom_bar(stat = ""identity"", fill = ""#39cf90"") +
  coord_flip() +
  theme_minimal() +
  theme(axis.title = element_blank()) +
  labs(title = ""Recent Dataquest blog posts by first week pageviews"") +
  scale_y_continuous(labels = comma)



Now we can see how useful it is to be able to compare blog posts on ""even footing""!
For more information on the googleAnalyticsR package and what you can do with it, check out its very helpful resource page.
 
Bio: Loryn Cole is a product marketer at Dataquest who enjoys learning to code with data. She rides motorcycles and sometimes writes about them.
Original. Reposted with permission.
Related:

Understanding Time Series with R
Wrapping Machine Learning Techniques Within AI-JACK Library in R
modelStudio and The Grammar of Interactive Explanatory Model Analysis"
https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html,Automate Microsoft Excel and Word Using Python,Integrate Excel with Word to generate automated reports seamlessly.,"comments
By Mohammad Khorasani, Data Scientist/Engineer Hybrid


Photo by Isaac Smith on Unsplash

 
Microsoft Excel and Word are without a shred of doubt the two most abundantly used software in the corporate and non-corporate world. They are practically synonymous with the term ‘work’ itself. Oftentimes, not a week goes by without us firing up the combination of the two and one way or another putting their goodness to use. While for the average daily purpose automation would not be solicited, there are times when automation can be a necessity. Namely, when you have a multitude of charts, figures, tables, and reports to generate, it can become an exceedingly tedious undertaking if you choose the manual route. Well, it doesn’t have to be that way. There is in fact a way to create a pipeline in Python where you can seamlessly integrate the two to produce spreadsheets in Excel and then transfer the results to Word to generate a report virtually instantaneously.
 
Openpyxl
 
 
Meet Openpyxl, arguably one of the most versatile bindings in Python that makes interfacing with Excel quite literally a stroll in the park. Armed with it you can read and write all current and legacy excel formats i.e. xlsx and xls. Openpyxl allows you to populate rows and columns, execute formulae, create 2D and 3D charts, label axes and titles, and a plethora of other abilities that can come in handy. Most importantly however, this package enables you to iterate over an endless numbers of rows and columns in Excel, thereby saving you from all that pesky number crunching and plotting that you had to do previously.
 
Python-docx
 
 
And then comes along Python-docx, this package is to Word what Openpyxl is to Excel. If you haven’t already studied their documentation, then you should probably take a look. Python-docx is without exaggeration one of the simplest and most self-explanatory toolkits I have worked with ever since I started working with Python itself. It allows you to automate document generation by inserting text, filling in tables and rendering images into your report automatically without any overhead whatsoever.
Without further ado let’s create our very own automated pipeline. Go ahead and fire up Anaconda (or any other IDE of your choice) and install the following packages:

pip install openpyxlpip install python-docx

 
 
Microsoft Excel Automation
 
 
Initially, we’ll load an Excel workbook that has already been created (shown below):

workbook = xl.load_workbook('Book1.xlsx')
sheet_1 = workbook['Sheet1']

 


Image by the author.

 
Subsequently, we’ll iterate over all of the rows in our spreadsheet to compute and insert the values for power by multiplying current by voltage:

for row in range(2, sheet_1.max_row + 1):
    current = sheet_1.cell(row, 2)
    voltage = sheet_1.cell(row, 3)
    power = float(current.value) * float(voltage.value)
    power_cell = sheet_1.cell(row, 1)
    power_cell.value = power

 
Once that is done, we will use the calculated values for power to generate a line chart that will be inserted into the specified cell as shown below:

values = Reference(sheet_1, min_row = 2, max_row = sheet_1.max_row, min_col = 1, max_col = 1)
chart = LineChart()
chart.y_axis.title = 'Power'
chart.x_axis.title = 'Index'
chart.add_data(values)
sheet_1.add_chart(chart, 'e2') 
workbook.save('Book1.xlsx')

 


Automatically generated Excel spreadsheet. Image by the author.

 
 
Extracting Chart
 
 
Now that we have generated our chart, we need to extract it as an image so that we can use it in our Word report. First, we’ll declare the exact location of our Excel file and also where the output chart image should be saved:

input_file = ""C:/Users/.../Book1.xlsx""
output_image = ""C:/Users/.../chart.png""

 
Then access the spreadsheet using the following method:

operation = win32com.client.Dispatch(""Excel.Application"")
operation.Visible = 0
operation.DisplayAlerts = 0
workbook_2 = operation.Workbooks.Open(input_file)
sheet_2 = operation.Sheets(1)

 
Subsequently, you can iterate over all of the chart objects in the spreadsheet (if there are more than one) and save them in the specified location as such:

for x, chart in enumerate(sheet_2.Shapes):
    chart.Copy()
    image = ImageGrab.grabclipboard()
    image.save(output_image, 'png')
    passworkbook_2.Close(True)
operation.Quit()

 
 
Microsoft Word Automation
 
 
Now that we have our chart image generated, we must create a template document that is basically a normal Microsoft Word Document (.docx) formulated exactly in the way we want our report to look, including typefaces, font sizes, formatting, and page structure. Then all we need to do is to create placeholders for our automated content i.e. table values and images and declare them with variable names as shown below.


Microsoft Word document template. Image by the author.

 
Any automated content can be declared inside a pair of double curly brackets {{variable_name}}, including text and images. For tables, you need to create a table with a template row with all the columns included, and then you need to append one row above and one row below with the following notation:
First row:

{%tr for item in variable_name %}

 
Last row:

{%tr endfor %}

 
In the figure above the variable names are

table_contents for the Python dictionary that will store our tabular data
Index for the dictionary keys (first column)
Power, Current, and Voltage for the dictionary values (second, third and fourth columns)

Then we import our template document into Python and create a dictionary that will store our table’s values:

template = DocxTemplate('template.docx')
table_contents = []for i in range(2, sheet_1.max_row + 1):
    table_contents.append({
        'Index': i-1,
        'Power': sheet_1.cell(i, 1).value,
        'Current': sheet_1.cell(i, 2).value,
        'Voltage': sheet_1.cell(i, 3).value
        })

 
Next we‘ll’ import the chart image that was previously produced by Excel and will create another dictionary to instantiate all of the placeholder variables declared in the template document:

image = InlineImage(template,'chart.png',Cm(10))context = {
    'title': 'Automated Report',
    'day': datetime.datetime.now().strftime('%d'),
    'month': datetime.datetime.now().strftime('%b'),
    'year': datetime.datetime.now().strftime('%Y'),
    'table_contents': table_contents,
    'image': image
    }

 
And finally, we’ll render the report with our table of values and chart image:

template.render(context)
template.save('Automated_report.docx')

 
 
Results
 
 
And there you go, an automatically generated Microsoft Word report with numbers and a chart created in Microsoft Excel. And with that, you have a fully automated pipeline that can be used to create as many tables, charts, and documents as you could possibly ever need.


Automatically generated report. Image by the author.

 
 
Source Code
 
 

 
If you want to learn more about data visualization and Python, then feel free to check out the following (affiliate linked) courses:
 
Data Visualization with Python
 
Python for Everybody Specialization
 
The source code and template for this tutorial can be found in the following GitHub repository.
 
mkhorasani/excel_word_automation
 
In addition, feel free to subscribe to Medium and explore more of my tutorials here.
 
Bio: Mohammad Khorasani is a hybrid of a data scientist and an engineer. Logistician. Candid. Realpolitik. Unlearning dogma one belief at a time. Read more of Mohammad's writings.
Original. Reposted with permission.
Related:

How to Generate Automated PDF Documents with Python
5 Tasks To Automate With Python
Data Scientists, You Need to Know How to Code"
https://www.kdnuggets.com/2021/03/begin-nlp-journey.html,How to Begin Your NLP Journey,"In this blog post, learn how to process text using Python.","comments
By Diego Lopez Yse, Data Scientist


Photo by PNG Design on Unsplash

 
Natural Language Processing (NLP) is one of the most exciting fields in Artificial Intelligence. It allows machines to process and understand human language in a variety of ways, and it’s triggering a revolution in the way we interact with systems and technology.
In a previous post I talked about NLP, its real-world applications, and some of its core concepts. Now I want to show you that NLP is as real as it gets, and anyone can start learning it. How? Let’s start with a simple text, and perform some Exploratory Data Analysis (EDA) around it using some NLP techniques. This way we can make sense out of data with simple and powerful tools before getting ourselves busy with any model or more complex tasks.
 
Define your text
 
Stephen Hawking once said:


“Artificial Intelligence (AI) is likely to be either the best or the worst thing to happen to humanity”


I couldn’t agree more with him, and time will tell what will actually happen. Nevertheless, this is a proper sentence to test some NLP techniques. To do that, let’s start by saving the phrase as a variable called “text”:

text = “Artificial Intelligence (AI) is likely to be either the best or the worst thing to happen to humanity.”


Using the langdetect library, we can check its language, and find out the probability of being written in that language:

import langdetect
from langdetect import detect_langs
print(detect_langs(text))





With a certainty of more than 99,9% we can state that this phrase is written in English language. You should also consider using spelling check functionalities to correct any grammatical mistakes.
What about the number of characters?

len(text)





We have 102 characters, including blank spaces. And the number of distinct characters?

len(set(text))





Let’s take a look at them:

print(sorted(set(text)))





There’s something interesting here. We’re not only counting the non-alphanumerical characters like ‘(‘ and ‘.’, but also the blank spaces, and what’s even more, capitalized letters are considered different characters in relation to the lowercased ones.
 
Tokenization
 
Tokenization is the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens. We use the NLTK library to perform this task:

import nltk
from nltk.tokenize import word_tokenize
tokenized_word = word_tokenize(text)
print(tokenized_word)





We can see that tokenization produces a list of words:

type(tokenized_word)





Which means we can call elements within it.

tokenized_word[2:9]





How many tokens we have?

len(tokenized_word)





And unique tokens?

len(set(tokenized_word))





Now we can calculate a measure related to the lexical richness of the text:

len(set(tokenized_word)) / len(tokenized_word)





This shows that the number of distinct words is 85,7% of the total number of words.
 
Lowercase & punctuation
 
Now let’s lowercase the text to standardize characters and for future stopwords removal:

tk_low = [w.lower() for w in tokenized_word]
print(tk_low)





Next, we remove non-alphanumerical characters:

nltk.download(“punkt”)
tk_low_np = remove_punct(tk_low)
print(tk_low_np)





Let’s visualize the cumulative frequency distribution of words:

from nltk.probability import FreqDist
fdist = FreqDist(tk_low_np)
fdist.plot(title = ‘Word frequency distribution’, cumulative = True)





We can see that the words “to” and “the” appear most often, but they don’t really add information to the text. They are what’s known as stopwords.
 
Stopwords removal
 
This process includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.
First, we need to create a list of stopwords and filter them our from our list of tokens:

from nltk.corpus import stopwords
stop_words = set(stopwords.words(“english”))
print(stop_words)





We’ll use this list from NLTK library, but bear in mind that you can create your own set of stop words. Let’s look for the word “the” in the list:

print(‘the’ in stop_words)





Now, let’s clean our text from these stopwords:

filtered_text = []
for w in tk_low_np:
   if w not in stop_words:
      filtered_text.append(w)
print(filtered_text)





We can see that the words “is”, “to”, “be”, “the” and “or” were removed from our text. Let’s update the cumulative frequency distribution of words:



Removing stop words should be done in a very conscious way, since it can bring huge problems while performing other tasks like sentiment analysis. If a word’s context is affected (e.g. by removing the word ‘not’, which is a negation of a component), that action can alter the meaning of the passage.
Beyond this example, it could be necessary to deal with other types of characteristics like contractions (like the word “doesn’t”, which should be expanded), or accents and diacritics (like the words “cliché” or “naïve”, which should be normalized by removing their diacritics).
 
Regular Expressions
 
Regular expressions (called REs, or RegExes) are a tiny, highly specialized programming language embedded inside Python and made available through the re module. By using them, you specify the rules for the set of possible strings that you want to match. You can ask questions such as “Does this string match the pattern?”, or “Is there a match for the pattern anywhere in this string?”.
For example, let’s search for words ending with “st”:

import re
[w for w in filtered_text if re.search(‘st$’, w)]





Or count the number of vowels in the first word (“artificial”):

len(re.findall(r’[aeiou]’, filtered_text[0]))





You can even modify texts based on conditions. For example, replace letters “ce” with letter “t” in the second word (“intelligence”):

x = re.sub('ce', 't', filtered_text[1])
print(x)





You can find more examples of regular expressions following this link.
 
Conclusion
 
We’ve only scratched the surface of all the possible and more complex NLP techniques out there. And it’s not just structured texts that you may want to analyze, but all that data generated from conversations, declarations or even tweets, which are examples of unstructured data. Unstructured data doesn’t fit neatly into the traditional row and column structure of relational databases, and represent the vast majority of data available in the actual world. It is messy and hard to manipulate.
NLP is seriously booming thanks to the huge improvements in the access to data and the increase in computational power, which allow us to achieve meaningful results in areas like healthcare, media, finance and human resources, among others.


My suggestion is: learn about NLP. Try different data sources and techniques. Experiment, fail and improve yourself. This discipline will impact every possible industry, and we are likely to reach a level of advancement in the coming years that will blow our minds.


Interested in these topics? Follow me on Linkedin or Twitter
 
Bio: Diego Lopez Yse is an experienced professional with a solid international background acquired in different industries (capital markets, biotechnology, software, consultancy, government, agriculture). Always a team member. Skilled in Business Management, Analytics, Finance, Risk, Project Management and Commercial Operations. MS in Data Science and Corporate Finance.
Original. Reposted with permission.
Related:

Exploratory Data Analysis on Steroids
Getting Started with 5 Essential Natural Language Processing Libraries
Natural Language Processing Pipelines, Explained"
https://www.kdnuggets.com/2021/08/common-data-science-interview-questions-answers.html,Most Common Data Science Interview Questions and Answers,"After analyzing 900+ data science interview questions from companies over the past few years, the most common data science interview question categories are reviewed in this guide, each explained with an example.","comments
By Nate Rosidi, Data Scientist and Product Manager.

Becoming a data scientist is considered a prestigious trait. Back in 2012, Harvard Business Review called 'data scientist' the sexiest job of the 21st century, and the growing trend of roles in the industry seems to be confirming that statement. To confirm this sexiness is still ongoing, the info from Glassdoor shows being a data scientist is the second-best job in America in 2021.

Source: Glassdoor.
To get such a prestigious job, you have to go through rigorous job interviews. Data science questions asked can be very broad and complex. This is expected, considering the role of a data scientist usually incorporates so many areas.  To help you prepare for the data science job interviews, I have reviewed all the applicable questions and separated them into different question categories. Here’s how I did that.
Description and Methodology of the Analysis
I gathered data from various job search boards and websites and company review platforms such as Glassdoor, Indeed, Reddit, and Blind App. To be more precise, there are 903 questions collected over the past four years.
The questions are sectioned into pre-determined categories. These categories are the result of an expert analysis of the interview experience description taken from our sources.
The categories are:

Coding
Modelling
Algorithms
Statistics
Probability
Product
Business case
System design
Technical

What types of interview questions should you expect?
This chart shows you the question type per category according to the collected data.

Translated to percentages, the chart looks like this:

As you can see, the coding and modelling questions are most dominant. More than half of all questions come from that area. It’s not surprising when you think about it. Coding and modelling are probably the two most important skills for a data scientist. Coding-type questions are widespread, comprising more than one-third of all questions. Other question types, such as algorithms and statistics, are also fairly significant; 24% of all questions come from these two categories. Other categories are not as represented. I find that reasonable, considering the nature of a data scientist role.
Now I want to guide you through every question category and show you some examples of the questions being asked.
The most tested concepts on data science interview questions
 
 
Coding​
 
As you already saw, coding questions are the single most important topic in data science. Such questions will require some sort of data manipulation using the code to identify insights. The questions are designed to test coding ability, problem-solving skills, and creativity. You’ll usually do that on a computer or a whiteboard.
Coding interview question example
One example from Microsoft is this one:
QUESTION: “Calculate the share of new and existing users. Output the month, share of new users, and share of existing users as a ratio. New users are defined as users who started using services in the current month. Existing users are users who started using services in the current month and used services in any previous month. Assume that the dates are all from the year 2020.”
You’ll be using the table fact_events, with the sample data looking like this:

To get the desired output, you should write this code:

with all_users as (
    SELECT date_part('month', time_id) AS month,
           count(DISTINCT user_id) as all_users
    FROM fact_events
    GROUP BY month),
new_users as (
    SELECT date_part('month', new_user_start_date) AS month,
           count(DISTINCT user_id) as new_users
    FROM
         (SELECT user_id,
           min(time_id) as new_user_start_date
          FROM fact_events
          GROUP BY user_id) sq
    GROUP BY month
)
SELECT
  au.month,
  new_users / all_users::decimal as share_new_users,
  1- (new_users / all_users::decimal) as share_existing_users
FROM all_users au
JOIN new_users nu ON nu.month = au.month



 
Writing a code in SQL is the most often tested concept when it comes to coding. It’s no surprise since SQL has been the most used tool in data science. One of the concepts you almost can’t avoid in the interviews is the joins. So make sure you know the difference between different joins and how to use them to get the required result.
Also, you can expect to group data using the GROUP BY clause very often. Some other concepts that are usually asked are filtering data using the WHERE and/or HAVING clause. You’ll also be asked to select distinct data. And also, make sure that you know the aggregate functions, such as SUM(), AVG(), COUNT(), MIN(), MAX().
Some concepts don’t occur that much often, but it’s worth mentioning them and being prepared for such questions. For example, Common Table Expressions or CTEs is one such topic. The other one is the CASE() clause. Also, don’t forget to refresh your memory on handling the string data types and dates.
 
Modeling
 
Modelling was the second-largest category in our research data, with 20% of all questions coming from here. These questions are designed to test your knowledge of building statistical models and implementing machine learning models.
Modelling interview question example
Regression, the most common technical data science concept asked in interviews. It’s not surprising, considering the nature of the statistical modelling.
One example from Galvanise would be the following:
QUESTION: “What is regularisation in regression?”
Here is how you could answer this question:
ANSWER: “A regularisation is a special type of regression where the coefficient estimates are constrained (or regularised) to zero. By doing this, it is possible to reduce the variance of the model while at the same time decreasing the sampling error. Regularisation is used to avoid or reduce overfitting. Overfitting happens when the model learns training data so well it undermines the model’s performance on new data. To avoid overfitting, Ridge or Lasso regularisations are usually used.”
Some of the concepts tested regularly are, again, other regression analysis concepts, such as logistic regression, Bayesian logistic regression, and naive Bayes classifiers. You can also be asked about the random forests, as well as testing and evaluating models.
 
Algorithms
 
Questions on algorithms are all questions that require solving a mathematical problem, mainly through code by using one of the programming languages. These questions involve a step-by-step process, usually requiring adjustment or computation to produce an answer. These questions test the basic knowledge of problem-solving and data manipulation, which can be implemented for complex problems at work.
Algorithm interview question example
The technical concept tested most under algorithms is solving a mathematical or syntax problem with a programming language.
Here is one example you can find on Leetcode:
QUESTION: “You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list.""
The example of the data could be something like this:

Source: Leetcode.
ANSWER: The code written in Java should be:

public ListNode addTwoNumbers(ListNode l1, ListNode l2) {
    ListNode dummyHead = new ListNode(0);
    ListNode p = l1, q = l2, curr = dummyHead;
    int carry = 0;
    while (p != null || q != null) {
        int x = (p != null) ? p.val : 0;
        int y = (q != null) ? q.val : 0;
        int sum = carry + x + y;
        carry = sum / 10;
        curr.next = new ListNode(sum % 10);
        curr = curr.next;
        if (p != null) p = p.next;
        if (q != null) q = q.next;
    }
    if (carry > 0) {
        curr.next = new ListNode(carry);
    }
    return dummyHead.next;
}



 
The other general concepts often tested by this type of question are arrays, dynamic programming, strings, greedy algorithm, depth-first search, tree, hash table, and binary search.
 
Statistics
 
The statistics interview questions are questions testing the knowledge of statistical theory and associated principles. These questions intend to try how familiar you are with the founding theoretical principles in data science. Being able to understand the theoretical and mathematical background of analyses being done is important. Answer those questions well, and every interviewer will appreciate you.
Statistics interview question example
The most mentioned technical concept is sampling and distribution. For a data scientist, this is one of the most commonly used statistics principles the data scientist implements daily.
For example, an interview question from IBM asks:
QUESTION: “What is an example of a data type with a non-Gaussian distribution?”
To answer the question, you could first define a Gaussian distribution. Then you could follow this by giving examples of the non-Gaussian distribution. Something like this:
ANSWER: “A Gaussian distribution is a distribution where a certain known percentage of the data can be found when examining standard deviations from the mean, otherwise known as a normal distribution. Some of the examples of the non-Gaussian distribution can be exponential distribution or binomial distribution.”
When preparing for the job interview, make sure you also cover the following topics: variance and standard deviation, covariance and correlation, the p-value, mean and median, hypothesis testing, and Bayesian statistics. These are all concepts you’ll need as a data scientist, so expect them in the job interviews too.
 
Probability
 
These questions require theoretical knowledge only on probability concepts. Interviewers ask these questions to get a deep understanding of your knowledge on the methods and uses of probability to complete the complex data studies usually performed in the workplace.
Probability interview question example
It’s highly probable, pun intended, that the question you’ll get is to calculate the probability of getting a certain card/number from a set of dice/cards. This seems to be the most common element of questioning for most companies in our research, as many of them have asked these types of questions.
An example of such a probability question from Facebook:
QUESTION: “What is the probability of getting a pair by drawing two cards separately in a 52-card deck?”
Here is how you can answer this:
ANSWER: “This first card you draw can be whatever, so it does not impact the result other than that there is one card less left in the deck. Once the first card is drawn, there are three remaining cards in the deck that can be drawn to get a pair. So, the chance of matching your first card with a pair is 3 out of 51 (remaining cards). This means that the probability of this event occurring is 3/51 or 5.89%.”
Since this is a kind of “specialised” question that deals only with probability, no other concepts are asked. The only difference is how imaginative the question is. But basically, you’ll always have to calculate the probability of some event and show your thinking.
 
Product
 
Product interview questions will ask you to evaluate the performance of a product/service through data. These questions test your knowledge of adapting and using data science principles in any environment, as is the case with daily work.
Product interview question example
The most prominent technical concept in this category is identifying a company’s product and proposing improvements from a data scientist’s perspective. The high variance in technical concepts tested on the product side can be explained by the nature of product questions and the higher level of creativity required to answer these.
An example of a product question from Facebook would be:
QUESTION: “What is your favourite Facebook product, and how would you improve it?”
ANSWER: Due to the nature of the question, we will let you answer this one yourself.
The general concepts tested heavily depend on the company that’s interviewing you. Just make sure you are familiar with the company’s business and their products (ideally, you’re their user, as well), and you’ll be fine.
 
Business Case
 
This category includes case studies and generic questions related to the business that would test a data science skill. The significance of knowing how to answer these questions can be enormous as some interviewers would like the candidates to know how to apply data science principles to solve a company’s specific problems before hiring them.
Business case question example
Due to the nature of the question type, I could not identify a single technical concept that stands out. Since most of the questions categorised here are case studies, they are unique in a certain way.
However, here is an example of a business case question from Uber:
QUESTION: “There is a pool of people who took Uber rides from two cities that were close in proximity, for example, Menlo Park and Palo Alto, and any data you could think of could be collected. What data would you collect so that the city the passenger took a ride from could be determined?”
ANSWER: “To determine the city, we need to have access to the location/geographical data. The data collected could be GPS coordinates, longitude/latitude, and ZIP code.”
 
System Design
 
System design questions are all questions related to designing technology systems. They are asked to analyse the candidate’s process in solving problems, creating, and designing systems to help customers/clients. Knowing system design can be quite important for a data scientist; even if your role is not to design a system, you will most likely play a role in an established system and need to know how it works in order to do your work.
System design interview question example
These questions cover different topics and tasks. But the one that stands out is building a database. Data scientists deal heavily with databases daily, so it makes sense to ask this question to see whether you can build a database from scratch.
Here is one question example from Audible uncovered in our research:
QUESTION: “Can you walk us through how you would build a recommendation system?”
ANSWER: Since there is such a variety of approaches to answer this question, we will leave you to come up with your own way of building one.
Again, to answer these questions, it’s essential to know the company’s business. Think a little about databases that the company most probably needs, and try to elaborate your approach a little before the interview.
 
Technical
 
Technical questions are all questions that are asking about the explanation of various data science technical concepts. The technical questions are theoretical and require knowledge of the technology you will be using at the company. Due to nature, they can seem similar to coding questions. Knowing the theory behind what you are doing is quite important, so technical questions can often be asked in interviews.
Technical interview question example
The most tested area is theoretical knowledge of Python and SQL. Not surprising, since these two languages are dominant in data science, along with R to complement Python.
An example of a real-world technical question from Walmart would be:
QUESTION: “What are the data structures in Python?”
ANSWER: “The data structures are used for storing data. There are four data structures in Python: List, Dictionary, Tuple, and Set. Those are the built-in data structures. Lists are used for creating lists that can contain different types of data. Dictionary is basically a set of keys; they are used to store a value with a key and getting the data using the same key. Tuples are the same as lists. The difference is that in a tuple, the data can’t be changed. Set contains the unordered elements with no duplicates. Along with the built-in data structures, there are also the user-defined data structures.”
These are catch-all types of questions. It’s a category for all the questions that can’t cleanly fit into other categories. Due to that, there are no specific concepts that occur more or less often.
Conclusion
This data science interview guide has been written to support the research undertaken to understand the types of questions being asked at a data science interview. The interview questions’ data are taken from dozens of companies over a four-year period and analysed. The questions have been categorised under nine different question types (algorithms, business case, coding, modelling, probability, product, statistics, system design, and technical questions).
As part of the analysis, I talked about some of the most common technical concepts from each question type category. For example, the most asked statistics questions have to do with sampling and distribution. Every question category is supported by one practical example of the real question.
The article is intended to serve you as an important guide for interview preparation or simply learning more about data science. I hope I have helped you to feel more comfortable about the data science interview process. Good luck with your interviews!
Original. Reposted with permission.
 
Related:

30 Most Asked Machine Learning Questions Answered
Top Python Data Science Interview Questions
How Can You Distinguish Yourself from Hundreds of Other Data Science Candidates?"
https://www.kdnuggets.com/2020/10/deploying-streamlit-apps-streamlit-sharing.html,Deploying Streamlit Apps Using Streamlit Sharing,Read this sneak peek into Streamlit’s new deployment platform.,"comments
By Tyler Richards, Data Scientist @ Facebook


Image by author

 
Over the past couple of weeks, I’ve been playing around with a new Streamlit feature called Streamlit sharing, which makes it super easy to deploy your custom apps. I’m going to go through a bit of background first, so if you want to see the docs for Streamlit sharing to get started you can find them here.
 
Streamlit background
 
For a bit of background, Streamlit is a framework that lets you quickly and confidently turn a python script into a web app and is an incredible tool for data scientists working on teams where they need to quickly share a model or an interactive analysis, or for data scientists working on personal projects they want to show the world. Here’s a Streamlit beginner tutorial if you want to try it out!
I’ve been using Streamlit for the past ~6 months, and it’s been so useful. Previously, if I knew I wanted to make a web app at the end of a project, I would always opt to switch to R for the wonderful R shiny framework, even though I am a much better python programmer than an R one. Going through Django or flask is just so much development friction to take on that it’s rarely worth it for a personal project and always takes too long for anything at work. But after using Streamlit, I now not only had options but found myself preferring python+Streamlit to R+shiny.
 
Streamlit sharing
 
This brings me to a couple of months ago. I started a DS project focused on analyzing reading habits using data from the Goodreads app. I decided to try Streamlit out, and it turned a multi-day long process of getting a Django/flask app running well locally into one that took around a half-hour for local Streamlit use. It really is as easy as throwing your analysis into a script, and calling Streamlit functions whenever you want to put a graph, widget, or text explainer on the app.
However, the most annoying process on Streamlit was the deployment and management process. The tutorial I followed was straightforward, and didn’t take that much time, but was fairly extensive. It required launching an ec2 instance, configuring SSH, using tmux, and going back to this terminal every time you wanted to change anything about your web app. It was doable but annoying.


Image by author

 
A few weeks ago, Streamlit saw my Goodreads app and asked if I wanted to test out their Streamlit sharing beta, which was supposed to remove the friction explained above. I, obviously, gave it a shot.
All I had to do was:

Push my app to a Github repo
Add a requirements.txt file that listed all the python libraries I used
Point Streamlit to my app via the link to the repository
Click Deploy

It genuinely was that easy to figure out. I had sectioned off a couple of hours to figure it out, as I expected various bugs to pop up (it is in beta!), but it took me fewer than 10 minutes to get it up and running.
I currently have three apps running, one is a test app, the second is the Goodreads book recommendation app I mentioned earlier, and the third is an interactive analysis of a tech survey that I spun up (from idea to functioning and deployed web app) in around an hour and a half.
Switching to Streamlit sharing has also saved me the ~$5 a month AWS bill, which I would gladly pay for this feature just for the savings in time spent on deployment alone.


Image by author

 
If I wanted to try out a new app, I could just click the new app button, point it to my repo, and they would handle literally everything else.


Image by author

 
If your Streamlit app uses any other packages, make sure to include a requirements.txt file in your repo — otherwise you’ll immediately get an error when deploying. You can use something like pip freeze to get requirements but that will give you all of the packages in the environment including those that you don’t use in your current project. And that will slow down your app deployment! So I’d suggest using something like pipreqs to keep it to just the core requirements for your app.

pip install pipreqs
pipreqs /home/project/location


If you have requirements for apt-get, add them to packages.txt -, one package per line.
 
Conclusion
 
So as a wrap-up, Streamlit sharing has saved me $ on both a development time saved and hosting cost basis (shoutout to the VC funds that make this all possible), has made my personal projects more interactive and prettier, and has taken away the headaches of deploying quick models or analyses. No wonder I’m a Streamlit fan.
Want to see more of this content? You can find me on Twitter, Substack, or on my portfolio site.
Happy Stream(lit)ing!
 
Bio: Tyler Richards is a Data Scientist at Facebook.
Original. Reposted with permission.
Related:

12-Hour Machine Learning Challenge: Build & deploy an app with Streamlit and DevOps tools
Build an app to generate photorealistic faces using TensorFlow and Streamlit
Machine Learning Model Deployment"
https://www.kdnuggets.com/2021/08/5-data-science-career-mistakes-avoid.html,5 Data Science Career Mistakes To Avoid,"Everyone makes mistakes, which can be a good thing when they lead to learning and improvements over time. But, we can also try to first learn from others to expedite our personal growth. To get started, consider these lessons learned the hard way, so you don’t have to.","comments
By Tessa Xie, Senior Data Scientist at Cruise

Photo by bruce mars on Unsplash.
 
When I first made the transition from finance to data science, I felt like I was on the top of the world — I got a job in my dream field, my career track is set, I will just keep my head down and work hard, what could go wrong? Well, there were a couple of things… For the following year as a data scientist, there were several mistakes that I’m glad I caught myself making early in my career. This way, I had time to reflect and course-correct before it was too late. After a while, I realized that these mistakes are quite common. In fact, I have observed a lot of DS around me still making these mistakes, unaware that they can hurt their data career in the long run.
If my 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist were what I learned from the best, the lessons in this article are those that I learned the hard way, and I hope I can help you avoid making the same mistakes.
 
Mistake 1: Seeing yourself as a foot soldier instead of a thought partner
 
 
Growing up, we have always been evaluated based on how well we can follow the rules and orders, especially in school. You will be the top student if you follow the textbook and practice exams and just put in the hard work. A lot of people seem to carry this “foot soldier” mindset into their working environment. In my opinion, this is the exact mindset that’s hindering a lot of data scientists from maximizing their impact and standing out from their peers. I have observed a lot of DS, especially junior ones, think they have nothing to contribute to the decision-making process and would rather retreat to the background and passively implement decisions made for them. This kicks off a vicious cycle — the less you contribute to those discussions, the less likely stakeholders will involve you in future meetings, and the less opportunity you will get to contribute in the future.
Let me give you a concrete example of the difference between a foot soldier and a thought partner in the case of model development. In the data collection and feature brainstorming meetings, the old me used to passively take notes on stakeholders’ suggestions so I can implement them “perfectly” later on. When someone proposed a feature that I knew we didn’t have data for, I would not say anything based on the assumption that they are more senior and they must know something that I overlooked. But guess what, they didn’t. I would later face the situation that 50% of the features we brainstormed would require additional data collection that would put our project deadline at risk. As a result, I often found myself in the undesirable position of the bad-news-bearing messenger in the end. Striving to be a thought partner nowadays, I involve myself early in the conversation and leverage my unique position as the person that’s closest to the data. This way, I can manage the expectations of stakeholders early on and make suggestions to help the team move forward.
How to avoid this:

Make sure you don’t hold back in meetings in which you can contribute something from the data perspective: are stakeholders’ definitions of metrics sufficient for what they want to measure? Is data available for measuring the set of metrics? If not, can we find proxies for the data we DO have?
Imposter syndrome is real, especially among junior DS. Make sure you are aware of this, and whenever you are questioning whether you should say something that “others might have already thought of” or ask a “stupid clarifying question,” YOU SHOULD.
Maintain a level of curiosity about what other people are working on. There are a lot of occasions where I found I could add value by noticing gaps other people may have overlooked due to their lack of understanding of the company’s data.

 
Mistake 2: Pigeonhole yourself into a specific area of data science
 
 
Do I want to be a data engineer or a data scientist? Do I want to work with marketing & sales data or do the geospatial analysis? You may have noticed that I have been using the term DS so far in this article as a general term for a lot of data-related career paths (e.g., data engineer, data scientist, data analyst, etc.). That’s because the lines are so blurred between these titles in the data world these days, especially in smaller companies. I have observed a lot of data scientists see themselves as ONLY data scientists building models and don’t pay attention to any business aspects or data engineers who only focus on data pipelining and don’t want to know anything about the modeling that’s going on in the company.
The best data talents are the ones who can wear multiple hats or are at least able to understand the processes of other data roles. This comes in especially handy if you want to work in an early stage or growth stage startup, where functions might not be as specialized yet, and you are expected to be flexible and cover a variety of data-related responsibilities. Even if you are in a clearly defined job profile, as you get more experience over time, you might discover that you are interested in transitioning into a different type of data role. This pivot will be much easier if you did not pigeonhole yourself and your skillset into the narrow focus of one specific role.
How to avoid this:

Again, be curious about the projects other data roles are working on. Schedule periodic meetings with colleagues to talk to each other about interesting projects or have different data teams share their work/projects with each other periodically.
If you can’t get exposure to other data roles at work, try to keep up/practice the data skills you don’t use during your free time. For example, if you are a data analyst and haven’t touched modeling in a while, consider practicing the skills through outside projects like a Kaggle competition.

 
Mistake 3: Not keeping up with the development in the field
 
 

Complacency Kills

 
Every soldier knows this, and every DS should, too. Being complacent about your data skills and not putting in the time to learn new ones is a common mistake. Doing this in the data field is more dangerous than in some other areas because data science is a field that’s relatively new and is still experiencing drastic changes and developments. There are constantly new algorithms, new tools, and even new programming languages being introduced.
If you don’t want to be that one data scientist who still only knows how to use STATA in 2021 (he exists, I worked with him), then you need to keep up with the developments in the field.
﻿
Don’t let this be you (GIF by GIPHY)
How to avoid this:

Sign up for online classes to learn about new concepts and algorithms or to brush up on the ones you already know but haven’t used in a while on the job. The ability to learn is a muscle everyone should keep practicing, and being a life-long learner is probably the best gift you can give to yourself.
Sign up for a DS newsletter or follow a DS blogger/publication on Medium and develop a habit of following the DS “news.”

 
Mistake 4: Overflexing your analytical muscle
 
 
If all you have is a hammer, everything looks like a nail. Don’t be that DS who tries to use ML on everything. When I first entered the world of data science, I was so excited about all the fancy models I learned in school and couldn’t wait to try all of them on real-world problems. But the real world is different from academic research, and the 80/20 rule is always at play.
In my previous article about “5 Lessons McKinsey Taught Me,” I wrote about how business impact and interpretability sometimes are more important than the extra several percentage points of your model’s accuracy. Sometimes maybe an assumptions-driven Excel model makes more sense than a multi-layered neural net. In those cases, don’t over-flex your analytical muscle and make your approach overkill. Instead, flex your business muscle and be the DS who also has business acumen.
How to avoid this:

Have a full range of analytical skills/tools in your armory, from simple Excel to advanced ML modeling skills, so you can always assess which tool is the best to use in the situation and not bring a gun to a knife fight.
Understand the business needs before delving into the analysis. Sometimes stakeholders would request an ML model because it’s a popular concept, and they have unrealistic expectations about what ML models can do. It’s your job as a DS to manage the expectations and help them find better and simpler ways to achieve their goals. Remember? Be a thought partner, not a foot soldier.

 
Mistake 5: Think building a data culture is someone else’s job
 
 
In my article “6 Essential Steps to Building a Great Data Culture,” I wrote about how the lives of data scientists can be horrible and unproductive if the company doesn’t have a great data culture. In fact, I have heard a lot of DS complaining about unproductive ad hoc data requests that should be easily handled by stakeholders in a self-sufficient fashion (for example, changing an aggregation from monthly to daily in Looker, which literally consists of two clicks). Don’t think changing that culture is someone else’s job. If you want to see changes, make them. After all, who is better positioned to build the data culture and educate stakeholders about data than data scientists themselves? Helping to build up the data culture in the company will make your life a lot easier down the road as well as your stakeholders.
How to avoid this:

Make it your responsibility to conduct training for the non-analytical stakeholders and develop self-serve resources.
Make sure you start practicing what you are preaching, start linking queries to slides, link data sources of truth to documents, and start documenting your code and databases. You can’t build up a data culture overnight, so it definitely takes patience.

I do want to point out that it’s OKAY to make mistakes in your career. The most important thing is to learn from those mistakes and to avoid them in the future. Or even better, write them down to help others avoid making the same mistakes.
 
Original. Reposted with permission.
Bio: Tessa Xie is an experienced Advanced Analytics Consultant skilled in data science, SQL, R, Python, Consumer Research and Economic Research with a strong engineering background following a Master's degree focused in Financial Engineering from MIT.
Related:

How a Single Mistake Wasted 3 Years of My Data Science Journey
Data Scientists think data is their #1 problem. Here’s why they’re wrong.
Learning from 3 big Data Science career mistakes"
https://www.kdnuggets.com/2021/07/brief-introduction-concept-data.html,A Brief Introduction to the Concept of Data,Every aspiring data scientist must know the concept of data and the kind of analysis they can run. This article introduces the concept of data (quantitative and qualitative) and the types of analysis.,"comments
By Angelica Lo Duca, Institute of Informatics and Telematics of the National Research Council
According to the Cambridge Dictionary [1], Data is information, especially facts or numbers, collected to be examined and considered and used to help decision-making, or information in an electronic form that can be stored and used by a computer. In other words, Data is a set of variables which can be quantitative or qualitative [2,3].
 
Data Types
 

Data can be either quantitative or qualitative. Understanding the difference between quantitative and qualitative data is very important, because they are treated and analyzed in different ways: for example, you cannot calculate statistics for qualitative data, or you cannot exploit Natural Language Processing techniques for quantitative data.

 
Quantitative Data
 

Quantitative data include data which can be expressed as numbers, thus they can be measured, counted and analysed through statistics computations. Quantitative data can be used to describe and analyze a phenomenon, in order to discover trends, compare differences and perform predictions. Often, quantitative data are already structured, thus it is quite easy to perform further analysis.
Quantitative data include:
 
1. Continuous data, which can take any numeric value.
Examples of continuous data are:

the average temperature over the years (e.g. 35°C or 84.2 °F)
price of a product over a month (e.g. $ 23.50 or 45.00 €)

Usually continuous data are distributed into an interval, which can assume both negative and positive values (interval data).
 
2. Discrete data, which can take only certain numeric values.
Examples of discrete data are:

scores of an exam, e.g 18 or 30
the shoes number, e.g. 42 EU

Usually discrete data are equidistant and non-negative (ratio data).
 
Qualitative Data
 

Qualitative data cannot be measured through standard computation techniques, because they express feelings, sensations and experiences. Qualitative data can be used to understand the context around a given phenomenon and discover new aspects. Often, qualitative data are unstructured, thus they require additional techniques to extract meaningful information.
Quantitative data include:
 
1. Nominal data, which are used to label quantities which cannot be measured, without following a specific order. Usually, nominal data group similar objects.
Examples of nominal data include:

the languages spoken by a person (e.g. English, Italian, French)
Colour palette (e.g. Red, Green)

 
2. Ordinal data, which differ from nominal data only for the fact that they can be ordered.
Examples of nominal data include:

the opinion regarding a given product (e.g. poor quality, medium quality, high quality)
the time of day (e.g. morning, afternoon, night)

 
Types of Data Analysis
 

The goal of data analysis is to discover hidden trends, patterns and relationships in the data.
According to the data type, different analyses can be performed: quantitative and qualitative analysis for quantitative and qualitative data, respectively.

 
Quantitative Analysis
 

Quantitative analysis [4] refers to quantitative data and includes the classical techniques for statistics:
 
1. Descriptive Statistics
Descriptive Statistics [5] analyses the past, by describing the basic features of data.
Descriptive Statistics is based on the calculation of some measures:

Frequency (count, percentage)
Central tendency (mean, median, mode)
Variability (maximum, minimum, range, quartile, variance)

 
2. Inferential Statistics
Inferential Statistics aims at building predictive models to understand the trend of a given phenomenon.
Inferential Statistics includes the following types of analysis:

Hypothesis testing (ANOVA, t-test, Box-Cox, …)
Confidence interval estimation

 
Qualitative Analysis
 

Qualitative analysis [6] exploits qualitative data and tries to understand data context. Since it is not possible to measure data, the following strategies can be adopted to analyse qualitative data:
 
1. Deductive Analysis
In Deductive Analysis, the researcher formulates some a-priori structures or questions to investigate data. This approach can be used when the research has at least a minimum overview of data.
 
2. Inductive Analysis
Inductive Analysis starts looking at data in the hope of extracting some useful information. This kind of analysis is quite time consuming, since it requires a deep investigation of data. Inductive Analysis is used when the researcher has no idea of data.
 
Conclusion
 
This article has introduced the basic concept of data, which include quantitative and qualitative data. Quantitative analysis focuses on numbers, while qualitative analysis focuses on categories. A great effort has been done in both types of analysis, but the research is still open.
 
References
 
[1] Cambridge Dictionary: Definition of Data:
https://dictionary.cambridge.org/dictionary/english/data
[2] Qualitative vs Quantitative Data: Definitions, Analysis, Examples:
https://www.intellspot.com/qualitative-vs-quantitative-data/
[3] ​​How to Understand the Quantitative and Qualitative Data in Your Business:
https://laconteconsulting.com/2020/02/14/quantitative-qualitative-data/
[4] Quantitative Data: Definition, Types, Analysis and Examples:
https://www.questionpro.com/blog/quantitative-data/
[5] A Gentle Introduction to Descriptive Analytics:
https://medium.com/analytics-vidhya/a-gentle-introduction-to-descriptive-analytics-8b4e8e1ad238
[6] Qualitative Data – Definition, Types, Analysis and Examples:
https://www.questionpro.com/blog/qualitative-data/
 
Bio: Angelica Lo Duca (Medium) works as post-doc at the Institute of Informatics and Telematics of the National Research Council (IIT-CNR) in Pisa, Italy. She is Professor of ""Data Journalism"" for the Master degree course in Digital Humanities at the University of Pisa. Her research interests include Data Science, Data Analysis, Text Analysis, Open Data, Web Applications and Data Journalism, applied to the fields of society, tourism and cultural heritage. She used to work on Data Security, Semantic Web and Linked Data. Angelica is also an enthusiastic tech writer.
Related:

The Brutal Truth About Data Science
Why and how should you learn “Productive Data Science”?
Top 10 Data Science Projects for Beginners"
https://www.kdnuggets.com/2021/09/top-18-low-code-no-code-machine-learning-platforms.html,Top 18 Low-Code and No-Code Machine Learning Platforms,"Machine learning becomes more accessible to companies and individuals when there is less coding involved. Especially if you are just starting your path in ML, then check out these low-code and no-code platforms to help expedite your capabilities in learning and applying AI.","comments
By Yulia Gavrilova, AI and Ethics of Tech at serokell.io.
You have probably heard the terms ‘low-code’ and ‘no-code’ before.
Low-code simply stands for a reduced amount of coding. A lot of elements can be simply dragged and dropped from the library. However, it is also possible to customize them by writing your own code, which gives increased flexibility.

No-code platforms require no knowledge of programming at all. They can be used by different people like artists, teachers, top managers. They need AI in their work but don’t want to dive deep into programming and computer science. No-code solutions are quite limited in functionality but allow you to build something simple quickly.
In practice, the border between no-code and low-code platforms is pretty thin. Platforms that promote themselves as ‘no-code’ still usually leave some space for customization.
Low-code platforms for beginners
Low-code libraries can be used even with minimal experience in coding.
 
PyCaret
 
This is an open-source machine learning library in Python that allows you to create and deploy machine learning models with minimal coding.
Basically, PyCaret is a low-code alternative that can replace hundreds of lines of code with just a few words. It greatly increases the speed of software development and makes it more accessible for beginners. PyCaret is a Python wrapper over several machine learning libraries such as scikit-learn, XGBoost, Microsoft LightGBM, spaCy, and many more.

 
Auto-ViML
 
AutoViML is a tool that enables anyone to build a machine learning model fast. It automatically renders your data through different machine learning models in order to discover which one gives the best results in each particular case. Another great plus is that you don’t have to preprocess your data because AutoViML automatically cleans, transforms, and normalizes it. The program works with different types of variables, including textual, numeral, and visual data.
 
H2O AutoML
 
H2O is an open-source machine learning platform. It has tools for deploying the most widely used machine learning algorithms such as gradient descent, linear regression, deep artificial neural networks, and others. What this platform is famous for is its cutting-edge AutoML. This feature provides for automating the process of building multiple models at once so you can create and test functional ML models even without prior experience.

No-code ML platforms you should use in 2021
Here is an assortment of no-code platforms that you can explore if you want to quickly deploy a machine learning element and integrate it with your existing software.
 
Google Cloud Auto ML
 
This no-code tool enables anyone to train and deploy custom machine learning models without any ML expertise. The platform works with different types of data and covers a broad range of use cases, from computer vision and video intelligence to natural language processing and translation. You will be able to prepare and store your datasets and use automatic tools for facilitated labeling. If you need more power and more flexible tools, you can upgrade to use Google Cloud.
 
Google ML Kit
 
This toolkit was made for Android and iOS developers who want to make their apps more engaging. Its API can be used to implement bar scanning, face detection, image labeling features, and more without having to create an ML model from scratch. All the necessary processing happens on the mobile device of the user in real-time, so there is no need for you to worry about setting up and hosting expensive servers.

 
Teachable Machine
 
Teachable Machine is another project by Google that facilitates the use of ML for apps and websites. This platform is easy to use even for non-tech-savvy people due to its user-friendly interface. The program works with images and allows you to train the machine to recognize and classify photos. It also processes sounds. The platform is interesting to play with if you’re a newbie, and it’s also free. But it is up to you to collect and prepare the data that you will use for training the model.
 
Runway AI
 
Runway AI was built for creators with no programming experience in the domains of video and photo editing with the green screen option, filtering, and other interesting features. This toolkit can help you expand your creativity with technological tools in a few simple clicks, turning your videos into top-notch cinema art.

 
Lobe
 
This ML platform has project templates that are easy to use, even for your first ML project. The project is relatively new, so only image classification is available right now. In the future, its creators also want to launch object detection and data classification templates. However, an image classifier is one of the most useful tools for retailers, advertisers, and business professionals, so be sure to check it out.
 
Obviously AI
 
If you are looking for a convenient tool for making predictions based on data without writing code, Obviously AI is for you. It can be used by marketers and business owners who want to forecast revenue flow, optimize business processes, build a more effective supply chain, and conduct personalized automated marketing campaigns. All you need is to provide data, pick a column based on which your custom ML algorithm would be created, and get your report.

 
CreateML
 
CreateML is a user-friendly drag-and-drop platform by Apple that allows you to train models on your Mac device. It can help you build classifiers and recommender systems. The tool can process images, videos, photos, tabular data, and texts. The model you get can be tested and deployed in IOS applications. You can preview the model’s performance and pause, save, resume, and extend your training process whenever you like. CreateML allows you to train multiple models on different datasets at once for a single project. It has standard Apple SDK and documentation that includes code samples and explanatory articles.

 
MakeML
 
MakeML enables iOS developers to implement object segmentation and object detection solutions. Using this tool, you can outline and edit elements not only in photos but also in videos. Create your own datasets, build custom ML models in a few clicks, and integrate your model into your app. This platform also allows you to work with AR.
 
Fritz AI
 
If you are looking for more exciting solutions for iOS and Android apps, you can also check out Fritz AI. It gives you flexibility in how much you want to be invested in ML model development ― you can train custom models in the Studio or use pre-trained models. In the program, you can create or import your own datasets, monitor the model’s performance, and re-train it. If you do Snapchat lens development, this tool will help you add no-code machine learning to your augmented reality filters.
 
SuperAnnotate
 
Making annotations to videos and texts is a tedious job, but it can be automated with SuperAnnotate. The solution covers a multitude of cases across different industries, such as aerial photography, autonomous driving, robotics, and medicine. If you quickly need to process images and you don’t want to hire a whole team of data scientists, we recommend checking it out.

 
Rapid Miner
 
RapidMiner is a tool created for data mining. It is based on the idea that business analysts or data analytics don’t necessarily have to program to do their job. At the same time, mining requires data, so the tool was equipped with a good set of operators solving a wide range of tasks for obtaining and processing information from various sources (databases, files). Overall, this tool makes data analytics simple enough for anyone to use it.
 
What-If Tool
 
This is a super useful tool to assess the performance of the models without coding. WIT visually displays how model behavior changes over time and over different subsets of data. You can also compare the performance of two models to see which one works best.
 
DataRobot
 
DataRobot is a platform that enables business analysts to build predictive analytics without knowledge of machine learning or programming. The platform uses automated machine learning (AutoML) to generate accurate predictive models in a short amount of time. DataRobot provides a user-friendly user interface for creating machine learning models. In just a few steps, a company can deploy a real-time predictive analytics service.

 
Nanonets AI
 
Intelligent document processing is possible with Nanonets. It captures data from documents automatically, saving you from hours of manual document management. Nanonets AI processes unseen, semi-structured documents even if they don’t follow a standard template, automatically validates data, and improves over time through multiple usages.
 
Monkey Learn Studio
 
MonkeyLearn Studio provides tools for working with textual data and is aimed at being used by companies. This platform can automatically tag business data, for example, support tickets or emails. It also helps with the visualization of data. MonkeyLearn makes it easy to work with machine learning because it has ready-made machine learning models that can be trained and built code-free.
Final words
These tools are cool for what they are: no-code platforms for quick deployment of simple projects by non-tech experts or newbies in ML. By no means can they substitute custom ML model development for high-load, data-intensive projects. So if you have a unique idea in mind that involves the processing of big data, automation of intensive industrial processes, or sensitive prediction models, contact us. Together, we can think of solutions that will fit your particular needs.
Original. Reposted with permission.
 
Related:

Pushing No-Code Machine Learning to the Edge
The next-generation of AutoML frameworks
Easy AutoML in Python"
https://www.kdnuggets.com/2021/03/simple-way-time-code-python.html,A Simple Way to Time Code in Python,Read on to find out how to use a decorator to time your functions.,"comments
By Edward Krueger, Senior Data Scientist and Tech Lead & Douglas Franklin, Aspiring Data Scientist and Teaching Assistant

Photo by Brad Neathery on Unsplash
 
Introduction
 
Our goal is to create an easy way to time functions in Python. We do this by coding a decorator with Python’s libraries functools and time. This decorator will then be applied to functions whose runtime we are interested in.
 
Timing Decorator: @timefunc
 
The code below represents a common decorator pattern that has a reusable and flexible structure. Notice the placement of functool.wraps. It is a decorator for our closure. This decorator preserves func’s metadata as it is passed to the closure.

Functools becomes significant on line 16, where we access func.__name__ in our print statement. If we did not use functools.wraps to decorate our closure, the wrong name would be returned.
This decorator returns the runtime of the function passed to timefunc(). On line 13, start initiates timing. Then, line 14's result stores the value of func(*args, **kwargs).After that, time_elapsed is calculated. The print statement reports func’s name and execution time.
 
Applying timefunc with the @ symbol
 
In Python, decorators can be easily applied with the @ symbol. Not all applications of decorators use this syntax, but all @ symbols are an application of a decorator.
We decorate single_thread with timefunc using the @ symbol.

Now that single_thread is decorated, when it’s called on line 13 we’ll see its func.__name__ and runtime.


Output of single_thread decorated by timefunc

 
If you want to know how this works, below we will go a little deeper into the why and how of coding a decorator to time functions.
 
Why One Might Time a Function
 
The reason is relatively straightforward. Faster functions are better functions.


Time is money, friend. — Gazlowe


The timing decorator shows us a function’s runtime. We can apply the decorator to several versions of a function, benchmark them and choose the fastest one. Additionally, it is useful to know how long executions will take when testing code. Got a five-minute runtime ahead? That's a nice window for getting up, moving your legs and refilling your coffee!
To write decorator functions in Python we rely on functools and an awareness of scope. Let's review scope and decoration.
 
Decoration, Closures and Scope
 
Decoration is a design pattern in Python that allows you to modify the behavior of a function. A decorator is a function that takes in a function and returns a modified function.
When writing closures and decorators, you must keep the scope of each function in mind. In Python, functions define scope. Closures have access to the scope of the function that returns them; the decorator’s scope.
It is important to preserve a decorated function's metadata as it is passed to a closure. Knowing our scope lets us properly decorate our closures with functools.wraps.
For more on these concepts read this three-minute piece.
Decorators and Closures by Example in Python
How to augment the behavior of a function using a decorator
 
On the reusability of this decorator
 
Notice that func is taken as an argument on line 7. Then on line 11, we pass *args, **kwargs, into our closure. These *args, **kwargs are used to calculate the result of func(*args, **kwargs) on line 10.

The flexibility of *args and **kwargsallow timefunc to work on almost any function. Our closure’s print statement is designed to access the functions __name__, args, kwargs and resultto create a useful timing output for func.
 
Conclusion
 
Decoration is a powerful tool to augment the behavior of functions. By coding a decorator to time your functions, you gain an elegant, reusable pattern to track a function’s runtime.
Feel free to copy timefunc into your codebase, or you can try coding your own timing decorator!
 
Edward Krueger is a Senior Data Scientist and Technical Lead at Business Laboratory and an Instructor at McCombs School of Business at The University of Texas at Austin.
Douglas Franklin is a Teaching Assistant at McCombs School of Business at The University of Texas at Austin.
Original. Reposted with permission.
Related:

15 common mistakes data scientists make in Python (and how to fix them)
How to Speed Up Pandas with Modin
11 Essential Code Blocks for Complete EDA (Exploratory Data Analysis)"
https://www.kdnuggets.com/2021/04/models-data-science-teams-chess-checkers.html,Models of Data Science teams: Chess vs Checkers,Should we still consider data scientists and data engineers as separate roles? When should a team grow with full-stack data developers? Introducing the Checkers-like data team.,"comments
By Marco Santoni, Data Product Manager
How many data engineers should we hire? Are they too many compared to our data scientists?
One of the key decisions to take when building a data science team is the mix of roles. This means choosing the right mix of background and of activities that each member of the team should have. I'll compare two models of teams I've experienced so far and define them as chess-team model and checkers-team model.
 
Chess-Team Model
 

The chess-team model is the common model we read about in literature. In a chess-team, each member of the team has a specific role. Roles are usually: data engineers, data scientists, and machine learning engineers. These roles typically correspond to different sets of skills (eg ML and statistics vs coding and devops) and to different set of activities (model selection vs data preparation vs model deployment).
Similarly to a chess piece which has a clear role that is different from the other pieces, a member of a data science chess-team is assigned a subset of the tasks that are part of the development pipeline. Let's consider a simplistic development pipeline:

data preparation -> data engineer
model development -> data scientists
model deployment -> machine learning engineer

The three activities of this development pipeline correspond to the three roles of the team, and there is little space for confusion. A data engineer probably won't work a lot on the model development and selection, while a data scientist probably won't be the one deploying the model in production.
 
Checkers-Team Model
 

The checkers-team model is a definition of a team model that I introduce in this post. In a checkers-team, each member of the team does not have a specific role because he may in charge of working on any step of the development pipeline. There are no roles like data engineer or data scientist because taking such a role implies limiting the scope of activities a team member should work on. Let' make an example. In a checkers-team, there is no data scientist because no one is in charge of model development only.
So, what is the role of someone working in a checkers-team? A member of the team can be defined as a full-stack data developer. A full-stack data developer is someone that for example works on data extraction AND model development AND model deployment. In a checkers-team, everyone works possibly on every piece of the development lifecycle. In this sense, the team is more similar to checkers pieces. There is no move that a piece can take and another piece cannot. Similarly, there is no activity that any team member cannot do. For example, everyone can contribute to building devops pipelines and automation.
Of course, every team member has a different background and a different set of skills from his/her teammates. One can come from a software engineering experience, another one can come from data science studies. However, the strategy of building a checkers-team is to invest in training team members to grow horizontally their set of skills.
 
Pros and Cons
 
Let's consider some key differences between a chess and a checkers team model.
Flexibility. The balance of types of activities is not stable over time in a team. There can be times when there is a peak of work items in data engineering and little or no work items in ML model development. These peaks can be due to different phases of the data product development cycle or due to varying business requirements. A checkers-team is flexible and can adapt quickly to these peaks. A checkers-team could for example dedicate the entire team to develop data engineering pipelines in a Scrum sprint if needed. The same flexibility is not as easy in a chess-team model where you have constraints due to different skills and different responsibilities.
Complexity. Not every data science team is facing the same level of complexity in their projects. Imagine a team that is building an AI model for self-driving cars. It is a complex problem to solve that requires advanced skills in computer vision and AI. These skills cannot be learned quickly but usually need a specific education or career path. When facing such problems, you need team members which are specialists in area like vision or AI. A chess-team is designed to host specialists in certain fields and is designed to grow vertically such skills. In a checkers-team, there are not such specialists.
Awareness. A member of a checkers-team knows in details every phase of the development cycle. While he is designing a ML model, he is aware at the same time of how the release pipeline and the operations of the model work. He may take decisions during model selection that take into consideration where the model will be hosted and possible constraints of the production platform. On the other hand, a data scientist of a chess-team knows less details (because he has not being working on it by himself) of how the model will be deployed and run. This minor awareness may lead to assumptions taken during model development, and these assumptions can bring to more complexity to those in charge of deploying such model.
Sense of Ownership. In a checkers-team, you are in charge of both engineering data pipelines, developing models, and deploying them. Any issue that may occur in these phases is also your issue. You can't delegate too much, and, therefore, you naturally feel responsible to contribute to the resolution. Distributing the ownership makes every team member more active in improving the development life cycle.
 
When is a Team Model Right?
 
The answer depends on the context and the organization you work at. Is the data science team is working on the core product of the company? If this is the case, the models that are developed may need a level of specialization that can't just be achieved by a checkers-team.
Or is the team rather working on adding tiny features or on improving the operations of the company? In this case, probably you won't be developing state-of-the-art AI models, and you can rely existing libraries or SaaS that make life easier for you. As complexity is not an obstacle, going for checkers-team may be a good option.
What is the size of your data science team? Or even how many teams do you have? Large organizations go for multiple data teams. These teams may be divided functionally (eg 1 team of data engineers + 1 separate team of data sciensts) or they may be divided by business units (eg 1 data team for marketing and 1 data team for recommender system). You can't of course adopt the checkers-team model in an large organization that design the data teams by functions, but you may still adopt this model in a large organization that creates multiple self-organized teams each dedicated to a specific business unit.
A last point to consider is the IT architecture. A checkers-team requires the same person to work on very different tasks. This is viable only if the complexity of such tasks is small. Adopting SaaS and PaaS resources simplifies every task by hiding the complexity of managing and running the resources. They let you focus on your goal. For example, building an API endpoint hosted by a function-as-a-service is something feasible by a data scientist with a mathematical background. Doing the same from scratch on an on-premise server is not as feasible.
Images courtesy of @pecanlie and @rafaelrex
 
Bio: Marco Santoni is currently a Data Product Manager with experience both as data scientist and software developer. He is passionate about data science and software engineering, and he is lucky to have turned them into my daily work. Always driven by curiosity and by an attitude for learning, he enjoys working both on the façade and on the foundations of a data product, ranging from machine learning to data engineering, devops, and data intensive architectures.
Original. Reposted with permission.
Related:

Six Tips on Building a Data Science Team at a Small Company
The Maslow’s hierarchy your data science team needs
The Missing Teams For Data Scientists"
https://www.kdnuggets.com/2021/08/expert-nlp-insights-music.html,NLP Insights for the Penguin Café Orchestra,We give an example of how to use Expert.ai and Python to investigate favorite music albums.,"Sponsored Post.
By Laura Gorrieri, expert.ai
Please find the notebook version of this thread here.
Let's build a small application to investigate one of my favourite artists. They are called ""The Penguin Café Orchestra"" and if you don't know them you are going to find out what they are about.
Our dataset: a list of their album's reviews that I took from Piero Scaruffi's website and saved in a dedicated folder.
Our goal: to understand more about an artist using album reviews.
Our practical goal: to see how expert.ai’s NL API works and what it can do.
What is The Penguin Café Orchestra about?
First let's see what comes out from the reviews just analysing the words used in them. We'll firstly concatenate all the reviews in one variable, in order to have a whole artist's review. Then we are going to take a look at the most frequent words in them, hoping that it will reveal more on the Penguin Café Orchestra.

## Code for iterating on the artist's folder and concatenate albums' reviews in one single artist's review
import os

artist_review = ''
artist_path = 'penguin_cafe_orchestra'
albums = os.listdir(artist_path)

for album in albums:
album_path = os.path.join(artist_path, album)
      with open(album_path, 'r', encoding = 'utf8') as file:
           review = file.read()
           artist_review += review

Using a shallow-linguistics approach we can investigate the artist review, which contains all the available reviews. To do so we use matplotlib and word cloud to produce a word cloud that will tell us more about the most frequent words in the text.
 
# Import packages

import matplotlib.pyplot as plt
%matplotlib inline

# Define a function to plot word cloud
def plot_cloud(wordcloud):
    # Set figure size
    plt.figure(figsize=(30, 10))
    # Display image
    plt.imshow(wordcloud)
    # No axis details
    plt.axis(""off"");

# Import package
from wordcloud import WordCloud, STOPWORDS

# Generate word cloud
wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='white', collocations=False, stopwords = STOPWORDS).generate(artist_review)

# Plot
plot_cloud(wordcloud)


Fig.1: A word cloud in which the most used words appear in a bigger font and the less used one in a smaller font.
How does their music make you feel?
Thanks to the word cloud, we know more about The Penguin Café Orchestra. We know that they use instruments such as the ukulele, piano and violin, and that they mix genres such as folk, ethnic, and classic.
Still, we have no idea of the style of the artist. We can know more by looking at what emotions come out of their work.
 
To do so, we are going to use expert.ai’s NL API. Please register here, find the documentation on the SDK here and on the features here.
### Install the python SDK

!pip install expertai-nlapi

## Code for initializing the client and then use the emotional-traits taxonomy

import os

from expertai.nlapi.cloud.client import ExpertAiClient
client = ExpertAiClient()

os.environ[""EAI_USERNAME""] = 'your_username'
os.environ[""EAI_PASSWORD""] = 'your_password'

emotions =[]
weights = []

output = client.classification(body={""document"": {""text"": artist_review}}, params={'taxonomy': 'emotional-traits', 'language': 'en'})

for category in output.categories:
    emotion = category.label
    weight = category.frequency
    emotions.append(emotion)
    weights.append(weight)

print(emotions)
print(weights)


['Happiness', 'Excitement', 'Joy', 'Amusement', 'Love']
[15.86, 31.73, 15.86, 31.73, 4.76]

For retrieving weights we used “frequency” which is actually a percentage.  The sum of all the frequencies is 100. This makes the frequencies of the emotions a good candidate for a pie chart, that is plotted using matplotlib.
# Import libraries

from matplotlib import pyplot as plt
import numpy as np

# Creating plot
colors = ['#0081a7','#2a9d8f','#e9c46a','#f4a261', '#e76f51']
fig = plt.figure(figsize =(10, 7))
plt.pie(weights, labels = emotions, colors=colors, autopct='%1.1f%%')

# show plot
plt.show()


Fig.2: A pie chart representing each emotion and its percentage.
What's their best album?
If you wanted to start to listen to them, to see if you feel the same emotions that Scaruffis found in their work, where could you start? We can take a look at the sentiment analysis for each album and get an idea of their best ones. To do so, we iterate on each album's review and use expert.ai NL API to retrieve their sentiment and its strength.
## Code for iterating on each album and retrieving the sentiment

sentiment_ratings = []
albums_names = [album[:-4] for album in albums]

for album in albums:
    album_path = os.path.join(artist_path, album)
    with open(album_path, 'r', encoding = 'utf8') as file:
        review = file.read()
        output = client.specific_resource_analysis(
            body={""document"": {""text"": review}}, params={'language': 'en', 'resource': 'sentiment' })
            sentiment = output.sentiment.overall sentiment_ratings.append(sentiment)

print(albums_names)
print(sentiment_ratings)

['Broadcasting From Home', 'Concert Program', 'Music From the Penguin Cafe', 'Signs of Life']
[11.6, 2.7, 10.89, 3.9]
 
Now we can visualize the sentiment for each review using a bar chart.  This will give us quick visual feedback on the best album of The Penguin Cafe Orchestra, and on their career. To do so we use once again matplotlib.

import matplotlib.pyplot as plt
plt.style.use('ggplot')

albums_names = [name[:-4] for name in albums]

plt.bar(albums_names, sentiment_ratings, color='#70A0AF') plt.ylabel(""Album rating"")
plt.title(""Ratings of Penguin Cafe Orchestra's album"")
plt.xticks(albums_names, rotation=70)
plt.show()


Originally posted here."
https://www.kdnuggets.com/2021/07/full-cross-validation-learning-curves-time-series.html,Full cross-validation and generating learning curves for time-series models,"Standard cross-validation on time series data is not possible because the data model is sequential, which does not lend well to splitting the data into statistically useful training and validation sets. However, a new approach called Reconstructive Cross-validation may pave the way toward performing this type of important analysis for predictive models with temporal datasets.","comments
By Mehmet Suzen, Theoretical Physicist | Research Scientist.
Time series analysis is needed almost in any quantitative field and real-life systems that collect data over time, i.e., temporal datasets. Building predictive models on temporal datasets for the future evolution of systems in consideration are usually called forecasting. The validation of such models deviates from the standard holdout method of having random disjoint splits of train, test, and validation sets used in supervised learning. This stems from the fact that time series are ordered, and order induces all sorts of statistical properties that should be retained. For this reason, applying direct cross-validation to time-series model building is not possible and only restricted to out-of-sample (OOS) validation, using the end-tail of a temporal set as a single test set. Recent work proposed an approach that overcomes the known limitation of achieving full cross-validation for time series. The approach opens up a possibility to produce learning curves for the time-series models as well, which is usually also not possible due to similar reasons.
 
Reconstructive Cross-validation (rCV): A meta-algorithm design principles
 
rCV is proposed recently in the paper titled Generalised learning of time-series: Ornstein-Uhlenbeck processes. The design principles of rCV for time-series aims at the following principles:

Figure 1: rCV meta-algorithm for time series cross-validation and learning curves.

Logically close to standard cross-validation: Arbitrary test-set size and number of folds.
Preserve correlations and data order.
Does not create the absurdity of predicting the past from the future data.
Applicable in a generic fashion regardless of the learning algorithm.
Applicable to multi-dimensional time series.
Evaluation metric agnostic.

 
Idea of introducing missing data: Temporal cross-validation and learning curves
 
The key idea of rCV is to create cross-validation sets via creating missing-data sets K-times, as in K-fold, with a given degree of missing ratio, i.e., random data point removal. Each fold will have a disjoint set of missing data points. By an imputation method, we would fill out the K-disjoint missing data sets and generate K-different training datasets.  This would allow us to have K-different models, and we could measure the generalised performance of the modelling approach by testing the primary model's prediction on the Out-of-sample (OOS) test set. To avoid confusion about what is a model?, what we are trying to achieve is to find out the hypothesis, i.e., the modelling approach.  By changing the ratio of missing data and repeating the cross-validation, the exercise will yield to set of the ratio of missing-missing data introduced and their corresponding rCV errors, where the plot is nothing but a learning curve from a supervised learning perspective.  Note that the imputation and prediction models are different models. The primary model we are trying to build is the prediction model we used for producing OOS predictions. The procedure is summarised in Figure 1.

Figure 2: Synthetic data and reconstructions.
 
Showcase with Gaussian process models on Ornstein-Uhlenbeck processes
 
To demonstrate the utility of rCV, the mentioned paper uses synthetic data generated by the Ornstein-Uhlenbeck process, i.e., Gaussian process with a certain parameter setting.  Figure 2 shows the synthetic data and example locations of generated missing dataset’s reconstruction errors. Figure 3 shows learning curves depending on the different ratios of the missing data setting.

Figure 3: Learning curves for the Gaussian Process model generated by rCV.
 
Conclusion
 
rCV provides a logically consistent way of practicing cross-validation in time series. It is usually not possible to produce learning curves on the same time window for the time series model, but using rCV with different ratios of missing data achieves this as well. rCV paves the way to do generalised learning for time series.
Further Reading
Apart from the paper Generalised learning of time-series: Ornstein-Uhlenbeck processes, the results can be reproduced with the Python prototype implementation here.
Original. Reposted with permission.
 
Related:

Training Sets, Test Sets, and 10-fold Cross-validation
Time Series Forecasting with PyCaret Regression Module
Visualizing Cross-validation Code"
https://www.kdnuggets.com/2020/10/fastcore-underrated-python-library.html,fastcore: An Underrated Python Library,A unique python library that extends the python programming language and provides utilities that enhance productivity.,"comments
By Hamel Husain, Staff Machine Learning Engineer at GitHub

 
Background
 
I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn't seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That's when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions.
And that's exactly what I did over the past month! I'm pleased to report that it has been the most efficient learning experience I've ever experienced. I've discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique.
For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I'm going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more!
 
Why fastcore is interesting
 

Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages.
You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems.
Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work.

 
A whirlwind tour through fastcore
 
Here are some things you can do with fastcore that immediately caught my attention.
 
Making **kwargs transparent
 
Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example:

def baz(a, b=2, c =3, d=4): return a + b + c

def foo(c, a, **kwargs):
    return c + baz(a, **kwargs)

inspect.signature(foo)




<Signature (c, a, **kwargs)>


Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates:

def baz(a, b=2, c =3, d=4): return a + b + c

@delegates(baz) # this decorator will pass down keyword arguments from baz
def foo(c, a, **kwargs):
    return c + baz(a, **kwargs)

inspect.signature(foo)




<Signature (c, a, b=2, d=4)>


You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs:

@delegates(baz, keep=True)
def foo(c, a, **kwargs):
    return c + baz(a, **kwargs)

inspect.signature(foo)




<Signature (c, a, b=2, d=4, **kwargs)>


You can also exclude arguments. For example, we exclude argument d from delegation:

def basefoo(a, b=2, c =3, d=4): pass

@delegates(basefoo, but= ['d']) # exclude `d`
def foo(c, a, **kwargs): pass

inspect.signature(foo)




<Signature (c, a, b=2)>


You can also delegate between classes:

class BaseFoo:
    def __init__(self, e, c=2): pass

@delegates()# since no argument was passsed here we delegate to the superclass
class Foo(BaseFoo):
    def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs)
        
inspect.signature(Foo)




<Signature (a, b=1, c=2)>


For more information, read the docs on delegates.
 
Avoid boilerplate when setting instance attributes
 
Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__?

class Test:
    def __init__(self, a, b ,c): 
        self.a, self.b, self.c = a, b, c



Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr:

class Test:
    def __init__(self, a, b, c): 
        store_attr()
        
t = Test(5,4,3)
assert t.b == 4



You can also exclude certain attributes:

class Test:
    def __init__(self, a, b, c): 
        store_attr(but=['c'])
    
t = Test(5,4,3)
assert t.b == 4
assert not hasattr(t, 'c')



There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail.
P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1
1. For example, store_attr does not rely on inheritance, which means you won't get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩
 
Avoiding subclassing boilerplate
 
One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example:

class ParentClass:
    def __init__(self): self.some_attr = 'hello'
        
class ChildClass(ParentClass):
    def __init__(self):
        super().__init__()

cc = ChildClass()
assert cc.some_attr == 'hello' # only accessible b/c you used super



We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass:

class NewParent(ParentClass, metaclass=PrePostInitMeta):
    def __pre_init__(self, *args, **kwargs): super().__init__()

class ChildClass(NewParent):
    def __init__(self):pass
    
sc = ChildClass()
assert sc.some_attr == 'hello' 



 
Type Dispatch
 
Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y:

collide_with(x::Asteroid, y::Asteroid) = ... 
# deal with asteroid hitting asteroid

collide_with(x::Asteroid, y::Spaceship) = ... 
# deal with asteroid hitting spaceship

collide_with(x::Spaceship, y::Asteroid) = ... 
# deal with spaceship hitting asteroid

collide_with(x::Spaceship, y::Spaceship) = ... 
# deal with spaceship hitting spaceship



Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks.
Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function:

@typedispatch
def f(x:str, y:str): return f'{x}{y}'

@typedispatch
def f(x:np.ndarray): return x.sum()

@typedispatch
def f(x:int, y:int): return x+y



Below is a demonstration of type dispatch at work for the function f:

f('Hello ', 'World!')




'Hello World!'



f(2,3)




5



f(np.array([5,5,5,5]))




20


There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask).
After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing.
 
A better version of functools.partial
 
functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values >= val:

test_input = [1,2,3,4,5,6]
def f(arr, val): 
    ""Filter a list to remove any values that are less than val.""
    return [x for x in arr if x >= val]

f(test_input, 3)




[3, 4, 5, 6]


You can create a new function out of this function using partial that sets the default value to 5:

filter5 = partial(f, val=5)
filter5(test_input)




[5, 6]


One problem with partial is that it removes the original docstring and replaces it with a generic docstring:

filter5.__doc__




'partial(func, *args, **keywords) - new function with partial application\n    of the given arguments and keywords.\n'


fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent:

filter5 = partialler(f, val=5)
filter5.__doc__




'Filter a list to remove any values that are less than val.'


 
Composition of functions
 
A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers:

def add(arr, val): return [x + val for x in arr]
def arrsum(arr): return sum(arr)

# See the previous section on partialler
add2 = partialler(add, val=2)

transform = compose(filter5, add2, arrsum)
transform([1,2,3,4,5,6])




15


But why is this useful? You might me thinking, I can accomplish the same thing with:

arrsum(add2(filter5([1,2,3,4,5,6])))



You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following:

def fit(x, transforms:list):
    ""fit a model after performing transformations""
    x = compose(*transforms)(x)
    y = [np.mean(x)] * len(x) # its a dumb model.  Don't judge me
    return y

# filters out elements < 5, adds 2, then predicts the mean
fit(x=[1,2,3,4,5,6], transforms=[filter5, add2])




[7.5, 7.5]


For more information about compose, read the docs.
 
A more useful __repr__
 
In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier).

class Test:
    def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously
    
Test(1)




<__main__.Test at 0x7ffcd766cee0>


We can use basic_repr to quickly give us a more sensible default:

class Test:
    def __init__(self, a, b=2, c=3): store_attr() 
    __repr__ = basic_repr('a,b,c')
    
Test(2)




Test(a=2, b=2, c=3)


 
Monkey Patching With A Decorator
 
It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so:

class MyClass(int): pass  

@patch
def func(self:MyClass, a): return self+a

mc = MyClass(3)



Now, MyClass has an additional method named func:

mc.func(10)




13


Still not convinced? I'll show you another example of this kind of patching in the next section.
 
A better pathlib.Path
 
When you see these extensions to pathlib.path you won't ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as:

Path.readlines: same as with open('somefile', 'r') as f: f.readlines()
Path.read: same as with open('somefile', 'r') as f: f.read()
Path.save: saves file as pickle
Path.load: loads pickle file
Path.ls: shows the contents of the path as a list.
etc.

Read more about this here. Here is a demonstration of ls:

from fastcore.utils import *
from pathlib import Path
p = Path('.')
p.ls() # you don't get this with vanilla Pathlib.Path!!




(#7) [Path('2020-09-01-fastcore.ipynb'),Path('README.md'),Path('fastcore_imgs'),Path('2020-02-20-test.ipynb'),Path('.ipynb_checkpoints'),Path('2020-02-21-introducing-fastpages.ipynb'),Path('my_icons')]


Wait! What's going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I'll go ahead and add another method to Path right now:

@patch
def fun(self:Path): return ""This is fun!""

p.fun()




'This is fun!'


That is magical, right? I know! That's why I'm writing about it!
 
An Even More Concise Way To Create Lambdas
 
Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let's create a lambda for taking the sum of a Numpy array:

arr=np.array([5,4,3,2,1])
f = lambda a: a.sum()
assert f(arr) == 15



You can use Self in the same way:

f = Self.sum()
assert f(arr) == 15



Let's create a lambda that does a groupby and max of a Pandas dataframe:

import pandas as pd
df=pd.DataFrame({'Some Column': ['a', 'a', 'b', 'b', ], 
                 'Another Column': [5, 7, 50, 70]})

f = Self.groupby('Some Column').mean()
f(df)







Another Column


Some Column





a
6


b
60



 
Read more about Self in the docs).
 
Notebook Functions
 
These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell:

from fastcore.imports import in_notebook, in_colab, in_ipython
in_notebook(), in_colab(), in_ipython()




(True, False, True)


This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment.
 
A Drop-In Replacement For List
 
You might be pretty happy with Python's list. This is one of those situations that you don't know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies.
The best way I can describe L is to pretend that list and numpy had a pretty baby:
define a list (check out the nice __repr__ that shows the length of the list!)

L(1,2,3)




(#3) [1,2,3]


Shuffle a list:

p = L.range(20).shuffle()
p




(#20) [8,7,5,12,14,16,2,15,19,6...]


Index into a list:

p[2,4,6]




(#3) [5,14,2]


L has sensible defaults, for example appending an element to a list:

1 + L(2,3,4)




(#4) [1,2,3,4]


There is much more L has to offer. Read the docs to learn more.
 
But Wait ... There's More!
 

There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn't demo in this blog post:
 
Utilities
 
The Utilites section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides.

mk_class: quickly add a bunch of attributes to a class
wrap_class: add new methods to a class with a simple decorator
groupby: similar to Scala's groupby
merge: merge dicts
fasttuple: a tuple on steroids
Infinite Lists: useful for padding and testing
chunked: for batching and organizing stuff

 
Multiprocessing
 
The Multiprocessing section extends python's multiprocessing library by offering features like:

progress bars
ability to pause to mitigate race conditions with external services
processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks

 
Functional Programming
 
The functional programming section is my favorite part of this library.

maps: a map that also composes functions
mapped: A more robust map
using_attr: compose a function that operates on an attribute

 
Transforms
 
Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post.
 
Further Reading
 
It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation.

The fastcore documentation site.
The fastcore GitHub repo.
Blog post on delegation.

 
Shameless plug: fastpages
 
This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages.
 
Bio: Hamel Husain is a Staff Machine Learning Engineer @ GitHub.
Original. Reposted with permission.
Related:

Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes
How to Automate Tasks on GitHub With Machine Learning for Fun and Profit
Automated Machine Learning — A Paradigm Shift That Accelerates Data Scientist Productivity"
https://www.kdnuggets.com/2021/03/worlddata-ai-kdnuggets-partner.html,Great News for KDnuggets subscribers! You now have access to the WorldData.AI Partners Plan at no cost,"Great News for KDnuggets subscribers! You now have access to the WorldData.AI Partners Plan at no cost, including access to some of the premium datasets only available to enterprise members. Connect your data to many of 3.5 Billion WorldData datasets and improve your Data Science and Machine Learning models! Subscribe to KDnuggets to get access.","Sponsored Post.

KDnuggets Partners with WorldData.AI to Empower Data Science Community with External Data, News Sentiment and Geo Intelligence

Brookline, MA & Houston, Texas: KDnuggets and WorldData.AI signed a partnership agreement that will join forces to empower data science community with the billions of curated External Data, News Sentiment, and Geo Intelligence for Data Science and AI initiatives.
Under the partnership agreement, KDnuggets subscribers will now have access to the WorldData.AI Partners Plan at no cost, including access to some of the premium datasets only available to enterprise members. Access to such extensive external data will allow them to connect their internal data with over 3.5 billion World Data and enhance their predictive intelligence.


If you are not a KDnuggets subscriber yet,
subscribe for free to KDnuggets here,
and get free access to WorldData.AI Partners Plan.

For any questions, please email contactus@worlddata.ai. 

WorldData.AI, the world’s largest external curated data platform, integrates data from all leading global sources. WorldData.AI database includes data on News Sentiments & Entity Analysis from a global news source, Global Geo-intelligence, billions of time series data covering data on Agriculture, Climate Change, Community, Demographics, Education, Energy (Oil & Gas), Environment, Natural Resources, Financial Market, Healthcare, Industry & Services, Labor Statistics, Macroeconomics, Finance, Politics, Population, Social Conditions, Science & Technology, Trade Macro Statistics, and Transportation.
""If utilized correctly, data can allow you to make intelligent business decisions, drive growth and improve profitability. Companies know they can gain valuable insights by analyzing the data they generate from their operations. But internally generated information can leave gaps, and companies are increasingly moving to incorporate new, nontraditional, and external sources of data into their analyses. This data can include almost anything, from historical demographic and weather data to satellite imagery and private company information. We look forward to working with KDnuggets and get Global Data Science Community access to World's largest curated external data platform,"" said Vivek Singh, Global Head of Marketing at WorldData.AI.
""Leading enterprises are now looking to connect their internal data with 1000s of external factors in real-time to enhance predictive and prescriptive intelligence. However, one of the most significant challenges Enterprises and Data Scientists faces is the lack of a comprehensive External Data source. WorldData.AI has solved this problem by streaming all major external data sources to accumulate billions of curated datasets under one roof. We are delighted to now offer WorldData.AI to our readers at no cost through this partnership,"" said Dr. Gregory Piatetsky-Shapiro, Co-Founder & President at KDnuggets.
 
About KDnuggets
KDnuggets is a leading site on AI, Analytics, Big Data, Data Science, and Machine Learning and is edited by Gregory Piatetsky-Shapiro and Matthew Mayo. KDnuggets received numerous awards/mentions as a leading publication. KDnuggets is currently reaching over 800,000 unique monthly visitors, and over 350,000 subscribers/followers via email and social media.
About WorldData.AI
WorldData.AI is a preeminent AI platform that empowers enterprises to leverage external data and build predictive intelligence. WorldData.AI offers over 3.5 billion trends and News Sentiments. WorldData.AI Machine Learning Lab enables data scientists to integrate outside data with WorldData.AI and build applications to solve a real-world problem. WorldData.AI founder Rajdeep Golecha is a recognized technology leader and entrepreneur."
https://www.kdnuggets.com/2021/09/roidna-aws-webinar-consumer-insights-data.html,Amazon Web Services Webinar: Boost customer satisfaction and sales with consumer insights data,"Join this webinar, Sep 27, to learn how to leverage external data to understand market needs and consumer behavior – helping you build a more customer-centric business.","Sponsored Post.







 



.
















                                                    How to use external consumer insights and marketing data to build
                                                    a customer-centric business
                                                






REGISTER NOW







.
                                    
























                                                                                        You're invited!
                                                                                    











                                                                                                    Mon, September 27
                                                                                                








                                                                                                    11am PT | 2pm eT
                                                                                                







                                                                                                    60 MIN SESSION
                                                                                                












REGISTER NOW















                                                                Join this webinar to learn how to leverage external data to understand market needs and consumer behavior – helping you
                                                                build a more customer-centric business.
                                                            












                                                    In this virtual session, AWS Data Exchange will host a discussion with thought leaders from companies such as Acxiom and
                                                    BlastPoint. They will share how organizations from big box retailers to automotive brands are using consumer insights
                                                    data to reach new customers, drive real business change, and increase longevity.
                                                









                                                    Key takeaways include:
                                                

















                                                                How marketers can leverage consumer-level data to reap insights in a dynamically changing marketplace
                                                            














                                                                Integrating external consumer insights data into personalized customer experiences with AI
                                                            














                                                                Visualizing data workflows using AWS Analytics tools to drive business intelligence
                                                            













                                                                How AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud
                                                            










                                        Moderator:
                                    






















                                                                                                    Claire O’Brien
                                                                                                



                                                                                                    Category Manager, Retail & Consumer Goods, AWS Data Exchange
                                                                                                














                                                                            Claire leads the Retail and Consumer Goods verticals for AWS Data Exchange’s Business Development team. In this role,
                                                                            Claire helps industry-leading providers grow their business through AWS Data Exchange and data subscribers discover and
                                                                            use third-party data on AWS.
                                                                        












                                        Presenters:
                                    






















                                                                                                    Linda Harrison
                                                                                                



                                                                                                    Director of Data Strategy, Acxiom
                                                                                                














                                                                            Linda Harrison often goes by the title of Data Guru. Her responsibilities include the ethical and appropriate use of
                                                                            third-party data and segmentation best practices for effective campaigns across all channels – email, direct mail, and
                                                                            digital.  Linda also oversees an RFP and Digital Hotline for recommendations and strategy for targeting using Acxiom and
                                                                            partner third party data. 
                                                                        































                                                                                                    Michele Fitzpatrick
                                                                                                



                                                                                                    Retail and Consumer Brands Strategy Consultant,
                                                                                                    Acxiom
                                                                                                














Michele is an experienced consumer
                                                                                experience (CX) consultant and customer relationship management (CRM)
                                                                                marketing
                                                                                strategist focused on helping her clients better understand their
                                                                                customers. By utilizing a combination of customer
                                                                                intelligence and data-enabled digital and traditional channels, Michele
                                                                                helps Acxiom’s major brands connect effectively
                                                                                with their consumers throughout the buyer journey and customer
                                                                                lifecycle.
Michele is passionate about helping brands
                                                                                connect with consumers in ways that matter to them, and which deliver
                                                                                value
                                                                                to the brand. This requires gaining unique insights into the hearts,
                                                                                minds, and behaviors of real people, then
                                                                                determining how, when, what, and most importantly, why they need
                                                                                something.
Prior to joining Acxiom, Michele held senior level CX consulting and CRM
                                                                                marketing strategy roles at MRM//McCann and
                                                                                Harte Hanks serving clients in retail, consumer brands, automotive,
                                                                                financial services, healthcare, agriculture, and
                                                                                associations.
































                                                                                                    Tomer Borenstein
                                                                                                



                                                                                                    Co-founder and Chief Technology Officer, BlastPoint
                                                                                                














                                                                            Tomer is credited with generating millions of dollars in sales and securing partnerships with a growing list of national
                                                                            and international corporate customers. A Business Times’ “30 under 30” honoree, Tomer has authored numerous articles and
                                                                            speaks regularly about the benefits of artificial intelligence and machine learning for customer engagement and business
                                                                            growth. His previous work in the startup space includes an exit to Apple. Tomer holds a Bachelor’s degree in Electrical
                                                                            and Computer Engineering and a Master’s in Machine Learning and Computer Vision from Carnegie Mellon University.
                                                                        































                                                                                                    Nam Le
                                                                                                



                                                                                                    Specialist Solutions
                                                                                                    Architect, AWS
                                                                                                














                                                                            Nam is a Specialist Solutions Architect at AWS covering AWS Marketplace, Service Catalog, Migration Services, and
                                                                            Control Tower. He helps customers implement security and governance best practices using native AWS Services and Partner
                                                                            products. He is an AWS-Certified Solutions Architect, and his skills include security, compliance, cloud computing,
                                                                            enterprise architecture, and software development. Nam has also worked as a consulting services manager, cloud
                                                                            architect, and technical marketing manager.
                                                                        












                                        *The views and opinions of Acxiom and BlastPoint are their own, and do not necessarily reflect the positions of AWS or
                                        AWS Marketplace.
                                    










About AWS Data Exchange:
                                        AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud. Once subscribed to
                                        a data
                                        product, you can use the AWS Data Exchange API to load data directly into Amazon S3 and then analyze it with a
                                        wide
                                        variety of AWS analytics and
                                        machine-learning services. Click here to browse thousands of data products now available
                                        from more than 80 qualified data providers in AWS Marketplace.
                                        Visit aws.amazon.com/data-exchange to learn more.
                                    






REGISTER NOW












                                        © 2﻿021 AWS Marketplace."
https://www.kdnuggets.com/2021/05/6-side-hustles-data-scientist.html,6 side hustles for an aspiring data scientist,"As an aspiring data scientist or an employed professional, many opportunities exist for you to offer your skills to a broader audience through side gigs. While the difficulty and risk vary, experiences from applying your data science practice to areas outside your immediate career path can increase your expertise while even increasing your bank account.","comments
By Ahmad Bin Shafiq, Machine Learning Student.

Photo by Sharon McCutcheon on Unsplash.
Data science is without a doubt the most in-demand field today. No wonder data scientists with proficient skills are handsomely rewarded in jobs across the world.
Now, you could be a data scientist who’s comfortable in the current job or an aspiring one looking to make inroads into data science. Regardless, the ideas I’m about to suggest would certainly help you upskill, earn a good side income as a data scientist, and most importantly, be your own boss.
 
1. Writing articles
 

Photo by Dan Counsell on Unsplash.
If you have a basic knowledge of data science and your writing skills are good, you can earn a handsome amount of money just by writing articles. Medium is the best platform for beginners to get started with writing. Along with this, there are different websites that pay writers for original blogs. You can write for those websites, or you can even start your own blog (this will not only help you financially, but it will also improve your resume as well). In the case of your own blog, you will write blogs related to AI/ML/DL/DS. Once your blog reaches a certain threshold (views/traffic-wise), you can apply for monetization. If your monetization is enabled, then you will earn money for every view of your blog.
 
2. Participating in Kaggle competitions
 

There are many competitions regarding data science on Kaggle. By participating in them, not only do you learn something new and improve your skills, but if you win those competitions, Kaggle offers cash prizes to its winners. Also, if you have an excellent Kaggle profile, this will definitely result in a lot of exposure from recruiters, which can help you in getting a job!
 
3. Doing freelance work
 

Photo by imtips.co.
Freelancing is the most trending way to earn money from home. Especially in 2020–21 when everything is locked down and people are in their homes. So, you can start doing freelance work regarding data science on websites like Fiverr, Upwork, freelancer, etc. And as you complete more and more projects, your profile becomes more valuable, and you start to get more orders.
To learn more about how to start your career in freelancing, refer to these articles:
5 Tips for Novice Freelance Data Scientists
How to become a freelance Data Scientist
 
4. Teaching
 

Photo by NeONBRAND on Unsplash.
There is a common saying:
The best way to learn something is to teach it
In order to become a skilled data scientist, all of your concepts should be crystal clear. So focusing on the above saying, you can teach data science at your university as an assistant professor or start an academy at home or somewhere that teaches data science. Start from the beginner level and change things along the way. By starting to teach, you not only earn a reasonable amount of money, but you also start giving back to the programmer's community.
 
5. Starting a YouTube channel
 

Photo by Sara Kurfeß on Unsplash.
Creating educational videos around the topic you have expertise in is another great way to earn revenue. YouTube offers impressive rewards to content creators. Creating educational videos also allow you to build a personal brand and create your own influencer network. On YouTube, you can upload any content as long as it doesn’t violate the content policies.
 
6. Job or Internship
 

Photo by Ant Rozetsky on Unsplash.
Recent studies show:
Internships can be an effective bridge to stable employment
This is especially true in STEM fields — such as data science — where internships are a game-changer. This is because some data science skills can only be solidified when learned in a hands-on environment.
While doing an internship, you get to learn from a team of professionals and build a strong network while gaining practical and meaningful experience. Also, internships help you financially as well, and they open many doors in the future.
Tips and Advice on How to Get Your First Data Science Internship
 
Conclusion
 
In this article, I talked about 6 ways I have used or seen others make extra income as a data scientist. These ways vary in difficulty, risk level, and the amount of money that you can make, but all of them have the potential to help you develop your skills and earn $$$.
 
 
Related:

How to become an online data science tutor
How to Succeed in Becoming a Freelance Data Scientist
Data careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space"
https://www.kdnuggets.com/2021/03/11-essential-code-blocks-exploratory-data-analysis.html,11 Essential Code Blocks for Complete EDA (Exploratory Data Analysis),This article is a practical guide to exploring any data science project and gain valuable insights.,"comments
By Susan Maina, Passionate about data, machine learning enthusiast, writer at Medium
Exploratory Data Analysis, or EDA, is one of the first steps of the data science process. It involves learning as much as possible about the data, without spending too much time. Here, you get an instinctive as well as a high-level practical understanding of the data. By the end of this process, you should have a general idea of the structure of the data set, some cleaning ideas, the target variable and, possible modeling techniques.
There are some general strategies to quickly perform EDA in most problems. In this article, I will use the Melbourne Housing snapshot dataset from kaggle to demonstrate the 11 blocks of code you can use to perform a satisfactory exploratory data analysis. The dataset includes Address, Type of Real estate, Suburb, Method of Selling, Rooms, Price, Real Estate Agent (SellerG), Date of Sale and, Distance from C.B.D. You can follow along by downloading the dataset here.
The first step is importing the libraries required. We will need Pandas, Numpy, matplotlib and seaborn. To make sure all our columns are displayed, use pd.set_option(’display.max_columns’, 100) . By default, pandas displays 20 columns and hides the rest.

import pandas as pd
pd.set_option('display.max_columns',100)import numpy as npimport matplotlib.pyplot as plt
%matplotlib inlineimport seaborn as sns
sns.set_style('darkgrid')


Panda’s pd.read_csv(path) reads in the csv file as a DataFrame.

data = pd.read_csv('melb_data.csv')


 
Basic data set Exploration
 
1. Shape (dimensions) of the DataFrame
The .shape attribute of a Pandas DataFrame gives an overall structure of the data. It returns a tuple of length 2 that translates to how many rows of observations and columns the dataset has.

data.shape### Results
(13580, 21)


We can see that the dataset has 13,580 observations and 21 features, and one of those features is the target variable.
 
2. Data types of the various columns
The DataFrame’s .dtypes attribute displays the data types of the columns as a Panda’s Series (Series means a column of values and their indices).

data.dtypes### Results
Suburb            object
Address           object
Rooms              int64
Type              object
Price            float64
Method            object
SellerG           object
Date              object
Distance         float64
Postcode         float64
Bedroom2         float64
Bathroom         float64
Car              float64
Landsize         float64
BuildingArea     float64
YearBuilt        float64
CouncilArea       object
Lattitude        float64
Longtitude       float64
Regionname        object
Propertycount    float64
dtype: object


We observe that our dataset has a combination of categorical (object) and numeric (float and int) features. At this point, I went back to the Kaggle page for an understanding of the columns and their meanings. Check out the table of columns and their definitions here created with Datawrapper.
What to look out for;

Numeric features that should be categorical and vice versa.

From a quick analysis, I did not find any mismatch for the datatypes. This makes sense as this dataset version is a cleaned snapshot of the original Melbourne data.
 
3. Display a few rows
The Pandas DataFrame has very handy functions for displaying a few observations. data.head()displays the first 5 observations, data.tail() the last 5, and data.sample() an observation chosen randomly from the dataset. You can display 5 random observations using data.sample(5)

data.head()
data.tail()
data.sample(5)





What to look out for:

Can you understand the column names? Do they make sense? (Check with the variable definitions again if needed)
Do the values in these columns make sense?
Are there significant missing values (NaN) sighted?
What types of classes do the categorical features have?

My insights; the Postcode and Propertycount features both changed according to the Suburb feature. Also, there were significant missing values for the BuildingArea and YearBuilt.
 
Distribution
 
This refers to how the values in a feature are distributed, or how often they occur. For numeric features, we’ll see how many times groups of numbers appear in a particular column, and for categorical features, the classes for each column and their frequency. We will use both graphs and actual summary statistics. The graphs enable us to get an overall idea of the distributions while the statistics give us factual numbers. These two strategies are both recommended as they complement each other.
 
Numeric Features
 
4. Plot each numeric feature
We will use Pandas histogram. A histogram groups numbers into ranges (or bins) and the height of a bar shows how many numbers fall in that range. df.hist() plots a histogram of the data’s numeric features in a grid. We will also provide the figsize and xrot arguments to increase the grid size and rotate the x-axis by 45 degrees.

data.hist(figsize=(14,14), xrot=45)
plt.show()




Histogram by author


What to look out for:

Possible outliers that cannot be explained or might be measurement errors
Numeric features that should be categorical. For example, Gender represented by 1 and 0.
Boundaries that do not make sense such as percentage values> 100.

From the histogram, I noted that BuildingArea and LandSize had potential outliers to the right. Our target feature Price was also highly skewed to the right. I also noted that YearBuilt was very skewed to the left and the boundary started at the year 1200 which was odd. Let’s move on to the summary statistics for a clearer picture.
 
5. Summary statistics of the numerical features
Now that we have an intuitive feel of the numeric features, we will look at actual statistics using df.describe()which displays their summary statistics.

data.describe()





We can see for each numeric feature, the count of values in it, the mean value, std or standard deviation, minimum value, the 25th percentile, the 50th percentile or median, the 75th percentile, and the maximum value. From the count we can also identify the features with missing values; their count is not equal to the total number of rows of the dataset. These are Car, LandSize and YearBuilt.
I noted that the minimum for both the LandSize and BuildingArea is 0. We also see that the Price ranges from 85,000 to 9,000,000 which is a big range. We will explore these columns in detailed analysis later in the project.
Looking at the YearBuilt feature, however, we note that the minimum year is 1196. This could be a possible data entry error that will be removed during cleaning.
 
Categorical features
 
6. Summary statistics of the categorical features
For categorical features, it is important to show the summary statistics before we plot graphs because some features have a lot of unique classes (like we will see for the Address) and the classes would be unreadable if visualized on a countplot.
To check the summary statistics of only the categorical features, we will use df.describe(include=’object’)

data.describe(include='object')




Categorical summary statistics by author


This table is a bit different from the one for numeric features. Here, we get the count of the values of each feature, the number of unique classes, the top most frequent class, and how frequently that class occurs in the data set.
We note that some classes have a lot of unique values such as Address, followed by Suburb and SellerG. From these findings, I will only plot the columns with 10 or less unique classes. We also note that CouncilArea has missing values.
 
7. Plot each categorical feature
Using the statistics above, we note that Type, Method and Regionname have less than 10 classes and can be effectively visualized. We will plot these features using the Seaborn countplot, which is like a histogram for categorical variables. Each bar in a countplot represents a unique class.
I created a For loop. For each categorical feature, a countplot will be displayed to show how the classes are distributed for that feature. The line df.select_dtypes(include=’object’) selects the categorical columns with their values and displays them. We will also include an If-statement so as to pick only the three columns with 10 or fewer classes using the line Series.nunique() < 10. Read the .nunique() documentation here.

for column in data.select_dtypes(include='object'):
    if data[column].nunique() < 10:
        sns.countplot(y=column, data=data)
        plt.show()




Count plots by author


What to look out for:

Sparse classes which have the potential to affect a model’s performance.
Mistakes in labeling of the classes, for example 2 exact classes with minor spelling differences.

We note that Regionname has some sparse classes which might need to be merged or re-assigned during modeling.
 
Grouping and segmentation
 
Segmentation allows us to cut the data and observe the relationship between categorical and numeric features.
 
8. Segment the target variable by categorical features.
Here, we will compare the target feature, Price, between the various classes of our main categorical features (Type, Method and Regionname) and see how the Price changes with the classes.
We use the Seaborn boxplot which plots the distribution of Price across the classes of categorical features. This tutorial, from where I borrowed the Image below, explains the boxplot’s features clearly. The dots at both ends represent outliers.


Image from www.geekeforgeeks.org


Again, I used a for loop to plot a boxplot of each categorical feature with Price.

for column in data.select_dtypes(include=’object’):
 if data[column].nunique() < 10:
 sns.boxplot(y=column, x=’Price’, data=data)
 plt.show()




Box plots by author


What to look out for:

which classes most affect the target variables.

Note how the Price is still sparsely distributed among the 3 sparse classes of Regionname seen earlier, strengthening our case against these classes.
Also note how the SA class (the least frequent Method class) commands high prices, almost similar prices of the most frequently occurring class S.
 
9. Group numeric features by each categorical feature.
Here we will see how all the other numeric features, not just Price, change with each categorical feature by summarizing the numeric features across the classes. We use the Dataframe’s groupby function to group the data by a category and calculate a metric (such as mean, median, min, std, etc) across the various numeric features.
For only the 3 categorical features with less than 10 classes, we group the data, then calculate the mean across the numeric features. We use display() which results to a cleaner table than print().

for column in data.select_dtypes(include='object'):
    if data[column].nunique() < 10:
        display(data.groupby(column).mean())











We get to compare the Type, Method and Regionname classes across the numeric features to see how they are distributed.
 
Relationships between numeric features and other numeric features
 
10. Correlations matrix for the different numerical features
A correlation is a value between -1 and 1 that amounts to how closely values of two separate features move simultaneously. A positive correlation means that as one feature increases the other one also increases, while a negative correlation means one feature increases as the other decreases. Correlations close to 0 indicate a weak relationship while closer to -1 or 1 signifies a strong relationship.


Image from edugyan.in


We will use df.corr() to calculate the correlations between the numeric features and it returns a DataFrame.

corrs = data.corr()
corrs





This might not mean much now, so let us plot a heatmap to visualize the correlations.
 
11. Heatmap of the correlations
We will use a Seaborn heatmap to plot the grid as a rectangular color-coded matrix. We use sns.heatmap(corrs, cmap=’RdBu_r’,annot=True).
The cmap=‘RdBu_r’ argument tells the heatmap what colour palette to use. A high positive correlation appears as dark red and a high negative correlation as dark blue. Closer to white signifies a weak relationship. Read this nice tutorial for other color palettes. annot=True includes the values of the correlations in the boxes for easier reading and interpretation.

plt.figure(figsize=(10,8))
sns.heatmap(corrs, cmap='RdBu_r', annot=True)
plt.show()




Heatmap by author


What to look out for:

Strongly correlated features; either dark red (positive) or dark blue(negative).
Target variable; If it has strong positive or negative relationships with other features.

We note that Rooms, Bedrooms2, Bathrooms, and Price have strong positive relationships. On the other hand, Price, our target feature, has a slightly weak negative correlation with YearBuilt and an even weaker negative relationship with Distance from CBD.
In this article, we explored the Melbourne dataset and got a high-level understanding of the structure and its features. At this stage, we do not need to be 100% comprehensive because in future stages we will explore the data more elaborately. You can get the full code on Github here. I will be uploading the dataset’s cleaning concepts soon.
 
Bio: Susan Maina is passionate about data, machine learning enthusiast, writer at Medium.
Original. Reposted with permission.
Related:

Powerful Exploratory Data Analysis in just two lines of code
Pandas Profiling: One-Line Magical Code for EDA
Statistical and Visual Exploratory Data Analysis with One Line of Code"
https://www.kdnuggets.com/2020/08/samsung-semiconductor-innovation-prevent-pandemic.html,How Semiconductor Innovation Could Help Prevent The Next Pandemic,Samsung Semiconductor technology has played a particularly essential role in the fight against Covid-19. Samsung technology powers many of the most innovative programs and AI platforms that are helping scientists conduct research and achieve breakthroughs at a speed that would have been impossible just a few years ago.,"Sponsored Post.
By Jim Elliott, Samsung Semiconductor

Over the past six months the world has been focused on the singular goal of developing treatments, vaccines, and containment strategies but what no one expected was how the tech world would rise to the challenge presented by Covid-19. While front line responders and essential workers put their lives on the line, researchers and scientists turned to artificial intelligence (AI) for answers. In mere months, emerging technologies have been rolled out and leveraged to full effect to vastly boost computing power, dramatically increase access to high-performance computing (HPC), and accelerate research by orders of magnitude. AI and HPC are being leveraged to not only assess and develop treatments but also to create potential vaccines, manage shutdowns and reopenings, analyze and enable access to digital medical records, and even to help develop better face masks.
Samsung Semiconductor technology has played a particularly essential role in the fight against Covid-19. Samsung technology powers many of the most innovative programs and AI platforms that are helping scientists conduct research and achieve breakthroughs at a speed that would have been impossible just a few years ago.
One prominent example of Samsung tech in action is the Covid-19 High Performance Computing Consortium. This consortium is a public-private initiative of IBM, the White House Office of Science and Technology Policy, and the US Department of Energy that is providing free access to HPC for groups researching and fighting the coronavirus. The consortium harnesses supercomputing resources from private companies, national laboratories, universities, and others to create an ever-expanding HPC system that (as of this writing) boasts 4.2 million CPU cores, 43,000 GPUs, and 600TB of Samsung’s new HBM2 high-bandwidth memory, providing 430 petaflops of processing power. This massive compute power enables researchers studying COVID-19 to get the answers they need in hours or days instead of weeks or months.

Source: COVID-19 High Performance Computing Consortium

 
One of many exciting projects utilizing the consortium’s HPC system is the Aaron Diamond AIDS Research Center (ADARC) at Columbia University. ADARC is studying neutralizing antibodies from convalescent COVID-19 donors in order to develop SARS-CoV-2 neutralizing antibodies that can be used in therapeutics such as monoclonal antibody treatments or ""passive"" immunization. Another research team utilizing the HPC system is led by Ryan Wang at Northeastern University. Wang’s team is studying how human contacts and voluntary contact tracing can impact the spread of COVID-19 using complex simulations. Their research might provide important insight into how to most effectively implement contact tracing in order to contain the spread of the virus.

Source: The Lancet

 
Another advanced AI system that leverages Samsung tech is Nvidia’s HBM2-equipped DGX A100. This system, which utilizes 320GB of Samsung’s new 2.4Gbps HBM2, consolidates the power of a 5-petaflop data center onto a single platform. The very first DGX A100 unit began operation at the Argonne National Laboratory in May of 2020 and is being used to conduct COVID-19 research.
Other Nvidia GPUs incorporating Samsung tech are also being used at Oxford Nanopore Technologies in the UK, where they are helping researchers sequence entire virus genomes in just 7 hours, at Plotly for real-time infection rate analysis, and at the National Institutes of Health for COVID-19 classification and reconstruction of virus spike protein structures.

Source: bioRxiv

 
At Oak Ridge National Laboratory, Nvidia V100 GPUs powered by Samsung HBM2 are being used to analyze a billion potential drug compounds per day. By early March, Oak Ridge was able to identify 77 different small-molecule drug compounds that are likely to bind to the virus’ spike protein and potentially inhibit its entry into host cells.
At the same time, Lawrence Livermore National Laboratory is upgrading its ‘Corona’ supercomputer using AMD MI50 GPUs which are also equipped with Samsung HBM2. The upgrade will almost double the supercomputer’s peak computer power and will be used for molecular modeling in support of COVID-19 research, potentially resulting in new designs for both antibodies and small molecules for therapeutics.

While the list of Covid-19 research projects leveraging Samsung tech is seemingly endless, the impact of semiconductor technology is being felt well beyond the laboratory. For example, the pandemic is also creating demand for high-bandwidth video streaming for virtual interaction between patients and frontline healthcare personnel. By providing server DRAM solutions with greater bandwidth and higher memory density per server, Samsung is helping to meet those needs as well.
The Covid-19 pandemic is an ongoing crisis that is presenting an extreme challenge to both our medical and research communities. I’m proud of the way Samsung and the semiconductor industry as a whole have risen to the challenge of supporting caregivers and researchers by rapidly rolling out the next generation of advanced technologies. As a result, lives have been saved, and the impact of the pandemic will, hopefully, be blunted.

For the time being, the pandemic is still raging and being complacent is not an option. It's also become clear that the risk of future such events is high—that this could happen all over again in 10, 20, 30 years or more. Our industry must continue to push boundaries and accelerate the rate of progress so that next time we can be more prepared.
Imagine a future in which AI can help not only predict but also prevent pandemics. And should future pandemics emerge, envision being ahead of the game with broad-spectrum protease inhibitors and other small molecule drugs, or potentially having universal vaccines for coronaviruses, Ebola, and influenzas on the ready to deploy at the first hint of an outbreak. As semiconductor technology continues to develop, these aspirations might well become reality. It is my belief that Samsung Semiconductor Inc. and the semiconductor industry on the whole can play a pivotal role in building a more prepared and more resilient world.
To learn more, please visit Samsung Semiconductor."
https://www.kdnuggets.com/2020/12/sqream-massive-data-video-challenge.html,SQream Announces Massive Data Revolution Video Challenge,"Data professionals are invited to share their massive data challenges from their own unique perspectives. Learn more about the Massive Data Revolution Video Challenge, get a $50 Amazon gift card, and be sure to submit your entry by December 16th.","Sponsored Post.

 
SQream calls on data practitioners to voice their analytics struggles in a video contest, and get a $50 Amazon gift card. 
In telecom, finance, healthcare, and virtually every other industry, organizations are experiencing an explosion of information. Big Data is becoming an outdated term as Massive Data takes hold. In this new environment, organizations are looking for ways to translate their most valuable asset into tangible business benefit. From better and faster decision-making to help enterprises face growing competition, tapping into their treasure troves of data is the key to successful business continuity and growth.
Yet as data grows to massive proportions, so do the challenges experienced by data practitioners in their daily struggle to access, prepare and analyze it. As data stores reach terabytes and petabytes, SQL queries can take hours or days, and in many cases, time out or are stopped by IT. Each change in perspective requires hours of data preparation, leaving little time for exploration and discovery. Analytic reports reflect only a small percentage of available data, leaving out critical business insights. Legacy systems that were built for significantly smaller volumes of data are simply unable to handle modern-day workloads, and data professionals are left struggling to deliver the reports required by business stakeholders.
Like many people who are looking to regain control in these turbulent times - so are data professionals, who are beginning to demand better and faster access to their key asset. The explosion of data combined with the growing demands of the industry has set the Massive Data Revolution in motion.
In light of the Revolution, SQream developed the leading data analytics acceleration platform for massive data, and is calling on data professionals to rise up and share their own massive data challenges in a 60-second video. Whether your struggles are around data preparation, ETL, SQL queries, or an inability to access and analyze enough of your data, SQream wants to hear about it from the source.
Qualifying participants will receive an Amazon gift card, as well as the opportunity to have their voice, and greatest analytics challenges, heard by industry experts, fellow practitioners, and solution providers.
""Data professionals have for too long been subject to severe limitations and unnecessary difficulties in their day-to-day work. Legacy systems are holding massive data hostage, with extremely long-running queries and the need for arduous data preparation standing in the way of critical insights,"" says David Leichner, CMO of SQream.
""What was acceptable for data volumes that existed ten or even five years ago is no longer enough. With the rise of the Massive Data Revolution, we want data practitioners to take an active part in creating a new status quo. By creating an open conversation about the biggest industry challenges, we raise the bar on what the data community expects and demands of data analytics systems and processes.""
Professionals across the data pipeline, from CIOs and CDOs to DBAs, Data Analysts, Data Engineers, and Data Scientists are invited to share their massive data challenges from their own unique perspectives. Visit this page to learn more about the Massive Data Revolution Video Challenge, and be sure to submit your entry by December 16th.
To learn how your organization can unleash the full power of its massive data, visit sqream.com."
https://www.kdnuggets.com/2021/03/evaluating-object-detection-models-using-mean-average-precision.html,Evaluating Object Detection Models Using Mean Average Precision,In this article we will see see how precision and recall are used to calculate the Mean Average Precision (mAP).,"By Ahmed Gad, KDnuggets Contributor.
comments
To evaluate object detection models like R-CNN and YOLO, the mean average precision (mAP) is used. The mAP compares the ground-truth bounding box to the detected box and returns a score. The higher the score, the more accurate the model is in its detections.
In my last article we looked in detail at the confusion matrix, model accuracy, precision, and recall. We used the Scikit-learn library to calculate these metrics as well. Now we'll extend our discussion to see how precision and recall are used to calculate the mAP.
Here are the sections covered in this tutorial:

From Prediction Score to Class Label
Precision-Recall Curve
Average Precision (AP)
Intersection over Union (IoU)
Mean Average Precision (mAP) for Object Detection

Let's get started.
 
From Prediction Score to Class Label
 
In this section we'll do a quick review of how a class label is derived from a prediction score.
Given that there are two classes, Positive and Negative, here are the ground-truth labels of 10 samples.

y_true = [""positive"", ""negative"", ""negative"", ""positive"", ""positive"", ""positive"", ""negative"", ""positive"", ""negative"", ""positive""]

 
When these samples are fed to the model it returns the following prediction scores. Based on these scores, how do we classify the samples (i.e. assign a class label to each sample)?

pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3]

 
To convert the scores into a class label, a threshold is used. When the score is equal to or above the threshold, the sample is classified as one class. Otherwise, it is classified as the other class. Let's agree that a sample is Positive if its score is above or equal to the threshold. Otherwise, it is Negative. The next block of code converts the scores into class labels with a threshold of 0.5.

import numpy

pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3]
y_true = [""positive"", ""negative"", ""negative"", ""positive"", ""positive"", ""positive"", ""negative"", ""positive"", ""negative"", ""positive""]

threshold = 0.5
y_pred = [""positive"" if score >= threshold else ""negative"" for score in pred_scores]
print(y_pred)

 

['positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative']

 
Now both the ground-truth and predicted labels are available in the y_true and y_pred variables. Based on these labels, the confusion matrix, precision, and recall can be calculated.

r = numpy.flip(sklearn.metrics.confusion_matrix(y_true, y_pred))
print(r)

precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=""positive"")
print(precision)

recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=""positive"")
print(recall)

 

# Confusion Matrix (From Left to Right & Top to Bottom: True Positive, False Negative, False Positive, True Negative)
[[4 2]
 [1 3]]

# Precision = 4/(4+1)
0.8

# Recall = 4/(4+2)
0.6666666666666666

 
After this quick review of calculating the precision and recall, in the next section we'll discuss creating the precision-recall curve.
 
Precision-Recall Curve
 
From the definition of both the precision and recall given in Part 1, remember that the higher the precision, the more confident the model is when it classifies a sample as Positive. The higher the recall, the more positive samples the model correctly classified as Positive.
 
When a model has high recall but low precision, then the model classifies most of the positive samples correctly but it has many false positives (i.e. classifies many Negative samples as Positive). When a model has high precision but low recall, then the model is accurate when it classifies a sample as Positive but it may classify only some of the positive samples.
 
Due to the importance of both precision and recall, there is a precision-recall curve the shows the tradeoff between the precision and recall values for different thresholds. This curve helps to select the best threshold to maximize both metrics.
There are some inputs needed to create the precision-recall curve:

The ground-truth labels.
The prediction scores of the samples.
Some thresholds to convert the prediction scores into class labels.

The next block of code creates the y_true list to hold the ground-truth labels, the pred_scores list for the prediction scores, and finally the thresholds list for different threshold values.

import numpy

y_true = [""positive"", ""negative"", ""negative"", ""positive"", ""positive"", ""positive"", ""negative"", ""positive"", ""negative"", ""positive"", ""positive"", ""positive"", ""positive"", ""negative"", ""negative"", ""negative""]

pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3, 0.7, 0.5, 0.8, 0.2, 0.3, 0.35]

thresholds = numpy.arange(start=0.2, stop=0.7, step=0.05)

 
Here are the thresholds saved in the thresholds list. Because there are 10 thresholds, 10 values for precision and recall will be created.

[0.2, 
 0.25, 
 0.3, 
 0.35, 
 0.4, 
 0.45, 
 0.5, 
 0.55, 
 0.6, 
 0.65]

 
The next function named precision_recall_curve() accepts the ground-truth labels, prediction scores, and thresholds. It returns two equal-length lists representing the precision and recall values.

import sklearn.metrics

def precision_recall_curve(y_true, pred_scores, thresholds):
    precisions = []
    recalls = []
    
    for threshold in thresholds:
        y_pred = [""positive"" if score >= threshold else ""negative"" for score in pred_scores]

        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=""positive"")
        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=""positive"")
        
        precisions.append(precision)
        recalls.append(recall)

    return precisions, recalls

 
The next code calls the precision_recall_curve() function after passing the three previously prepared lists. It returns the precisions and recalls lists that hold all the values of the precisions and recalls, respectively.

precisions, recalls = precision_recall_curve(y_true=y_true, 
                                             pred_scores=pred_scores,
                                             thresholds=thresholds)

 
Here are the returned values in the precisions list.

[0.5625,
 0.5714285714285714,
 0.5714285714285714,
 0.6363636363636364,
 0.7,
 0.875,
 0.875,
 1.0,
 1.0,
 1.0]

 
Here is the list of  values in the recalls list.

[1.0,
 0.8888888888888888,
 0.8888888888888888,
 0.7777777777777778,
 0.7777777777777778,
 0.7777777777777778,
 0.7777777777777778,
 0.6666666666666666,
 0.5555555555555556,
 0.4444444444444444]

 
Given the two lists of equal lengths, it is possible to plot their values in a 2D plot as shown below.

matplotlib.pyplot.plot(recalls, precisions, linewidth=4, color=""red"")
matplotlib.pyplot.xlabel(""Recall"", fontsize=12, fontweight='bold')
matplotlib.pyplot.ylabel(""Precision"", fontsize=12, fontweight='bold')
matplotlib.pyplot.title(""Precision-Recall Curve"", fontsize=15, fontweight=""bold"")
matplotlib.pyplot.show()

 
The precision-recall curve is shown in the next figure. Note that as the recall increases, the precision decreases. The reason is that when the number of positive samples increases (high recall), the accuracy of classifying each sample correctly decreases (low precision). This is expected, as the model is more likely to fail when there are many samples.

The precision-recall curve makes it easy to decide the point where both the precision and recall are high. According to the previous figure, the best point is (recall, precision)=(0.778, 0.875).
Graphically deciding the best values for both the precision and recall might work using the previous figure because the curve is not complex. A better way is to use a metric called the f1 score, which is calculated according to the next equation.

The f1 metric measures the balance between precision and recall. When the value of f1 is high, this means both the precision and recall are high. A lower f1 score means a greater imbalance between precision and recall.
According to the previous example, the f1 is calculated according to the code below. According to the values in the f1 list, the highest score is 0.82352941. It is the 6th element in the list (i.e. index 5). The 6th elements in the recalls and precisions lists are 0.778 and 0.875, respectively. The corresponding threshold value is 0.45.

f1 = 2 * ((numpy.array(precisions) * numpy.array(recalls)) / (numpy.array(precisions) + numpy.array(recalls)))

 

[0.72, 
 0.69565217, 
 0.69565217, 
 0.7,
 0.73684211,
 0.82352941, 
 0.82352941, 
 0.8, 
 0.71428571, 0
 .61538462]

 
The next figure shows, in blue, the location of the point that corresponds to the best balance between the recall and the precision. In conclusion, the best threshold to balance the precision and recall is 0.45 at which the precision is 0.875 and the recall is 0.778.

matplotlib.pyplot.plot(recalls, precisions, linewidth=4, color=""red"", zorder=0)
matplotlib.pyplot.scatter(recalls[5], precisions[5], zorder=1, linewidth=6)

matplotlib.pyplot.xlabel(""Recall"", fontsize=12, fontweight='bold')
matplotlib.pyplot.ylabel(""Precision"", fontsize=12, fontweight='bold')
matplotlib.pyplot.title(""Precision-Recall Curve"", fontsize=15, fontweight=""bold"")
matplotlib.pyplot.show()

 

After the precision-recall curve is discussed, the next section discusses how to calculate the average precision.
 
Average Precision (AP)
 
The average precision (AP) is a way to summarize the precision-recall curve into a single value representing the average of all precisions. The AP is calculated according to the next equation. Using a loop that goes through all precisions/recalls, the difference between the current and next recalls is calculated and then multiplied by the current precision. In other words, the AP is the weighted sum of precisions at each threshold where the weight is the increase in recall.

It is important to append the recalls and precisions lists by 0 and 1, respectively. For example, if the recalls list is0.8,0.60.8,0.6, then it should have 0 appended to be0.8,0.6,0.00.8,0.6,0.0. The same happens for the precisions list but have 1 rather than 0 appended (e.g.0.8,0.2,1.00.8,0.2,1.0).
Given that both recalls and precisions are NumPy arrays, the previous equation is modeled according to the next Python line.

AP = numpy.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])

 
Here is the complete code that calculates the AP.

import numpy
import sklearn.metrics

def precision_recall_curve(y_true, pred_scores, thresholds):
    precisions = []
    recalls = []
    
    for threshold in thresholds:
        y_pred = [""positive"" if score >= threshold else ""negative"" for score in pred_scores]

        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=""positive"")
        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=""positive"")
        
        precisions.append(precision)
        recalls.append(recall)

    return precisions, recalls

y_true = [""positive"", ""negative"", ""negative"", ""positive"", ""positive"", ""positive"", ""negative"", ""positive"", ""negative"", ""positive"", ""positive"", ""positive"", ""positive"", ""negative"", ""negative"", ""negative""]
pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3, 0.7, 0.5, 0.8, 0.2, 0.3, 0.35]
thresholds=numpy.arange(start=0.2, stop=0.7, step=0.05)

precisions, recalls = precision_recall_curve(y_true=y_true, 
                                             pred_scores=pred_scores, 
                                             thresholds=thresholds)

precisions.append(1)
recalls.append(0)

precisions = numpy.array(precisions)
recalls = numpy.array(recalls)

AP = numpy.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])
print(AP)

 
This is all about the average precision. Here is a summary of the steps to calculate the AP:

Generate the prediction scores using the model.
Convert the prediction scores to class labels.
Calculate the confusion matrix.
Calculate the precision and recall metrics.
Create the precision-recall curve.
Measure the average precision.

The next section talks about the intersection over union (IoU) which is how an object detection generates the prediction scores.
 
Intersection over Union (IoU)
 
To train an object detection model, usually, there are 2 inputs:

An image.
Ground-truth bounding boxes for each object in the image.

The model predicts the bounding boxes of the detected objects. It is expected that the predicted box will not match exactly the ground-truth box. The next figure shows a cat image. The ground-truth box of the object is in red while the predicted one is in yellow. Based on the visualization of the 2 boxes, is the model made a good prediction with a high match score?
It is difficult to subjectively evaluate the model predictions. For example, someone may conclude that there is a 50% match while someone else notices that there is a 60% match.

Image without labels from Pixabay by susannp4

 
A better alternative is to use a quantitative measure to score how the ground-truth and predicted boxes match. This measure is the intersection over union (IoU). The IoU helps to know if a region has an object or not.
The IoU is calculated according to the next equation by dividing the area of intersection between the 2 boxes by the area of their union. The higher the IoU, the better the prediction.

The next figure shows 3 cases with different IoUs. Note that the IoUs at the top of each case are objectively measured and may differ a bit from the reality but it makes sense.
For case A, the predicted box in yellow is so far from being aligned on the red ground-truth box and thus the IoU score is 0.2 (i.e. there is only a 20% overlap between the 2 boxes).
For case B, the intersection area between the 2 boxes is larger but the 2 boxes are still not aligned well and thus the IoU score is 0.5.
For case C, the coordinates of the 2 boxes are so close and thus their IoU is 0.9 (i.e. there is a 90% overlap between the 2 boxes).
Note that the IoU is 0.0 when there is a 0% overlap between the predicted and ground-truth boxes. The IoU is 1.0 when the 2 boxes fit each other 100%.

To calculate the IoU for an image, here is a function named intersection_over_union(). It accepts the following 2 parameters:

gt_box: Ground-truth bounding box.
pred_box: Predicted bounding box.

It calculates the intersection and union between the 2 boxes in the intersection and union variables, respectively. Moreover, the IoU is calculated in the iou variable. It returns all of these 3 variables.

def intersection_over_union(gt_box, pred_box):
    inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]
    inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]

    inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]
    inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]

    intersection = inter_box_w * inter_box_h
    union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection
    
    iou = intersection / union

    return iou, intersection, union

 
The bounding box passed to the function is a list of 4 elements which are:

The x-axis of the top-left corner.
The y-axis of the top-left corner.
Width.
Height.

Here are the ground-truth and predicted bounding boxes of the car image.

gt_box = [320, 220, 680, 900]
pred_box = [500, 320, 550, 700]

 
Given that the image is named cat.jpg, here is the complete that draws the bounding boxes over the image.

import imageio
import matplotlib.pyplot
import matplotlib.patches

def intersection_over_union(gt_box, pred_box):
    inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]
    inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]

    inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]
    inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]

    intersection = inter_box_w * inter_box_h
    union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection
    
    iou = intersection / union

    return iou, intersection, union

im = imageio.imread(""cat.jpg"")

gt_box = [320, 220, 680, 900]
pred_box = [500, 320, 550, 700]

fig, ax = matplotlib.pyplot.subplots(1)
ax.imshow(im)

gt_rect = matplotlib.patches.Rectangle((gt_box[0], gt_box[1]),
                                       gt_box[2],
                                       gt_box[3],
                                       linewidth=5,
                                       edgecolor='r',
                                       facecolor='none')

pred_rect = matplotlib.patches.Rectangle((pred_box[0], pred_box[1]),
                                         pred_box[2],
                                         pred_box[3],
                                         linewidth=5,
                                         edgecolor=(1, 1, 0),
                                         facecolor='none')
ax.add_patch(gt_rect)
ax.add_patch(pred_rect)

ax.axes.get_xaxis().set_ticks([])
ax.axes.get_yaxis().set_ticks([])

 
The next figure shows the image with the bounding boxes.

To calculate the IoU, just call the intersection_over_union() function. Based on the bounding boxes, the IoU score is 0.54.

iou, intersect, union = intersection_over_union(gt_box, pred_box)
print(iou, intersect, union)

 

0.5409582689335394 350000 647000

 
The IoU score 0.54 means there is a 54% overlap between the ground-truth and predicted bounding boxes. Looking at the boxes, someone may visually feel it is good enough to conclude that the model detected the cat object. Someone else may feel the model is not yet accurate as the predicted box does not fit the ground-truth box well.
To objectively judge whether the model predicted the box location correctly or not, a threshold is used. If the model predicts a box with an IoU score greater than or equal to the threshold, then there is a high overlap between the predicted box and one of the ground-truth boxes. This means the model was able to detect an object successfully. The detected region is classified as Positive (i.e. contains an object).
On the other hand, when the IoU score is smaller than the threshold, then the model made a bad prediction as the predicted box does not overlap with the ground-truth box. This means the detected region is classified as Negative (i.e. does not contain an object).

Let's have an example to clarify how the IoU scores help to classify a region as an object or not. Assume the object detection model is fed by the next image where there are 2 target objects with their ground-truth boxes in red and the predicted boxes are in yellow.
The next code reads the image (given it is named pets.jpg), draws the boxes, and calculates the IoU for each object. The IoU for the left object is 0.76 while the other object has an IoU score of 0.26.

import matplotlib.pyplot
import matplotlib.patches
import imageio

def intersection_over_union(gt_box, pred_box):
    inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]
    inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]

    inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]
    inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]

    intersection = inter_box_w * inter_box_h
    union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection
    
    iou = intersection / union

    return iou, intersection, union, 

im = imageio.imread(""pets.jpg"")

gt_box = [10, 130, 370, 350]
pred_box = [30, 100, 370, 350]

iou, intersect, union = intersection_over_union(gt_box, pred_box)
print(iou, intersect, union)

fig, ax = matplotlib.pyplot.subplots(1)
ax.imshow(im)

gt_rect = matplotlib.patches.Rectangle((gt_box[0], gt_box[1]),
                                       gt_box[2],
                                       gt_box[3],
                                       linewidth=5,
                                       edgecolor='r',
                                       facecolor='none')

pred_rect = matplotlib.patches.Rectangle((pred_box[0], pred_box[1]),
                                         pred_box[2],
                                         pred_box[3],
                                         linewidth=5,
                                         edgecolor=(1, 1, 0),
                                         facecolor='none')
ax.add_patch(gt_rect)
ax.add_patch(pred_rect)

gt_box = [645, 130, 310, 320]
pred_box = [500, 60, 310, 320]

iou, intersect, union = intersection_over_union(gt_box, pred_box)
print(iou, intersect, union)

gt_rect = matplotlib.patches.Rectangle((gt_box[0], gt_box[1]),
                                       gt_box[2],
                                       gt_box[3],
                                       linewidth=5,
                                       edgecolor='r',
                                       facecolor='none')

pred_rect = matplotlib.patches.Rectangle((pred_box[0], pred_box[1]),
                                         pred_box[2],
                                         pred_box[3],
                                         linewidth=5,
                                         edgecolor=(1, 1, 0),
                                         facecolor='none')
ax.add_patch(gt_rect)
ax.add_patch(pred_rect)

ax.axes.get_xaxis().set_ticks([])
ax.axes.get_yaxis().set_ticks([])

 
Given that the IoU threshold is 0.6, then only the regions with IoU scores greater than or equal to 0.6 are classified as Positive (i.e. having objects). Thus, the box with IoU score 0.76 is Positive while the other box with IoU of 0.26 is Negative.

Image without Labels from hindustantimes.com

 
If the threshold changed to be 0.2 rather than 0.6, then both predictions are Positive. If the threshold is 0.8, then both predictions are Negative.
As a summary, the IoU score measures how close is the predicted box to the ground-truth box. It ranges from 0.0 to 1.0 where 1.0 is the optimal result. When the IoU is greater than the threshold, then the box is classified as Positive as it surrounds an object. Otherwise, it is classified as Negative.
The next section shows how to benefit from the IoUs to calculate the mean average precision (mAP) for an object detection model.
 
Mean Average Precision (mAP) for Object Detection
 
Usually, the object detection models are evaluated with different IoU thresholds where each threshold may give different predictions from the other thresholds. Assume that the model is fed by an image that has 10 objects distributed across 2 classes. How to calculate the mAP?
To calculate the mAP, start by calculating the AP for each class. The mean of the APs for all classes is the mAP.
Assuming that the dataset used has only 2 classes. For the first class, here are the ground-truth labels and predicted scores in the y_true and pred_scores variables, respectively.

y_true = [""positive"", ""negative"", ""positive"", ""negative"", ""positive"", ""positive"", ""positive"", ""negative"", ""positive"", ""negative""]

pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.75, 0.2, 0.8, 0.3]

 
Here are the y_true and pred_scores variables of the second class.

y_true = [""negative"", ""positive"", ""positive"", ""negative"", ""negative"", ""positive"", ""positive"", ""positive"", ""negative"", ""positive""]

pred_scores = [0.32, 0.9, 0.5, 0.1, 0.25, 0.9, 0.55, 0.3, 0.35, 0.85]

 
The list of IoU thresholds starts from 0.2 to 0.9 with 0.25 step.

thresholds = numpy.arange(start=0.2, stop=0.9, step=0.05)

 
To calculate the AP for a class, just feed its y_true and pred_scores variables to the next code.

precisions, recalls = precision_recall_curve(y_true=y_true, 
                                             pred_scores=pred_scores, 
                                             thresholds=thresholds)

matplotlib.pyplot.plot(recalls, precisions, linewidth=4, color=""red"", zorder=0)

matplotlib.pyplot.xlabel(""Recall"", fontsize=12, fontweight='bold')
matplotlib.pyplot.ylabel(""Precision"", fontsize=12, fontweight='bold')
matplotlib.pyplot.title(""Precision-Recall Curve"", fontsize=15, fontweight=""bold"")
matplotlib.pyplot.show()

precisions.append(1)
recalls.append(0)

precisions = numpy.array(precisions)
recalls = numpy.array(recalls)

AP = numpy.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])
print(AP)

 
For the first class, here is its precision-recall curve. Based on this curve, the AP is 0.949.

The precision-recall curve of the second class is shown below. Its AP is 0.958.

Based on the APs of the 2 classes (0.949 and 0.958), the mAP of the object detection model is calculated according to the next equation.

Based on this equation, the mAP is 0.9535.

mAP = (0.949 + 0.958)/2 = 0.9535

 
 
Conclusion
 
This tutorial discussed how to calculate the mean average precision (mAP) for an object detection model. We started by discussing how to convert a prediction score to a class label. Using different thresholds, a precision-recall curve is created. From that curve, the average precision (AP) is measured.
For an object detection model, the threshold is the intersection over union (IoU) that scores the detected objects. Once the AP is measured for each class in the dataset, the mAP is calculated.
 
Bio: Ahmed Gad received his B.Sc. degree with excellent with honors in information technology from the Faculty of Computers and Information (FCI), Menoufia University, Egypt, in July 2015. For being ranked first in his faculty, he was recommended to work as a teaching assistant in one of the Egyptian institutes in 2015 and then in 2016 to work as a teaching assistant and a researcher in his faculty. His current research interests include deep learning, machine learning, artificial intelligence, digital signal processing, and computer vision.
Original. Reposted with permission.
Related:

Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall
Working With The Lambda Layer in Keras
How to Create Custom Real-time Plots in Deep Learning"
https://www.kdnuggets.com/2021/07/overview-albumentations-open-source-library-advanced-image-augmentations.html,Overview of Albumentations: Open-source library for advanced image augmentations,With code snippets on augmentations and integrations with PyTorch and Tensorflow pipelines.,"comments
By Olga Chernytska, Senior Machine Learning Engineer
Native PyTorch and TensorFlow augmenters have a big disadvantage – they cannot simultaneously augment an image and its segmentation mask, bounding box, or keypoint locations. So there are two options – either write functions on your own or use third-party libraries. I tried both, and the second option is just better 🙂
 
Why Albumentations?
 
Albumentations was the first library that I’ve tried, and I’ve stuck with it, because:

It is open-source,
Intuitive,
Fast,
Has more than 60 different augmentations,
Well-documented,
And, what is most important, can simultaneously augment an image and its segmentation mask, bounding box, or keypoint locations.

There are two more similar libraries – imgaug and Augmentor. Unfortunately, I cannot provide any comparison, as I haven’t tried them yet. Till this moment Albumentations was just enough.
 
Short Tutorial
 
In this short tutorial, I’ll show how to augment images for segmentation and object detection tasks – easily with few lines of code.
If you’d like to follow this tutorial:

Install Albumentations. I really recommend checking if you have the latest version, as older ones may be buggy. I use version ‘1.0.0’ and it works fine.
Download a test image with labels below. It is just a random image from COCO dataset. I modified it a bit and stored it in the format required by Albumentations. This library accepts images as NumPy arrays, segmentation masks as NumPy arrays, and bounding boxes as lists.

Download
Let’s load the image, its binary pixel-wise segmentation mask, and a bounding box. The bounding box is defined as a 4-element list – [x_min, y_min, width, height].

import pickle
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# load data
with open(""image_data.pickle"", ""rb"") as handle:
    image_data = pickle.load(handle)

image = image_data[""image""]
mask = image_data[""mask""]
bbox = image_data[""bbox_coco""]

# visualize data
fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].imshow(image)
ax[0].set_title(""Image"")
ax[1].imshow(image)
bbox_rect = patches.Rectangle(
    bbox[:2], bbox[2], bbox[3], linewidth=2, edgecolor=""r"", facecolor=""none""
)
ax[1].add_patch(bbox_rect)
ax[1].imshow(mask, alpha=0.3, cmap=""gray_r"")
ax[1].set_title(""Image + BBox + Mask"")
plt.show()


After loading and visualizing the image, you should get this:
Image. The output when running code for image and its labels visualization.
Segmentation mask is visualized as a transparent black-white image (1 is black, ‘horse’).

 
Mask Augmentation for Segmentation. And now we can start with Albumentations. Transformations here are defined very similarly to PyTorch and TensorFlow (Keras API):

Define transformation via combining several augmentations using a Compose object.
Each augmentation has argument `p`, the probability to be applied, and additionally augmentations-specific arguments, like `width` and `height` for RandomCrop.
Use defined transformation as a function to augment the image and its mask. This function returns a dictionary with keys – `image` and `mask`.

Below is the code on how to augment the image (and its mask) with random 256×256 crop (always) and horizontal flip (only in 50% cases).

import albumentations as A

# define augmentation
transform = A.Compose([
    A.RandomCrop(width=256, height=256, p=1), 
    A.HorizontalFlip(p=0.5),
])

# augment and visualize images
fig, ax = plt.subplots(2, 3, figsize=(15, 10))
for i in range(6):
    transformed = transform(image=image, mask=mask) 
    ax[i // 3, i % 3].imshow(transformed[""image""])
    ax[i // 3, i % 3].imshow(transformed[""mask""], alpha=0.3, cmap=""gray_r"")
plt.show()


As a result, you should get something like this. Your augmented images will be different, as Albumentations produces random transformations. For a detailed tutorial on mask augmentation refer to original documentation.
Image. The output when running code for simultaneous image and mask augmentation.
Segmentation mask is visualized as a transparent black-white image (1 is black, ‘horse’)

 
Bounding Boxes Augmentation for Object Detection. It is similar to augmentation for segmentation masks, however:

Additionally, define `bbox_params`, where specify the format of the bounding box and argument for bounding box classes. `coco` means bounding box in COCO dataset format – [x_min, y_min, width, height]. And argument `bbox_classes` will be used later to pass classes for bounding boxes.
`transform` accepts bounding boxes as a list of lists. Additionally, it requires bounding box classes (as a list) even if there is a single bounding box in the image.

Below is the code that does RandomCrop and HorizonalFrip simultaneously for the image and its bounding box.

# define augmentation
transform = A.Compose([
    A.RandomCrop(width=256, height=256, p=1),
    A.HorizontalFlip(p=0.5),
], bbox_params=A.BboxParams(format='coco', label_fields=[""bbox_classes""]))

# augment and visualize images
bboxes = [bbox] #`transform` accepts bounding boxes as a list of lists.
bbox_classes = [""horse""]

fig, ax = plt.subplots(2, 3, figsize=(15, 10))
for i in range(6):
    transformed = transform(
        image=image, 
        bboxes=bboxes, 
        bbox_classes=bbox_classes
    )
    ax[i // 3, i % 3].imshow(transformed[""image""])
    trans_bbox = transformed[""bboxes""][0]
    bbox_rect = patches.Rectangle(
        trans_bbox[:2],
        trans_bbox[2],
        trans_bbox[3],
        linewidth=2,
        edgecolor=""r"",
        facecolor=""none"",
    )
    ax[i // 3, i % 3].add_patch(bbox_rect)
plt.show()


And here are the results. In case you need some specific bounding box augmentations – refer to the original documentation.
Image. The output when running code for simultaneous image and bounding box augmentation.

 
Simultaneous augmentation of multiple targets. Besides allowing to simultaneously augment several masks or several bounding boxes, Albumentations has a feature to simultaneously augment different types of labels, for instance, a mask and a bounding box.
When calling a `transform` simply give it everything you have:

# define augmentation
transform = A.Compose([
    A.RandomCrop(width=256, height=256, p=1),
    A.HorizontalFlip(p=0.5),
], bbox_params=A.BboxParams(format='coco', label_fields=[""bbox_classes""]))

# augment and visualize images
bboxes = [bbox]
bbox_classes = [""horse""]

fig, ax = plt.subplots(2, 3, figsize=(15, 10))
for i in range(6):
    transformed = transform(
        image=image, 
        mask=mask, 
        bboxes=bboxes, 
        bbox_classes=bbox_classes
    )
    ax[i // 3, i % 3].imshow(transformed[""image""])
    trans_bbox = transformed[""bboxes""][0]
    bbox_rect = patches.Rectangle(
        trans_bbox[:2],
        trans_bbox[2],
        trans_bbox[3],
        linewidth=2,
        edgecolor=""r"",
        facecolor=""none"",
    )
    ax[i // 3, i % 3].add_patch(bbox_rect)
    ax[i // 3, i % 3].imshow(transformed[""mask""], alpha=0.3, cmap=""gray_r"")
plt.show()


Your result will look like in the image below. And here is more detailed documentation on that.
Image. The output when running code for a simultaneous image, segmentation mask, and bounding box augmentation.
Segmentation mask is visualized as a transparent black-white image (1 is black, ‘horse’).

 
And More. Albumentations has much more features available, such as augmentation for keypoints and AutoAugment. And it includes about 60 different augmentation types – literally for any task you need.
 
Compatibility with PyTorch & TensorFlow
 
Most likely you are going to use Albumentations as a part of PyTorch or TensorFlow training pipeline, so I’ll briefly describe how to do it.
PyTorch. When creating a Custom dataset, define Albumentations transform in the `__init__` function and call it in the `__getitem__` function. PyTorch models require input data to be tensors, so make sure you add `ToTensorV2` as the last step when defining `transform` (a trick from one of Albumentations tutorials).

from torch.utils.data import Dataset
from albumentations.pytorch import ToTensorV2

class CustomDataset(Dataset):
    def __init__(self, images, masks):
        self.images = images  # assume it's a list of numpy images
        self.masks = masks  # assume it's a list of numpy masks
        self.transform = A.Compose([
            A.RandomCrop(width=256, height=256, p=1),
            A.HorizontalFlip(p=0.5),
            ToTensorV2(),
        ])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        mask = self.masks[idx]
        transformed = self.transform(image=image, mask=mask)
        transformed_image = transformed[""image""]
        transformed_mask = transformed[""mask""]
        return transformed_image, transformed_mask


TensorFlow (Keras API) also allows creating Custom datasets, similar to PyTorch. So define Albumentations transform in the `__init__` function and call it in the `__getitem__` function. Pretty simple, isn’t it?

from tensorflow import keras

class CustomDataset(keras.utils.Sequence):
    def __init__(self, images, masks):
        self.images = images
        self.masks = masks
        self.batch_size = 1
        self.img_size = (256, 256)
        self.transform = A.Compose([
            A.RandomCrop(width=256, height=256, p=1), 
            A.HorizontalFlip(p=0.5),
        ])

    def __len__(self):
        return len(self.images) // self.batch_size

    def __getitem__(self, idx):
        """"""Returns a batch of samples""""""
        i = idx * self.batch_size
        batch_images = self.images[i : i + self.batch_size]
        batch_masks = self.masks[i : i + self.batch_size]
        batch_images_stacked = np.zeros(
            (self.batch_size,) + self.img_size + (3,), dtype=""uint8""
        )
        batch_masks_stacked = np.zeros(
            (self.batch_size,) + self.img_size, dtype=""float32""
        )
        for i in range(len(batch_images)):
            transformed = self.transform(
                image=batch_images[i], 
                mask=batch_masks[i]
            )
            batch_images_stacked[i] = transformed[""image""]
            batch_masks_stacked[i] = transformed[""mask""]
        return batch_images_stacked, batch_masks_stacked


That’s it! Hope this tutorial encouraged you to try Albumentations next time you are working on segmentation, object detection or keypoint localization task. Let me know if it did!
 
Bio: Olga Chernytska is a Senior Machine Learning Engineer in a large Eastern European outsourcing company; was involved in various data science projects for top US, European and Asian companies; main specialization and interest is Deep Computer Vision.
Original. Reposted with permission.
Related:

The only Jupyter Notebooks extension you truly need
How to create an interactive 3D chart and share it easily with anyone
Extraction of Objects In Images and Videos Using 5 Lines of Code"
https://www.kdnuggets.com/2021/01/mastering-tensorflow-variables-5-easy-steps.html,Mastering TensorFlow Variables in 5 Easy Steps,"Learn how to use TensorFlow Variables, their differences from plain Tensor objects, and when they are preferred over these Tensor objects | Deep Learning with TensorFlow 2.x.","comments
By Orhan G. Yalçın, AI Researcher
WARNING: Do not confuse this article with “Mastering TensorFlow Tensors in 5 Easy Steps”!
 
If you are reading this article, I am sure that we share similar interests and are/will be in similar industries. So let’s connect via Linkedin! Please do not hesitate to send a contact request! Orhan G. Yalçın — Linkedin

Figure 1. Photo by Crissy Jarvis on Unsplash
 

 
In this tutorial, we will focus on TensorFlow Variables. After the tutorial, you will be able to create, update, and manage TensorFlow Variables effectively. As usual, our tutorial will deliver code examples with detailed explanations as well as conceptual explanations. We will master TensorFlow Variables in 5 easy steps:

Step 1: Definition of Variables →A Brief Introduction, Comparison with Tensors
Step 2: Creation of Variables → Instantiating tf.Variable Objects
Step 3: Qualifications of Variables → Characteristics and Features
Step 4: Operations with Variables → Basic Tensor Operations, Indexing, Shape Manipulation, and Broadcasting
Step 5: Hardware Selection for Variables → GPUs, CPUs, TPUs

Fasten your belts, and let’s start!
 
Definition of Variables
In this step, we will briefly cover what Variables are and understand the difference between plain Tensor objects and Variable objects.
 
A Brief Introduction
A TensorFlow Variable is the preferred object type representing a shared and persistent state that you can manipulate with any operation, including TensorFlow models. Manipulation refers to any value or parameter update. This characteristic is the most distinguishing feature of Variables compared to tf.Tensor objects. TensorFlow Variables are recorded as tf.Variable objects. Let’s make a brief comparison between tf.Tensor and tf.Variable objects to understand their similarities and differences.

Figure 2. Variable Values can be Updated (Figure by Author)
 

 
Comparison with Tensors
So, the most important difference between Variables and Tensors is mutability. The values in a Variable object can be updated (e.g., with the assign() function) as opposed to Tensors.
“The values of tensor objects cannot be updated, and you can only create a new Tensor object with the new values.”
Variable objects are mainly used to store model parameters, and since these values are constantly updated during training, using Variables, instead of Tensors, is a necessity rather than a choice.
The shape of a Variable object can be updated with the reshape() instance function just like the shape of a Tensor object. Since Variable objects are built on top of Tensor objects, they have common attributes such as .shape and .dtype. But, Variables also have unique attributes such as .trainable,.device, and .name attributes that the Tensors do not have.

Figure 3. A Tensorflow Variable is actually a wrapper around a TensorFlow Tensor with additional features (Figure by Author)
 

 
Let’s see how we can create tf.Variable objects!
 
Creation of Variables
We can instantiate (i.e., create) tf.Variableobjects with the tf.Variable() function. The tf.Variable() function accepts different data types as parameter such as integers, floats, strings, lists, and tf.Constant objects.
Before showing different Variable object examples with these different data types, I want you to start a new Google Colab notebook and import TensorFlow library with the following code:

Now, we can start creating tf.Variable objects.
1 — We can pass a tf.constant() object as the initial_value:

2 — We can pass a single integer as the initial_value:

3 — We can pass a list of integers or floats as the initial_value:

4 — We can pass a single string as the initial_value:

5 — We can pass a list of strings as the initial_value:

As you can see, there are several data types that th etf.Variable() function accepts as the initial_value argument. Now let’s take a look at the characteristics and features of variables.
 
Qualifications of Variables
Every Variable must have some properties such as value, name, uniform data type, shape, rank, size, and more. In this section, we will see what these properties are and how we can view these properties in a Colab notebook.
 
Value
Every Variable must specify an initial_value. Otherwise, TensorFlow raises an error and says that Value Error: initial_value must be specified. Therefore, make sure that you pass on an initial_valueargument when creating Variable objects. To be able to view a Variable’s values, we can use the .value() function as well as the .numpy() function. See the example below:


Output:
The values stored in the variables:
tf.Tensor( [[1. 2.]  
            [1. 2.]], shape=(2, 2), dtype=float32)The values stored in the variables:
[[1. 2.]
[1. 2.]]

 
 
Name
Name is a Variable attribute which helps developers to track the updates on a particular variable. You can pass a name argument while creating the Variable object. If you don’t specify a name, TensorFlow assigns a default name, as shown below:


Output:
The name of the variable:  Variable:0

 
 
Dtype
Each Variable must have a uniform data type that it stores. Since there is a single type of data stored for every Variable, you can also view this type with the .dtype attribute. See the example below:


Output:
The selected datatype for the variable:  <dtype: 'float32'>

 
 
Shape, Rank, and Size
The shape property shows the size of each dimension in the form of a list. We can view the shape of the Variable object with the .shape attribute. Then, we can view the number of dimensions that a Variable object has with the tf.size() function. Finally, Size corresponds to the total number of elements a Variable has. We need to use the tf.size() function to count the number of elements in a Variable. See the code below for all three properties:


Output:
The shape of the variable:  (2, 2)
The number of dimensions in the variable: 2
The number of dimensions in the variable: 4

 
 
Operations with Variables
There are several basic operations you can easily conduct with math operators and TensorFlow functions. On top of what we covered in Part 2 of this tutorial series, you may also use the following math operators for Variable operations.
 
Basic Tensor Operations
 

Figure 4. You May Benefit from Basic Math Operators (Figure by Author)
 

 
Addition and Subtraction: We can conduct addition and subtraction with + and — signs.


Addition by 2:
tf.Tensor( [[3. 4.]  [3. 4.]], shape=(2, 2), dtype=float32)Substraction by 2:
tf.Tensor( [[-1.  0.]  [-1.  0.]], shape=(2, 2), dtype=float32)

 
Multiplication and Division: We can conduct multiplication and division with * and / signs.


Multiplication by 2:
tf.Tensor( [[2. 4.]  [2. 4.]], shape=(2, 2), dtype=float32)Division by 2:
tf.Tensor( [[0.5 1. ]  [0.5 1. ]], shape=(2, 2), dtype=float32)

 
Matmul and Modulo Operations: Finally, you can also do matmul and modulo operations with @ and % signs:


Matmul operation with itself:
tf.Tensor( [[3. 6.]  [3. 6.]], shape=(2, 2), dtype=float32)Modulo operation by 2:
tf.Tensor( [[1. 0.]  [1. 0.]], shape=(2, 2), dtype=float32)

 
These are elementary examples, but they can be extended into complex calculations, which creates the algorithms that we use for deep learning applications.

Note: These operators also work on regular Tensor objects.

 
 
Assignment, Indexing, Broadcasting, and Shape Manipulation
 
Assignment
With the tf.assign() function, you may assign new values to a Variable object without creating a new object. Being able to assign new values is one of the advantages of Variables, where value reassignment is required. Here is an example of reassignment of values:


Output:
...array([[  2., 100.],
          [  1.,  10.]],...

 
 
Indexing
Just as in Tensors, you may easily access particular elements using index values, as shown below:


Output:
The 1st element of the first level is: [1. 2.]
The 2nd element of the first level is: [1. 2.]
The 1st element of the second level is: 1.0
The 3rd element of the second level is: 2.0

 
 
Broadcasting
Just as with Tensor objects, when we try to do combined operations using multiple Variable objects, the smaller Variables can stretch out automatically to fit larger Variables, just as NumPy arrays can. For example, when you attempt to multiply a scalar Variable with a 2-dimensional Variable, the scalar is stretched to multiply every 2-dimensional Variable element. See the example below:


tf.Tensor([[ 5 10]
           [15 20]], shape=(2, 2), dtype=int32)

 
 
Shape Manipulation
Just as in Tensor objects, you can reshape Variable objects as well. For the reshape operation, we can use the tf.reshape() function. Let's use the tf.reshape() function in code:


tf.Tensor( [[1.]
            [2.]
            [1.]
            [2.]], shape=(4, 1), dtype=float32)

 
 
Hardware Selection for Variables
As you will see in the upcoming Parts, we will accelerate our model training with GPUs and TPUs. To be able to see what type of device (i.e., processor) our variable is processed with, we can use .device attribute:


The device which process the variable:   /job:localhost/replica:0/task:0/device:GPU:0

 
We can also set which device should process a particular calculation with the tf.device() function by passing the device name as an argument. See the example below:


Output:
The device which processes the variable a: /job:localhost/replica:0/task:0/device:CPU:0The device which processes the variable b: /job:localhost/replica:0/task:0/device:CPU:0The device which processes the calculation: /job:localhost/replica:0/task:0/device:GPU:0

 
Even though you will not have to set this manually while training a model, there might be circumstances where you have to choose a device for a particular calculation or data processing work. So, beware of this option.
 
Congratulations
We have successfully covered the basics of TensorFlow’s Variable objects.
Give yourself a pat on the back!
 
This should give you a lot of confidence since you are now much more informed about the main mutable Variable object type used for all kinds of operations in TensorFlow.
If this is your first post, consider starting from Part 1 of this tutorial series:
Beginner’s Guide to TensorFlow 2.x for Deep Learning Applications
Understanding the TensorFlow Platform and What it has to Offer to a Machine Learning Expert
or check out Part 2:
Mastering TensorFlow Tensors in 5 Easy Steps
Discover how the building blocks of TensorFlow works at the lower level and learn how to make the most of Tensor…
 
Subscribe to the Mailing List for the Full Code
If you would like to have access to full code on Google Colab and the rest of my latest content, consider subscribing to the mailing list:

Slide to SubscribeFinally, if you are interested in more advanced applied deep learning tutorials, check out some of my other articles:
Image Classification in 10 Minutes with MNIST Dataset
Using Convolutional Neural Networks to Classify Handwritten Digits with TensorFlow and Keras | Supervised Deep Learning
Image Noise Reduction in 10 Minutes with Convolutional Autoencoders
Using Deep Convolutional Autoencoders to Clean (or Denoise) Noisy Images with the help of Fashion MNIST | Unsupervised…
Image Generation in 10 Minutes with Generative Adversarial Networks
Using Unsupervised Deep Learning to Generate Handwritten Digits with Deep Convolutional GANs using TensorFlow and the…
Fast Neural Style Transfer in 5 Minutes with TensorFlow Hub & Magenta
Transferring the van Gogh’s Unique Style to Photos with Magenta’s Arbitrary Image Stylization Network and Deep Learning
Bio: Orhan G. Yalçın is an AI Researcher in the legal domain. He is a qualified lawyer with business development and data science skills, and has previously worked as a legal trainee for Allen & Overy on capital markets, competition, and corporate law matters.
Original. Reposted with permission.
Related:

Mastering TensorFlow Tensors in 5 Easy Steps
Deploying Trained Models to Production with TensorFlow Serving
Pruning Machine Learning Models in TensorFlow"
https://www.kdnuggets.com/2021/06/bigquery-snowflake-comparison-data-warehouse-giants.html,BigQuery vs Snowflake: A Comparison of Data Warehouse Giants,In this article we are going to compare the two topmost data warehouses: BigQuery and Snowflake.,"comments
By Anji Velagana, Freelance Content Writer

It's essential to understand data warehousing depending on your requirements and business. Many organizations struggle in selecting the data warehouse that suits them. Hence, people are opting for the BigQuery/Snowflake course to understand data warehousing. Here, we are going to compare the two topmost data warehouses: BigQuery and Snowflake.
Let's move right into knowing them.
 
What is BigQuery?
 
BigQuery is a serverless and fully managed data warehouse that can enable a scalable analysis of the data. It is referred to as ""Platform-as-a-Service (PaaS)."" By using ANSI SQL, BigQuery supports querying. An enterprise data warehouse is used to solve problems using Google infrastructure processing power by enabling super-fast SQL.
Significance of BigQuery
It is very hard to manage the data spread across the applications as the business grows. Also, it isn't easy to analyze the data within meaningful insight systems. The precious engineering sources are often deployed in setting up a centralized data store. With BigQuery, the developers can focus on essential activities like analyzing business-critical data. The REST API of BigQuery enables businesses in building mobile front-ends and App Engine-based dashboards.
 
What is Snowflake?
 
It is a cloud-based data warehousing company founded in 2012. Snowflake offers analytics services, cloud-based data storage services. In general, Snowflake is termed as a ""Data warehouse as a service."" The data cloud of Snowflake is powered by the advanced data platform with Software-as-a-Service (SaaS). It enables data processing, storage, and analytics solutions that are faster, easier, and more flexible than traditional offerings.
Significance of Snowflake
Snowflake has an architecture of multi-cluster shared data. The architecture separates their compute and storage layer. This can help them in scaling up automatically as per the demand without impacting their performance. Micro-partitioning is featured in Snowflake architecture that is able to manage both structured and semi-structured data. Hence, they are able to manage Parque, JSON and so on. Within the Snowflake. One of the vital points is that Snowflake can be delivered as a service. Besides, it is extremely easy to use with zero management. After the data migration into Snowflake, everything is taken care of, and there's no requirement to prune, index, etc., allowing the users to focus more on the value in the data.
 
Which is better, BigQuery or Snowflake?
 
If you want to know which is better among BigQuery and Snowflake, let’s know their characteristics.
Pricing
For computing resources, Snowflake uses a time-based pricing model, in which the users charge for execution time. Whereas, BigQuery uses a model of query-based pricing for computing the resources, in which the users charge for data. BigQuery storage is less expensive than Snowflake storage.
Architecture
The architecture of Snowflake is a shared-nothing database and a hybrid traditional shared-disk architecture. Also, Snowflake uses the central data repository from all the compute nodes for persisted data. Similarly to shared-nothing architectures, Snowflake processes all the queries by using Massively Parallel Processing (MPP) to compute the clusters.
Performance
Both BigQuery and Snowflake perform well under different load levels. One must run the benchmarks by using his own data. Also, they can be used to handle the workloads of many companies with good performance. In terms of raw speed, on an average of 10.74 seconds, the Snowflake edged out BigQuery. It is a known factor that BigQuery clocked at 14.32 seconds per query. As per the benchmarks of independent third-party, the performance of Snowflake is better than the performance of BigQuery.
Ease of Use
Both BigQuery and Snowflake are user-friendly in nature when it comes to the case ease of use. Snowflake has received a 9.2 rating on the G2 business software review website.
Scalability
Often, Snowflake allows the users to scale their storage and compute resources independently. It includes workload monitoring and automatic performance tuning to improve query times. Whereas, Bigquey handles scalability questions under the hood entirely. BigQuery can automatically provide the compute resources. It makes it very easy to process the data in just a few minutes.
Security
Both BigQuery and Snowflake use AES encryption on the data to support customer-managed keys. Besides, they depend on the roles to provide access to the resources. Often, Snowflake allows federated user access through SAML 2.0 compliant vendors and Microsoft Active Directory (ADFS). BigQuery also allows federated user access by using Active Directory. Snowflake provides granular permissions for views, schemas, procedures, tables, and other objects.
Maintenance and Management
Both BigQuery and Snowflake require low maintenance because automated management is going on in the background. This can imply in Snowflake that queries are optimized and tuned in the background sometimes. Since the platform of Bigquery is serverless, the customers are hardly aware of the considerations.
 
Conclusion
 
I hope you found the detailed insights of BigQuery and Snowflake. Now, you are aware of both and choose to pick as per your needs. Still, you have any queries regarding BigQuery and Snowflake, feel free to comment in the below section.
 
Bio: Anji Velagana is a B.Tech graduate turned Freelance Content Writer, pursuing a Masters in Journalism & Mass Communication with passion.
 
Related:

ETL in the Cloud: Transforming Big Data Analytics with Data Warehouse Automation
Apache Spark on Dataproc vs. Google BigQuery
Cloud Data Warehouse is The Future of Data Storage"
https://www.kdnuggets.com/2021/04/time-series-forecasting-predict-weather.html,Want To Get Good At Time Series Forecasting? Predict The Weather,This article is designed to help the reader understand the components of a time series.,"comments
By Michael Grogan, Data Science Consultant


Source: Photo by geralt from Pixabay

 
For someone who originally comes from an economics background, it might seem quite strange that I would spend some time building models that can predict weather patterns.
I often questioned it myself — but there is a reason for it. Temperature patterns are one of the easiest time series to forecast.
 
Time Series Components
 
When a time series is decomposed — or broken into its individual elements — a series consists of the following components:

Trend: The general direction of the time series over a significant period of time
Seasonality: Patterns that frequently repeat themselves in a time series
Random: Random fluctuations in a time series

When one thinks about it — the components of temperature data are very pronounced.
In most parts of the Northern Hemisphere at least, the general trend is for temperature to rise as one heads into the summer months, with a decreasing trend towards the winter months.
For instance, here is the mean temperature fluctuations for Dublin Airport, Ireland from 2015–2018, sourced from Met Éireann:


Source: RStudio

 
Decomposing this time series visually reveals the following:
Trend


Source: RStudio

 
Seasonal


Source: RStudio

 
Random


Source: RStudio

 
As we can see in the graph above, the seasonal patterns show clear evidence of a yearly cycle. For instance, an autocorrelation function reveals a strong correlation in temperature data every 12 months. This makes sense, as it stands to reason that temperatures in January will show the most correlation with recorded January temperatures in other years. Same when comparing temperatures across July, and so on.


Source: RStudio

 
An intuitive understanding of these components allows for a better appreciation of how they apply across other time series.
For instance, the seasonal pattern for air passenger numbers (at least before COVID-19) has been a higher incidence of air passengers in the summer months, with lower passengers on the whole for winter. Here is an example of passenger number fluctuations which was generated using data from San Francisco Open Data:


Source: Jupyter Notebook Output

 
Domain knowledge is also important in discerning the components of a time series. For instance, a data scientist who specialises in analysis of the energy markets would intuitively know that commercial electricity consumption tends to follow a weekly, rather than a yearly seasonal pattern. i.e. consumption tends to peak on days of high usage such as Monday, while decreasing significantly over the weekend.
 
Can A Time Series Be Forecasted In The First Place?
 
All too often, those who are new to time series analysis will attempt to forecast a series with a lot of inherent randomness present in the data.
For instance, stock prices tend to follow a very stochastic (or random) pattern. These time series are often driven by cyclicality rather than seasonality, whereby the peaks and troughs in the time series do not occur at specified intervals.
As such, while the overall trend may give a longer-term view of the stock’s direction — it still remains a lot harder to forecast the time series outright, as the patterns in the time series often do not repeat themselves.
That said, all too often — one might attempt to forecast a stock price using a model such as ARIMA without fully taking the time to understand the components of that time series. I’m also guilty of having made this mistake in the past.
Additionally, it is noteworthy that temperature data cannot be influenced by human intervention. However, many time series can (including stock prices), and as such, past data cannot account for these interventions.
Using a separate example, suppose I were to attempt to use Community Mobility data from Google to try and forecast mobility trends for a major city in six months time.
Such a forecast would make no sense — as it is completely dependent on factors external to the time series itself, i.e. government lockdowns, COVID-19 circulation, etc.
 
Conclusion
 
Strictly speaking, you don’t have to start with weather patterns when predicting a time series. However, you should start with a set of data that is easy to forecast and has predictable trend and seasonality patterns.
The big mistake that people make is in trying to forecast a time series that has a lot of inherent randomness baked in. Not only will you be unable to generate credible forecasts on such data, but terms such as autocorrelation function will make no intuitive sense to you.
Predicting a time series that is not influenced by external factors (temperature patterns being one of the very few) will allow you to better understand why factors such as autocorrelation, stationarity, and others are of theoretical relevance. Indeed, when it does come time to predict something more complex such as sales data — you will be better equipped to 1) understand the theoretical workings of the model and 2) the advantages and disadvantages of the use of different time series models with respect to the data.
Disclaimer: This article is written on an “as is” basis and without warranty. It was written with the intention of providing an overview of data science concepts, and should not be interpreted as professional advice. The findings and interpretations in this article are those of the author and are not endorsed by or affiliated with any third-party mentioned in this article.
 
Bio: Michael Grogan is a Data Science Consultant. He posesses expertise in time series analysis, statistics, Bayesian modeling, and machine learning with TensorFlow.
Original. Reposted with permission.
Related:

Working With Time Series Using SQL
Why Automated Feature Selection Has Its Risks
Rejection Sampling with Python"
https://www.kdnuggets.com/2020/12/facebook-open-sources-rebel-new-reinforcement-learning-agent.html,"Facebook Open Sources ReBeL, a New Reinforcement Learning Agent",The new model tries to recreate the reinforcement learning and search methods used by AlphaZero in imperfect information scenarios.,"By Jesus Rodriguez, Intotheblock.
comments


Source: https://smilegate.ai/en/2020/12/07/facebook-rebel/

 

I recently started a new newsletter focus on AI education. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:




 
Poker has been considered by many the core inspiration for the formalization of game theory. John von Neuman was reportedly an avid poker fan and use many analogies of the card game while creating the foundation of game-theory. With the advent of artificial intelligence(AI) there have been many attempts to master different forms of poker, most of them with very limited results. Last year, researchers from Facebook and Carnegie Mellon University astonishing the AI world by unveiling Pluribus, an AI agent that beat elite human professional players in the most popular and widely played poker format in the world: six-player no-limit Texas Hold’em poker. Since then, a question that has hunted AI researchers is whether the skills acquired by models like Pluribus can be used in other imperfect information games. A few days ago, Facebook again used poker as the inspiration for Recursive Belief-based Learning (ReBeL), a reinforcement learning model that is able to master several imperfect-information games.
The inspiration from ReBeL comes from DeepMind’s AlphaZero. After setting up new records in the Go game with the development of AlphaGo, DeepMind expanded iits efforts to other perfect-information games such as Chess, or Shogi. The result was AlphaZero, a reinforcement agent that was able to master all these games from scratch. Of course, recreating the magic of AlphaZero in imperfect-information games like poker entails a different level of complexity.
Games like poker in which players keep their cards secret represent a major obstacle for reinforcement learning + search algorithms. Most of these techniques assume that each player’s action has a fixed value regardless of the probability of that action being executed. For instance, in chess, a good move is good regardless of whether is played or not. Now let’s think about a game like poker in which the players bluff all the time. In many scenarios, the value of a bluff action diminishes the more its used as the opponents can adjust their strategy to it. How could we possibly leverage reinforcement learning + search methods across many imperfect-information games.
 
Enter ReBeL
 
The idea behind ReBeL is SO SIMPLE as it is clever. If AlphaZero showed success with reinforcement learning + search strategies in perfect-information games, then why not to transform imperfect-information games to perfect-information equivalents? I know, I know, it sounds too good to be true but let’s look at an example.
Let’s imagine a simplified version of poker in which a single card is dealt to each player which can then choose between three actions: fold, call or raise. Now consider a variation of this game in which the cards are not dealt to the players directly but, instead, they can be seen only by an third-party referee. Instead of taking an action directly, the players will announce how likely they are to take a specific action given the current hand. The referee will take an action based on the player’s analysis. In terms of strategy, this modified game is identical to the original game with the difference that it contains no private information. Instead, the modified game can be considered a continuous-state perfect-information game.


Source: https://ai.facebook.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/

 
The transformation of an imperfect-information game to a perfect-information environment, opens the door to utilizing the same techniques that worked for AlphaZero. The main challenge at this point is efficiency, as the search space is fairly larger than most perfect-information games. To address this, ReBeL uses an optimization technique known as counterfactual regret minimization (CFR) to improve the efficiency of the search.
Facebook evaluated ReBeL in two games: heads-up no-limit Texas Hold’em and Liar’s Dice with very strong performance in both.
ReBeL represents an important milestone in order to use reinforcement learning + search in order to generically solve imperfect-information games. There are still plenty of challenges including the fact that it knows the rules of the game in advance which is not the case of many real world scenarios. Facebook also open sourced the implementation of the Liar’s Dice game to allow the research community to improve in these ideas.
 
Original. Reposted with permission.
Related:

Facebook Open Sourced New Frameworks to Advance Deep Learning Research
Microsoft and Google Open Sourced These Frameworks Based on Their Work Scaling Deep Learning Training
Remembering Pluribus: The Techniques that Facebook Used to Master World’s Most Difficult Poker Game"
https://www.kdnuggets.com/2021/01/deep-learning-pioneer-geoff-hinton-research-future-ai.html,Deep Learning Pioneer Geoff Hinton on his Latest Research and the Future of AI,Geoff Hinton has lived at the outer reaches of machine learning research since an aborted attempt at a carpentry career a half century ago. He spoke to Craig Smith about his work In 2020 and what he sees on the horizon for AI.,"comments
By Craig Smith, Eye on AI

Editor's note: This is a transcript of a conversation between Craig Smith and Geoff Hinton, from an episode of the Eye on AI podcast. You can also find a video version of the interview, as well as an audio version, including a scrolling transcript with speed controls.

 
CRAIG: Hi, I'm Craig Smith, and this is Eye on AI.
This week I speak to Geoff Hinton, who has lived at the outer reaches of machine learning research since an aborted attempt at a carpentry career a half century ago. After that brief dogleg, he came back into line with his illustrious ancestors, George Boole, the father of Boolean logic and George Everest, British surveyor general of India and eponym of the world's tallest mountain. Geoff is one of the pioneers of deep learning and shared the 2018 Turing award with colleagues, Yoshua Bengio, and Yann LeCun. A year earlier, he had introduced capsule networks, an alternative to convolutional neural networks that take into account the pose of objects in a 3D world, solving the problem in computer vision, in which elements of an object change their position when viewed from different angles.
He has been largely silent since then, and I'm delighted to have him on the podcast.
We began like so many of us do today trying to get the teleconferencing system to work. I hope you find the conversation as engrossing as I did.
CRAIG: I don't think I need to introduce you or that you need to introduce yourself. I do want to sort of recap what's gone on in the last year. It's been quite a year. Capsule networks had sort of faded from view, at least from the layman's point of view, and resurfaced at NeurIPS last December with your introduction of stacked capsule autoencoders.
Then, in February at the AAAI conference, you talked about capsule networks as key to unsupervised learning. And in April you revived the idea of backpropagation as a learning function in the brain with the introduction of neural gradient representation by activity differences or NGRADs,
GEOFF: I think it would be better if we started with capsules and we do three different topics.
We do capsules.
CRAIG: Okay.
GEOFF: Then we do SimCLR. And then we do the NGRAD stuff.
CRAIG: Okay. Can you talk about your new capsule idea? Not new - a year old or longer now - but how that has influenced your research?
GEOFF: Okay. So, several things have changed and more things are changing right now.
So originally capsules, we used supervised learning and we thought it would be easy to get things working like that, even though I don't really believe in supervised learning. And last year we switched to unsupervised learning and we also switched to using set transformers.
So what capsules are trying to do is recognize whole objects by recognizing their parts and the relationships between the parts.
So, if you see something that might be an eye, and you see something that might be a nose, the possible eye could say where the face should be and the possible nose could say where the face should be. And if they agree on where the face should be, then you say, 'Hey, they're in the right relation to make a face, so we'll instantiate a face. We'll activate the face capsule.'
So, there's various problems with that. One is the issue of whether you try and train it supervised or unsupervised, and it's going to be much better to use unsupervised because then you don't need labels. But the other problem, which we overcame with stacked capsule autoencoders, is that if you see say a circle in a line drawing, you don't know whether it's a left eye or a right eye or the front wheel of a car or the back wheel of a car.
And so, it has to vote for all sorts of objects it might be a part of, and so, you know, if it's the back wheel of a car, it knows roughly where the car should be and it can vote for, 'there should be a car there.' But of course, it might not be that. It might be a doorknob or it might be a left eye. And so, it makes lots and lots of votes.
And now what happens is every higher-level capsule gets a huge cloud of votes, nearly all of which are wrong. But one way to try and rectify that is to say, well, if any other capsule likes the vote, if any other capsule can make use of that vote, to make be part of this object, then route the vote there and don't route it to me.
And so that was the idea of dynamic routing; that you try and get all the bad votes to go to the places where they're good votes.
That's complicated to make it work. The alternative which we use in stacked capsule autoencoders s is to say, if you discover parts, suppose you discovered a circle and a triangle and a rectangle, you don't really know what they're parts of. There's many, many things they could be parts of. So, what you want to do is have them interact with each other a bit, and use the spatial relations between them to allow each part to become more confident by what kind of part it is. So, if you're a circle and there's a triangle in the right relative position to be a nose, if you're a left eye, then you get more confident than you’re a left eye. And that's what transformers are very good at.
Transformers have a representation of, in the case of language, a word fragment. So, it might be the fragment 'may,' which happens to be a whole word. And they don't know whether that's a modal, like would and should, or whether it's a month, like June and July. And so, what they do is, the representation of that fragment interacts with the representations of other fragments. And if there's another fragment in the sentence, for example, June, then the representation for May gets more month-like. Whereas if there's another fragment that's, would or should, it gets more modal like. And after a few layers of that, the fragments have been disambiguated. That is, you know, much better what each fragment is meant to be.
So, in language, that means you have a contextually sensitive representation of the word which s disambiguated between different meanings. In vision, if you have something like a circle, you'd like to know whether that circle is an eye, or the wheel of a car. And you can do that without yet creating a face or a car, by this interaction between parts. And in stacked capsule autoencoders, that's what we do.
We take the first level of parts and they all interact with each other. So, they become more confident about what kind of a part they are. Well, once they are more confident about what kind of a part they are, then they vote for what whole they might be a part of. And that way they can have far more specific, confident votes.
So, they don't make lots of crazy votes. Once you become convinced that a circle is probably a left eye, it doesn't vote for being the back wheel of a car. That means you've got far fewer votes to deal with it and so it's far easier to find the clusters. We made it so instead of trying to learn by supervision, by giving it labels, it learned to create a whole that was good at reconstructing the parts.
And so that's unsupervised learning.
CRAIG: At some point you need to connect it to language.
GEOFF: Yeah. So, all the learning in stacked capsule autoencoders, almost all the learning, is unsupervised. That is, you have these parts, and you recognize these parts, which are sort of templates that occur a lot – it’s a little bit more complicated than that, but - and then you recognize wholes that are combinations of these parts.
And the objective function is to find wholes that are good at reconstructing the parts, in particular, find wholes so if I tell you the pose of the whole thing, you can tell me the pose of the part. Like if I tell you there's a small face at 45 degrees in the bottom right-hand corner of the image, you can tell me that there should be a nose of 45 degrees that's even smaller in the bottom right hand corner of the image. So, the whole can predict the parts. And I didn't need any labels for that.
CRAIG: Right.
GEOFF: Now once you've done that, once you've got these wholes, you can then learn what they're called. So then supervised learning consists of taking wholes and learning their names. But you're not learning to recognize them when you're doing that, you're just learning what things you can already recognize are called, much like a little child learns to recognize cows and sheep. It doesn't know that cows are called cows and sheep are called sheep, and that's why it needs its mother to tell it. But its mother is not the one who tells it how to tell the difference between a cow and a sheep.
CRAIG: Yeah. Would this kind of unsupervised learning, in a larger system, also be able to make assumptions or inferences about relationships between objects or, or the laws of physics, for example?
GEOFF: Those are two somewhat different questions.
CRAIG: Yeah.
GEOFF: In the long run, we'd like it to do that, but let's return to the laws of physics later on when we talk about SimCLR.
Okay. for now, he recognizes objects by seeing parts in the correct relationships. And you recognize scenes by seeing objects in the correct relationships. In a scene the relationships between objects are typically somewhat looser, but yes, it can do that. It can recognize that objects are related in the right way to make a particular kind of a scene.
CRAIG: SimCLR [Simple framework for Contrastive Learning of visual Representations] came up later in the year. Can you talk about SimCLR and how that relates?
GEOFF: So that's a different learning algorithm. That's different in many ways. It's not, for example, focusing on the problem of dealing with viewpoint equivariance, that is, as the viewpoint changes, you get a representation that changes so that you can cope with viewpoint easily.
That's not the primary goal of SimCLR. What SimCLR is doing is saying, 'I want to learn to represent a patch of an image in such a way that other patches of the same image have similar representations.' So, what you do is you take a crop of an image and then you take another crop of the same image.
And you say, we're going to have a neural net that converts those crops into a vector representation, pattern of neural activities. And we want those patterns to be similar if the crops came from the same image and different, if they came from different images. If you just say, make them similar, that's easy.
You just make all of the vectors be identical. The trick is you have to make them similar if they came from the same image and different if they came from different images. And so that's called contrastive learning. And Ting Chen in the Google lab in Toronto, with some help from others of us, made that work extremely well.
He wasn't the originator of the idea. In fact, the first comes from work I did with Sue Becker in 1993 or 92, and then later work I did in 2002. But we never really made it work well for images and other people revived the idea in 2018 and got contrastive learning working for crops of images. And then Ting Chen made it work considerably better, and that made people sit up. And so, what happens is once you've got this representation of a patch of an image, or, this neural net that can convert a patch from an image into a representation, such that you get similar representations with two patches coming from the same image, then you can use those representations to try and recognize what the objects are in the image. And that stage is supervised learning, but that doesn't require a deep net.
So, the idea is you do unsupervised learning by using this deep net to try and get the same representation or very similar representations for two different patches of the same image. And different representations for patches of different images. After you've used the deep net to do that, so Ting uses a ResNet, which is a [inaudible] kind of deep net, after you've done that, you then just directly learn to turn those representations with no extra hidden layers into class labels. So that's called a linear classifier.
It doesn't have hidden layers in it. And he does remarkably well. So, a linear classifier based on those representations that we've got by pure unsupervised learning with no knowledge of the labels can do as well now on ImageNet as a supervised method, provided for the unsupervised learning, we use a bigger ResNet.
If you use a standard sized ResNet on ImageNet, you get a certain error rate and we can get pretty much the same error rate by using a bigger ResNet training it entirely unsupervised with no knowledge of labels. And then on top of the representations we extract, training a linear classifier.
CRAIG: And in that training, in one of the things I read, you talked about using augmented data.
GEOFF: Yes. It's very important when you do this. You can think of the two different crops as different ways of getting representations of the same image, but it's perhaps the major thing you do, but you also have to do things like mess with the color balance. So, for example, if I give you two different crops from the same image you can often recognize that they're from the same image by looking at the relative distribution of red, green, and blue - the color histogram.
And we don't want it doing that. So, to stop it cheating like that, you take two different crops of the same image and on one of the crops you change the color balance. And now it can't recognize that they're the same just by using the color distribution. And those are the two most important ones, doing different crops and changing the color balance.
CRAIG: Yeah. Is that augmentation something that the data scientist does in the data prep? It's not part of the model, the model doesn't automatically augment the data.
GEOFF: Well, it's not part of the data prep really, as you're training on the data, you'll, you'll get an image, you'll take two different crops of the image, and then you will augment those crops. You'll change the color balance.
CRAIG: Right.
GEOFF: So, you can't really think of it as modifying the data so much as given an image, you then get these crops with modified color balance, and you can modify all sorts of other things like orientation and stuff like that.
CRAIG: And that sounds very similar, from a layman's point of view, to what Yann LeCun is doing with video, where, where he, he, he takes a video and, and tries to predict what the next frame will be, in an unsupervised manner.
Am I wrong in that?
GEOFF: Well, it's not the same as trying to predict the next frame of a video. It Is the same, however, as trying to extract your representation from the next frame, that's easily predicted by the representation you extracted from the current frame. So that's contrastive learning. You can do contrastive learning for videos.
And you can say, you're really asking the question, ‘Did these two frames come from the same video?’ And that's a bit like asking, ‘did these two crops come from the same image,’ and you can use the same contrastive learning techniques for that.
CRAIG: Yeah. And then at AAAI, when you were talking on the stage with Yann and Yoshua Bengio, you talked about capsule networks as a form of unsupervised learning that has promise going forward. This SimCLR is another method. Are they related or can they be blended in making unsupervised methods more powerful?
GEOFF: They're somewhat different approaches at present. You could clearly try and combine them.
We're not doing that at present. Yeah.

 
CRAIG: In Nature. there was a paper, I believe it was in Nature, about sort of reviving the idea of back propagation as a function of learning in the brain. And you introduced this idea of, of NGRADs, neural gradient representation by activity differences. Can you talk about that?
GEOFF: Neuroscientists have been very skeptical about whether the brain can do anything like back propagation.
Well, one of the big problems has been, how does the brain communicate gradients? Because in back propagation, you need to change your weight in proportion to the gradient of the error with respect to that weight, whatever your error function is. And the idea is that you represent an error by the rate of change in neural activity.
And that's nice because it can have both signs, that is, neural activity can be going up or it can be going down, so you can represent both signs of error. And it also implies that the learning rule, which uses a gradient, is going to be something called spike timing dependent plasticity. That is when you change your synapse strength, you're going to change it in proportion to the error derivative.
And that means you're going to want to change it in proportion to the rate of change of the post synaptic activity. It is going to be the presynaptic activity times the rate of change of the post synaptic activity. And that's called spike timing dependent plasticity, which they found in the brain. And in fact, I've been suggesting for a long time that we use activity differences.
I had a paper with James McClelland in 1987, suggesting that temporal differences of activity be used as error derivatives. And that was actually before spike timing dependent plasticity had been discovered. By 2005, I got interested in activity differences again. And much more recently people have managed to make that work quite well.
I'm still somewhat skeptical. I think the brain could do back prop if it wanted to that way. It's a little clumsy and I'm now skeptical because I think back prop is too good an algorithm for the brain. So, the brain is actually dealing with a very different problem from what most neural nets are dealing with.
Most of the neural nets want to get a lot of knowledge represented in a modest number of parameters, like only a billion parameters, for example. For brain, that's a tiny number of parameters. That's the number of parameters you're having a cubic millimeter of brain, roughly. So, we have trillions and trillions of parameters.
But, we don't have many training examples. We only live for like a billion seconds or 2 billion seconds.
And so, we, we don't get much experience and we've got a huge number of parameters and neural nets mostly were in the other regime. They get lots of training and they don't have many parameters. Now, if you've got lots and lots of parameters and not much training data, what you want to do is somewhat different from backpropagation, I think.
So, I got very interested in the idea that there is one way of making this activity difference method work nicely; of trying to generate agreement between a top down representation and a bottom up representation. So, the idea is, you have, say, some hierarchy of parts. You look at an image, you instantiate parts at different levels.
And then from the high-level parts, you top down predict the low-level parts. And what you'd like to see is agreement between the top-down prediction, which depends on a larger context, and the bottom up extraction of a part, which depends on a smaller context. So, from some local region of the image you extract a part; from many of those parts, you predict a whole; from the whole, you now, top-down predict the individual parts. But those predictions of the parts have used more information because they're based on the whole, it got to see more.
And what you want is, agreement between the top-down prediction and the bottom up extraction of part representation. And, you want it to be significant agreement, so what you really want is on the same image, they agree, but on different images they disagree. So, if you take the parts from one image and the top-down predictions or another image, they should disagree.
And that's contrastive learning as in SimCLR. But it also suggests a learning algorithm for the brain that is somewhat different from back prop. And I got very excited. It's not quite as efficient as back prop, but it's much easier to put into a brain because you don't need to go backwards through many layers.
You just need to compare a top-down prediction with a bottom up prediction. I call it back relaxation. And, over many times steps, it will get information backwards, but it won't get information backwards in one trial. And back propagation sends information all the way backwards through a multi-layer net on a single presentation of an image and back relaxation just gets it back one layer each time, and it needs multiple presentations of the same image to get it back all the way.
So, I got really interested in back relaxation and whether that might explain how the brain was doing this learning of multi-layer nets. But then I discovered that sort of pure greedy bottom up learning did just about as well. I hadn't done the controls carefully enough. The bottom up algorithm that I introduced in 2006 actually worked as well as this back relaxation.
And that was a huge disappointment to me. I still want to go back and see if I can make back relaxation work better than greedy bottom up.
CRAIG: I see, and thus the June tweet.
GEOFF: Yeah, that's when I discovered that back relaxation doesn't work any better than greedy bottom up learning.
CRAIG: Is the assumption that, that the brain is so efficient that even if greedy bottom up can do it on its own that there wouldn't be this top-down function, or is it possible that that top down function exists as a, as a, an optimizer or something?
GEOFF: Well, you'd like this top-down prediction - and making it agree with the bottom up extraction - you'd like that to be better than just training a stack of autoencoders, one layer at a time. Otherwise it's not worth doing and training a stack of autoencoders, one hidden layer at a time, turns out to be pretty good.
And what's happened recently in these big neural nets is, deep learning really got going in about 2006 when we discovered that if you train stacks of autoencoders or restricted Boltzmann machines, one, one hidden layer at a time, and then you fine tune it, it works very well.
And that got neural nets going again. People then did things like speech. And vision on ImageNet, where, they said you don't need the pre-training. You don't need to train these stacks of autoencoders. You can just train the whole thing supervised.
And that was fine for a while. But then when they got even bigger data sets and even bigger networks, people have gone back to this unsupervised pre-training. So that's what Bert is doing. Bert is unsupervised pre-training. And GPT-3 uses unsupervised pre-training. And that is important now. So, there was this on again, off again, where there was supervised learning and then I introduced unsupervised pre-training and then people said, 'Oh, but we don't need that. We just use supervised learning.' But now they're back to saying, 'Oh, but we do need some unsupervised learning.'
CRAIG: Right.
GEOFF: But the unsupervised learning algorithms are now getting more sophisticated.
CRAIG: Yeah. and again, the, the SimCLR is, at least as it relates to computer vision, is one method. The stacked capsule autoencoders is another method, and there may be others still. The learning and the brain, you know, I had a long conversation, about a year ago with Rich Sutton about temporal difference learning. And there is a view that, that that algorithm is, describes what's happening in lower brain function. and what you're talking about is cortex learning, and, it, at what point do they - are they completely different systems?
GEOFF: Yes. The big successes of computational neuroscience have been taking the work that Rich Sutton and others did on temporal differences and relating it to experimental studies on the brain and dopamine. Peter Dayan in particular, was very important in showing the relationship between this theoretical learning algorithm and what's actually going on in the brain. But that's for reinforcement learning.
And I think reinforcement learning is kind of the icing on the cake. Most of the learning is going to be unsupervised learning. You have to learn how the world works and you don't want to learn how the world works by using reinforcement signals. You don't want to learn to do vision by stubbing your toe all the time. You want to learn to do vision some other way.
CRAIG: Yeah. This is giving you further insight into, into learning in the brain. I remember that that was really your initial impulse in getting involved in all this study.
GEOFF: Yeah. My main goal in life has been to understand how the brain works, and all of this technology that's come out of attempts to understand how the brain works, aren't really how the brain works. It's useful spinoff. But it's not what I was really after.
CRAIG: Is that all part of one general stream that you're pursuing that's headed to a particular goal?
GEOFF:  It's like this. if your research has been around for a while, you have a number of, kind of, deep intuitions about how things should be, and then you have particular projects that are like particular instances that combine those intuitions. And often projects that seem quite separate, eventually merge. But for now, the work on capsules is somewhat different.
Although all three of them could merge together. That is, if we, if we can get the idea of top-down predictions and bottom up predictions agreeing in a contrastive sense, that is, they agree well for the same image and they're very different for different images, that will fit in with stacked capsule autoencoders.
But it will also, it's also an example of contrastive learning as in SimCLR. It may also explain how the brain can learn multi-layer nets. So obviously I would like to, I'd like to have one solution to everything. That's what everybody always wants. it's just, you have to be more realistic and get parts of this. You can't get the whole thing all at once.
CRAIG: Yeah. With the rise of transformers in models like GPT-3 and now, in capsule networks, which is primarily computer vision, there's kind of a convergence between computer vision and natural language processing. How do you see that convergence progressing? And, and those are the two principle components of consciousness, if I'm not wrong. So are we working towards, a model that can perceive the world, an AI model that can perceive the world that's closer to a human perception in that it blends...
GEOFF: One of the big motivations of capsule s was that it would be, it would have representations more like the representations we use.
So, a classic example is, if you see a square rotated through 45 degrees, you have two completely different ways of perceiving that one is as a tilted square and the other is there's an upright diamond. And what you know about it is totally different, depending on which representation you use. Now, convolutional nets don't have two different representations of that.
They just have one representation of that. To get two different representations, you need something that imposes a frame of reference. And a very strong feature of our perception is that we impose frames of reference on things and understand them relative to those imposed frames. And if you get someone to impose a different frame, they'll understand things quite differently.
That was one of the big motivations for capsules. It's also one for computer graphics. So, in computer graphics, you represent a house with a particular coordinate frame. And then relative to that coordinate frame, you know, where the windows and the door are.
Again, that's the kind of representation we need to get into neural nets, if neural nets are going to get more like us at representing objects. At present deep neural nets are very good at doing classification where they do it a completely different way from people. So, they're relying much more on things like texture.
And they can see all sorts of complex textures that we aren't sensitive to. And that's why you get these adversarial examples where two things look totally different to us, but very similar to a neural net and vice versa.
CRAIG: Google has just filed for a patent on capsule networks. Is that, because of the successes of stacked capsule autoencoders?
GEOFF: You know, I don't know all the motivations for filing the patent, but I think the main motivation, which is true for most of the patent filings Google does, is protective in the sense they don't want other people to sue them for using stuff they developed. And so, Google really, isn't interested in making its money out of patents. It's interesting in make is making its money out of having great products.
And it doesn't want to be prevented from using its own research and its great products. And the patent laws have changed in such a way that the first to file - it's not the first to invent, it's the first person to file it. And so, you have to file patents, just protectively.
CRAIG: There's this paper, that that's, under review right now, transformers for image recognition at scale.
Does that relate at all to this use of transformers in capsule networks?
GEOFF: Yes, it does a bit. So, what it is showing is that the kind of interactions between parts that work so well in things like Bert, for words, where you're getting word fragments to interact, also works when you're getting representations of patches of images to interact.
And it's also what's happening in stacked capsule autoencoders, where we have a set transformer that's getting the representations of parts to interact with one another and become refined. But then in stacked capsule autoencoders, we then jump to high level representations and we're doing it all unsupervised.
Whereas in the paper with the 16 by 16 patches, they're training it supervised to perform classification. They're not training it unsupervised. So somewhat different. But this general trend of, extract some pieces and then get them to interact so you get clearer about what the pieces are, which is what transformers do, that seems to be a very good way to go about doing, building layers of representation.
CRAIG: Yeah. I'm going to ask a question that I'll probably cut out because it's going to sound. Ignorant, but in, in transformers, both, in capsule networks, or in natural language processing models like Bert or GPT-3, it relies on, massive, parameters, right? Billions of parameters.
GEOFF: Actually, less parameters than convolutional neural nets, but...
CRAIG: Okay. but, but it, it goes out into the world of the internet in this case and, and ingests all of this. And then it looks to me like it's a kind of search it's going out and, and, and finding, something that matches a representation.
And then fills it in with, with what's already out there. Is that wrong?
GEOFF: That's wrong. Yeah. You can go and find the closest thing. I mean, if you give it a story to complete, you can find the closest match on the web. And it'll do completions that are nothing like the closest match on the web. Basically, it's taking all this information in this data it's observed and it's boiling it down into these parameters that allow it to produce similar stuff, but not by matching to particular instances it's seen already.
CRAIG: And in the same way, capsule networks are creating a new representation
GEOFF: Yeah, and capsules networks should be able to deal with a new view of the same object.
CRAIG: Yeah. So where is your research going now? I mean, on these three streams or are there other streams?
GEOFF: My main interest always been unsupervised learning because I think that's what most human learning is. I'm interested in developing capsules further and in things like SimCLR,
I'm also interested in making distillation work better. So, the idea of distillation is you have a great big model and you've trained it on data and it's extracted the regular patterns in the data and got them into its parameters.
And now you want to train a much smaller model that will be as good as the big model, almost as good as a big model, but you couldn't have trained directly on the data. And so, we see this all over the place. So, insects are like this. The way insects, roughly speaking, the way most insects work is they have one stage that's just about extracting nutrients from the environment and that's called the larva.
Okay. And it's just an eating machine. And this great fat, ugly grub, or a caterpillar for a butterfly, just gets fat. That's its role in life. And then it basically gets turned into a soup. And out of that soup, you build the adult, which may look nothing like the larva. I mean, a caterpillar and a butterfly are very different things.
And they're optimized for different things. So, the larva is optimized for sucking nutrients out of the environment. And then the butterfly is optimized for traveling around and mating. And those are very different activities from sucking nutrients out of the environment. Now butterflies also get nutrients out of the environment, but they're not machines for doing that like larva. You also see it in mining - a nice Canadian example. So, if you want gold, first you take a chunk of the earth, then you convert it to pay dirt. and you have one way of doing that. And then you take the pay dirt and you heat it up very hot to try and get the gold out. I think that's how it works.
And the same for data mining. So, you've got a big set of data. And you you'd like to end up with a small agile model that can look at a new example and tell you what class it is, for example, but the kinds of models that are good at sucking structure out of the data are not necessarily the same as the models that are going to be small and agile and easy to use on your cell phone for making the right decisions.
And so, the idea is you use one kind of model for sucking structure to the data, a great big model. Once you suck the structure of the data, you get the great big model to train a small model. And it turns out the big model is much better at training a small model than just the raw data. And it's like an apprenticeship. Or like the way science works. Once scientists have done their research and figured out how things work, they can then teach school kids how things work. So, sort of any smart school kid can learn Newton's mechanics. But not any smart schoolkid could have invented Newton's mechanics. Once Newton invented it, which is kind of tricky, you can then explain it quite well. And you can, you can instill a model of it in a school kid. And so, the idea of distillation is we use great big neural networks for getting structure out of the data and then much smaller, more agile networks for actually using what we discovered. And it's now being quite widely used.
It's used in Bert, for example, to get more agile networks. But I think there's probably ways of making it much better. And that's another thing I'm working on.
CRAIG: Yeah. on, capsule networks in the, in the, AAAI talk. You all agreed, you and Yann LeCun and Yoshua Bengio, on the, on the, on the direction of your research in unsupervised learning and a lot of what Yann LeCun has been doing with video sounds similar to what you talk about both with capsule networks, and SimCLR.
GEOFF: Yann and I share a lot of intuitions. We, we, we worked together for a while. We have a very similar view of the world.
CRAIG: So, can you talk about how, again, at some point all of these ideas will converge? How his research relates to your research, particularly his research on video?
GEOFF: Our goals are very similar and the methods are quite similar.
And as we start applying SimCLR-like methods to video, they're going to get even more similar. So, this idea of contrastive representation learning seems to be very powerful and Yann's exploiting it. Ting Chen made it work really well for static images. And we're now trying to extend that to video, but we're trying to extend it using attention, which is going to be very important for video because you can't possibly process everything in a video at a high resolution.
CRAIG: Yeah. Yeah. And, and that's interesting, when, when you relate, this, machine learning to learning in the brain and certainly attention is, is, critical. Yeah. and can you talk a little bit about how, these models, even if they're not using the algorithms that you think are operating in the brain, how they are analogous to, to human learning. I mean, there's this huge amount of unsupervised learning that goes on with, as you were saying with capsules that at the end, there's a little bit of supervised learning that that puts labels to representation.
GEOFF: Let me just clarify that the first few versions of capsules we did were all using supervised learning because we thought that would make life easier, even though that's not really what we believed in, but now we're doing unsupervised learning and it works better.
And it's, it's ideologically far more satisfactory.
CRAIG: Yeah. but in, in the unsupervised capsule networks, at the, at the end, you connected it to language through ...
GEOFF: That's just to show that it's learned something sensible. I mean, obviously you want to connect to language. There's very nice work going on at Google now in robotics, where they're using deep learning for getting robot arms to do things, to manipulate things. But they're also interfacing it with language. So, you can tell a robot what to do, and the robot can also tell you what it's doing. And that seems very important. It also seems that if the robot can tell you what it's doing, like, you know, it's opening the drawer.
The objections of people like Gary Marcus have to natural language processing, saying it doesn't really understand what's going on. But you know, if it says I'm opening the drawer and I'm taking out a block and he opens his drawer and takes out a block, it's very hard to say it doesn't understand what's going on.
CRAIG: we mentioned at the beginning the laws of physics, learning the laws of physics, and you don't need language to learn the laws of physics. You do need a linguistic interface, to, to look at a tree and a car and be able to identify them as a tree in a car.
Can you talk about learning, something like the laws of physics that doesn't require language to be attached to it, but nonetheless, there's learning that takes place?
GEOFF: Yeah. At high school you may learn the laws of physics. We learn sort of common-sense physics. So, we learn, you know, if you throw something up it comes down again and if we're good, we learn how to throw a basketball so it goes through the hoop. And that's a very impressive skill cause you're throwing it from like 20 feet away and you have to get it right to a few inches. That's an amazing thing to be able to do.
And we, we don't learn that by being told how to do it. We don't run that using language at all. We learn it from people who say trial and error, but we're understanding how the world works just by observing the world. also, by acting in the world. So just passively observing the world will allow you to understand it, but it's not nearly as good as acting in the world.
And in fact, if you think about perception for robots or wander around an act in the world, it changes your view of high perception should work. So, if you're just taking images or videos and just passively processing them, it doesn't make you think about attention. But as soon as you have a robot, that's moving around in the world, it's got to decide what to look at.
And the sort of primary question in vision is where should I look next? And that's been sort of widely ignored by people that just process static images. Attention is crucial when it's sort of central to how human vision works.
CRAIG: Can you sum up a little bit, everyone likes to hear about convergence of all of these things, you know, convergence of, of computer vision with, natural language processing, convergence of unsupervised learning wit. supervised learning and reinforcement learning. is, is that beyond what, what you're really focused on? because you're focused on, on, the basic research, not necessarily, building models that pulls it all together.
GEOFF: Let me just say something about supervised learning versus unsupervised learning, because, it sounds like a very simple distinction. but actually, it's very confusing. So, if you ask. When, when a kid's mother says that's a cow, we tend to think of it in machine learning as the mother supplied a label.
But what's really happening is this. The child has some sensory input and the child is getting a correlation between the visual sensory input and the auditory sensory input. Now the top level, the auditory thing gives you the word cow and the visual thing gives you whatever your visual is and you learn they go together. But actually, supervision when you actually get it in reality, it's just another correlation and there's, so it's all about complex correlations in the sensory input, called supervised and unsupervised learning.
And then there's correlations with payoffs. And that's reinforcement learning, but I think the correlations with payoffs don't have enough structure in them for you to do most of the learning. So most of the learning's unsupervised.
CRAIG: Okay, well, let's, let's leave it there. I really appreciate it. And, it's been a fascinating conversation and I'll edit it down to be a coherent on my side.
GEOFF: Bye for now.
CRAIG: Yeah, bye-bye. That's it for this week's podcast. I want to thank Geoff for his time. If you want to learn more about the episode today, you can find a scrolling transcript on our website, www.eye-on.ai.
 
Related:

12 Deep Learning Researchers and Leaders
Some Musings on Capsule Networks and DLPaper2Code
2020: A Year Full of Amazing AI Papers — A Review"
https://www.kdnuggets.com/2020/11/5-things-doing-wrong-pycaret.html,5 Things You Are Doing Wrong in PyCaret,PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few words only. This makes experiments exponentially fast and efficient. Find out 5 ways to improve your usage of the library.,"comments
By Moez Ali, Founder & Author of PyCaret


Photo by Ben White on Unsplash

 
PyCaret
 
PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is an end-to-end machine learning and model management tool that speeds up the machine learning experiment cycle and makes you more productive.
In comparison with the other open-source machine learning libraries, PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few words only. This makes experiments exponentially fast and efficient.
Official: https://www.pycaret.org
Docs: https://pycaret.readthedocs.io/en/latest/
Git: https://www.github.com/pycaret/pycaret
 
👉 compare_models does more than what you think
 
When we had released version 1.0 of PyCaret in Apr 2020, compare_models function was comparing all the models in the library to return the averaged cross-validated performance metrics. Based on which you would use create_model to train the best performing model and get the trained model output that you can use for predictions.
This behavior was later changed in version 2.0. compare_models now returns the best model based on the n_select parameter which by default is set to 1 which means that it will return the best model (by default).


compare_models(n_select = 1)

 
By changing the default n_select parameter to 3, you can get a list of top 3 models. For example:


compare_models(n_select = 3)

 
The returned objects are trained models, you really don’t need to call create_model again to train them. You can use these models to generate diagnostic plots or to even use them for predictions, if you would like. For example:


predict_model function

 
👉You think you are limited to scikit-learn models only
 
We recieve a lot of requests to include non scikit-learn models in the model library. Many people don’t realize that you are not limited to the default models only. create_model function also accepts untrained model object in addition to the ID’s of models available in the model library. As long as your object is compatible with scikit-learn fit/predict API, it will work just fine. For example, here we have trained and evaluated NGBClassifier from ngboost library by simply importing untrained NGBClassifier:


create_model with external models

 
You can also pass the untrained models in the include parameter of the compare_models and it will just work normally.


compare_models with untrained object

 
Notice that include parameters include ID’s for three untrained model from the model library i.e. Logistic Regression, Decision Tree and K Neighbors and one untrained object from ngboost library. Also, notice that the index represents the position of the model entered in the include parameter.
 
👉You don’t know about the pull( ) function
 
All training functions (create_model, tune_model, ensemble_model, etc.) in PyCaret displays a score grid but it doesn’t return the score grid. Hence you cannot store the score grid in an object like pandas.DataFrame. However, there is a function called pull that allows you to do that. For example:


pull function with create_model

 
This will also work for holdout score grid when you use predict_model function.


pull function with predict_model

 
Now that you can access metrics as pandas.DataFrame, you can do wonders. For example, you can create a loop to train a model with different parameters and create a comparison table with this simple code:


create_model and pull function

 
👉 You think PyCaret is a black-box, it is not.
 
Another common confusion is that all the preprocessing is happening behind the scenes and is not accessible to users. As such, you cannot audit what happens when you ran the setup function. This is not True.
There are two functions in PyCaret get_config and set_config that allows you to access and change everything in the background, from your training set to the random state of your model. You can check the documentation of get_config function by simply calling help(get_config) to see which variables are accessible to you:


help(get_config)

 
You can access the variable by calling it inside the get_config function. For example to access X_train transformed dataset, you will write this:


get_config(‘X_train’)

 
You can use the set_config function to change the environment variables. With what you know so far about pull, get_config, and set_config function, you can create some pretty sophisticated workflows. For example, you can resample holdout set N times to evaluate averaged performance metrics instead of relying on one holdout set:

import numpy as np
Xtest = get_config('X_test')
ytest = get_config('y_test')AUC = []for i in np.random.randint(0,1000,size=10):
    Xtest_sampled = Xtest.sample(n = 100, random_state = i)
    ytest_sampled = ytest[Xtest_sampled.index]
    set_config('X_test', Xtest_sampled)
    set_config('y_test', ytest_sampled)
    predict_model(dt);
    AUC.append(pull()['AUC'][0])>>> print(AUC)[Output]: [0.8182, 0.7483, 0.7812, 0.7887, 0.7799, 0.7967, 0.7812, 0.7209, 0.7958, 0.7404]>>> print(np.array(AUC).mean())[Output]: 0.77513


 
👉You are not logging your experiments
 
If you are not logging your experiments, you should start logging them now. Whether you want to use MLFlow backend server or not, you should still log all your experiments. When you perform any experiment, you generate a lot of meta data which is impossible to keep track of manually.
PyCaret’s logging functionality will generate a nice, light-weight, easy to understand excel spreadsheet when you use get_logs function. For example:

# loading dataset
from pycaret.datasets import get_data
data = get_data('juice')# initializing setup
from pycaret.classification import *
s = setup(data, target = 'Purchase', silent = True, log_experiment = True, experiment_name = 'juice1')# compare baseline models
best = compare_models()# generate logs
get_logs()




get_logs()

 
In this very short experiment we have generated over 3,000 meta data points (metrics, hyperparameters, runtime, etc.). Imagine how you would have manually kept track of these datapoints? Perhaps, it’s not practically possible. Fortunately, PyCaret provides a simple way to do it. Simply set log_experiment to True in the setup function.
 
There is no limit to what you can achieve using the lightweight workflow automation library in Python. If you find this useful, please do not forget to give us ⭐️ on our GitHub repo.
To hear more about PyCaret follow us on LinkedIn and Youtube.
To learn more about all the updates in PyCaret 2.2, please see the release notes or read this announcement.
 
Important Links
 
User Guide
Documentation
Official Tutorials
Example Notebooks
Other Resources
 
Want to learn about a specific module?
 
Click on the links below to see the documentation and working examples.
Classification
Regression
Clustering
Anomaly Detection
Natural Language Processing
Association Rule Mining
 
Bio: Moez Ali is a Data Scientist, and is Founder & Author of PyCaret.
Original. Reposted with permission.
Related:

5 Things You Don’t Know About PyCaret
Deploy a Machine Learning Pipeline to the Cloud Using a Docker Container
GitHub is the Best AutoML You Will Ever Need"
https://www.kdnuggets.com/2021/07/10-machine-learning-model-training-mistakes.html,10 Machine Learning Model Training Mistakes,These common ML model training mistakes are easy to overlook but costly to redeem.,"comments
By Sandeep Uttamchandani, Ph.D., Both a Product/Software Builder (VP of Engg) & Leader in operating enterprise-wide Data/AI initiatives (CDO)


Image by Tumisu from Pixabay

 
ML model training is the most time-consuming and resource-expensive part of the overall model-building journey. Training by definition is iterative, but somewhere during the iterations, mistakes seep into the mix. In this article, I share the ten deadly sins during ML model training — these are the most common as well as the easiest to overlook.
 
Ten Deadly Sins of ML Model Training
 

1. Blindly increasing the number of epochs when the model is not converging
 
During model training, there are scenarios when the loss-epoch graph keeps bouncing around and does not seem to converge irrespective of the number of epochs. There is no silver bullet as there are multiple root causes to investigate — bad training examples, missing truths, changing data distributions, too high a learning rate. The most common one I have seen is bad training examples related to a combination of anomalous data and incorrect labels.
 
2. Not shuffling the training dataset
 
Sometimes there are scenarios where the model seems to be converging, but suddenly the loss value increases significantly, i.e., loss value reduces and then increases significantly with epochs. There are multiple reasons for this kind of exploding loss. The most common one I have seen is outliers in the data that are not evenly distributed/shuffled in the data. Shuffling, in general, is an important step including for patterns where the loss is showing a repeating step function behavior.
 
3. In multiclass classification, not prioritizing specific per-class metrics accuracy
 
For multiclass prediction problems, instead of tracking just the overall classification accuracy, it is often useful to prioritize the accuracy of specific classes and iteratively work on improving the model class by class. For instance, in classifying different forms of fraudulent transactions, focus on increasing the recall of specific classes (such as foreign transactions) based on business needs.
 
4. Assuming specificity will lead to lower model accuracy
 
Instead of building a generic model, imagine building a model for a specific geographic region or specific user persona. Specificity will make the data more sparse but can lead to better accuracy for those specific problems. It is important to explore the specificity and sparsity trade-off during tuning.
 
5. Ignoring prediction bias
 
Prediction bias is the difference between the average of predictions and the average of labels in the dataset. Prediction bias serves as an early indicator of model issues. A big nonzero prediction bias is indicative of a bug somewhere in the model. There’s an interesting Facebook paper in the context of ad CTR. Typically, the bias is useful to measure across prediction buckets.
 
6. Calling it a success just on model accuracy numbers
 
Accuracy of 95% means 95 of 100 predictions were correct. Accuracy is a flawed metric with a class imbalance in the dataset. Instead investigate deeply into metrics, such as precision/recall and how it correlates to overall user metrics such as spam detection, tumor classification, etc.
 
7. Not understanding the impact of regularization lambda
 
Lambda is a key parameter in striking the balance between simplicity and training-data fit. High lambda → simple model → possibly underfitting. Low lambda → complex model → potential overfitting your data (won’t be able to generalize to new data). The ideal value of lambda is one that generalizes well to previously unseen data: data-dependent and requires analysis.
 
8. Using the same test set over and over
 
The more the same data is used for parameter and hyperparameter settings, the lesser confidence that the results will actually generalize. It is important to collect more data and keep adding to the test and validation sets.
 
9. Not paying attention to initiation value in neural networks
 
Given non-convex optimization in NN, initialization matters.
 
10. Assuming wrong labels always need to be fixed
 
When wrong labels are detected, it is tempting to jump in and get them fixed. It is important to first analyze misclassified examples for the root cause. Oftentimes, errors due to incorrect labels may be a very small percentage. There might be a bigger opportunity to better train for specific data slices that might be the predominant root cause.
 
To summarize, avoiding these mistakes puts you significantly ahead of most other teams. Incorporate these as a checklist in your process.
 
Bio: Sandeep Uttamchandani, Ph.D.: Data + AI/ML -- Both a Product/Software Builder (VP of Engg) & Leader in operating enterprise-wide Data/AI initiatives (CDO) | O'Reilly Book Author | Founder - DataForHumanity (non-profit)
Original. Reposted with permission.
Related:

How to Determine if Your Machine Learning Model is Overtrained
Write and train your own custom machine learning models using PyCaret
How to break a model in 20 days — a tutorial on production model analytics"
https://www.kdnuggets.com/2021/07/high-performance-deep-learning-part3.html,"High-Performance Deep Learning: How to train smaller, faster, and better models – Part 3","Now that you are ready to efficiently build advanced deep learning models with the right software and hardware tools, the techniques involved in implementing such efforts must be explored to improve model quality and obtain the performance that your organization desires.","By Gaurav Menghani, Software Engineer at Google AI.
comments

In the previous parts (Part 1 & Part 2), we discussed why efficiency is important for deep learning models to achieve high-performance models that are pareto-optimal, as well as the focus areas for efficiency in Deep Learning. Let us now dive deeper into examples of tools and techniques that fall in these focus areas.
 
Compression Techniques
 
Compression techniques, as mentioned earlier, are generic techniques that can help achieve a more efficient representation of one or more layers in a neural network, with a possible quality trade-off. The efficiency could come from improving one or more of the footprint metrics, such as model size, inference latency, training time required for convergence, etc., in exchange for as little quality loss as possible. Often the model could be over-parameterized. In such cases, these techniques help improve generalization on unseen data as well.
Pruning: One of the popular compression techniques is Pruning, where we prune unimportant network connections, hence making the network sparse. LeCun et al. [1], in their paper titled ""Optimal Brain Damage"", trimmed the number of parameters (connections between layers) in their neural network by a factor of four while increasing both the inference speed and generalization.

An illustration of pruning in neural networks.
A similar approach was followed by the Optimal Brain Surgeon work (OBD) by Hassibi et al. [2] and by Zhu et al. [3]. These methods take a network that has been pre-trained to reasonable quality and then iteratively prune the parameters that have the lowest saliency score, which measures the importance of a particular connection, such that the impact on the validation loss is minimized. Once pruning concludes, the network is fine-tuned with the remaining parameters. The process is repeated until the network is pruned to the desired level.
Amongst the various works on pruning, the differences occur in the following dimensions:

Saliency: This is the heuristic for determining which connection should be pruned. This can be based on second-order derivatives [1, 2] of the connection weight with respect to the loss function, the magnitude of the connection weight [3], and so on.
Unstructured v/s Structured: The most flexible way of pruning is unstructured (or random) pruning, where all given parameters are treated equally. In structured pruning, parameters are pruned in blocks of size > 1 (such as pruning row-wise in a weight matrix or pruning channelwise in a convolutional filter (example: [4,5]). Structured pruning allows easier leveraging of inference-time gains in size and latency since these blocks of pruned parameters can be intelligently skipped for storage and inference.


Unstructured vs. Structured Pruning of a weight matrix, respectively.

Distribution: One could set a pruning budget that is the same for each layer, or it could be allocated on a per-layer basis [6]. The intuition being that certain layers are more amenable to pruning than others. For example, often, the first few layers are already small enough that they cannot tolerate significant sparsity [7].
Scheduling: Yet additional criteria are how much to prune and when? Do we want to prune an equal number of parameters every round [8], or do we prune at a higher pace in the beginning and gradually slow down [9]?
Regrowth: In some cases, the network is allowed to regrow the pruned connections [9], such that the network constantly operates with the same percentage of connections pruned.

In terms of practical usage, structured pruning with a meaningful block size can help improve latency. Elsen et al. [7] construct sparse convolutional networks that outperform their dense counterparts by 1.3 - 2.4× with ≈ 66% of the parameters while retaining the same Top-1 accuracy. They do this via their library to convert from the NHWC (channels-last) standard dense representation to a special NCHW (channels-first) ‘Block Compressed Sparse Row’ (BCSR) representation which is suitable for fast inference using their fast kernels on ARM devices, WebAssembly etc. [10]. Although they also introduce some constraints on the kinds of sparse networks that can be accelerated. Overall, this is a promising step towards practical improvements in footprint metrics with pruned networks.
Quantization:  Quantization is another popular compression technique. It exploits the idea that almost all the weights of a typical network are in 32-bit floating-point values, and if we are okay with losing some model quality (accuracy, precision, recall, etc.), we can store these values in a lower precision format (16-bit, 8-bit, 4-bit, etc.).
For example, when a model is persisted, we can map the minimum value in a weight matrix to 0 and the maximum value to 2b-1 (where b is the number of bits of precision), and linearly extrapolate all values between them to an integer value. Often, this might be sufficient for the purposes of reducing model size. For example, if b = 8, we are mapping the 32-bit floating-point weights to 8-bit unsigned integers. This would lead to a 4x reduction in space. When doing inference (computing model predictions), we can recover a lossy representation of the original floating-point value (due to the rounding error), using the quantized value and the min & max floating-point values of the array. This step is referred to as Weight Quantization since we are quantizing the model’s weights.

Mapping continuous high-precision values to discrete low-precision integer values. Source
The lossy representation and the rounding error might be okay for larger networks with built-in redundancy due to a large number of parameters but might lead to a drop in accuracy for smaller networks, which would likely be sensitive to these errors.
We can solve this issue (in an experimental manner) by simulating the rounding behavior of weight quantization during the training. We do this by adding nodes in the model training graph that quantize and dequantize the activations and weight matrices, such that the training-time inputs to a neural network operation look identical to what they would have during the inference stage. Such nodes are referred to as Fake Quantization nodes. Training in such a manner makes the networks more robust to the behavior of quantization in inference mode. Note that we are doing Activation Quantization along with Weight Quantization during training now. This step of training-time simulated quantization is described in detail by Jacob et al. and Krishnamoorthi et al. [11,12]

Original model training graph and the graph with fake quantization nodes. Source
Since both weights and activations are run in simulated quantized mode, that means all layers receive inputs that could be represented in lower-precision, and after the model is trained, it should be robust enough to do the math operations directly in lower-precision. As an example, if we train the model to replicate quantization in the 8-bit domain, the model can be deployed to do the matrix multiplication and other operations with 8-bit integers.
On resource-constrained devices (such as mobile, embedded, and IoT devices), 8-bit operations can be sped up between 1.5 - 2x using libraries like GEMMLOWP [13] which rely on hardware support for such acceleration such as the Neon intrinsics on ARM processors [14]. Further, frameworks such as Tensorflow Lite enable their users to directly use quantized operations without having to bother about the lower-level implementations.

Top-1 Accuracy vs. Model Latency for models with and without quantization. Source
Apart from Pruning and Quantization, there are other techniques like Low-Rank Matrix Factorization, K-Means Clustering, Weight-Sharing etc. which are also actively being used for model compression [15].
Overall, we saw that compression techniques could be used to reduce a model’s footprint (size, latency, etc.) while trading off some quality (accuracy, precision, recall, etc.) in return.
 
Learning Techniques
 
Distillation: As mentioned earlier, Learning techniques try to train a model differently in order to obtain a better performance. For example, Hinton et al. [16], in their seminal work, explored how smaller networks can be taught to extract dark knowledge from larger models/ensembles of larger models. They use a larger teacher model to generate soft labels on existing labeled data.
The soft labels assign a probability to each possible class instead of hard binary values in the original data. The intuition is that these soft labels capture the relationship between the different classes from which the model can learn. For example, a truck is more similar to a car than to an apple, which the model might not be able to learn directly from hard labels. The student network learns to minimize the cross-entropy loss on these soft labels, along with the original ground-truth hard labels. The weights of each of these loss functions can be scaled based on the results from experimentation.

Distillation of a smaller student model from a larger pre-trained teacher model.
In the paper, Hinton et al. [16] were able to closely match the accuracy of a 10-model ensemble for a speech recognition task with a single distilled model. There are other comprehensive studies [17,18] demonstrating the significant improvements in model quality of smaller models. As an example, Sanh. et al. [18] were able to distill a student model that retains 97% of the performance of BERT-Base while being 40% smaller and 60% faster on CPU.
Data Augmentation: Typically for large models and complex tasks, the more data you have, the higher the chances of improving your model’s performance. However, getting high-quality labeled data is often both slow and expensive since they usually require a human in the loop. Learning from this data which has been labeled by humans, is referred to as supervised learning. It works very well when we have the resources to pay for the labels, but we can and should do better.
Data Augmentation is a nifty way of improving the performance of the model. Usually, it involves making transformations to your data, such that it does not require re-labeling (label-invariant transformations). For example, if you were teaching your neural network to classify an image to contain a dog or a cat, rotating the image would not change the label. Other transformations could be horizontal/vertical flipping, stretching, cropping, adding Gaussian noise, etc. Similarly, if you were detecting the sentiment of a given piece of text, introducing a typo would likely not change the label.
Such label-invariant transformations have been used across the board in popular deep learning models. They are especially handy when you have a large number of classes and/or few examples for certain classes.

Some common types of data augmentations. Source
There are other transformations such as Mixup [19], which mix inputs from two different classes in a weighted manner and treat the label to be a similarly weighted combination of the two classes. The idea is that the model should be able to extract out features that are relevant for both classes.
These techniques are really introducing data efficiency to the pipeline. It is not very different from teaching a kid to identify real-life objects in different contexts.
Self-Supervised Learning: There is rapid progress in an adjacent area, where we can learn generic models that completely skip the need for labels for extracting meaning out of data. With methods like contrastive learning [20], we can train a model such that it learns a representation for the input, such that similar inputs would have similar representations, while unrelated inputs should have very dissimilar representations. These representations are n-dimensional vectors (embeddings) which can then be useful as features in other tasks where we might not have enough data to train models from scratch. We can view the first step of using unlabeled data as pre-training and the next step as fine-tuning.

Contrastive Learning. Source
This two-step process of pre-training on unlabeled data and fine-tuning on labeled data has also gained rapid acceptance in the NLP community. ULMFiT [21] pioneered the idea of training a general-purpose language model, where the model learns to solve the task of predicting the next word in a given sentence.
The authors found that using a large corpus of preprocessed but unlabeled data such as the WikiText-103 (derived from English Wikipedia pages) was a good choice for the pre-training step. This was sufficient for the model to learn general properties about the language. The authors found that fine-tuning such a pre-trained model for a binary classification problem required only 100 labeled examples (as compared to 10,000 labeled examples otherwise).

Rapid convergence with ULMFiT. Source

High-level approach of pre-training on a large corpus and fine-tuning on the relevant dataset. Source
This idea was also explored in BERT models, where the pre-training steps involve learning a  bi-directional masked language model, such that the model has to predict a missing word in the middle of a sentence.
Overall, learning techniques help us improve model quality without impacting the footprint. This could be used for improving model quality for deployment. If the original model quality was satisfactory, you could also exchange the newfound quality gains for improving model size and latency by simply reducing the number of parameters in your network until you go back to the minimum viable model quality.
In our next part, we will continue to go over examples of tools and techniques that fit in the remaining three focus areas. Also, feel free to go over our survey paper that explores this topic in detail.
 
References
[1] Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal brain damage. In Advances in neural information processing systems. 598–605.
[2] Babak Hassibi, David G Stork, and Gregory J Wolff. 1993. Optimal brain surgeon and general network pruning. In IEEE international conference on neural networks. IEEE, 293–299.
[3] Michael Zhu and Suyog Gupta. 2018. To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings. OpenReview.net. https://openreview.net/forum?id=Sy1iIDkPM
[4] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. 2017. Structured pruning of deep convolutional neural networks. ACM Journal on Emerging Technologies in Computing Systems (JETC) 13, 3 (2017), 1–18.
[5] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2016. Pruning Filters for Efficient ConvNets. In ICLR (Poster).
[6] Xin Dong, Shangyu Chen, and Sinno Jialin Pan. 2017. Learning to prune deep neural networks via layer-wise optimal brain surgeon. arXiv preprint arXiv:1705.07565 (2017).
[7] Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan. 2020. Fast sparse convnets. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 14629–14638.
[8] Song Han, Jeff Pool, John Tran, and William J Dally. 2015. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626 (2015).
[9] Tim Dettmers and Luke Zettlemoyer. 2019. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840 (2019).
[10] XNNPACK. https://github.com/google/XNNPACK
[11] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713
[12] Raghuraman Krishnamoorthi. 2018. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv (Jun 2018). arXiv:1806.08342 https://arxiv.org/abs/1806.08342v1
[13] GEMMLOWP. https://github.com/google/gemmlowp
[14] Arm Ltd. 2021. SIMD ISAs | Neon – Arm Developer. https://developer.arm.com/architectures/instruction-sets/simdisas/neon [Online; accessed 3. Jun. 2021].
[15] Rina Panigrahy. 2021. Matrix Compression Operator. https://blog.tensorflow.org/2020/02/matrix-compressionoperator-tensorflow.html
[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015).
[17] Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, and Matt Richardson. 2016. Do deep convolutional nets really need to be deep and convolutional? arXiv preprint arXiv:1603.05691 (2016).
[18] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).
[19] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2017. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017).
[20] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597–1607.
[21] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146 (2018).
Bio: Gaurav Menghani is a Staff Software Engineer at Google Research where he leads research projects geared towards optimizing large machine learning models for efficient training and inference on devices ranging from tiny microcontrollers to Tensor Processing Unit (TPU)-based servers. His work has positively impacted > 1 Billion of active users across YouTube, Cloud, Ads, Chrome, etc. He is also an author of an upcoming book with Manning Publication on Efficient Machine Learning. Before Google, Gaurav worked at Facebook for 4.5 years and has contributed significantly to Facebook’s Search system and large-scale distributed databases. He has an M.S. in Computer Science from Stony Brook University.
twitter.com/GauravML
linkedin.com/in/gauravmenghani
www.gaurav.im
Related:

High Performance Deep Learning, Part 1
High-Performance Deep Learning: How to train smaller, faster, and better models – Part 2
5 Challenges to Scaling Machine Learning Models"
https://www.kdnuggets.com/2021/05/essential-math-data-science-basis-change-basis.html,Essential Math for Data Science: Basis and Change of Basis,"In this article, you will learn what the basis of a vector space is, see that any vectors of the space are linear combinations of the basis vectors, and see how to change the basis using change of basis matrices.","By Hadrien Jean, Machine Learning Scientist.
comments

 
Basis and Change of Basis
 
One way to understand eigendecomposition is to consider it as a change of basis. You’ll learn in this article what is the basis of a vector space.
You’ll see that any vector of the space are linear combinations of the basis vectors and that the number you see in vectors depends on the basis you choose.
Finally, you’ll see how to change the basis using change of basis matrices.
It is a nice way to consider matrix factorization as eigendecomposition or Singular Value Decomposition. As you can see in Chapter 09 of Essential Math for Data Science, with eigendecomposition, you choose the basis such that the new matrix (the one that is similar to the original matrix) becomes diagonal.
 
Definitions
 
The basis is a coordinate system used to describe vector spaces (sets of vectors). It is a reference that you use to associate numbers with geometric vectors.
To be considered as a basis, a set of vectors must:

Be linearly independent.
Span the space.

Every vector in the space is a unique combination of the basis vectors. The dimension of a space is defined to be the size of a basis set. For instance, there are two basis vectors in ℝ2 (corresponding to the x and y-axis in the Cartesian plane), or three in ℝ3.
As shown in section 7.4 of Essential Math for Data Science, if the number of vectors in a set is larger than the dimensions of the space, they can’t be linearly independent. If a set contains fewer vectors than the number of dimensions, these vectors can’t span the whole space.
As you saw, vectors can be represented as arrows going from the origin to a point in space. The coordinates of this point can be stored in a list. The geometric representation of a vector in the Cartesian plane implies that we take a reference: the directions given by the two axes x and y.
Basis vectors are the vectors corresponding to this reference. In the Cartesian plane, the basis vectors are orthogonal unit vectors (length of one), generally denoted as i and j.

Figure 1: The basis vectors in the Cartesian plane.
 
For instance, in Figure 1, the basis vectors i and j point in the direction of the x-axis and y-axis respectively. These vectors give the standard basis. If you put these basis vectors into a matrix, you have the following identity matrix (for more details about identity matrices, see 6.4.3 in Essential Math for Data Science):

Thus, the columns of I2 span ℝ2. In the same way, the columns of I3 span ℝ3 and so on.

Orthogonal basis
Basis vectors can be orthogonal because orthogonal vectors are independent. However, the converse is not necessarily true: non-orthogonal vectors can be linearly independent and thus form a basis (but not a standard basis).

The basis of your vector space is very important because the values of the coordinates corresponding to the vectors depend on this basis. By the way, you can choose different basis vectors, like in the ones in Figure 2 for instance.

Figure 2: Another set of basis vectors.
 
Keep in mind that vector coordinates depend on an implicit choice of basis vectors.
 
Linear Combination of Basis Vectors
 
You can consider any vector in a vector space as a linear combination of the basis vectors.
For instance, take the following two-dimensional vector v:


Figure 3: Components of the vector v.
 
The components of the vector v are the projections on the x-axis and on the y-axis (vx and vy, as illustrated in Figure 3). The vector v corresponds to the sum of its components: v = vx + vy, and you can obtain these components by scaling the basis vectors: vx = 2i and vy = −0.5j. Thus, the vector v shown in Figure 3 can be considered as a linear combination of the two basis vectors i and j:

 
Other Bases
 
The columns of identity matrices are not the only case of linearly independent columns vectors. It is possible to find other sets of n vectors linearly independent in ℝn.
For instance, let’s consider the following vectors in ℝ2:

and

The vectors v and w are represented in Figure 4.

Figure 4: Another basis in a two-dimensional space.
 
From the definition above, the vectors v and w are a basis because they are linearly independent (you can’t obtain one of them from combinations of the other) and they span the space (all the space can be reached from the linear combinations of these vectors).
It is critical to keep in mind that, when you use the components of vectors (for instance vx and vy, the x and y components of the vector v), the values are relative to the basis you chose. If you use another basis, these values will be different.
You can see in Chapter 09 and 10 of Essential Math for Data Science that the ability to change the bases is fundamental in linear algebra and is key to understand eigendecomposition or Singular Value Decomposition.
 
Vectors Are Defined With Respect to a Basis
 
You saw that to associate geometric vectors (arrows in the space) with coordinate vectors (arrays of numbers), you need a reference. This reference is the basis of your vector space. For this reason, a vector should always be defined with respect to a basis.
Let’s take the following vector:

The values of the x and y components are respectively 2 and -0.5. The standard basis is used when not specified.
You could write Iv to specify that these numbers correspond to coordinates with respect to the standard basis. In this case i is called the change of basis matrix.

You can define vectors with respect to another basis by using another matrix than i.
 
Linear Combinations of the Basis Vectors
 
Vector spaces (the set of possible vectors) are characterized in reference to a basis. The expression of a geometrical vector as an array of numbers implies that you choose a basis. With a different basis, the same vector v is associated with different numbers.
You saw that the basis is a set of linearly independent vectors that span the space. More precisely, a set of vectors is a basis if every vector from the space can be described as a finite linear combination of the components of the basis and if the set is linearly independent.
Consider the following two-dimensional vector:

In the ℝ2 Cartesian plane, you can consider v as a linear combination of the standard basis vectors i and j, as shown in Figure 5.

Figure 5: The vector v can be described as a linear combination of the basis vectors i and j.
 
But if you use another coordinate system, v is associated with new numbers. Figure 6 shows a representation of the vector v with a new coordinate system (i′ and j′).

Figure 6: The vector v with respect to the coordinates of the new basis.
 
In the new basis, v is a new set of numbers:

 
The Change of Basis Matrix
 
You can use a change of basis matrix to go from a basis to another. To find the matrix corresponding to new basis vectors, you can express these new basis vectors (i′ and j′) as coordinates in the old basis (i and j).
Let’s take again the preceding example. You have:

and

This is illustrated in Figure 7.

Figure 7: The coordinates of the new basis vectors with respect to the old basis.
 
Since they are basis vectors, i′ and j′ can be expressed as linear combinations of i and j.:


Let’s write these equations under the matrix form:

To have the basis vectors as columns, you need to transpose the matrices. You get:

This matrix is called the change of basis matrix. Let’s call it CC:

As you can notice, each column of the change of basis matrix is a basis vector of the new basis. You’ll see next that you can use the change of basis matrix CC to convert vectors from the output basis to the input basis.

Change of basis vs linear transformation 
The difference between change of basis and linear transformation is conceptual. Sometimes it is useful to consider the effect of a matrix as a change of basis; sometimes you get more insights when you think of it as a linear transformation. Either you move the vector or you move its reference. This is why rotating the coordinate system has an inverse effect compared to rotating the vector itself. For eigendecomposition and SVD, both of these views are usually taken together, which can be confusing at first. Keeping this difference in mind will be useful throughout the end of the book. The main technical difference between the two is that change of basis must be invertible, which is not required for linear transformations.

 
Finding the Change of Basis Matrix
 
A change of basis matrix maps an input basis to an output basis. Let’s call the input basis B1 with the basis vectors i and j, and the output basis B2 with the basis vectors i′ and j′. You have:

and

From the equation of the change of basis, you have:

If you want to find the change of basis matrix given B1 and B2, you need to calculate the inverse of B1 to isolate C:

In words, you can calculate the change of basis matrix by multiplying the inverse of the input basis matrix (, which contains the input basis vectors as columns) by the output basis matrix (, which contains the output basis vectors as columns).

Converting vectors from the output to the input basis 
Be careful, this change of basis matrix allows you to convert vectors from  to  and not the opposite. Intuitively, this is because moving an object is the opposite to moving the reference. Thus, to go from  to , you must use the inverse of the change of basis matrix .

Note that if the input basis is the standard basis (), then the change of basis matrix is simply the output basis matrix:


Invertible Change of Basis Matrix
Since the basis vectors are linearly independent, the columns of C are linearly independent, and thus, as stated in section 7.4 of Essential Math for Data Science, C is invertible.

 
Example: Changing the Basis of a Vector
 
Let’s change the basis of a vector v, using again the geometric vectors represented in Figure 6.
 
Notation
 
You’ll change the basis of v from the standard basis to a new basis. Let’s denote the standard basis as  and the new basis as . Remember that the basis is a matrix containing the basis vectors as columns. You have:

and

Let’s denote the vector v relative to the basis  as :

The goal is to find the coordinates of v relative to the basis , denoted as .

Square bracket notation
To distinguish the basis used to define a vector, you can put the basis name (like ) in subscript after the vector name enclosed in square brackets. For instance,  denotes the vector v relative to the basis , also called the representation of v with respect to 

 
Using Linear Combinations
 

 

 
Let’s code this:

v_B1 = np.array([2, 1])
B_2 = np.array([
    [0.8, -1.3],
    [1.5, 0.3]
])

v_B2 = np.linalg.inv(B_2) @ v_B1
v_B2




array([ 0.86757991, -1.00456621])



These values are the coordinates of the vector v relative to the basis . This means that if you go to 0.86757991i′−1.00456621j′ you arrive to the position (2, 1) in the standard basis, as illustrated in Figure 6.
 
Conclusion
 
Understanding the concept of basis is a nice way to approach matrix decomposition (also called matrix factorization), like eigendecomposition or singular value decomposition (SVD). In these terms, you can think of matrix decomposition as finding a basis where the matrix associated with a transformation has specific properties: the factorization is a change of basis matrix, the new transformation matrix, and finally the inverse of the change of basis matrix to come back into the initial basis (more details in Chapter 09 and 10 of Essential Math for Data Science).
 
Bio: Hadrien Jean is a machine learning scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog (hadrienj.github.io).
Original. Reposted with permission.
Related:

Essential Math for Data Science: Scalars and Vectors
Essential Math for Data Science: Introduction to Matrices and the Matrix Product
Essential Math for Data Science: Information Theory"
https://www.kdnuggets.com/2021/04/dijkstra-principle-data-science.html,Data science is not about data – applying Dijkstra principle to data science,"What is Data Science really about? Is it the data, or the algorithms, or something else? Similar foundational philosophical struggles exist with other scientific fields, including computer science, and maybe we can look to these resolutions to better understand the true 'meaning' of data science.","comments
By Mehmet Suzen, Theoretical Physicist | Research Scientist.

Image source: Wikipedia.
Edsger Dijkstra was a Dutch theoretical physicist turned computer scientist and probably one of the most influential earlier pioneers in the field. He had deep insight into what computer science is and a well-founded notion of how it should be taught in academics. In this post, we extrapolate his ideas into data science. We develop something called the Dijkstra principle for data science that is driven by his ideas on what does computer science entail.
 
Computer Science and Astronomy
 
Astronomy is not about telescopes. Indeed, it is about how the universe works and how its constituent parts are interacting. Telescopes, either being optical or radio observations or similar detection techniques, are merely tools to practice and do investigation for astronomy. A formed analogy goes into computer science as well, and this is the quote from Dijkstra:
Computer science is no more about computers than astronomy is about telescopes.  - Edsger Dijkstra

Dijkstra in Zurich, 1984 (Wikipedia). 
The idea of Computer Science not being about computers is rather strange at first glance. However, what Dijkstra had in mind are abstract mechanisms and mathematical constructs that one can map real problems to and solve as a computer science problem, such as graph algorithms. Though Computer Science had many subfields, its inception can be considered as being rooted in applied mathematics.
 
Dijkstra principle for data science
 
By using Dijkstra's approach now, we are in a position to formulate a principle for data science.
Data science is no more about data than computer science is about computers. - Dijkstra principle for data science
This sounds absurd. If data science is not about data, then what is it about? Apart from the definition of data science as an emergent field, as an amalgamation of multiple fields from statistics to high-performance computing,  the idea that data not being the core tenant of data science implies the practice does not aim at data itself rather a higher purpose. Data is used similar to a telescope in astronomy, and the purpose is to reveal the empirical truths about the representations that the data conveys. There are no unique ways to achieve this purpose.
 
Conclusion
 
Such a Dijkstra principle for data science would be very helpful in understanding the data science practice as being not data-centric, contrary to the mainstream dogma, rather as a science-centric practice with the data being the primary tool to leverage, using a multitude of techniques. The implication is that machine learning is a secondary tool on top of data in practicing data science. This attitude would help causality play a major role in shifting modern data science forward.
Original. Reposted with permission.
 
Related:

Data Science: (not) the preferred nomenclature
Data Science Books You Should Start Reading in 2021
How to frame the right questions to be answered using data"
https://www.kdnuggets.com/2021/04/imerit2-bias-variance-unpredictability.html,"The Three Edge Case Culprits: Bias, Variance, and Unpredictability","Edge cases occur for three basic reasons: Bias – the ML system is too ‘simple’; Variance – the ML system is too ‘inexperienced’; Unpredictability – the ML system operates in an environment full of surprises. How do we recognize these edge cases situations, and what can we do about them?","Sponsored Post.

iMerit is a leader in providing high-quality data for training data sets. In our latest blog post, we bring you the latest in recognising and handling edge cases in your ML systems. If you wish to talk to an expert and learn more about conquering your edge cases, please contact us. 
Through training, machine learning (ML) systems learn to recognize patterns by associating inputs (e.g., pixel values, audio samples, text) to categories (e.g., object identities, words, sentiments). This association can be thought of as dividing the multi-dimensional space of possible inputs into regions representing categories, the regions being defined by decision boundaries. When, during testing or operation, an input falls beyond the edge of the region assigned to its category, the input will be mis-categorized. We call this an edge case.
Edge cases occur for three basic reasons:

Bias – the ML system is too ‘simple’
Variance – the ML system is too ‘inexperienced’
Unpredictability – the ML system operates in an environment full of surprises.

How do we recognize these edge cases situations, and what can we do about them?
 
Bias
 
Bias shows up when an ML system cannot achieve good performance on its training data set. This is an indication that the architecture of the ML system, its model, does not have a structure that can represent the nuances in the training data. A very simple example is shown below:

This chart shows the linear decision boundary implemented by a two-input, single-layer, two-neuron neural network. Having been trained to try to distinguish the purple from the yellow dots, it cannot do a good job because its simple linear decision boundary is inadequate to separate the classes.

If we make the ML system a bit more complex by adding a pre-processing layer to transform the original inputs to polar coordinates, the purple and yellow dots line up differently in the new input space presented to the linear part of the ML system, and we get a decision boundary that looks like this. This more complex ML system performs well on the training data.
“The structure of the layers themselves can also make a big difference.”
Often the additional complexity needed to overcome bias in an ML system can be accomplished simply by adding processing units and layers. However, the structure of the layers themselves can also make a big difference. The ImageNet visual database has been used since 2010 to benchmark the performance of object recognition ML systems. The best performance on this data set jumped 10.8 percentage points in 2012, when Convolutional Neural Networks replaced earlier architectures that used pre-processing and Support Vector Machines, essentially a more sophisticated version of the simple ML system shown above.
 
Variance
 
When an ML system achieves good performance on its training data, but performs poorly in testing, the problem is often that the training data set is too small to adequately reflect the range of variability in the ML system’s operational environment.

The primary way to reduce edge cases caused by variance is to gather more training data.
For example, the object recognition system shown here does fine with adults, but apparently doesn’t have much experience with children.
Edge cases caused by variance issues is a large problem for automatic facial recognition systems. While these systems can be highly accurate in recognizing smartphone users, for example, they can produce inaccurate and troubling results when used for law enforcement. In a 2018 test, 28 members of the US Congress were falsely matched to mug shots in a database of 25,000 criminals.
The primary way to reduce edge cases caused by variance is to gather more training data. Studies have shown that ML system performance consistently improves as training data set size increases, even for data sets that are already very large.
In some cases, data augmentation can be used to increase the size of training data sets, by making small changes to the original data to represent variations such as object rotation, lighting, or pose. However, it is particularly important to ensure that training data is representative of important but possibly rare situations encountered in the operational environment, such as the appearance of children or extreme weather conditions.
 
Unpredictability
 
Machine learning relies on finding regular patterns in input data. There is always statistical variation in the data, but with an appropriate architecture and enough training data, an ML system can often find enough data regularity (achieve small enough bias and variance), to make reliable decisions and minimize edge cases.

However autonomous driving is an example of an environment that presents ML systems with such a high degree of variability that the possible situations are virtually endless. Training data can be gathered through many millions of miles of driving without encountering important edge cases.  For example, in 2018 a woman was struck and killed by a self-driving car, because the ML system had never been trained to recognize and respond to a jaywalker with a bicycle.
Handling the virtually unlimited number of edge cases encountered in the unpredictable autonomous driving application is a particularly difficult challenge. Two avenues are being pursued to deal with this problem. One approach is to develop ‘checker’ ML systems, specifically trained to recognize dangerous or questionable systems and take overriding action to maintain safety.
Another approach is to train autonomous vehicle ML systems to better handle edge cases. One way is to use virtual road training environments to expand the range of training situations. Another way uses adversarial training, the technology famous for creating ‘deep fakes’,  to greatly expand training sets, allowing ML systems to better handle novelty.
 
Overcoming Edge Cases
 
The table below summarizes how to recognize and deal with the edge cases resulting from ML bias, variance, and unpredictability:


Culprit
How it Shows Up
What to Try


Bias
Poor training performance
Better ML system model


Variance
Poor test performance
More training data


Unpredictability
Operational surprises
Simulation Adversarial training



Key takeaways

Ensure your training and test data sets are large and diverse enough, (and of course accurately labeled!), to expose weaknesses in ML design and to sufficiently characterize the diversity of the operational environment
For operational environments with high unpredictability, consider augmenting training data with examples generated through simulation and adversarial training.
Monitor operations and respond to the inevitable appearance of new edge cases."
https://www.kdnuggets.com/2021/04/secret-analysing-large-complex-datasets-constraint.html,"The secret to analysing large, complex datasets quickly and productively?","Data is beautiful, and lots of data is simply sublime, but be wary of the pitfalls. Sometimes you have so much data you can waste hours exploring without answering the important questions. These 5 tips will show you how to analyse large complex datasets productively by constraining yourself.","comments
By Thomas Richardson, Behavioural scientist at U. of Manchester.

The joys and stresses of too much data. Photo is me!
Researchers these days often have access to a lot of data. You might be analysing The European Social Survey with hundreds of variables and tens of thousands of respondents. Or perhaps you collected tons of data yourself; You might do a study where participants do a huge battery of tests, leaving you with heaps of data per person.
How do you go about analysing this beautiful mess?
 
Well, I’ll tell you what not to do:
 
Dive in headfirst and start correlating and plotting and building multilevel models but then realise that you might have to control for some variables and you get a result, but then it goes away with covariates B and C but not when you also add A, and then you find something seemingly robust, but you’ve tested like 15 things by now so maybe it's a false positive, so you learn about cross-validation and what the hell am I even doing here and when did it become Friday?!?
But seriously, sometimes we have more data than we know what to do with. There are worse problems, I’ll admit, but this has been a problem for me during my PhD. I’ve lost entire days digging around in big datasets. Leaving aside the problem of p-hacking (I generally try to validate any results found on a new dataset, and so should you!), this can really eat into your time.
You can also get suckered into this when your goal is to “get to know the data inside and out.” We often hear statisticians recommending that we do more exploratory data analysis, plot our data etc. This is valuable, but it’s not necessary to know every corner of the European Social Survey. It’s often not productive to see how a model is influenced by 10 different covariates that are barely related to the dependent variable. You have to draw the line somewhere and say “enough”!

Graph by the author.
I’m a big fan of the idea that constraints on our choice can be liberating. If that sounds paradoxical, these great articles [here and here] will explain it. I’ve found that when it comes to data, more options aren’t always better.
 
Solutions
 
1. Be question-driven
Before looking at your data, decide what your primary questions are. These are the questions you will set out to answer, and after, you will stop (at least until you’ve written the paper/chapter). What are the questions that, once answered, could be publishable? Not to be too cynical, but in academia, papers are the name of the game. Papers aren’t usually a collection of interesting observations stapled together. They are focused, and you should be too.
Do all the analysis required to answer a paper-sized question in a satisfactory manner (i.e., by checking the robustness of the result, ruling out alternative explanations), and then stop. All other questions in the data are unimportant or less important. You can always come back to them later.
2. Discuss the data with colleagues or lab members
Talking them through your data and analyses will clarify it in your mind. The idea of gaining a deeper understanding of something through explaining it to others is often called the Feynman technique, and I’m a big fan. They may be able to advise what in the data is interesting and what isn’t.
Remember, you’re trying to produce knowledge that is valuable to your field. Therefore, it makes sense to ask a few members of said field if they think your analysis is valuable.
3. Keep detailed notes of what you try
With so many t-tests and regression models flying about, you might end up forgetting you ran some analysis previously and end up running it again! Also, sometimes models don’t yield insight in isolation but in combination with other models.
For example, you run model A, and predictor A1 shows an effect that you don’t understand. So you see if A1 correlates with a few other things, and that might explain it. You build model B and eureka! Now record what you did. Interpret each test before moving onto the next one. Don’t just go running tests willy-nilly without thinking about what they mean. Running tests for the hell of it is a great way to find false positives. If you keep notes and interpret what’s going on as you go, every test truly does teach you something about the data.
4. Set a time limit on analysis
Set yourself an amount of time to do it in. Deadlines, even self-imposed ones, are great for clarifying the mind. Having years to do a PhD is not an excuse to waste time. Any data stuff not done after this time will have to wait for another day. It’s not a big deal if you miss something important, as your supervisor/colleagues/reviewers on the eventual paper will find them. You can even come back to the dataset and take a few more days on it later when you’ve got some more time. This also has the advantage of returning to the data refreshed. Give yourself a time limit. Don’t just wander leisurely through the data.
5. Pre-registration
Constrain what analyses you’ll do in a formal way. Write down what analyses you’ll do before you look at the data. Aspredicted is a good tool to help you with this. Pre-registration is brilliant for many other reasons: give it a Google, read this or ask your local Open Science Methods Police for more information (If you’re at the University of Manchester, that may well be me!).

Original. Reposted with permission.
 
Related:

The question that makes your data project more valuable
How to frame the right questions to be answered using data
Powerful Exploratory Data Analysis in just two lines of code"
https://www.kdnuggets.com/2021/08/top-stories-2021-jul.html,Top July Stories: Data Scientists and ML Engineers Are Luxury Employees,Also: Top 6 Data Science Online Courses in 2021; Advice for Learning Data Science from Google's Director of Research; 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist,"By Gregory Piatetsky, KDnuggets.  

Here are the most popular July 2021 stories on KDnuggets. The top story, ""Data Scientists and ML Engineers Are Luxury Employees"" touched upon the same topic as our recent poll - the demand for Data Scientists and related professions, but our poll results show that the majority of KDnuggets readers remain confident that Data Scientists will not go extinct in 10 years. 
Rather, KDnuggets readers want to improve their skills, and two very popular posts about it were Advice for Learning Data Science from Google's Director of Research and 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist. Check the other top stories below!

Stories in green font are also the winners of KDnuggets Top Blog Rewards for July.

Most Viewed - Platinum Badge (>32,000 UPV)

 Data Scientists and ML Engineers Are Luxury Employees, by Adrien Biarnes (*)
 Top 6 Data Science Online Courses in 2021, by Natassha Selvaraj (*)


Most Viewed - Gold Badge (>16,000 UPV)

 Advice for Learning Data Science from Google's Director of Research, by Benjamin Obi Tayo (*)
 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist, by Tessa Xie
 Pandas not enough? Here are a few good alternatives to processing larger and faster data in Python, by DaurEd (*)
 A Learning Path To Becoming a Data Scientist, by Sara Metwalli (*)
 Geometric foundations of Deep Learning, by Michael Bronstein, Joan Bruna, Taco Cohen, and PV (*)
 GitHub Copilot Open Source Alternatives, by Matthew Mayo


Most Viewed - Silver Badge (> 8,000 UPV)

 How Can You Distinguish Yourself from Hundreds of Other Data Science Candidates?, by Tirthajyoti Sarkar (*)
 5 Python Data Processing Tips & Code Snippets, by Matthew Mayo (*)
 Design patterns in machine learning, by Agoston Torok (*)
 How to Get Practical Data Science Experience to be Career-Ready, by Terence Shin (*)




Most Shared - Platinum Badge (>1400 shares)

 Data Scientists and ML Engineers Are Luxury Employees, by Adrien Biarnes (*)
 Relax! Data Scientists will not go extinct in 10 years, but the role will change, by Gregory Piatetsky (*)
 Pandas not enough? Here are a few good alternatives to processing larger and faster data in Python, by DaurEd (*)


Most Shared - Gold Badge (>700 shares)

 Why and how should you learn ""Productive Data Science""?, by Tirthajyoti Sarkar
 Become an Analytics Engineer in 90 Days, by Tuan Nguyen
 Not Only for Deep Learning: How GPUs Accelerate Data Science & Data Analytics, by Kevin Vu (*)
 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist, by Tessa Xie
 GitHub Copilot: Your AI pair programmer - what is all the fuss about?, by Matthew Mayo


Most Shared - Silver Badge (>400 shares)

 11 Important Probability Distributions Explained, by Terence Shin
 Geometric foundations of Deep Learning, by Michael Bronstein, Joan Bruna, Taco Cohen, and PV
 A Learning Path To Becoming a Data Scientist, by Sara Metwalli
 A Brief Introduction to the Concept of Data, by Angelica Lo Duca (*)
 Top 6 Data Science Online Courses in 2021, by Natassha Selvaraj


(*) indicates that badge added or upgraded based on these monthly results.

Most Shareable (Viral) Blogs
Among the top blogs, here are the blogs with the highest ratio of shares/unique views, which suggests that people who read it really liked it. 

 Relax! Data Scientists will not go extinct in 10 years, but the role will change, by Gregory Piatetsky
 The Brutal Truth About Data Science, by Prad Upadrashta
 Streamlit Tips, Tricks, and Hacks for Data Scientists, by Kaveh Bakhtiyari
 Not Only for Deep Learning: How GPUs Accelerate Data Science & Data Analytics, by Kevin Vu
 5 Mistakes I Wish I Had Avoided in My Data Science Career, by Tessa Xie"
https://www.kdnuggets.com/2021/09/speeding-neural-network-training-multiple-gpus-dask.html,Speeding up Neural Network Training With Multiple GPUs and Dask,A common moment when training a neural network is when you realize the model isn’t training quickly enough on a CPU and you need to switch to using a GPU. It turns out multi-GPU model training across multiple machines is pretty easy with Dask. This blog post is about my first experiment in using multiple GPUs with Dask and the results.,"comments
By Jacqueline Nolis, Head of Data Science at Saturn Cloud

The talk this blog post was based on.
 
A common moment when training a neural network is when you realize the model isn’t training quickly enough on a CPU and you need to switch to using a GPU. A less common, but still important, moment is when you realize that even a large GPU is too slow to train a model and you need further options.
One option is to connect multiple GPUs together across multiple machines so they can work as a unit and train a model more quickly. For most of the time, I’ve been using neural networks with the idea that connecting GPUs seemed outrageously difficult and maybe even only feasible for engineering teams trying to show off training large models. Thankfully, it turns out multi-GPU model training across multiple machines is pretty easy with Dask. This blog post is about my first experiment in using multiple GPUs with Dask and the results.
To see how to use Dask with GPUs to more quickly train a model, I first needed a model to try training. I decided to lean on my old friend, a neural network trained on Seattle pet license data, to generate realistic sounding pet names. I had previously used this neural network as an example in R, so switching it to Python and Dask seemed fun. I decided to use PyTorch for the framework, but still with a multi-layer LSTM to capture the patterns in the words.

Some adorable names generated for adorable pets.
 
This whole blog post can be run for free on the Saturn Cloud Hosted free tier using the free Jupyter server and Dask clusters.
 
Model architecture
 
 
To train a neural network to generate realistic text, I needed the data to be properly formatted. Specifically, I want to predict each character in a name (as opposed to word in a sentence), so the base data was the beginning sequences of characters in text and the target data was next letter in the sequence of text. For example, to train a model on the pet name “SPOT” I would need to train it that a first letter is “S”, after “S” is “P,” after “SP”, is “O”, and so on. I used blanks used as filler and a special stop character indicate the name was complete. This was be made into a matrix as shown below (and if the data is too long I’d just truncate the earlier letters):


X_1
X_2
X_3
X_4
X_5
Y


(blank)
(blank)
(blank)
(blank)
(blank)
S


(blank)
(blank)
(blank)
(blank)
S
P


(blank)
(blank)
(blank)
S
P
O


(blank)
(blank)
S
P
O
T


(blank)
S
P
O
T
(stop)


 
Each character was then 1-hot encoded (so given a dictionary value like A=1, B=2, etc and converted into vectors of 0s with one 1). With that, I got a 3-dimensional binary matrix as input to the model and 2-dimensional binary matrix as the target.
The network was four LSTM layers, plus a dense layer with a node for each character in the dictionary. The four LSTM layers would find the patterns within the text, and the dense mapped the results to each character-level prediction. For a single GPU, I would train the model using a single function:

def train():
    device = torch.device(0) # Set GPU as the device
    dataset = OurDataset(pet_names, device=device)
    loader = DataLoader(dataset, batch_size=batch_size, 
                        shuffle=True, num_workers=0)
    model = Model()
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    for epoch in range(num_epochs):
        dataset.permute()
        for i, (batch_x, batch_y) in enumerate(loader):
            optimizer.zero_grad()
            batch_y_pred = model(batch_x)
            loss = criterion(batch_y_pred.transpose(1, 2), batch_y)
            loss.backward()
            optimizer.step()
        time = datetime.datetime.now().isoformat()
        print(f""{time} - epoch {epoch} - loss {loss.item()}"")
    return model


(Full code for the final model can be found on this Saturn Cloud GitHub repo.)
 
Training with multiple GPUs
 
 
The basic idea to train with multiple GPUs is to use PyTorch’s Distributed Data Parallel (DDP) function. DDP will let multiple models being trained at the same time pass parameters between each other after each batch and compute the gradient concurrently. Thus, if you have three devices training a model with DDP, it’s equivalent to training a single model with a batch size that’s three times as large. Since the devices are all training concurrently, it’s theoretically possible that in this scenario, the model could be trained 3x faster than a model on a single device with a batch size 3x larger. In practice, this won’t be the case because of latency between the models when they communicate. But still, the results can be pretty good.
To turn my training code into a multi-GPU set up, I needed to make a few adjustments. First, I needed to add the PyTorch DDP wrapper around the model in the training future (and again the full code is available on GitHub):

model = Model()
model = model.to(device) # Was in single-GPU scenario
model = DDP(model) # Added for multi-GPU scenario


 
Next, any place in my training function where I might only want a single machine to do something (like write the output at the end of an Epoch), I needed to wrap in a “only have one worker do this” logical statement. I used worker number 0 as the one to run these commands.

worker_rank = int(torch.distributed.get_rank())
if worker_rank == 0:
   # ...


 
Then, I used dask.delayed to have multiple versions of the training function running concurrently. After adding a @dask.delayed decorator above the training function, I used dask_pytorch_ddp as a simpler wrapper around the functions to run them:

from dask_pytorch_ddp import dispatch
futures = dispatch.run(client, train)


 
Finally, to log the model results rather than having to coordinate where results are being saved within the Dask cluster, I used Weights and Biases as a place to store results and models.
 
Comparing results
 
 
For the analysis, I decided to try adjusting a number of parameters and see how a version with multiple-GPUs compared to a single GPU. Note that for simplicity in comparing, in the single GPU case I still used the DDP-based code but only with one machine (rather than removing that entirely)

Networked GPUs - 1, 4, or 16
Training batch size, per GPU - 1024, 4096, or 16384
Model learning rate - 0.001, 0.004, or 0.016

Then, I used a Saturn Cloud Dask cluster to iterate though each of the combinations. Note that you have to be careful when comparing the results of the different experiments. For an equivalent comparison, if the number of networked GPUs went up by 4x you need to lower the training size per GPU to 1/4th the value. I, uh, may have spent hours trying to figure out what was wrong before I realized my comparisons weren’t equivalent. With that, here are the results:

As you can see, using more GPUs can dramatically reduce the amount of time it takes to train the model, while still providing the same quality of model accuracy.
Here is a graph of the same data visualized in a different way–instead of showing the curves over the epochs of the training run, this is the amount of time it takes before the model hits a reasonable level of accuracy. You can see that increasing the number of GPUs by 4x can give you a nearly 4x increase in speed.

 
A few notes about these pretty amazing results. First, these techniques work best when your models need larger batch sizes because at that point. multiple GPUs aren’t just speeding up your results, but also you can avoid out of memory errors. So, if your problem is sufficiently simple, you might not get as much value out of using multiple GPUs, although there should still be a speed increase. Although, if the time it takes for your GPUs to communicate is high, then that can reduce the value of this too. Additionally, besides having multiple machines all coordinating GPU model training, one other option you have is to train with multiple GPUs on the same machine with resources like V100 graphics cards. That can be done using only DDP–having Dask communicate locally could still work, but would probably be redundant.
Besides using multiple GPUs with Dask to train the same model in a coordinated way, you could use Dask to concurrently train models with different sets of parameters. This allows you to explore a parameter space much faster–you could go faster still with tools like Dask-ML for fast hyper-parameter tuning.
Lastly, just to see the fruits of all this labor, below is a set of pet names the model generated (and you can generate them yourself here).
['Karfa', 'Samalpidells', 'Avexen', 'Ejynsp', 'Charlie', 'Kooie', 'Winnundie', 'Wunla', 'Scarlie A', 'Fli', 'Argio', 'Kucka', 'Karlor', 'Litr', 'Bheneng', 'Gelo', 'Cangalantynes', 'Mop', 'Nimi', 'Hary', 'Cassm Tur', 'Ullyne', 'Hollylof', 'Sopa', 'Ghugy', 'Bigde', 'Nucha', 'Mudser', 'Wridney', 'Mar Bas', 'Rugy Bi', 'Roba', 'Ruedel', 'Henrie', 'Sharlia Hu', 'Yellaj', 'Balil']
 
Bio: Jacqueline Nolis has over a decade of experience in using data science to solve business problems, and has lead data science, machine learning, and AI projects at Microsoft, Adobe, Airbnb, and T-Mobile, and has helped build data science teams from scratch.
Original. Reposted with permission.
Related:

Pandas not enough? Here are a few good alternatives to processing larger and faster data in Python
GPU-Powered Data Science (NOT Deep Learning) with RAPIDS
Introducing Packed BERT for 2x Training Speed-up in Natural Language Processing"
https://www.kdnuggets.com/2021/03/10-amazing-machine-learning-projects-2020.html,10 Amazing Machine Learning Projects of 2020,"So much progress in AI and machine learning happened in 2020, especially in the areas of AI-generating creativity and low-to-no-code frameworks. Check out these trending and popular machine learning projects released last year, and let them inspire your work throughout 2021.","comments
By Anupam Chugh, iOS Developer | Writer @Medium | BITS Pilani.

Photo by Paul Hanaoka on Unsplash.
A lot happened in the machine learning community during the past year. Here’s a tour through the most popular and trending open-source research projects, demos, and prototypes. It ranges from photo editing to NLP to training models with “no-code,” and I hope they inspire you to build incredible AI-powered products this year. You can also find additional machine learning projects here.
 
1. Background Matting v2
 
Background Matting v2 takes a cue from the popular The World is Your Green Screen open-source project and showcases how to remove or change the background in real-time. It gives better performance (30fps at 4K and 60fps at FHD) and can be used with Zoom, the popular videoconferencing app.
The technique uses an additional captured frame of the background and uses it in recovering the alpha matte and the foreground layer. To process high-resolution images in real-time, two neural networks are used.
This project is certainly helpful if you’re looking to remove a person from a video whilst preserving the background.

Demo
 
2. SkyAR
 
Here’s another amazing project that does a video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles.
Based on Pytorch, the project partially adapts code from the pytorch-CycleGAN-and-pix2pix project, and it leverages sky matting, motion estimation through optical flow, and image blending to provide artistic backgrounds on videos in real-time.
The above open-source has incredible potential in movies and video games such as adding fake rain/sunny weather/etc.

Source
 
3. AnimeGAN v2
 
Cartoonifying photos is always a fun machine learning project. Isn’t it?
This project, AnimeGANv2, is the improved version of AnimeGAN. Specifically, it combines neural style transfer and generative adversarial network (GAN) to accomplish the task while also ensuring that the generation of high-frequency artifacts is prevented.

Source
 
4. txtai
 
AI-refined search engines and QA Chatbots are always the need of the hour. And that’s exactly what this project does.
By using sentence-transformers, transformers, and faiss, txtai builds an AI-powered engine for contextual search and extractive question-answering.
Essentially, txtai supports building text indices to perform similarity searches and create extractive question-answering based systems.

Source
 
5. Bringing-Old-Photos-Back-to-Life
 
Next up, we have Microsoft’s latest photo restoration project that auto-fixes damaged photos.
Specifically, it restores old photos that suffer from complex degradation through a deep learning implementation in PyTorch by leveraging scratch detection, face enhancements, and other techniques.
As per their research paper: “We train two variational autoencoders (VAEs) to respectively transform old photos and clean photos into two latent spaces. And the translation between these two latent spaces is learned with synthetic paired data. This translation generalizes well to real photos because the domain gap is closed in the compact latent space. Besides, to address multiple degradations mixed in one old photo, we design a global branch with a partial nonlocal block targeting the structured defects, such as scratches and dust spots, and a local branch targeting the unstructured defects, such as noises and blurriness.”
The model certainly outperforms the conventional state of the art methods, as evident in the below demo:

Source
 
6. Avatarify
 
Deepfake projects have taken the machine learning and AI community by storm. This project shows a classic example of that by letting you create photo-realistic avatars in real-time video conferencing apps.
Basically, it uses the First Order Model to do a motion extraction from the video and apply it to the target avatar image by using optical flow. In doing so, you can generate avatars on a virtual camera or even animate classic paintings. From Elon Musk to Mona Lisa, you can impersonate anyone for fun!

Source
 
7. Pulse
 
Here’s an AI model that showcases how to generate a realistic face image from a low-resolution one.
PULSE, which stands for Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models, provides an alternative formulation of the super-resolution problem based on creating realistic SR images that also downscale back correctly.

Source
 
8. pixel2style2pixel
 
Based on the research paper “Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation,” this project uses a Pixel2Pixel framework, and it’s designed to address a wide range of image-to-image tasks using the same architecture in order to avoid any possible locality bias.
Based on a novel encoder network, this network can be trained to align a face image to a frontal pose, conditional image synthesis, and create super-resolution images.
From generating almost real-life people out of cartoonist pics to converting sketches or face segmentations to photo-realistic images, there are so many things that you can do with this.

Source
 
9. igel
 
It could be due to budget issues or a lack of a clear vision, but finding people with relevant machine learning expertise is always a challenge for startups. More so since the field is always a work in progress.
Hence, there has been a surge in no-code machine learning platforms lately, with the likes of Google and Apple releasing their own toolsets to quickly train models.
This delightful open-source machine learning project does just that by allowing you to train/fit, test, and use models without writing code. While the GUI drag and drop version is still a work in progress, there’s a lot that you can achieve with the command line tools of this project:

//train or fit a model
igel fit -dp 'path_to_your_csv_dataset.csv' -yml 'path_to_your_yaml_file.yaml'

//evaluate
igel evaluate -dp 'path_to_your_evaluation_dataset.csv'

//predict
igel predict -dp 'path_to_your_test_dataset.csv'


 
There’s also a single command igel experiment to combine all the phases: train, evaluate, and predict. For more details, refer to the documentation here.

Source
 
10. Pose Animator
 
Last but not least, we have a web animation tool. Basically, this project uses PoseNet and FaceMesh landmark results to bring an SVG vector image to life by leveraging some TensorFlow.js models.
You can animate your own designs or skeleton images in the following way:

Source
 
Original. Reposted with permission.
 
Related:

2020: A Year Full of Amazing AI Papers — A Review
20+ Machine Learning Datasets & Project Ideas
All Machine Learning Algorithms You Should Know in 2021"
https://www.kdnuggets.com/2021/06/10-mistakes-avoid-data-science-beginner.html,10 Mistakes You Should Avoid as a Data Science Beginner,Read this article on how to gain a competitive advantage in the data science job market.,"comments
By Isabelle Flückiger, Senior Executive | International Advisor | Speaker | Thought Leader | Learning Leadership | Lecturer | Startup Advisor


Image by Steve Buissinne from Pixabay

 
Data science is a success. Thousands of students around the globe sign up for online courses or even a data science master program.
The data science field is a very competitive market, especially to get one of the (supposed) dream jobs at one of the big tech companies. The positive news is that you have it in your hand to gain a competitive advantage for such a position by preparing yourself adequately.
On the other hand, there are (too) many MOOCs, master programs, bootcamps, blogs, videos and data science academies. As a beginner, you feel lost. Which course should I attend? What topics should I learn? On what methods do I need to focus? What tool and programming language must I study?
The truth is that every data scientist has her/his individual journey and is biased towards that learning path. So, without knowing you, it is difficult to say what’s the best approach for you.
But there are common mistakes made over and over again by all data scientists. Even when knowing them, you will not avoid them altogether, but eventually, stop earlier in doing them and find faster back to the road to success.
Based on my 20+ years of experience in data science, leading teams up to 150 people, and still giving lectures on a part-time basis at one of the leading global universities, I summarized for you the core mistakes to avoid reaching your dream faster.
The mistakes are given in the order of the learning progress as a beginner data scientist.
 
#1 Investing too much time in assessing all the different types and options of courses available before you finally start — or eventually never start
 
I know that you are overwhelmed by all the courses, and you try not to make any mistakes. You want to invest your time and money effectively and select the right approach which promises the fastest and best success.
Unfortunately, there is no immediate success like in any technical and scientific field, and for the best possible success, you will not have any comparison.
The fact is that today, all established platforms, academies, and institutes have good courses. So, do not overthink and over-analyze the courses. Be brave and choose one, complete that course and then select another one.
The most crucial aspect is starting and doing. You cannot make a mistake here because you neither know your journey nor how it would have been different when choosing another one. No one can tell you that. Period.
It is also important to realize that learning is circular and not linear. Taking one data science course does not exclude that you are taking another one.
I do still data science, machine learning, and AI training after all my years of experience. In every still so “simply” beginner course, I detect a new aspect and a new view on the topic. And this is exactly what finally makes a high-demanded data scientist. It’s to understand all the different perspectives on a topic.
 
#2 You want to learn too many methods and tools at once instead of learning and understanding the methods one by one
 
Many aspiring data scientists think that having as many as possible methods mentioned in the CV help to get a job faster. But the contrary is true. When applying for a job and you started only six months ago with data science for every recruiter, it’s clear that it is buzzword dropping with no substance behind it.
If we look at regression models, there are many books only about regression. There are more than 50 regression types, and each comes with different preconditions. So, only have “regression” in your CV does not say anything. Also, regression models are still the most important models for applications and to set the basis of understanding for data science in general.
You must understand what is solved by a method; what are the assumptions; what do the parameters mean; what are pitfalls; and so on, and so forth.
Based on the CV and how the knowledge of regression is described, every experienced recruiter — or today, the algorithms behind the process — can identify the depth of your understanding.
It is better to have in-depth knowledge and experience in only a handful of methods than knowing many with no substance.
 
#3 You code everything from the beginning because you think this helps you to program better and faster
 
When starting coding, people think they must quickly begin coding and re-programming as many algorithms as possible. Also, here you should focus on understanding a few and not on quantity.
First, you need to understand the prerequisites of coding: linear algebra, mathematical induction, discrete mathematics, geometry — yes, this is the strength of the excellent programmers but often forgotten by data scientists, statistics and probability theory, calculus, Boolean algebra, and graph theory.
I did not become better and faster by coding more. I got good at programming by understanding the mathematical basis, reviewing the code of others, and run and test them on different data and problems.
Yes, coding is essential, but more important is to understand the (good) architecture of code. And this can only be learned by reviewing other code.
A fact is that code becomes more and more a commodity, and there are even no-code tools. The differentiator will not be anymore between the ones that can code and those that cannot, but the ones that understand its architecture and those that do not.
I show you another example: I assume you have already used TensorFlow. But do you understand what it is? What does it do? And why it is called “TensorFlow”? Do you know what a tensor is? Not just the mechanical calculation of a tensor product, but what does it mean geometrically?
 
#4 By learning the theory, you think you know everything but miss enough practical experience
 
Learning data science is try and error. Only when you make as much experience as possible, making all the errors and resolving them, you get a deeper understanding.
The theory is okay and vital. You need an understanding of the fundamentals.
Unfortunately, in practice, it rarely works like in theory. On the contrary, it often works precisely in a way, as you have learned you should not do it.
So, you must start from the beginning with practical examples. Often, you will not feel ready to do practical work: not enough knowledge of the basics or not enough programming experience.
But I strongly advise: start at the beginning even though you do not feel ready to do exercises. It has not to be a daylong or week-long project. A small 1–2 hours project is enough.
You can either start with a no-code tool like RapidMiner or KNIME or take somebody else’s code and apply it. E.g. take a simple sentiment analysis code and use it to Tweets or product description. Then you can start to alter the code for other examples and compare the results.
When you learned talking as a small child, you started with single words or expressions of two or three words. And step by step, you built up a feeling for the language. It is the same with practical experience in data science.
Pro tip: Learning is circular. So, store your work. Later you can come back, improve it, move it to GitHub, and add visualizations with Tableau.
 
#5 You think that certifications are a competitive advantage to get a data science job
 
Certifications are okay. There are many voices out there that tell you that you should not do certifications. But they can serve as a motivation, and finally, they show officially your progress and your eagerness to learn. I still do certificates. There is nothing wrong with it, and when you invest time, it is legitimate to have it.
But it is not a differentiator in the market. The fact is that there are thousands of people that have the same certifications. So, to have a competitive advantage, you must go beyond that.
For example, a student of mine approached me for support for an internship opportunity in the finance field. He wanted to apply what he has learned and get to know the culture and cooperation within a data science team. I could place him with a bank, and he writes a semester thesis from that. Yes, it is stressful to do the study and the internship and semester thesis in parallel. But it will give him an invaluable competitive advantage for job offers.
 
#6 You worry about the opinion of other people instead of building your own opinion based on facts
 
Most aspiring data scientists worry about the opinion of other data scientists. And the more arguments they hear, the more confused they are. Even though confusion is required to the path of clarity, it should not remain a steady state.
Each data scientist is an individual with her/his experience, learning and career path and opinion. I am used to saying, “if you have two data scientist in a room, you have at least four different opinions.”
It is good to take opinions as inspiration and as a guide to search for information, but not as the information itself.
Search for hard facts. Draw your logical conclusions, validate, and update them again. This is an important skill to progress successfully in your data science career.
 
#7 Not caring about business and domain knowledge
 
Many data scientists think they can apply the methods to every problem and industry, but I can tell you that’s wrong from more than 20 years of experience.
Too often, I saw data scientists presenting findings to the business people, and the reaction was, “oh, we know this already. What we need is ‘why that happens’ and ‘how to solve it.’ Or, in the worst case, ‘this is absolute nonsense because this is not how our business works.’ Boom!
It is more important to have domain knowledge than knowing all the sexist and fanciest methods. A data scientist is solving a business problem, not a technical problem. By solving a business problem, you bring value to the company’s business, and you have only so much value as the value of your solution. You do this successfully when you know the business.
I worked in many different industries. Each time before I even started to engage with the business, I read a lot about the industry.

I started with Wikipedia, learned the big picture and about the companies
I looked up the annual reports and investor relations information of the top 10 companies in an industry
I read all the news articles of the last few years about this industry and companies
I contacted my LinkedIn contacts who work in this industry

Only then, I started to interact with the business.
Half of your learning should contain the development of industry and business knowledge.
 
#8 You are not studying and learning on a consistent and ongoing basis
 
It is very easy to be distracted or give up early because you do not understand the topic. Learning data science is a marathon and not a sprint. So, it is essential to build up a routine to study ongoing and consistent. Like in marathon training, you train in small units but daily.
Also, as written before, learning is circular. Having once studied a topic does not mean that you have mastered it.
Let me give you an example. In the mathematical finance lectures, I had to learn many limit theorems. The exam went excellent, and I was convinced that I understand them. But seven years later, when I had to review code for the valuation of complex structure financial products, the scales fell from my eyes, and I realized that I did not understand it until that moment of code review.
So, book daily, or at least weekly, a few hours to learn. It does not matter whether you are an aspiring or already a senior data scientist.
The learning should consist of new data science topics, already learned topics but from another perspective, e.g. another course or book, new technologies and technology trends, industry and business knowledge, data visualization and data storytelling, and applications to data.
It adds layer and layer of understanding, and in the job interview, you will be able to give convincing answers by presenting the holistic view from different perspectives.
 
#9 No storytelling with the data
 
In a data science job, you will primarily communicate your findings to non-technical people, notably, the people from the business. And the business is financing your job. Without their commitment, your job and the data science team would not exist.
Your job is to bring value to the business. It is not to apply fancy methods only for the sake of application.
A friend of mine is the data science lead of a global bank. When they are hiring data scientists, they send them two weeks in advance a dataset and ask for a 20 minutes presentation. No further input is given. They want to see the storytelling. They are not interested in the methods applied — except a candidate would tell absolute nonsense about the methods used. What they want to see is, first, the framing of the business problem and why it is important to solve. Second, what should be solved and last, how it is solved, and the result in a business context. “This is the most important work we do the whole day. A candidate must not be perfect in that but show that she/he has understood what is important in our job.”
So, learn data storytelling — there are even free courses about that — and learn visualization of data in a business context.
 
#10 Learning on your own without interactions with the data science community
 
Many people think they can learn data science through their own hard work. All the other data scientists are seen as competitors, and one is reluctant to exchange knowledge.
But living in your world where you only read and learn based on your selection is highly biased, and many perspectives on a topic or method are missing. Further, the open discourse about a topic and gaining experience in argumentation is missing — a skill needed by any data scientist.
Any experienced recruiter knows after one or two questions if you are a one-person show or if you have a vivid network that helps you to gain knowledge exponentially. This benefits the company and increases your market value and demand.
So, it’s crucial to develop a network. This can be done by attending bootcamps, hackathons, and Meetup meetings.
Now, you know theoretically what you should avoid.
Any of these mistakes is a potential showstopper for your data science job.
I know that you still will make several of these mistakes. I am not different. It is in human nature to think that “I am different” — even though the data says the contrary. But the awareness of these potential mistakes will help you to re-adjust your path faster and thus become more effectively a demanded data scientist.
 
Do you like my story? Here you can find more.
 
Hands-On Step-By-Step Guidance to Grow Your Job Opportunities
How to leverage Meetup meetings strategically to get your dream data science job
 
The Ultimate Guide on the Data Science MicroMasters Programs on edX 2021
Which of the 6 programs should you choose?
 
The Top Technology Trends and Their Impact on Data Science, Machine Learning and AI
An action plan for you and your career
 
Bio: Isabelle Flückiger is a Senior Executive with international C-level advisory experience in end-to-end digital, data and new technology transformation projects, with key industry experience in banking, insurance, chemicals, utilities and pharma/life sciences.
Original. Reposted with permission.
Related:

How to Land a Data Analytics Job in 6 Months
Top 10 Data Science Projects for Beginners
Data Science is Not Becoming Extinct in 10 Years, Your Skills Might"
https://www.kdnuggets.com/2021/08/train-bert-model-scratch.html,How to Train a BERT Model From Scratch,"Meet BERT’s Italian cousin, FiliBERTo.","comments
By James Briggs, Data Scientist


BERT, but in Italy — image by author

 
Many of my articles have been focused on BERT — the model that came and dominated the world of natural language processing (NLP) and marked a new age for language models.
For those of you that may not have used transformers models (eg what BERT is) before, the process looks a little like this:

pip install transformers
Initialize a pre-trained transformers model — from_pretrained.
Test it on some data.
Maybe fine-tune the model (train it some more).

Now, this is a great approach, but if we only ever do this, we lack the understanding behind creating our own transformers models.
And, if we cannot create our own transformer models — we must rely on there being a pre-trained model that fits our problem, this is not always the case:


A few comments asking about non-English BERT models

 
So in this article, we will explore the steps we must take to build our own transformer model — specifically a further developed version of BERT, called RoBERTa.
 
An Overview
 
 
There are a few steps to the process, so before we dive in let’s first summarize what we need to do. In total, there are four key parts:

Getting the data
Building a tokenizer
Creating an input pipeline
Training the model

Once we have worked through each of these sections, we will take the tokenizer and model we have built — and save them both so that we can then use them in the same way we usually would with from_pretrained.
 
Getting The Data
 
 
As with any machine learning project, we need data. In terms of data for training a transformer model, we really are spoilt for choice — we can use almost any text data.


Video walkthrough for downloading OSCAR dataset using HuggingFace’s datasets library

 
And, if there’s one thing that we have plenty of on the internet — it’s unstructured text data.
One of the largest datasets in the domain of text scraped from the internet is the OSCAR dataset.
The OSCAR dataset boasts a huge number of different languages — and one of the clearest use-cases for training from scratch is so that we can apply BERT to some less commonly used languages, such as Telugu or Navajo.
Unfortunately, the only language I can speak with any degree of competency is English — but my girlfriend is Italian, and so she — Laura, will be assessing the results of our Italian-speaking BERT model — FiliBERTo.
So, to download the Italian segment of the OSCAR dataset we will be using HuggingFace’s datasets library — which we can install with pip install datasets. Then we download OSCAR_IT with:

Let’s take a look at the dataset object.

Great, now let’s store our data in a format that we can use when building our tokenizer. We need to create a set of plaintext files containing just the text feature from our dataset, and we will split each sample using a newline \n.

Over in our data/text/oscar_it directory we will find:


The directory containing our plaintext OSCAR files

 
 
Building a Tokenizer
 
 
Next up is the tokenizer! When using transformers we typically load a tokenizer, alongside its respective transformer model — the tokenizer is a key component in the process.


Video walkthrough for building our custom tokenizer

 
When building our tokenizer we will feed it all of our OSCAR data, specify our vocabulary size (number of tokens in the tokenizer), and any special tokens.
Now, the RoBERTa special tokens look like this:

So, we make sure to include them within the special_tokens parameter of our tokenizer’s train method call.

Our tokenizer is now ready, and we can save it file for later use:

Now we have two files that define our new FiliBERTo tokenizer:

merges.txt — performs the initial mapping of text to tokens
vocab.json — maps the tokens to token IDs

And with those, we can move on to initializing our tokenizer so that we can use it as we would use any other from_pretrained tokenizer.
 
Initializing the Tokenizer
 
 
We first initialize the tokenizer using the two files we built before — using a simple from_pretrained:

Now our tokenizer is ready, we can try encoding some text with it. When encoding we use the same two methods we would typically use, encode and encode_batch.

From the encodings object tokens we will be extracting the input_ids and attention_mask tensors for use with FiliBERTo.
 
Creating the Input Pipeline
 
 
The input pipeline of our training process is the more complex part of the entire process. It consists of us taking our raw OSCAR training data, transforming it, and loading it into a DataLoader ready for training.


Video walkthrough of the MLM input pipeline

 
 
Preparing the Data
 
 
We’ll start with a single sample and work through the preparation logic.
First, we need to open our file — the same files that we saved as .txt files earlier. We split each based on newline characters \n as this indicates the individual samples.

Then we encode our data using the tokenizer — making sure to include key parameters like max_length, padding, and truncation.

And now we can move onto creating our tensors — we will be training our model through masked-language modeling (MLM). So, we need three tensors:

input_ids — our token_ids with ~15% of tokens masked using the mask token <mask>.
attention_mask — a tensor of 1s and 0s, marking the position of ‘real’ tokens/padding tokens — used in attention calculations.
labels — our token_ids with no masking.

If you’re not familiar with MLM, I’ve explained it here.
Our attention_mask and labels tensors are simply extracted from our batch. The input_ids tensors require more attention however, for this tensor we mask ~15% of the tokens — assigning them the token ID 3.

In the final output, we can see part of an encoded input_ids tensor. The very first token ID is 1 — the [CLS] token. Dotted around the tensor we have several 3 token IDs — these are our newly added [MASK] tokens.
 
Building the DataLoader
 
 
Next, we define our Dataset class — which we use to initialize our three encoded tensors as PyTorch torch.utils.data.Dataset objects.

Finally, our dataset is loaded into a PyTorch DataLoader object — which we use to load our data into our model during training.
 
Training the Model
 
 
We need two things for training, our DataLoader and a model. The DataLoader we have — but no model.



 
Initializing the Model
 
 
For training, we need a raw (not pre-trained) BERTLMHeadModel. To create that, we first need to create a RoBERTa config object to describe the parameters we’d like to initialize FiliBERTo with.

Then, we import and initialize our RoBERTa model with a language modeling (LM) head.

 
Training Preparation
 
 
Before moving onto our training loop we need to set up a few things. First, we set up GPU/CPU usage. Then we activate the training mode of our model — and finally, initialize our optimizer.

 
Training
 
 
Finally — training time! We train just as we usually would when training via PyTorch.

If we head on over to Tensorboard we’ll find our loss over time — it looks promising.


Loss / time — multiple training sessions have been threaded together in this chart

 
 
The Real Test
 
 
Now it’s time for the real test. We set up an MLM pipeline — and ask Laura to assess the results. You can watch the video review at 22:44 here:



We first initialize a pipeline object, using the 'fill-mask' argument. Then begin testing our model like so:

“ciao come va?” is the right answer! That’s as advanced as my Italian gets — so, let’s hand it over to Laura.
We start with “buongiorno, come va?” — or “good day, how are you?”:

The first answer, “buongiorno, chi va?” means “good day, who is there?” — eg nonsensical. But, our second answer is correct!
Next up, a slightly harder phrase, “ciao, dove ci incontriamo oggi pomeriggio?” — or “hi, where are we going to meet this afternoon?”:

And we return some more positive results:

✅ ""hi, where do we see each other this afternoon?""
✅ ""hi, where do we meet this afternoon?""
❌ ""hi, where here we are this afternoon?""
✅ ""hi, where are we meeting this afternoon?""
✅ ""hi, where do we meet this afternoon?""

 
Finally, one more, harder sentence, “cosa sarebbe successo se avessimo scelto un altro giorno?” — or “what would have happened if we had chosen another day?”:

We return a few good more good answers here too:

✅ ""what would have happened if we had chosen another day?""
✅ ""what would have happened if I had chosen another day?""
✅ ""what would have happened if they had chosen another day?""
✅ ""what would have happened if you had chosen another day?""
❌ ""what would have happened if another day was chosen?""

 
Overall, it looks like our model passed Laura’s tests — and we now have a competent Italian language model called FiliBERTo!
That’s it for this walkthrough of training a BERT model from scratch!
We’ve covered a lot of ground, from getting and formatting our data — all the way through to using language modeling to train our raw BERT model.
I hope you enjoyed this article! If you have any questions, let me know via Twitter or in the comments below. If you’d like more content like this, I post on YouTube too.
Thanks for reading!
 

70% Off! Natural Language Processing: NLP With Transformers in Python

Transformer models are the de-facto standard in modern NLP. They have proven themselves as the most expressive…
 
*All images are by the author except where stated otherwise
 
Bio: James Briggs is a data scientist specializing in natural language processing and working in the finance sector, based in London, UK. He is also a freelance mentor, writer, and content creator. You can reach the author via email (jamescalam94@gmail.com).
Original. Reposted with permission.
Related:

How to Apply Transformers to Any Length of Text
Understanding BERT with Hugging Face
Topic Modeling with BERT"
https://www.kdnuggets.com/2021/01/google-trillion-parameter-switch-transformer-model.html,Six Times Bigger than GPT-3: Inside Google’s TRILLION Parameter Switch Transformer Model,Google’s Switch Transformer model could be the next breakthrough in this area of deep learning.,"By Jesus Rodriguez, Intotheblock.
comments


Source: https://arxiv.org/pdf/2101.03961.pdf

 

I recently started a new newsletter focus on AI education and already has over 50,000 subscribers. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:


 
OpenAI’s GPT-3 is, arguably , the most famous deep learning models created in the last few years. One of the things that impresses the most about GPT-3 is its size. In some context, GPT-3 is nothing but GPT-2 with a lot of more parameters. With 175 billion parameters, GPT-3 was about four times bigger than its largest predecessor.
Knowing that, how would you then feel about a model that is 6 times larger than GPT-3? This is precisely what a team from Google Research achieved with their novel Switch Transformer architecture. The new model features an unfathomable 1.6 trillion parameters which makes it effectively six times larger than GPT-3.
1.6 trillion parameters is certainly impressive but that’s not the most impressive contribution of the Switch Transformer architecture. With this new model, Google is essentially unveiling a method that maximize the parameter count of a transformer model in a simple and computationally efficient way. Transformer models like GPT-3 are not only huge but also computationally expensive which limits its adoption in mainstream scenarios.
The key modification of the Switch Transformer architecture is based on introducing a Mixture of Experts (MoE) routing layer that facilitates learning sparse models instead of a super large dense model. This is not as confusing as it reads so let me try to explain. Typical transformer architectures are composed by the famous attention layer followed by a dense feed forward network. Among other things, that dense layer is responsible for the large cost of training transformer models. Google’s Switch Transformer proposes replacing that layer with what they called a Switch FFN layer. That layer processes the input tokens and decides which smaller feed forward network should process it. The Switch FFN layer includes three main benefits:

The router computation is very small as it only routes to a single expert.
The capacity of each expert network can remain manageable.
The router implementation is super simple.



Source: https://arxiv.org/pdf/2101.03961.pdf

 
With the new optimizations, Google was able to train a Switch Transformer model to an astonishing 1.6 trillion parameters! The training speed improved to up seven times compared to previous architectures.
Miraculously, the Switch Transformer release has managed to remain under the radar. Somehow, it reminds me of the original BERT paper that trigger the whole transformer movement. However, if the hype behind GPT-3 is any indication of what’s next to come, keep an eye for new milestones using the Switch Transformer.
 
Original. Reposted with permission.
Related:

Microsoft Uses Transformer Networks to Answer Questions About Images With Minimum Training
OpenAI Releases Two Transformer Models that Magically Link Language and Computer Vision
Top 5 Artificial Intelligence (AI) Trends for 2021"
https://www.kdnuggets.com/2020/09/geographical-plots-python.html,Geographical Plots with Python,"When your data includes geographical information, rich map visualizations can offer significant value for you to understand your data and for the end user when interpreting analytical results.","comments
By Ahmad Bin Shafiq, Machine Learning Student.

 
Plotly
 
Plotly is a famous library used for creating interactive plotting and dashboards in Python. Plotly is also a company, that allows us to host both online and offline data visualisatoins.
In this article, we will be using offline plotly to visually represent data in the form of different geographical maps.
Installing Plotly

pip install plotly
pip install cufflinks



 
Run both commands in the command prompt to install plotly and cufflinks and all of their packages on our local machine.
Choropleth Map
Choropleth maps are popular thematic maps used to represent statistical data through various shading patterns or symbols on predetermined geographic areas (i.e., countries). They are good at utilizing data to easily represent the variability of the desired measurement across a region.
How does a Choropleth map works?
Choropleth Maps display divided geographical areas or regions that are colored, shaded, or patterned in relation to a data variable. This provides a way to visualize values over a geographical area, which can show variation or patterns across the displayed location.
Using Choropleth with Python
Here, we will be using a dataset of power consumption of different countries throughout the world in 2014.
Okay, so let’s get started.
Importing libraries

import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode,iplot,plot
init_notebook_mode(connected=True)

import pandas as pd



 
Here, init_notebook_mode(connected=True) connects Javascript to our notebook.
Creating/Interpreting our DataFrame

df = pd.read_csv('2014_World_Power_Consumption')
df.info()



 

Here we have 3 columns, and all of them have 219 non-null entries.

df.head()




Compiling our data into dictionaries

data = dict(
        type = 'choropleth',
        colorscale = 'Viridis',
        locations = df['Country'],
        locationmode = ""country names"",
        z = df['Power Consumption KWH'],
        text = df['Country'],
        colorbar = {'title' : 'Power Consumption KWH'},
      )



 
type = ’choropleth': defines the type of the map, i.e., choropleth in this case.
colorscale = ‘Viridis': displays a color map (for more color scales, refer here).
locations = df['Country']: add a list of all countries.
locationmode = 'country names’: as we have country names in our dataset, so we set location mode to ‘country names’.
z: list of integer values that display the power consumption of each state.
text = df['Country']: displays a text when hovering over the map for each state element. In this case, it is the name of the country itself.
colorbar = {‘title’ : ‘Power Consumption KWH’}: a dictionary that contains information about the right sidebar. Here, colorbar contains the title of the sidebar.

layout = dict(title = '2014 Power Consumption KWH',
              geo = dict(projection = {'type':'mercator'})
             )



 

layout — a Geo object that can be used to control the appearance of the base map onto which data is plotted.

It is a nested dictionary that contains all the relevant information about how our map/plot should look like.
Generating our plot/map

choromap = go.Figure(data = [data],layout = layout)
iplot(choromap,validate=False)



 

Cool! Our choropleth map for the ‘2014 World Power Consumption’ has been generated, and from the above, we can see that each country displays its name and its power consumption in kWh when hovering over each element on the map. The more concentrated the data is in one particular area, the deeper the shade of color on the map. Here ‘China’ has the largest power consumption, and so its color is deepest.
 
Density Maps
 
Density mapping is simply a way to show where points or lines may be concentrated in a given area.
Using Density Maps with Python
Here, we will be using a worldwide dataset of earthquakes and their magnitudes.
Okay, so let’s get started.
Importing libraries

import plotly.express as px
import pandas as pd



 
Creating/Interpreting our DataFrame

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')
df.info()



 

Here, we have 4 columns, and all of them have 23412 non-null entries.

df.head()



 

Plotting our data

fig = px.density_mapbox(df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
                        center=dict(lat=0, lon=180), zoom=0,
                        mapbox_style=""stamen-terrain"")
fig.show()



 
lat='Latitude': takes in the Latitude column of our data frame.
lon='Longitude': takes in the Longitude column of our data frame.
z: list of integer values that display the magnitude of the earthquake.
radius=10: sets the radius of influence of each point.
center=dict(lat=0, lon=180): sets the center point of the map in a dictionary.
zoom=0: sets map zoom level.
mapbox_style='stamen-terrain': sets the base map style. Here, ""stamen-terrain"" is the base map style.
fig.show(): displays the map.
Map

Great! Our Density map for the ‘Earthquakes and their magnitudes’ has been generated, and from the above, we can see that it covers all the territories that suffered from the earthquake, and also shows the magnitude of the earthquake of every region when we hover over it.
Geographical plotting using plotly can be a bit challenging sometimes due to its various formats of data, so please refer to this cheat sheet for all types of syntaxes of plotly plots.
 
 
Related:

Exploring the Impact of Geographic Information Systems
Top 10 Data Visualization Tools for Every Data Scientist
7 Techniques to Visualize Geospatial Data"
https://www.kdnuggets.com/2020/12/predictions-ai-machine-learning-data-science-research.html,"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021","2020 is finally coming to a close. While likely not to register as anyone's favorite year, 2020 did have some noteworthy advancements in our field, and 2021 promises some important key trends to look forward to. As has become a year-end tradition, our collection of experts have once again contributed their thoughts. Read on to find out more.","By Matthew Mayo, KDnuggets.
comments
To the chagrin of absolutely no one, 2020 is finally drawing to a close. It has been a rollercoaster of a year, one defined almost exclusively by the COVID-19 pandemic. But other things have happened, including in the fields of AI, data science, and machine learning as well. To that end, it's time for KDnuggets annual year end expert analysis and predictions. This year we posed the question:
What were the main developments in AI, Data Science, Machine Learning Research in 2020 and what key trends do you see for 2021?
Last year's noted main developments and predictions included continued advancements in many research areas, NLP in particular. While there can be debate as to whether 2020's big NLP advancement was as formidable as some may have originally thought (or continue to think), there is no doubt that there was a continued and intense focus on NLP research in 2020. It should not be difficult to surmise that this continues into 2021 as well.
Topics such as ethics and diversity were taking center stage in 2019, and this past year they stayed there. There seems to have been a transition from thinking of diversity and ethics and related subjects as periphery concerns in machine learning to viewing them as core considerations alongside technology. Let's hope this trend continues into 2021 and beyond.
What did our panel come up with as the main developments of 2020, and what do they see as the most likely key trends for 2021? Our group this year consists of Dan Becker, Pedro Domingos, Ajit Jaokar, Ines Montani, Brandon Rohrer, Dipanjan Sarkar, Rosaria Silipo, Rachael Tatman, and Daniel Tunkelang. More so than any other year, we thank our contributors for taking the time out of their busy schedules in these tumultuous times to share their insights with our readers.
This is the first in a series of 3 such posts over the coming week. While they will be split up into research, technology, and industry, there is considerable and understandable overlap between these disciplines, and as such we recommend you check out all 3 as they are published.

  
Without further delay, here are the 2020 key trends and 2021 predictions from this year's group of experts.
 
Dan Becker (@dan_s_becker) is Founder of Decision AI, and previously founded Kaggle Learn.
ML research followed some established themes this year:

Transformers: GPT-3 received the most attention of any development this year, and it shows the continued evolution of Transformer models trained on huge corpora. We also saw first successful experiments to use transformers for computer vision, which was historically dominated by convolutional networks.
Generative Models: Research like Vid2Player shows computer-generated video at a level of quality beyond what we've seen in the past. The social impact of generative models will be huge and hard to predict.
Reinforcement Learning: I saw less attention to RL in 2020 than I saw in the previous couple years. But the transfer learning across tasks in One Policy To Rule Them All looks hugely promising. I expect this to be less important than GPT-3 over the next couple years, but likely far more important over a longer time horizon. Most people don't realize the huge impact RL will have once it works more reliably.

2021:

Probabilistic Programming and Bayesian Models: We've seen a lot of experimentation in new probabilistic programming languages. This reminds me of the experimentation I saw in deep learning frameworks 5 years ago. So I hope probabilistic programming is a key trend in 2021, though it will also require more education for users to take advantage of the new tools.
GPT-4: As more people experiment with GPT-3, I think we'll find it falls a little short of most practical usefulness... Extrapolating from recent trends, GPT-4 will be much better and will likely cross that threshold of practical usefulness.
GPUs for structured data: The NVIDIA RAPIDS team is creating data science tools that promise a sudden speedup beyond anything we've seen in the last decade. My sense is that this software isn't yet ready for prime time, but that could come in 2021.
AutoML becomes uninteresting: Most data scientists are still tuning parameters through ad hoc experimentation. It's a matter of time until we all use more automatic solutions, and the time may come in the next year.
Reinforcement learning becomes practically useful: This is what I'm most excited about. Conventional machine learning is focused on prediction, but few data scientists optimize the decision layer that translates those predictions into real-world business decisions. This has resulted in models that are accurate without being useful. We'll see a mindset shift in 2021 to use models for optimal decision-making in complex environments.

 
Pedro Domingos (@pmddomingos) is a Professor in the Dept. of Computer Science & Engineering, University of Washington.
To my mind, the main developments in 2020 were the emergence of graph neural networks and neuro-symbolic AI as major research directions. I think in 2021 we'll see the latter subsume the former: GNNs are a limited form of relational learning, and before long we'll have neuro-symbolic approaches that accomplish all that GNNs do, and then some. After this, where you turn the dial of representational power for specific applications is mainly the usual matter of overfitting control and scalability. At the high end, how far neuro-symbolic AI gets us toward human-level AI is the trillion dollar question.
 
Ajit Jaokar (@AjitJaokar) is the course director of the ""Artificial Intelligence: Cloud and Edge implementations"" course at the University of Oxford, and is an entrepreneur.
2020 was the year of COVID but also of tech. AI matured through MLOps deployments. The Cloud platforms (ex: AWS, Azure, GCP) continue to drive innovation in all areas of AI including AI on Edge devices. I expect to see much more innovation in this space after the acquisition of ARM by Nvidia.
In the AI world, the big trend was NLP (GPT-3 and other models). For 2021, the real question is – would few shot learner models (like GPT-3) change the way models are built? Instead of the traditional sequence of building a model from data reflecting a problem, we could flip it. We can think of just a forward pass with very large models i.e. Model – Problem – Inference. Of course, we need a massive pre-trained model like GPT-3. If this trend does take off – it will be transforming for AI over the next two years.
In 2021, traditional machine learning models could become a commodity in the sense that everyone would be using some form of basic ML or DL. So, we could shift from data science to decision science. The output of data science is a model with a performance metrics (for example accuracy). With decision science, we could take this further by suggesting actions and execute these actions. That means algorithms like reinforcement learning could be a part of 2021 and beyond
 
Ines Montani (@_inesmontani) is a software developer working on Artificial Intelligence and Natural Language Processing technologies, and the co-founder of Explosion.
2020 has been an extraordinary year and while we've seen many exciting advancements in the field, the most important developments in my opinion have been about consolidation rather than revolution. In previous years, the technology was evolving so quickly that for many companies, it was wise to wait. That calculation has changed now, and there's a much better understanding of what projects are likely to succeed. Building prototypes and applying machine learning to business problems has never been easier but what remains challenging is closing the gap between prototyping and shipping successful projects into production. In 2021, we'll likely keep seeing more focus on the whole lifecycle of a machine learning project: from prototype to production, and from iterative development to ongoing maintenance and monitoring.
 
Brandon Rohrer (@_brohrer_) is Principal Data Scientist at iRobot and Instructor at End-to-End Machine Learning.
Convolutional and recurrent neural networks are beginning to show they can’t solve every problem as well as we would like. Two papers this year sum up this trend. The Hardware Lottery (https://arxiv.org/abs/2009.06489v1) describes how much serendipity can be involved in which algorithms rise to prominence and become entrenched as an industry standard. And the tour de force paper Underspecification Presents Challenges for Credibility in Modern Machine Learning (https://arxiv.org/abs/2011.03395) casts a harsh light how we have been evaluating models and measuring progress. These are good things. In 2021, if we choose to, we can invest in exploration and in solving new families of problems.
Also, because we’ve been left no choice, we’ve sprung to develop tools and practices for remote instruction, distributed teams, and asynchronous work. The machine learning research environment of 2020 would be unrecognizable to our 2019 selves. In 2021 I predict the quality and quality of online instruction and collaboration will double.
 
Dipanjan Sarkar is a Data Science Lead at Applied Materials, a Google Developer Expert in Machine Learning, a published author, and consultant.
Based on my prediction last year, 2020 has rightly been the year of NLP, with transformers paving the way to solve tough problems like question answering, search and translation with ease. Explainable AI has also started moving out of the ‘inflated expectations’ Gartner Hype Cycle phase, with a lot of practical implementations being available and used to explain complex models across diverse problems and data.
For 2021, I am sure we are going to see the advent of powerful, yet efficient models especially for both vision and natural language processing. We have already seen progress with efficient transformer models like DistilBERT, Reformer and Performer. Deep Learning frameworks like TensorFlow are focusing on ML for mobile and IoT devices with TFLite and TF.js with edge and on-device computing being in-demand.
I also foresee more progress in areas pertaining to unsupervised and self-supervised learning in the field of deep learning with methodologies like SimCLR, SimSiam and SwAV which have achieved huge success in pre-training models to give a better performance during the adaptation phase. Last, but not the least, low-code automated machine learning platforms and responsible AI are two other areas to watch out for as we can definitely expect some interest advancements there.
 
Rosaria Silipo (@DMR_Rosaria) is Principal Data Scientist at KNIME.
In this strange year of 2020, given the uncertainty about the future, attention has focused on getting data science solutions ready to work and produce results: safe deployment, application monitoring, and secure solutions. This trend will probably continue in 2021. 
Deployment remains the critical phase in a data science project, where all unnoticed mistakes from the previous steps resurface. So, in addition to the classic enterprise features, we are starting to feel the need for productionizing an application from within the training  environment to avoid needless mistakes during the transfer.
Some focus in 2021 will also be on the interpretation of the data analysis process, especially in the life sciences via machine learning interpretability (MLI) or eXplainable AI (XAI) techniques for black-box models.
On a side note, I really suspect that if the COVID-isolation persists in many countries around the world, the number of books about machine learning and artificial intelligence will skyrocket.
 
Rachael Tatman (@rctatman) is Developer Advocate at Rasa working on natural language processing.
I know a lot of folks would probably consider GPT-3 to be a major new development in NLP this year, but I'd consider it a pretty straightforward extension of existing NLP methods on a scale that's utterly impractical for the vast majority of NLP applications. What I personally find far more exciting is the growing trend of focusing on small, efficient models that still perform well. The first SustainNLP workshop (https://sites.google.com/view/sustainlp2020/home) is a great example of this. From a research perspective, I think finding ways to get really excellent model performance with limited data and compute resources is going to be a huge challenge in the field, but also really rewarding. 
 
Daniel Tunkelang is an independent consultant specializing in search, discovery, and ML/AI.
In 2020, AI has continued to improve incrementally. We've seen iterations on transformer-based models for natural language understanding and generation, the most notable being OpenAI's GPT-3. Autonomous vehicles continue to be almost ready for mainstream use. More broadly, AI has moved from being a buzzword to a critical capability for companies in all industries.
Meanwhile, 2020 has been dominated by the Covid-19 pandemic. While AI has played a part in the fight against the virus, what's been more interesting is how, because of the pandemic, most people working on or studying machine learning are doing so from home. If the mainstream acceptance of remote work and education persists after the pandemic -- which seems likely -- then we can look forward to two competing trends. On one hand, AI expertise will become truly global rather than tied to specific hubs. On the other hand, technology powerhouses will recruit talent globally, at the expense of smaller regional firms.
And yet, even as remote work drives the globalization of AI, the growing conflict between the United States and China splinters it. It seems likely that we will spend the next decade in an AI arms race.
ps. On November 30, the submission deadline for this post, DeepMind researchers announced that their AlphaFold system has solved the Critical Assessment of protein Structure Prediction (CASP) grand challenge by predicting protein folding grand challenge with revolutionary accuracy and speed. It's too early to digest this announcement, but this may indeed turn out to be the biggest AI breakthrough of 2020.
 
Related:

AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020
Machine Learning & AI Main Developments in 2018 and Key Trends for 2019
AI, Analytics, Machine Learning, Data Science, Deep Learning Technology Main Developments in 2019 and Key Trends for 2020"
https://www.kdnuggets.com/2021/04/cogitotech-6-mistakes-avoid-training-machine-learning.html,6 Mistakes To Avoid While Training Your Machine Learning Model,"While training the AI model, multi-stage activities are performed to utilize the training data in the best manner, so that outcomes are satisfying. So, here are the 6 common mistakes you need to understand to make sure your AI model is successful.","Sponsored Post.

Developing an AI or a ML model is not a child’s play. It requires lot of knowledge and skills with enriched experience to make the model work successfully in multiple scenarios.

Additionally, you need high-quality computer vision training data especially to train your visual perception based AI model. The most crucial stage in AI development is acquiring & collecting the training data and using this data while training the models. 
Any mistake while training your model will not only makes your model perform inaccurately but also could be disastrous while making crucial business decisions, especially in certain areas such as Healthcare or Self Driving Cars.
While training the AI model, multi-stage activities are performed to utilize the training data in the best manner, so that outcomes are satisfying. So, here are the 6 common mistakes you need to understand to make sure your AI model is successful. 
 
#1 Using Unverified and Unstructured Data
 
The use of unverified & unstructured data is one of the most common mistakes machine learning engineers do in AI developments. The unverified data might have errors such as duplication, conflicting data, lack of categorization, data conflict, errors and other data issues that could create anomalies during the training process.
Hence, before you use the data for your machine learning training, carefully examine your raw data set and eliminate the unwanted or irrelevant data, helping your AI model work with better accuracy.
 
#2 Using the Already Used Data to Test Your Model 
 
One should avoid re-using the data that has already been used to test the model. Hence, such mistakes should be avoided. For example, if someone has already learned something and has applied that knowledge to their area of work; using the same learnings on another area of work could lead to one being biased and repetitive in inferencing.
Similarly, in machine learning, the same logic applies, AI can learn with the bulk of datasets to predict the answers accurately. Using the same training data on Models or AI based applications could lead the model to be biased and derive results which are the resultant of their previous learning. Hence, while testing the capabilities of your AI model, it is very important to test using the new datasets that were not used earlier for machine learning training. 
 
#3 Using the Insufficient Training Data Sets
 
To make your AI model successful you need to use the right training data so that it can predict with highest level of accuracy. Lack of sufficient data for training is one of the primary reasons behind the failure of the model.
However, depending on the type of AI model or industries, the fields of requirement of training data is varied. For deep learning, you need more quantitative as well as qualitative datasets to make sure it can work with the high precision. 
 
#4 Making Sure Your AI Model is Unbiased
 
It is not possible to develop an AI model that can give a hundred per cent accurate results in various scenarios. Just like humans, machines could also be biased due to various factors such as age, gender, orientation, and income level etc., which could affect the results one way or another. Hence, you need to minimize this by using statistical analysis to find how each personal factor is affecting the data and AI training data in process.
 
#5 Relying on AI Model Learning Independently
 
Though, you need experts to train in your AI model, using a huge amount of training datasets. But if AI is using the repetitive machine learning process that needs to be considered while training such models.
Here, as a machine learning engineer, you need to make sure that your AI model is learning with the right strategy. To ensure this you must frequently check the AI training process and its results at regular intervals to get the best outcomes.
However, while developing the machine learning AI, you need to keep asking yourself important questions such as; is your data sourced from a trustworthy reliable source? Does your AI cover a wide demographic and is there anything else affecting the results?
 
#6 Not Using the Properly Labelled Datasets
 
To achieve the winning streak while developing an AI model through machine learning you need a well-defined strategy. This will not only help you to get the best outcomes but also to make the machine learning models more reliable among the end-users.
Though, mentioned above are the key points you need to keep in mind while training your model. But training data accurately with highest level of precision is highly crucial in making the AI successful and work with the best level of accuracy in various scenarios. If your data is not properly labelled it will affect the performance of the model.
If your machine learning model is Computer Vision-oriented, to get the right training data, image annotation is the precise technique to create such datasets. Getting the right labelled data is another challenge for AI companies while training the model. But there are many companies offering data labeling for machine learning and AI"
https://www.kdnuggets.com/2021/04/best-machine-learning-frameworks-extensions-tensorflow.html,The Best Machine Learning Frameworks & Extensions for TensorFlow,Check out this curated list of useful frameworks and extensions for TensorFlow.,"By Derrick Mwiti, Data Scientist.
comments
TensorFlow has a large ecosystem of libraries and extensions. If you’re a developer, you can easily add them into your ML work without having to build new functions.
In this article, we will explore some of the TensorFlow extensions that you can start using right away.
To start, let’s check out domain-specific pre-trained models from TensorFlow Hub.
Let’s get to it!
 
TensorFlow Hub
 
TensorFlow Hub is a repository with hundreds of trained and ready-to-use models. You can find models for:

natural language processing
object detection
image classification
style transfer
video action detection
sound classification
pitch recognition

To use a model, you first need to identify it at tfhub.dev. You’re going to need to check its documentation. For example, here are instructions to load this ImageNet classification model.

model = tf.keras.Sequential([
    hub.KerasLayer(""https://tfhub.dev/google/imagenet/inception_v1/classification/4"")
])


Models can be used as they are, or you can fine-tune them. The model’s documentation offers instructions on how to do this.
For example, we can fine-tune the above model by passing ‘trainable=True’ to ‘hub.kerasLayer’.

hub.KerasLayer(""https://tfhub.dev/google/imagenet/inception_v1/classification/4"",
               trainable=True, arguments=dict(batch_norm_momentum=0.997))


 
TensorFlow Model Optimization Toolkit
 
This is a collection of tools that you can use to optimize models for execution and deployment.
Why is this important?

it reduces the latency of models on mobile devices,
it reduces the cost of cloud, because models become small enough for edge device deployment.

Optimizing models might lead to a reduction in accuracy. Depending on the problem, you’ll need to decide if a slightly less accurate model is worth the advantage of model optimization.
Optimization can be applied to pre-trained models from tfhub.dev, as well as your own trained models. You can also download optimized models from tfhub.dev.
One of the techniques for model optimization is pruning. In this technique, unnecessary values in the weight tensor are eliminated. This results in smaller models, with accuracy that’s very close to the baseline model.
The first step in pruning a model is to define the pruning parameters.
Setting a sparsity of 50% means that 50% of the weights will be zeroed. The ‘PruningSchedule’ is responsible for controlling pruning during training.

from tensorflow_model_optimization.sparsity.keras import ConstantSparsity
pruning_params = {
    'pruning_schedule': ConstantSparsity(0.5, 0),
    'block_size': (1, 1),
    'block_pooling_type': 'AVG'
}


After that, you can prune the entire model using the above parameters.

from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude
model_to_prune = prune_low_magnitude(
    keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(1, activation='relu')
    ]), **pruning_params)


An alternative is to use quantization aware training that uses lower-precision, for example 8-bit instead of 32-bit float.

import tensorflow_model_optimization as tfmot
quantize_model = tfmot.quantization.keras.quantize_model
q_aware_model = quantize_model(model)


At this point, you’ll have a model that’s quantization aware, but not yet quantized.
After you compile and train the model, you can create the quantized model using the TFLite Converter.

converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

quantized_tflite_model = converter.convert()


You can also quantize certain layers of the model.
The other model optimization strategy is weight clustering. In this technique, the number of unique weight values is reduced.
 
TensorFlow Recommenders
 
TensorFlow Recommenders (TFRS) is a library for building recommender system models.
You can use it for preparing data, formulating the model, training, evaluation, and deployment. This Notebook contains a full example of how to use TFRS.
 
TensorFlow Federated
 
TensorFlow Federated (TFF) is an open-source library for machine learning on decentralized data. In federated learning, devices can collaboratively learn from a shared model.
The model will be trained on a server using proxy data. Each device will then download the model and improve it using the data on that device.
What’s good about this approach is that sensitive user data is never uploaded to the server. One way this has been used is in phone keyboards.
TensorFlow Federated is made up of two layers:

Federated Learning (FL) API
Federated Core (FC) API

Using the Federated Learning (FL) API, developers can apply federated training and evaluation on existing TensorFlow models.
The Federated Core (FC) API is a system of low-level interfaces for writing federated algorithms.
If you’re interested, check out official TensorFlow Federated tutorials to learn more.
 
TensorFlow Graphics
 
To build more efficient neural network architectures, you can insert differentiable graphic layers.
Modeling geometric priors and constraints to neural networks leads to architectures that can be trained more robustly and efficiently.
The combination of computer graphics and computer vision lets us use unlabelled data in machine learning problems. Tensorflow Graphics provides a suite of differentiable graphics, geometry layers and 3D viewer functionalities.
Here’s an example of the output produced by a code snippet from the official docs.

import numpy as np
import tensorflow as tf
import trimesh

import tensorflow_graphics.geometry.transformation as tfg_transformation
from tensorflow_graphics.notebooks import threejs_visualization

# Download the mesh.
!wget https://storage.googleapis.com/tensorflow-graphics/notebooks/index/cow.obj
# Load the mesh.
mesh = trimesh.load(""cow.obj"")
mesh = {""vertices"": mesh.vertices, ""faces"": mesh.faces}
# Visualize the original mesh.
threejs_visualization.triangular_mesh_renderer(mesh, width=400, height=400)
# Set the axis and angle parameters.
axis = np.array((0., 1., 0.))  # y axis.
angle = np.array((np.pi / 4.,))  # 45 degree angle.
# Rotate the mesh.
mesh[""vertices""] = tfg_transformation.axis_angle.rotate(mesh[""vertices""], axis,
                                                        angle).numpy()
# Visualize the rotated mesh.
threejs_visualization.triangular_mesh_renderer(mesh, width=400, height=400)



 
TensorFlow Privacy 
 
This library is for training machine learning models with training data privacy. Some of the tutorials provided for this include:

training a language model with differential privacy
a convolutional neural network on MNIST with differential privacy

Differential privacy is expressed using epsilon and delta.
 
Tensor2tensor
 
This is a library of models and datasets aimed at making deep learning more accessible and accelerate research in machine learning.
 
TensorFlow Probability
 
According to the official docs:
“TensorFlow Probability is a library for probabilistic reasoning and statistical analysis in TensorFlow”
You can use the library to encode domain knowledge, but it also has:

support for many probability distributions
tools for building deep probabilistic models
variational inference and Markov chain Monte Carlo
optimizers such as Nelder-Mead, BFGS, and SGLD

Here’s an example model based on the Bernoulli distribution:

model = tfp.glm.Bernoulli()
coeffs, linear_response, is_converged, num_iter = tfp.glm.fit(
    model_matrix=features[:, tf.newaxis],
    response=tf.cast(labels, dtype=tf.float32),
    model=model)


 
TensorFlow Extended (TFX)
 
TensorFlow Extended (TFX) is a platform that you can use to bring your machine learning pipeline to production.
Plus, using TensorFlow’s ModelServer lets you use a RESTful API to access your model.
Assuming you have it installed and configured, the server can be started by running:

$ tensorflow_model_server -- rest_api_port=8000 
                               -- model_config_file=models.config 
                               -- model_config_file_poll_wait_seconds=300


The API will be available on port 8000 on localhost. Setting up this server requires some knowledge of server administration.
 
TensorBoard
 
TensorBoard is TensorFlow’s open-source visualization toolkit. You can use it as a callback in your model training in order to track the process. It can be used to track various metrics such as log loss and accuracy. TensorBoard also provides several tools that can be used for experimentation.  You can use it to:

visualize images
check model weights and biases
visualize the architecture of the model
see the performance of your application via profiling

just to mention a few.
Note: As an alternative, you can also track and visualize model training runs,  and version your models in Neptune.
For instance, here is how you can log your Keras experiments using Neptune.

PARAMS = {'lr': 0.01, 'epochs': 10}
neptune.create_experiment('model-training-run', params=PARAMS)

model.fit(x_train, y_train,
          epochs=PARAMS['epochs'],
          callbacks=[NeptuneMonitor()])

neptune.log_artifact('model.h5')



See Neptune TensorFlow/Keras integration
 
TensorFlow Agents
 
This library can be used for designing, implementing, and testing reinforcement learning algorithms. It provides modular components that are extensively tested. Components can be modified and extended.
This notebook shows how to train a DQN (Deep Q Networks) agent on the Cartpole environment. The initialization code looks like this:

import tensorflow as tf
from tf_agents.networks import q_network
from tf_agents.agents.dqn import dqn_agent

q_net = q_network.QNetwork(
  train_env.observation_spec(),
  train_env.action_spec(),
  fc_layer_params=(100,))

agent = dqn_agent.DqnAgent(
  train_env.time_step_spec(),
  train_env.action_spec(),
  q_network=q_net,
  optimizer=optimizer,
  td_errors_loss_fn=common.element_wise_squared_loss,
  train_step_counter=tf.Variable(0))

agent.initialize()


 
Final thoughts
 
In this article, we explored several libraries that can be used to extend TensorFlow’s functionalities. Try using the code snippets I provided to familiarize yourself with the tools.
We talked about:

using pre-trained models from TensorFlow Hub,
optimizing your models using TensorFlow Model Optimization Toolkit,
building recommenders using TensorFlow Recommenders,
training models on decentralized data using TensorFlow Federated,
training in private mode with TensorFlow Privacy.

And that’s quite a lot, so choose one of these to start with, and go through the list to see if any tools fit your machine learning workflow.
 
Bio: Derrick Mwiti is a data scientist who has a great passion for sharing knowledge. He is an avid contributor to the data science community via blogs such as Heartbeat, Towards Data Science, Datacamp, Neptune AI, KDnuggets just to mention a few. His content has been viewed over a million times on the internet. Derrick is also an author and online instructor. He also trains and works with various institutions to implement data science solutions as well as to upskill their staff. You might want to check his Complete Data Science & Machine Learning Bootcamp in Python course.
Original. Reposted with permission.
Related:

The Best Machine Learning Frameworks & Extensions for Scikit-learn
Top Python Libraries for Data Science, Data Visualization & Machine Learning
Top Python Libraries for Deep Learning, Natural Language Processing & Computer Vision"
https://www.kdnuggets.com/2020/11/dalex-explain-tensorflow-model.html,"tensorflow + dalex = :) , or how to explain a TensorFlow model","Having a machine learning model that generates interesting predictions is one thing. Understanding why it makes these predictions is another. For a tensorflow predictive model, it can be straightforward and convenient develop an explainable AI by leveraging the dalex Python package.","comments
By Hubert Baniecki, Research Software Engineer at MI2DataLab.

I will showcase how straightforward and convenient it is to explain a tensorflow predictive model using the dalex Python package. The introduction to this topic can be found in Explanatory Model Analysis: Explore, Explain, and Examine Predictive Models.
For this example, we will use the data from the World Happiness Report and predict the happiness scored according to economic production, social support, etc., for any given country.


Data from the World Happiness Report (Kaggle.com).
Let’s first train the basic tensorflow model incorporating the experimental normalization layer for a better fit.

The next step is to create a dalex Explainer object, which takes model and data as input.

Now, we are ready to explain the model using various methods: model level methods explain the global behavior, while predict level methods focus locally on a single observation from the data. We can start by evaluating model performance.


Model performance for the happiness regression task.
Which features are the most important? Let’s compare the two methods, one of which is implemented in the shap package.



Comparison of the two Feature Importance methods, one of which is implemented in the shap package.
What are the continuous relationships between variables and predictions? We use Partial Dependence profiles, which point out that not always the more, the better.


Partial Dependence profiles display the continuous relationships between variables and predictions.
What about the residuals? These plots are useful to visualize where the model is wrong.


Residual diagnostics can help assess weaknesses in our model.
One can be more curious about the variable attributions for a specific country,


Variable attributions for Poland.
or several countries to compare the results.


Variable attributions for multiple countries.
Should you be interested in surrogate approximation, there is a possibility to produce the lime package explanations using the unified interface.


Surrogate approximation - an explanation from the lime package.
Finally, if an interpretable model is needed, we can approximate the black-box with an easy-to-understand decision tree.


Decision tree trained on predicted values of the black-box model.
I hope that this journey brought you some happiness as it is accessible and user-friendly to explain predictive models nowadays. Of course, there are more explanations, results, and plots in the dalex toolkit. We prepared various resources listed in the package README .
The code for this piece is available at http://dalex.drwhy.ai/python-dalex-tensorflow.html.
 

pip install tensorflow dalex shap statsmodels lime scikit-learn


 
:)
Original. Reposted with permission.
 
Bio: Hubert Baniecki is a Research Software Engineer, developing R & Python tools for Explainable AI, and researching ML in the context of interpretability and human-model interaction.
Related:

Interpretability, Explainability, and Machine Learning – What Data Scientists Need to Know
Explaining the Explainable AI: A 2-Stage Approach
Explainability: Cracking open the black box, Part 1"
https://www.kdnuggets.com/2021/08/ai21-jurassic1-language-models.html,Jurassic-1 Language Models and AI21 Studio,"AI21 Labs’ new developer platform offers instant access to our 178B-parameter language model, to help you build sophisticated text-based AI applications at scale.","By AI21 Labs 

We are thrilled to announce the launch of AI21 Studio, our new developer platform where you can use our state-of-the-art Jurassic-1 language models to build your own applications and services. Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers. AI21 Studio is currently in open beta, allowing anyone to sign up and immediately start querying Jurassic-1 using our API and interactive web environment.
From a technical perspective, Jurassic-1 Jumbo enjoys a slight size advantage relative to GPT-3 (excess 3B parameters), but also, it introduces several conceptual novelties into this arena of huge language models. The depth-to-width ratio of Jurassic-1 Jumbo’s core Transformer architecture was optimized for its size -- It is shallower (76 vs 96 layers) and wider (13824 vs 12288 hidden dimension) than GPT-3, with more computational parameters per layer over fewer layers. This modification is aimed at maximizing the expressivity of the network, following theoretical insights published in the last NeurIPS. From a practical perspective, a shallower and wider network allows more parallelization between compute operations, reducing latency. Moreover, the Jurassic-1 models utilize a unique 250,000-token vocabulary which is not only much larger than most existing vocabularies (5x or more), but also the first to include multi-word tokens such as expressions, phrases, and named entities. Because of this, Jurassic-1 needs fewer tokens to represent a given amount of text, thereby improving computational efficiency and further reducing latency. Check out the white paper for more technical details, as well as a thorough evaluation of our models. 
In order to help developers scale their applications beyond a proof-of-concept and efficiently serve production-scale traffic, AI21 Studio allows developers to train custom versions of Jurassic-1 models. Training a custom model is easy and requires as few as 50-100 training examples. Once trained, your custom model is served in AI21 Studio and immediately available for your exclusive use.
We created AI21 Studio to democratize access to cutting-edge AI technology. Using Jurassic-1 within AI21 Studio, you can quickly build text-based applications that rival those being dreamed up in the world’s biggest labs, even if you have no prior experience. We’ve been using AI21 Studio internally to power our own applications, and it has propelled our product development immensely. Now it’s your turn.
GET STARTED NOW
To learn more, visit our blog posts highlighting different use-cases for Jurassic-1 and demonstrating how to bootstrap a custom model in AI21 Studio or read the AI21 Studio documentation."
https://www.kdnuggets.com/2021/03/learning-from-machine-learning-mistakes.html,Learning from machine learning mistakes,Read this article and discover how to find weak spots of a regression model.,"comments
By Emeli Dral, CTO and Co-founder of Evidently AI


Image by Author

 
When we analyze machine learning model performance, we often focus on a single quality metric. With regression problems, this can be MAE, MAPE, RMSE, or whatever fits the problem domain best.
Optimizing for a single metric absolutely makes sense during training experiments. This way, we can compare different model runs and can choose the best one.
But when it comes to solving a real business problem and putting the model into production, we might need to know a bit more. How well does the model perform on different user groups? What types of errors does it make?
In this post, I will present an approach to evaluating the regression model performance in more detail.
 
Regression errors: too much or too little?
 
When we predict a continuous variable (such as price, demand, and so on), a common-sense definition of error is simple: we want the model predictions to be as close to actual as possible.
In practice, we might care not only about the absolute error value but also other criteria. For example, how well we catch the trend, if there is a correlation between the predicted and actual value — and what is the sign of our error, after all.
Underestimating and overestimating the target value might have different business implications. Especially if there is some business logic on top of the model output.
Imagine you are doing demand forecasting for a grocery chain. Some products are perishables, and delivering too much based on the wrong forecast would lead to waste. Overestimation has a clear cost to factor in.


Image by author. Source images from Unsplash: 1, 2.

 
In addition to classic error analysis, we might want to track this error skew (the tendency to over- or underestimate) and how it changes over time. It makes sense both when analyzing model quality during validation and in production monitoring.
To explain this concept of analyzing the error bias, let’s walk through an example.
 
Evaluating the model performance
 
Let’s say we have a model that predicts the demand for city bike rentals. (If you want to play with this use case, this Bike Demand Prediction dataset is openly available).
We trained a model, simulated the deployment, and compared its performance in “production” to how well it did on the training set.
In practice, we need to know the ground truth for that. Once we learn the actual demand, we can calculate our model’s quality and estimate how far off we are in our predictions.
Here, we can see a major increase in error between Reference performance in training and current Production performance.


Screenshot from the Evidently report.

 
To understand the quality better, we can look at the error distribution. It confirms what we already know: the error increased. There is some bias towards overestimation, too.


Screenshot from the Evidently report.

 
Things do not look ideal, and we want to dig deeper into what is going on. As do our business stakeholders. Why do these errors happen? Where exactly? Will retraining help us improve the quality? Do we need to engineer new features or create further post-processing?
Here is an idea of how to explore it.
 
Looking at the edges
 
Aggregate quality metrics show us the mean performance. However, these are the extreme cases that can often give us helpful information. Let us look directly there!
We can group the predictions where we have high errors and learn something useful from them.
How can we implement this approach?
Let’s take each individual prediction and calculate the error. Then, we create two groups based on the types of errors:

Overestimation. Cases where the model predicts the values that are higher than actual.
Underestimation. Cases where the model predicts the values that are lower than actual.

Let us limit the size of each group by choosing only 5% of the most extreme examples with the largest error. This way, we have the top-5% of predictions where the model overestimates and the top-5% where the model underestimates.
The rest 90% of predictions are the “majority.” The error in this group should be close to the mean.
That is how we can visualize the proposed segments. That is a sort of situation we’d like to see: most of the predictions are close to the actual values. Analyzing outliers can bring meaningful insight.


Image by Author.

 
How can it be useful?
Let’s take a time series example. If we built a great model and “learned” all the signal from the data, the error should be random. There should be no pattern. Except for a few likely outliers, the error would be close to the average in all groups. Sometimes slightly larger, sometimes smaller. But on average, about the same.
If there is some useful signal in the data that can explain the error, the situation can look differently. There can be a large error in specific groups. There can also be a clear skew towards under- or overestimation.
In these cases, the error may be dependent on specific feature values. What if we could find and describe the instances where it is higher than usual? That is precisely what we want to investigate!
 
Spotting the flaws
 
In our case, we can see that the error both in the over- and underestimation groups are significantly higher than the one in the “majority” group.


Screenshot from the Evidently report.

 
We can then try to investigate and explore the new patterns.
To do that, we look at the objects inside both 5%-groups and see what feature values correspond to them. Feature by feature, if we can.
Our goal is to identify if there is a relationship between the specific feature values and high error. To get deeper insight, we also distinguish between over- or under-estimation.
Imagine that we predict healthcare costs and consistently over-estimate the price for patients of certain demographics? Or, the error is unbiased but large, and our model fails on a specific segment? That is a sort of insight we want to find.


Image by Author.

 
We can make a complex (and computationally heavy) algorithm to perform this search for underperforming segments. As a reasonable replacement, we can just do this analysis feature by feature.
How can we do it? Let’s plot the feature distributions and our target demand and color-code the examples where we made high errors.
In our bike demand prediction use case, we can already get some insights. If we plot the “humidity” feature, we can notice that our model now significantly overestimates the demand when the humidity values are between 60 and 80 (plotted to the right).
We saw these values in our training dataset (plotted to the left), but the error was unbiased and similar on the whole range.


Screenshot from the Evidently report.

 
We can notice other patterns, too. For example, in temperature. The model also overestimates the demand when the temperature is above 30°C.


Screenshot from the Evidently report.

 
We can now suspect that something happened to the weather, and new related patterns emerged. In reality, we trained the model using the data from only cold months of the year. When it went to “production,” summer just started. With the new weather came new seasonal patterns that the model failed to grasp before.
The good news is that by looking at these plots, we can see that there seems to be some useful signal in the data. Retraining our model on new data would likely help.
 
How to do the same for my model?
 
We implemented this approach in the Evidently open-source library. To use it, you should prepare your model application data as a pandas DataFrame, including model features, predictions, and actual (target) values.
The library will work with a single DataFrame or two — if you want to compare your model performance in production with your training data or some other past period.


Image by Author.

 
The Regression performance report will generate a set of plots on model performance and an Error Bias table. The table helps explore the relations between the feature values and the error type and size.
You can also quickly sort the features to find those where the “extreme” groups look differently from the “majority.” It helps identify the most interesting segments without manually looking at each feature one by one.
You can read the full docs on Github.
 
When is this useful?
 
We believe this sort of analysis can be helpful more than once in your model lifecycle. You can use it:

To analyze the results of the model test. For example, once you validate your model an offline test or after A/B test or shadow deployment.
To perform ongoing monitoring of your model in production. You can do this at every run of a batch model or schedule it as a regular job.
To decide on the model retraining. Looking at the report, you can identify if it is time to update the model or if retraining would help.
To debug models in production. If the model quality fails, you can spot the segments where the model underperforms and decide how to address them. For example, you might provide more data for the low-performing segments, rebuild your model or add business rules on top of it.

If you want a practical example, here is a tutorial on debugging the performance of the machine learning model in production: “How to break a model in 20 days”.
 
Bio: Emeli Dral is a Co-founder and CTO at Evidently AI where she creates tools to analyze and monitor ML models. Earlier she co-founded an industrial AI startup and served as the Chief Data Scientist at Yandex Data Factory. She is a co-author of the Machine Learning and Data Analysis curriculum at Coursera with over 100,000 students.
Original. Reposted with permission.
Related:

A Machine Learning Model Monitoring Checklist: 7 Things to Track
MLOps: Model Monitoring 101
Evaluating Object Detection Models Using Mean Average Precision"
https://www.kdnuggets.com/2021/06/determined-ai-speed-up-deep-learning-language-model.html,How to speed up a Deep Learning Language model by almost 50X at half the cost,"In this blog post, we show how to accelerate fine-tuning the ALBERT language model while also reducing costs by using Determined’s built-in support for distributed training with AWS spot instances.","Sponsored Post.
By Armand McQueen

One of the big headaches in deep learning is that models take forever to train. As an ML engineer, waiting hours or days for training to complete makes iteratively improving your model a slow and frustrating process. You can speed up model training by using more GPUs, but this raises two challenges:

Distributed training is a hassle because it requires changing your model code and dealing with DevOps headaches like server management, cluster scheduling, networking, etc.
Using many GPUs at once can quickly cause your training costs to skyrocket, especially when using on-demand cloud GPUs.

In this blog post, we show how to accelerate fine-tuning the ALBERT language model while also reducing costs by using Determined’s built-in support for distributed training with AWS spot instances. Originally, ALBERT took over 36 hours to train on a single V100 GPU and cost $112 on AWS. With distributed training and spot instances, training the model using 64 V100 GPUs took only 48 minutes and cost only $47! That’s both a 46x performance improvement and a 58% reduction in cost!
Best of all, realizing these performance gains and cost reductions required nothing more than changing a few configuration settings. As we detail in the blog post, switching to distributed training and leveraging spot instances in Determined can be done without changing your model code, without needing to understand the details of using spot instances, and with no manual server wrangling required.
In the full article, we show you how we fine-tuned ALBERT on the SQuAD 2.0 dataset (using the Huggingface implementation), and how to save money by training with Determined using Spot Instances. You can read the full article “ALBERT on Determined: Distributed Training with Spot Instances” on our blog, and see the experiment in the Determined repository here. 
To learn more about Determined and how it can help make your training easier, faster and cheaper, check out our GitHub repo or hop on our community Slack."
https://www.kdnuggets.com/2020/10/data-science-cloud-dask.html,Data Science in the Cloud with Dask,Scaling large data analyses for data science and machine learning is growing in importance. Dask and Coiled are making it easy and fast for folks to do just that. Read on to find out how.,"comments
By Hugo Bowne-Anderson, Head of Data Science Evangelism and VP of Marketing at Coiled
The capability to scale large data analyses is growing in importance when it comes to data science and machine learning, and at a rapid rate. Fortunately, tools like Dask and Coiled are making it easy and fast for folks to do just that.
Dask is a popular solution for scaling up analyses when working in the PyData Ecosystem and Python. This is the case because Dask is designed to parallelize any PyData library, and hence seamlessly works with the host of PyData tools.
Scaling up your analysis to utilize all the cores of a sole workstation is the first step when starting to work with a large dataset.
Next, to leverage a cluster on the cloud (Azure, Google Cloud Platform, AWS, and so on) you might need to scale out your computation.
Read on and we will:

Use pandas to showcase a common pattern in data science workflows,
Utilize Dask to scale up workflows, harnessing the cores of a sole workstation, and
Demonstrate scaling out our workflow to the cloud with Coiled Cloud.

Find all the code here on github.
Note: Before you get started, it’s important to think about if scaling your computation is actually necessary. Consider making your pandas code more efficient before you jump in. With machine learning, you can measure if more data will result in model improvement by plotting learning curves before you begin.
 
PANDAS AND ETL: A COMMON PATTERN IN DATA SCIENCE
 
First, we’ll use pandas on an in-memory dataset to introduce a common data science pattern. This is a 700 MB subset of the NYC taxi dataset, which is about 10 GB in total.
We want to see the scaling shine bright, so we picked a relatively boring workflow. Now we read in the data, massage it, create a feature, and save it to Parquet (not human-readable but vastly more efficient than CSV).

# Import pandas and read in beginning of 1st file
import pandas as pd
df = pd.read_csv(""data_taxi/yellow_tripdata_2019-01.csv"")

# Alter data types for efficiency
df = df.astype({
    ""VendorID"": ""uint8"",
    ""passenger_count"": ""uint8"",
    ""RatecodeID"": ""uint8"",
    ""store_and_fwd_flag"": ""category"",
    ""PULocationID"": ""uint16"",
    ""DOLocationID"": ""uint16"",    
})

# Create new feature in dataset: tip_ratio
df[""tip_ratio""] = df.tip_amount / df.total_amount

# Write to parquet
df.to_parquet(""data_taxi/yellow_tripdata_2019-01.parq"")

 
This took roughly 1 minute on my laptop, a wait time for analysis we can tolerate (maybe).
Now we want to perform the same analysis on the dataset at large.
 
DASK: SCALING UP YOUR DATA SCIENCE
 
The 10GB size of the data set is more than the RAM on my laptop, so we can’t store it in memory.
Instead we could write a for loop:

for filename in glob(""~/data_taxi/yellow_tripdata_2019-*.csv""):
	df = pd.read_csv(filename)
	...
	df.to_parquet(...)

 
However, the multiple cores on my laptop aren’t taken advantage of through this method, nor is this a graceful solution. Here comes Dask for single machine parallelism.
Importing several aspects of Dask, we’ll spin up a local cluster and launch a Dask client:

from dask.distributed import LocalCluster, Client
cluster = LocalCluster(n_workers=4)
client = Client(cluster)
client

 
Then we import Dask DataFrame, lazily read in the data, and perform the ETL pipeline just as we did with pandas before.

import dask.dataframe as dd

df = dd.read_csv(
	""data_taxi/yellow_tripdata_2019-*.csv"",
	dtype={'RatecodeID': 'float64',
   	'VendorID': 'float64',
   	'passenger_count': 'float64',
   	'payment_type': 'float64'}
)

# Alter data types for efficiency
df = df.astype({
	""VendorID"": ""UInt8"",
	""passenger_count"": ""UInt8"",
	""RatecodeID"": ""UInt8"",
	""store_and_fwd_flag"": ""category"",
	""PULocationID"": ""UInt16"",
	""DOLocationID"": ""UInt16"",    
})

# Create new feature in dataset: tip_ratio
df[""tip_ratio""] = df.tip_amount / df.total_amount
# Write to Parquet
df.to_parquet(""data_taxi/yellow_tripdata_2019.parq"")

 
Taking about 5 minutes on my laptop, we’ll call this tolerable (I guess). But, if we wanted to do something marginally more complex (which we commonly do!), this time would quickly increase.
If I had access to a cluster on the cloud, now would be the time to utilize it!
But first, let’s reflect on what we’ve just worked out:

We used a Dask DataFrame - a large, virtual dataframe divided along the index into various Pandas DataFrames
We’re working on a local cluster, made of:

A scheduler (which organizes and send the work / tasks to workers) and,
Workers, which compute those tasks

We’ve launched a Dask client, which is “the user-facing entry point for cluster users.”

In short - the client lives wherever you are writing your Python code and the client talks to the scheduler, passing it the tasks.

 
 
COILED: SCALING OUT YOUR DATA SCIENCE
 
And now what we’ve been waiting for - it’s time to burst to the cloud. If you had access to cloud resources (like AWS) and knew how to configure Docker and Kubernetes containers, you could get a Dask cluster launched in the cloud. This would be time consuming, however.
Enter a handy alternative: Coiled, which we’ll use here. To do so, I've signed into Coiled Cloud (the Beta is currently free compute!), pip installed coiled, and authenticated. Feel free to follow along and do this yourself.

pip install coiled --upgrade
coiled login   # redirects you to authenticate with github or google

 
We then perform our necessary imports, spin up a cluster (takes roughly a minute), and instantiate our client:

import coiled
from dask.distributed import LocalCluster, Client
cluster = coiled.Cluster(n_workers=10)
client = Client(cluster)

 
Next we import our data (this time from s3), and perform our analysis:

import dask.dataframe as dd

# Read data into a Dask DataFrame
df = dd.read_csv(
	""s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv"",
	dtype={
    	'RatecodeID': 'float64',
   	'VendorID': 'float64',
   	'passenger_count': 'float64',
   	'payment_type': 'float64'
	},
	storage_options={""anon"":True}
)

# Alter data types for efficiency
df = df.astype({
	""VendorID"": ""UInt8"",
	""passenger_count"": ""UInt8"",
	""RatecodeID"": ""UInt8"",
	""store_and_fwd_flag"": ""category"",
	""PULocationID"": ""UInt16"",
	""DOLocationID"": ""UInt16"",    
})

# Create new feature in dataset: tip_ratio
df[""tip_ratio""] = df.tip_amount / df.total_amount

# Write to Parquet
df.to_parquet(""s3://hugo-coiled-tutorial/nyctaxi-2019.parq"")

 
How long did this take on Coiled Cloud? 30 seconds. This is an order of magnitude less time than it took on my laptop, even for this relatively straightforward analysis.
It’s easy to see the power of being able to do this set of analyses in a single workflow. We didn’t need to switch contexts or environments. Plus, it is straightforward to go back to using Dask from Coiled on my local workstation or pandas when we’re done. Cloud computing is great when needed, but can be a burden when it’s not. We just had an experience that was a lot less burdensome.
 
DO YOU NEED FASTER DATA SCIENCE?
 
You can get started on a Coiled cluster for free right now. Coiled also handles security, conda/docker environments, and team management, so you can get back to doing data science and focus on your job. Get started today on Coiled Cloud.
 
Bio: Hugo Bowne-Anderson Hugo Bowne-Anderson is Head of Data Science Evangelism and VP of Marketing at Coiled (@CoiledHQ, LinkedIn). He has extensive experience as a data scientist, educator, evangelist, content marketer, and data strategy consultant, in industry and basic research. He also has experience teaching data science at institutions such as Yale University and Cold Spring Harbor Laboratory, conferences such as SciPy, PyCon, and ODSC and with organizations such as Data Carpentry. He is committed to spreading data skills, access to data science tooling, and open source software, both for individuals and the enterprise.
Related:

Machine Learning in Dask
Why and How to Use Dask with Big Data
K-means Clustering with Dask: Image Filters for Cat Pictures"
https://www.kdnuggets.com/2020/09/autograd-best-machine-learning-library-not-using.html,Autograd: The Best Machine Learning Library You’re Not Using?,"If there is a Python library that is emblematic of the simplicity, flexibility, and utility of differentiable programming it has to be Autograd.","By Kevin Vu, Exxact Corp.
comments
Autograd: The Missing Machine Learning Library
 
Wait, people use libraries other than TensorFlow and PyTorch?
 
Ask a group of deep learning practitioners for their programming language of choice and you’ll undoubtedly hear a lot about Python. Ask about their go-to machine learning library, on the other hand, and you’re likely to get a picture of a two library system with a mix of TensorFlow and PyTorch. While there are plenty of people that may be familiar with both, in general commercial applications in machine learning (ML) tend to be dominated by the use of TensorFlow, while research projects in artificial intelligence/ML mostly use PyTorch. Although there’s significant convergence between the two libraries with the introduction of eager execution by default in TensorFlow 2.0 released last year, and the availability of building static executable models using Torchscript, most seem to stick to one or the other for the most part.
While the general consensus seems to be that you should pick TensorFlow for its better deployment and edge support if you want to join a company, and PyTorch for flexibility and readability if you want to work in academic research, there’s more to the world of AI/ML libraries than just PyTorch and TensorFlow. Just like there’s more to AI/ML than just deep learning. In fact, the gradients and tensor computations powering deep learning promise to have a wide-ranging impact in fields ranging from physics to biology. While we would bet that the so-called shortage of ML/AI researchers is exaggerated (and who wants to dedicate their most creative years to maximizing ad engagement and recommending more addictive newsfeeds?), we expect that the tools of differentiable programming will be increasingly valuable to a wide variety of professionals for the foreseeable future.
 
Differentiable Computing is Bigger than Deep Learning
 
Deep learning, the use of many-layered artificial neural networks very loosely based on ideas about computation in mammalian brains, is well known for its impacts on fields like computer vision and natural language processing. We’ve also seen that many of the lessons in hardware and software developed alongside deep learning in the past decade (gradient descent, function approximation, and accelerated tensor computations) have found interesting applications in the absence of neural networks.
Automatic differentiation and gradient descent over the parameters of quantum circuits offers meaningful utility for quantum computing in the era of Noisy Intermediate-Scale Quantum (NISQ) computing devices (i.e. quantum computing devices that are available now). The penultimate step in DeepMind’s impressive upset at the CASP13 protein folding prediction conference and competition used gradient descent applied directly over predicted amino acid positions, rather than a deep neural network as the Google Alphabet subsidiary is well known for. These are just a few examples of the power of differentiable programming unbound by the paradigm of artificial neurons.

Deep learning can be categorized as a subspace of the more general differentiable programming. Deep neuroevolution refers to the optimization of neural networks by selection, without explicit differentiation or gradient descent.
 
Differentiable programming is a broader programming paradigm that encompasses most of deep learning, excepting gradient-free optimization methods such as neuroevolution/evolutionary algorithms. Yann LeCun, Chief AI Scientist at Facebook, touted the possibilities of differentiable programming in a Facebook post (content mirrored in a Github gist). To hear LeCun tell it, differentiable programming is little more than a rebranding of modern deep learning, incorporating dynamic definitions of neural networks with loops and conditionals.
I would argue that the consequences of widespread adoption of differentiable programming are closer to what Andrej Karpathy describes as “Software 2.0”, although he also limits his discussion largely to neural networks. It’s reasonable to argue that software 2.0/differentiable programming is a broader paradigm in its entirety than either LeCun or Karpathy described. Differentiable programming represents a generalization beyond the constraint of neural networks as function approximators to facilitate gradient-based optimization algorithms for a wide range of systems. If there is a Python library that is emblematic of the simplicity, flexibility, and utility of differentiable programming it has to be Autograd.
 
Combining Deep Learning with Differentiable Programming
 
Differentiating with respect to arbitrary physical simulations and mathematical primitives presents opportunities for solutions where deep neural networks are inefficient or ineffective. That’s not to say you should throw away all your deep learning intuition and experience. Rather, the most impressive solutions will combine elements of deep learning with the broader capabilities of differentiable programming, such as the work of Degrave et al. 2018, whose authors combined a differentiable physics engine with a neural network controller to solve robotic control tasks.
Essentially they extended the differentiable parts of the environment beyond the neural network to include simulated robot kinematics. They could then backpropagate through the parameters of the robot environment into the neural network policy, speeding up the optimization process by about 6x to 8x in terms of sample efficiency. They chose to use Theano as their automatic differentiation library, which prevented them from differentiating through conditional statements, limiting the types of contact constraints they could implement. A differentiable physics simulator built with Autograd or even recent versions of PyTorch or Tensorflow 2.0, which support differentiating through dynamic branching, would have even more possibilities for optimizing a neural network robot controller, e.g. offering more realistic collision detection.
The universal approximation power of deep neural networks makes them an incredible tool for problems in science, control, and data science, but sometimes this flexibility is more liability than utility, as anyone who has ever struggled with over-fitting can attest. As a famous quote from John von Neumann puts it: “With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.” (an actual demonstration of this concept can be found in “Drawing an elephant with 4 complex parameters” by Mayer et al. [pdf]).
In modern machine learning practice, that means being careful not to mismatch your model to your dataset, a feat that for small datasets is all too easy to stumble into. In other words a big conv-net is likely to be overkill for many bespoke datasets with only a few hundred to a few thousand samples. In many physics problems, for example, it will be better to describe your problem mathematically and run gradient descent over the free parameters. Autograd is a Python package well suited to this approach, especially for Pythonicly-inclined mathematicians, physicists, and others who are well-practiced at describing problems at a low level with Python matrix and array computational package NumPy.
 
Autograd: Anything you can NumPy, you can differentiate
 
Here’s a simple example of what Autograd can do:

import autograd.numpy as np
from autograd import elementwise_grad as egrad

import matplotlib.pyplot as plt

x = np.linspace(-31.4,31.4, 256)

sinc = lambda x: np.sin(x) / x

plt.figure(figsize=(12,7))

plt.title(“sinc function and derivatives”, fontsize=24)

my_fn = sinc

for ii in range(9):

    plt.plot(x, my_fn(x), lw=3, label=”d{} sinc(x)/dx{}”.format(ii,ii))

    plt.legend(fontsize=18)

    plt.axis([-32, 32, -0.50, 1.2])

    plt.savefig(“./sinc_grad{}.png”.format(ii))

    my_fn = egrad(my_fn) 

 

Differentiation with Autograd. In this case Autograd was able to differentiate up to the 7th derivative before running into some numerical stability problems around x=0 (note the sharp olive green spike in the center of the figure).
 
Autograd is a powerful automatic differentiation library that makes it possible to differentiate native Python and NumPy code. Derivatives can be computed to an arbitrary order (you can take derivatives of derivatives of derivatives, and so on), and assigned to multiple arrays of parameters so long as the final output is a scalar (e.g. a loss function). The resulting code is Pythonic, a.k.a. it is readable and maintainable, and it doesn’t require learning new syntax or style. That means we don’t have to worry about memorizing complex APIs like the contents of torch.nn or tf.keras.layers, and we can concentrate on the details of our problem, e.g. translating mathematics into code. Autograd+NumPy is a mature library that is maintained but no longer developed, so there’s no real danger of future updates breaking your project.
You can implement a neural network easily with Autograd, as the mathematical primitives of dense neural layers (matrix multiplication) and convolution (you can easily use Fourier transforms for this, or use convolve2d from scipy) have relatively fast implementations in NumPy. To try out a simple MLP demonstration on scikit-learn’s diminutive digits dataset, download this Github gist, (you may also be interested in studying the official example in the autograd repository).
If you copy the gist and run it in a local virtual environment you’ll need to pip install both autograd, and scikit-learn, the latter for its digits dataset. Once all set up, running the code should yield progress reports like the following:

epoch 10, training loss 2.89e+02, train acc: 5.64e-01, val loss 3.94e+02, val accuracy 4.75e-01
total time: 4.26, epoch time 0.38

epoch 20, training loss 8.79e+01, train acc: 8.09e-01, val loss 9.96e+01, val accuracy 7.99e-01

total time: 7.73, epoch time 0.33

epoch 30, training loss 4.54e+01, train acc: 9.20e-01, val loss 4.55e+01, val accuracy 9.39e-01

total time: 11.49, epoch time 0.35

…

epoch 280, training loss 1.77e+01, train acc: 9.99e-01, val loss 1.39e+01, val accuracy 9.83e-01

total time: 110.70, epoch time 0.49

epoch 290, training loss 1.76e+01, train acc: 9.99e-01, val loss 1.39e+01, val accuracy 9.83e-01

total time: 115.41, epoch time 0.43

 
That’s a reasonably good result of 98.3% validation accuracy after just under two minutes of training. With a little tweaking of hyperparameters, you could probably push that performance to 100% accuracy or very near. Autograd handles this small dataset easily and efficiently (while Autograd and NumPy operations don’t run on the GPU, primitives like matrix multiply do take advantage of multiple cores). But if all you wanted to do was build a shallow MLP you could do so more quickly in terms of both development and computational time with a more mainstream and modern machine learning library.
There is some utility in building simple models at a low-level like this where control is prioritized or as a learning exercise, of course, but if a small dense neural network was the final goal we’d recommend you stick to PyTorch or TensorFlow for brevity and compatibility with hardware accelerators like GPUs. Instead let’s dive into something a bit more interesting: simulating an optical neural network. The following tutorial does involve a bit of physics and a fair bit of code: if that’s not your thing feel free to skip ahead to the next section where we’ll touch on some of Autograd’s limitations.
 
Simulating an Optical Neural Network with Autograd
 
Optical neural networks (ONNs) are an old idea, with the scientific journal Applied Optics running special issues on the topic in 1987 and again in 1993. The concept has recently been revisited by academics (e.g. Zuo et al. 2019) and by startups such as Optalysys,  Fathom Computing, and Lightmatter and Lightelligence, the last two of which were spun out of the same lab at MIT by co-authors on a high-profile paper published in Nature.
Light is an attractive physical phenomenon for implementing neural networks due to the similarity in the mathematics used to describe both neural networks and optical propagation. Thanks to the Fourier Transform property of lenses and the convolution property of the Fourier transform, convolutional layers can be implemented with a perturbative element placed after 2 focal lengths and one lens away from an input plane (this is known as a 4f correlator) while a matrix multiply can be implemented by placing the element 2 focal lengths and 1 lens from that. But this isn’t an optics lecture, it’s a coding tutorial, so let’s see some code!
To install the necessary dependencies, activate your desired virtual environment with your environment manager of choice and use pip to install Autograd and scikit-image if you haven’t already.
pip install autograd
pip install scikit-image
We’ll be simulating an optical system that essentially operates as a single-output generator, processing a flat input wavefront by passing it through a series of evenly-spaced phase images. To keep the tutorial relatively simple and the line count down, we will attempt to match only a single target image, shown below (you can download the image to your working directory if you want to follow along). After completing this simple tutorial, you may be inclined to experiment with building an optical classifier, autoencoder, or some other image transformation.

Now for some Python, starting with importing the packages we’ll need.

import autograd.numpy as np
from autograd import grad

import matplotlib.pyplot as plt 

import time

import skimage

import skimage.io as sio 

 
We’ll use the angular spectrum method to simulate optical propagation. This is a good method for near-field conditions where the aperture size of your lens or beam is similar to the propagation distance. The following function executes angular spectrum method propagation given a starting wavefront and its dimensions, wavelength of light, and propagation distance.

def asm_prop(wavefront, length=32.e-3, \
wavelength=550.e-9, distance=10.e-3):

    if len(wavefront.shape) == 2:

        dim_x, dim_y = wavefront.shape

    elif len(wavefront.shape) == 3:

        number_samples, dim_x, dim_y = wavefront.shape

    else:

        print(“only 2D wavefronts or array of 2D wavefronts supported”)

    assert dim_x == dim_y, “wavefront should be square”

    px = length / dim_x

    l2 = (1/wavelength)**2

    fx = np.linspace(-1/(2*px), 1/(2*px) – 1/(dim_x*px), dim_x)

    fxx, fyy = np.meshgrid(fx,fx)

    q = l2 – fxx**2 – fyy**2

    q[q<0] = 0.0

    h = np.fft.fftshift(np.exp(1.j * 2 * np.pi * distance * np.sqrt(q)))

    fd_wavefront = np.fft.fft2(np.fft.fftshift(wavefront))

    if len(wavefront.shape) == 3:

        fd_new_wavefront = h[np.newaxis,:,:] * fd_wavefront

        New_wavefront = np.fft.ifftshift(np.fft.ifft2(\

                fd_new_wavefront))[:,:dim_x,:dim_x]

    else:

        fd_new_wavefront = h * fd_wavefront

        new_wavefront = np.fft.ifftshift(np.fft.ifft2(\

                fd_new_wavefront))[:dim_x,:dim_x]

    return new_wavefront

 
Instead of restricting our ONN to either convolution or matrix multiplication operations, we’ll propagate our beam through a series of evenly spaced phase object images. Physically, this is similar to shining a coherent beam of light through a series of thin, wavy glass plates, only in this case we’ll use Autograd to backpropagate through the system to design them so that they direct light from the input wavefront to match a given target pattern at the end. After passing through the phase elements, we’ll collect the light on the equivalent of an image sensor. This gives us a nice nonlinearity in the conversion from a complex field to real-valued intensity that we could use to build a more complex optical neural network by stacking several of these together.
Each layer is defined by passing through a series of phase images separated by short distances. This is described computationally as  propagation over a short distance, followed by a thin phase plate (implemented as multiplication):

def onn_layer(wavefront, phase_objects, d=100.e-3):
    for ii in range(len(phase_objects)):

        wavefront = asm_prop(wavefront * phase_objects[ii], distance=d)

    return wavefront

 
The key to training a model in Autograd is in defining a function that returns a scalar loss. This loss function can then be wrapped in Autograd’s grad function to compute gradients. You can specify which argument contains the parameters to compute gradients for the argnum argument to grad, and remember that the loss function must return a single scalar value, not an array.

def get_loss(wavefront, y_tgt, phase_objects, d=100.e-3):
    img = np.abs(onn_layer(wavefront, phase_objects, d=d))**2

    mse_loss = np.mean( (img – y_tgt)**2 + np.abs(img-y_tgt) )

    return mse_loss

get_grad = grad(get_loss, argnum=2)

 
First, let’s read in the target image and set up the input wavefront. Feel free to use a 64 by 64 image of your choosing, or download the grayscale smiley image from earlier in the article.

# target image
tgt_img = sio.imread(“./smiley.png”)[:, :, 0]

y_tgt = 1.0 * tgt_img / np.max(tgt_img)

# set up input wavefront (a flat plane wave with an 16mm aperture)

dim = 128

side_length = 32.e-3

aperture = 8.e-3

wavelength = 550.e-9

k0 = 2*np.pi / wavelength

px = side_length / dim

x = np.linspace(-side_length/2, side_length/2-px, dim)

xx, yy = np.meshgrid(x,x)

rr = np.sqrt(xx**2 + yy**2)

wavefront = np.zeros((dim,dim)) * np.exp(1.j*k0*0.0)

wavefront[rr <= aperture] = 1.0 

 
Next, define the learning rate, propagation distance, and the model parameters.

lr = 1e-3
dist = 50.e-3

phase_objects = [np.exp(1.j * np.zeros((128,128))) \

        for aa in range(32)]

losses = []

 
If you’re familiar with training neural networks with PyTorch or similar librarie​s, the training loop should look familiar. We call the gradient function we defined earlier (which is a function transformation of the function we wrote to calculate loss), and apply the resulting gradients to the parameters of our model. I found the model to get much better results by updating parameters (phase_objects) by only the phase of the gradient, rather than the raw complex gradient itself. The real-valued phase component of the gradient is accessed by using NumPy’s np.angle, and it’s converted back into complex values by np.exp(1.j * value).

for step in range(128):
    my_grad = get_grad(wavefront, y_tgt, phase_objects, d=dist)

    for params, grads in zip(phase_objects, my_grad):

        params -= lr * np.exp( -1.j * np.angle(grads))

     loss = get_loss(wavefront, y_tgt, phase_objects,d=dist)

     losses.append(loss)

     img = np.abs(onn_layer(wavefront, phase_objects))**2

     print(“loss at step {} = {:.2e}, lr={:.3e}”.format(step, loss, lr))

     fig = plt.figure(figsize=(12,7))

     plt.imshow(img / 2.0, cmap=”jet”)

     plt.savefig(“./smiley_img{}.png”.format(step))

     plt.close(fig)

fig = plt.figure(figsize=(7,4))

plt.plot(losses, lw=3)

plt.savefig(“./smiley_losses.png”)

 
If everything worked out you should see monotonically decreasing mean squared error loss and the code will save a series of figures depicting optical network’s output as it gets closer and closer to matching the target image.

Optimization of the optical system attempting to match the target image. Each of the numbered images with a blue background is the model output at different training steps. Unsurprisingly for training with a single sample, the loss decreases smoothly over the course of training.
 
That’s it! We’ve simulated an optical system acting as a single-output generator. If you have any trouble getting the code to run, try copying the code from this Github gist all in one go to prevent introducing typos.
 
Autograd Uses and Limitations
 
Autograd is a flexible automatic differentiation package that has influenced mainstream machine learning libraries in many ways. It’s not always easy to determine the ancestry of how different ideas influence one another in a rapidly developing space like machine learning. However, the imperative, define-by-run approach features prominently in Chainer, PyTorch, and to some extent TensorFlow versions after 2.0 that feature eager execution. According to libraries.io ten other Python packages depend on Autograd, including packages for solving inverse kinematics, sensitivity analysis, and Gaussian processes. My personal favorite is the quantum machine learning package PennyLane.
Autograd may not be as powerful as PyTorch or TensorFlow, and it doesn’t have implementations of all the latest deep learning tricks, but in some ways this can be an advantage during certain stages of development. There aren’t a lot of specialized APIs to memorize and the learning curve is particularly gentle for anyone who is familiar with Python and/or NumPy. It doesn’t have any of the bells and whistles for deployment or scaling, but it is simple and efficient to use for projects where control and customization is important. It’s particularly well-suited to mathematicians and physicists who need to translate abstract ideas from math to code to build arbitrary machine learning or optimization solutions at a low-level of implementation.
The biggest con to using Autograd in our opinion is a lack of support for hardware acceleration. Perhaps there’s no better way to describe this drawback than the 4-year-long discussion on this Github issue, which discusses various ways of introducing GPU support. If you worked your way through the optical neural network tutorial in this post you’ll have already noticed that running an experiment with even a modestly sized model could require a prohibitively high amount of computational time. Computation speed with Autograd is enough of a drawback that we don’t actually recommend using it for projects much larger than the MLP or ONN generator demonstrations described above.
Instead, consider JAX, an Apache 2.0 licensed library developed by Google Brain researchers, including the Autograd developers. JAX combines hardware acceleration and just-in-time compilation for substantial speedups over native NumPy code, and in addition, JAX offers a set of function transformations for automatically parallelizing code. JAX can be slightly more complicated than a direct NumPy replacement with Autograd, but its powerful features can more than make up for that. We’ll compare JAX to Autograd as well as the popular PyTorch and TensorFlow in a future article.
 
Original. Reposted with permission.
Related:

PyTorch for Deep Learning: The Free eBook
Batch Normalization in Deep Neural Networks
Deep Learning for Signal Processing: What You Need to Know"
https://www.kdnuggets.com/2021/06/nij-recidivism-forecasting-challenge.html,"Submit Your Algorithm for a Chance to Win Prizes Totaling $700,000+",Can your algorithm make fair and accurate #recidivism forecasts? Take part in US National Institute of Justice “Recidivism Forecasting Challenge” with prize money totaling over $700K.,"Sponsored Post.

The U.S. National Institute of Justice’s (NIJ) “Recidivism Forecasting Challenge” (the Challenge) aims to increase public safety and improve the fair administration of justice across the United States. The Challenge offers an opportunity for contestants to win prize money totaling over $700,000 for their development of a recidivism forecasting model using data provided by NIJ. The winning Challenge forecasts will be used to help improve recidivism rates, the likelihood of a past criminal offender to reoffend, and inform policies and practices.
In accordance with priorities set by the U.S. Department of Justice, NIJ supports the research, development, and evaluation of strategies to reduce violent crime, and to protect police and other public safety personnel by reducing recidivism. Results from the Challenge will provide critical information to community corrections departments that may help facilitate more successful re-integration into society for previously incarcerated persons and persons on parole. 
As the research, development, and evaluation agency of the U.S. Department of Justice, NIJ invests in scientific research across diverse disciplines to serve the needs of the criminal justice community. NIJ seeks to use and distribute rigorous evidence to inform practice and policy; often relying on data analytic methods to do so. The Challenge aims to improve the ability to forecast recidivism using person- and place-based variables with the goal of improving outcomes for those serving a community supervision sentence. In addition to the Challenge data provided, NIJ encourages contestants to consider a wide range of potential supplemental data sources that are available to community corrections agencies to enhance risk determinations, including the incorporation of dynamic place-based factors along with the common static and dynamic risk factors. 
The Challenge will have three categories of contestants: students; individuals/small teams/businesses; and large businesses. NIJ will evaluate all entries on how accurately they forecast the outcome of recidivism. Recidivism is defined in this Challenge as an arrest for a new crime. To receive prize money, (114 total prizes available, up to 15 per contestant/team) winning applicants must provide a comprehensive document detailing the lessons learned about what variables did and did not matter to their final forecasting model and, when applicable, what type of models outperformed other models. Contestants are encouraged to provide additional intellectual property regarding specific techniques, weighting, or other sensitive decisions.
The Challenge uses data from the State of Georgia about persons released from prison to parole supervision for the period January 1, 2013, through December 31, 2015. Contestants will submit forecasts (percent likelihoods) of whether individuals in the dataset recidivated within one year, two years, or three years after release. 
The final submission deadline is June 30, 2021, 11:59:59 pm ET.
NIJ expects that new and more nuanced information will be gained from the Challenge and help address high recidivism among persons under community supervision. Findings could directly impact the types of factors considered when evaluating risk of recidivism and highlight the need to support people in specific areas related to reincarceration. Additionally, the Challenge could provide guidance on gender specific considerations and strategies to account for racial bias during risk assessment.  
To receive notices on NIJ’s Recidivism Challenge and data science-related resources, subscribe to email updates."
https://www.kdnuggets.com/2021/04/build-impressive-data-science-resume.html,How to Build an Impressive Data Science Resume,Every one of us needs a resume to showcase our skills and experience but how much effort are we putting into it to make it impactful. It is undeniable that resumes play a key role in our job application process. This article will explore some simple strategies to significantly improve the presentation as well as the content of data science resumes.,"comments
By Sharan Kumar Ravindran, Data Science Professional and Author


Photo by Glenn Carstens-Peters on Unsplash

 
Every one of us needs a resume to showcase our skills and experience but how much effort are we putting into it to make it impactful. It is undeniable that resumes play a key role in our job application process. This article will explore some simple strategies to significantly improve the presentation as well as the content of data science resumes.
 
First, Why is it important to focus on the resume?
 
Getting a data science job is becoming very competitive, though the number of opportunities is historically high the number of people applying for these jobs is extremely high as well.
For example, below is a screenshot of a job posting from LinkedIn, this job posting has a total of 1200+ views and if we consider approximately one-tenth apply for the job then it is a total of 120+ applications and this is just one way to apply for the job there would be people applying for this job from other sources, through references and directly as well and hence the total number of applications would be approximately 200+. The same logic would apply to any data science job position hence resume plays a critical role in getting shortlisted.


Screenshot from LinkedIn

 
In this article, I am going to guide you on building a high-impact resume that can help you in getting shortlisted for the job application. The topics covered in this article are,

Basic rules in resume preparation
Customizing your resume and cover letter
Google’s X-Y-Z formula to make impactful statements
Tools to help build a stunning resume

If you have a preference for video format, check out here.
 
Basic rules in resume preparation
 
Resume formatting
 
Most job applications accept resumes in both pdf and word format. But I would suggest you stick to the pdf version as this ensures the formatting is preserved, that is the recruiter sees the resume the same way you see it.
 
Profile summary
 
Profile summary is key to a resume, consider it as an elevator pitch. It should be persuasive and should cover information like who you are, what are your skills and strengths. This part of the resume will be the main driver for the first impression also in influencing the recruiter’s decision hence spend enough time to ensure it includes the key details about you.
My people include career objectives at the beginning of the resume. I personally advocate removing the career objective from the resume and instead use that space for a better profile summary. Because most recruitment happens based on your achievements, strengths, and skill and not based on your aspirations. So make an intelligent decision and efficiently use the real-estate of your resume especially the start.
 
Use bullet points
 
Make sure the details you include in your resume are in bullet points, be it the profile summary or professional/project experience. It is very difficult to focus on a long paragraph and hence keeping it simple and in bullet point ensures better readability as demonstrated in the below screenshots.


Profile summary in one long paragraph

 


Profile Summary in bullet points

 
Try to restrict each bullet point to 2–3 lines and make key phrases bold as it helps while scanning through quickly.
 
Consistency in format
 
The contents of the resume should be in a consistent format, the titles, subtitles, bullet points, and other texts in the resume should all be in a consistent format. Below are few things that will ensure consistency,

Pick one font and use it across the resume
Titles used in your resume like for highlighting Experience and Education should be in a consistent format. You can choose to use a bigger font but let it be consistent across the resume
If your resume is more than one page then ensure the margin, alignments and spaces are consistent across all the pages
You can choose to capitalize the first words in the titles but then let it be consistent across the resume

 
Avoid typos
 
Always check for typographical and grammatical errors as they might turn off the recruiter. While there is a good chance for the typographical and grammatical errors to go unnoticed but when caught they send out wrongs signals like,

You are not detailed enough to catch those mistakes
As a data scientist communication is a key aspect and having spelling or grammatical mistakes is definitely not good
Firms are increasingly using automated tools to filter resumes, these tools most likely reject resume with typographical errors

 
Include contact details
 
Your contact details are important for the recruiter to contact you hence ensure that you double-check your details. Many people start editing their resumes based on their colleague’s or friend’s resumes in those cases ensure the hyperlinks are also edited when you edit the text. Like when you edit the email id ensure the email in the hyperlink is edited as well.
 
Include links to your profile and portfolios
 
Ensure your resume has links to your LinkedIn profile, git repository, and other websites or profiles like Kaggle you would like to highlight to the recruiter
 
Customizing your resume and cover letter
 
When you are competing with a lot of people on a job application, simple things like customization can be a differentiator and could help you in getting the immediate attention of the recruiter. When I say customizing your resume to the job posting it doesn’t mean completely re-writing your resume to every job you apply but just making a few minor tweaks to ensure your profile highlights the requirement and the expectation the job demands.
Customizing your resume helps you in

Ensuring that your resume is tailored to the job posting
Ensure your resume passes the automated keyword-based filtering
Sends out a positive signal to the recruiter since you have done your groundwork

There are few components in your resume that you can customize, they are

The target job title in your resume, as well as the cover letter, should match the job posting
Ensure skills highlighted in your resume includes some of the skills requested in the job description
Make simple modifications to your profile summary to ensure the expectation mentioned in the job posting are addressed
If you are applying for a job position away from your current city or you are applying for a job that demands traveling then explicitly mention in the resume or the cover letter your willingness to relocate or travel

 
Google X-Y-Z formula to make impactful statements
 
This is an amazing formula that helps to convert your accomplishments into a high-impact statement. It was first introduced by Laszlo Bock in his article here. This is a very effective technique that can be used to write an impactful resume. This formula means,


Accomplished “X” as measured by “Y” by doing “Z”


I will use some simple examples to exactly explain how this formula can be applied in your data science resume.
 
Example 1:
“Built a recommendation system”
this is a simple statement, not attractive at all because it doesn’t exactly mention the impact of the use case. We can try to improve it by including details of its impact by using below the statement,
“Built a recommendation system that increased revenue by 10%”
Now, this is much better than the previous statement but this can be further improved by using the Google X-Y-Z formula as below (X, Y, and Z of the formula are highlighted below)
“Built a recommendation system (X) that helped to increase the revenue by 10% and improved the customer engagement (Y)on the platform by using Collaborative Filtering Algorithm (Z)”
 
Example 2:
“Participated in a kaggle competition”
this is again a simple statement which just says you participated in a kaggle competition but doesn’t talk about your performance hence this can be improved by including some details like below,
“Finished at 20th Place in a Kaggle competition”
This is now better but we can make it more impactful by using the google X-Y-Z formula like,
“Participated in a Kaggle competition (X) and finished at 20th position out of 1250 teams (Y) by working with 3 colleagues building an ensemble predictive model (Z)”
Now, use this formula to convert your accomplishments into more powerful statements.
 
Tools to help build a stunning resume
 
There are great tools out there to help you in building a stunning resume. Below are two of my favorites,
 
Resume.io
 

This is a paid platform to build your resume and cover letter, though it doesn’t require any payment to use the platform and build a resume payment will be required for you to download the resume
They have a lot of resume templates catering to several job categories to help you with getting started
While this platform supports several templates and has many options to make changes but some parts of the template remain rigid. But I personally feel its better this way as it makes you focus only on things that need attention and it also ensures consistency in your resume

 
Flowcv.io
 

This is a free platform to build your resume and cover letter, if you wish to have more than one version of your resume then it is required to make a one-off payment
This platform offers several configurations to make any changes to define how things should look. I feel having more options means more decision to make hence time-consuming and might lead to some inconsistencies

These tools are very helpful in creating stunning resumes.
 
To stay connected
 

If you like this article and interested in similar ones, follow me on Medium
I teach and talk about various data science topics on my YouTube Channel. Subscribe to my channel here.
Sign up to my email list here for more data science tips and to stay connected with my work

 
Bio: Sharan Kumar Ravindran (LinkedIn, GitHub) is a data science professional with over 10 years of experience and has authored two data science-related books.
Original. Reposted with permission.
Related:

Data careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space
Using NLP to improve your Resume
How Data Professionals Can Add More Variation to Their Resumes"
https://www.kdnuggets.com/2021/03/top-10-python-libraries-2021.html,Top 10 Python Libraries Data Scientists should know in 2021,"So many Python libraries exist that offer powerful and efficient foundations for supporting your data science work and machine learning model development. While the list may seem overwhelming, there are certain libraries you should focus your time on, as they are some of the most commonly used today.","By Terence Shin, Data Scientist | MSc Analytics & MBA student.
comments

Photo by David Clode on Unsplash.
Learning data science can be overwhelming. There are hundreds of tools and resources out there, and it’s not always obvious what tools you should be focusing on or what you should learn.
The short answer is that you should learn what you enjoy because data science offers a wide range of skills and tools. That being said, I wanted to share with you what I believe are the top 10 Python libraries that are most commonly used in data science.
With that said, here are the Top 10 Python Libraries for Data Science.
 
1. Pandas
 
You’ve heard the saying. 70 to 80% of a data scientist’s job is understanding and cleaning the data, aka data exploration and data munging.
Pandas is primarily used for data analysis, and it is one of the most commonly used Python libraries. It provides you with some of the most useful set of tools to explore, clean, and analyze your data. With Pandas, you can load, prepare, manipulate, and analyze all kinds of structured data. Machine learning libraries also revolve around Pandas DataFrames as an input.
Where to Learn Pandas

Kaggle Tutorial
75 Pandas Exercises with Solutions
Pandas Practice Problems

 
2. NumPy
 
NumPy is mainly used for its support for N-dimensional arrays. These multi-dimensional arrays are 50 times more robust compared to Python lists, making NumPy a favorite for data scientists.
NumPy is also used by other libraries such as TensorFlow for their internal computation on tensors. NumPy also provides fast precompiled functions for numerical routines, which can be hard to manually solve. To achieve better efficiency, NumPy uses array-oriented computations, so working with multiple classes becomes easy.
Where to Learn NumPy

NumPy.org
TutorialsPoint

 
3. Scikit-learn
 
Scikit-learn is arguably the most important library in Python for machine learning. After cleaning and manipulating your data with Pandas or NumPy, scikit-learn is used to build machine learning models as it has tons of tools used for predictive modelling and analysis.
There are many reasons to use scikit-learn. To name a few, you can use scikit-learn to build several types of machine learning models, supervised and unsupervised, cross-validate the accuracy of models, and conduct feature importance.
Where to Learn Scikit-learn

Scikit-learn tutorial
An Introduction to Machine Learning (Udacity)

 
4. Gradio
 

Image taken by Gradio (with permission)
Gradio lets you build and deploy web apps for your machine learning models in as little as three lines of code. It serves the same purpose as Streamlit or Flask, but I found it much faster and easier to get a model deployed.
Gradio is useful for the following reasons:

It allows for further model validation. Specifically, it allows you to interactively test different inputs into the model.
It’s a good way to conduct demos.
It’s easy to implement and distribute because the web app is accessible by anyone through a public link.

Where to Learn Gradio

Getting Started page

 
5. TensorFlow
 
TensorFlow is one of the most popular libraries of Python for implementing neural networks. It uses multi-dimensional arrays, also known as tensors, which allows it to perform several operations on a particular input.
Because it is highly parallel in nature, it can train multiple neural networks and GPUs for highly efficient and scalable models. This feature of TensorFlow is also called pipelining.
Where to Learn TensorFlow

TensorFlow’s website
Kaggle’s Intro to Deep Learning
Google’s Intro to TensorFlow

 
6. Keras
 
Keras is mainly used for creating deep learning models, specifically neural networks. It’s built on top of TensorFlow and Theano and allows you to build neural networks very simply. Since Keras generates a computational graph using back-end infrastructure, it is relatively slow compared to other libraries.
Where to Learn Keras

Keras’s Website
Google’s Intro to Keras

 
7. SciPy
 
As the name suggests, SciPy is mainly used for its scientific functions and mathematical functions derived from NumPy. Some useful functions which this library provides are stats functions, optimization functions, and signal processing functions. To solve differential equations and provide optimization, it includes functions for computing integrals numerically. Some of the applications which make SciPy important are:

Multi-dimensional image processing
Ability to solve Fourier transforms, and differential equations
Due to its optimized algorithms, it can do linear algebra computations very robustly and efficiently

Where to Learn SciPy

SciPy Website
Guru99 Tutorial

 
8. Statsmodels
 
Statsmodels is a great library for doing hardcore statistics. This multifunctional library is a blend of different Python libraries, taking its graphical features and functions from Matplotlib, for data handling, it uses Pandas, for handling R-like formulas, it uses Pasty, and is built on NumPy and SciPy.
Specifically, it’s useful for creating statistical models, like OLS, and also for performing statistical tests.
Where to Learn Statsmodels

Statsmodels: Getting Started

 
9. Plotly
 
Plotly is definitely a must-know tool for building visualizations since it is extremely powerful, easy to use, and has a big benefit of being able to interact with the visualizations.
Along with Plotly is Dash, which is a tool that allows you to build dynamic dashboards using Plotly visualizations. Dash is a web-based python interface that removes the need for JavaScript in these types of analytical web applications and allows you to run these plots online and offline.
Where to Learn Plotly

Beginner Visualizations in Plotly
Advanced Visualizations in Plotly

 
10. Seaborn
 
Built on the top of Matplotlib, Seaborn is an effective library for creating different visualizations.
One of the most important features of Seaborn is the creation of amplified data visuals. Some of the correlations that are not obvious initially can be displayed in a visual context, allowing Data Scientists to understand the models more properly.
Due to its customizable themes and high-level interfaces, it provides well-designed and extraordinary data visualizations, hence making the plots very attractive, which can, later on, be shown to stakeholders.
Where to Learn Seaborn

Elite Data Science
Seaborn User Guide

 
Related:

More Data Science Cheatsheets
Are You Still Using Pandas to Process Big Data in 2021? Here are two better options
Data Science Learning Roadmap for 2021"
https://www.kdnuggets.com/2021/09/springboard-difference-data-engineers-data-scientists.html,What Is The Real Difference Between Data Engineers and Data Scientists?,"To launch your data career, you’ll need both theoretical knowledge and applied skills. Bootcamp programs like Springboard’s Data Science Career Track and Data Engineering Career Track can help make you job-ready through hands-on, project-based learning and one-on-one mentorship. Wondering which data career path is right for you? Read on to find out.","Sponsored Post.

 
Although data engineers and data scientists have overlapping skill sets, they fulfill different roles within the fields of big data and AI system development. Data scientists develop analytical models, while data engineers deploy those models in production. As such, data scientists focus primarily on analytics, and data engineers focus more heavily on programming.
To launch your data career, you’ll need both theoretical knowledge and applied skills. Bootcamp programs like Springboard’s Data Science Career Track and Data Engineering Career Track can help make you job-ready through hands-on, project-based learning and one-on-one mentorship. Wondering which data career path is right for you? Read on to find out.  
 
What Do Data Engineers Do?
 
 
Data engineers create and maintain key data infrastructures like databases, data warehouses, and data pipelines. Data engineers also prepare data for production by converting raw, unstructured data into a structured format that can be analyzed and interpreted. 
The work of data engineers is foundational to big data analytics. Data engineers construct data pipelines that capture data from users, SaaS platforms, and other data producers. Data pipelines process this data in real-time and store it in warehouses for analysis. This process is referred to as ETL (extract, transform, load).
The responsibilities of a data engineer vary depending on organizational size. A data engineer at a small company might build data ecosystems and manage the entirety of the data flow, similar to a full-stack data scientist. At a mid-sized company, data engineers craft custom tools to support big data analytics. At large companies that handle large, complex volumes of data, data engineers often focus on optimizing ETL processes.   
 
What Do Data Scientists Do?
 
 
Data scientists analyze and interpret data to solve business problems. Initially, data scientists explore data and conduct market research in order to formulate business questions around a specific trend or pain point. Data scientists must then frame business questions as data analytics problems. 
To identify critical patterns within a data set, data scientists use advanced analytical techniques powered by machine learning and statistics. Data scientists build models to establish relationships between data objects. Predictive models forecast future events based on historical data, while prescriptive models recommend actionable changes in business strategy based on current and historical data.
Data scientists must also interpret the results of their analyses to design data-driven business solutions. When data scientists present their findings to stakeholders, they must build a cohesive narrative that communicates the meaning of their results and how those results can inform business strategy. 
 
Key Data Engineering Skills 
 
 
Data engineers need a robust software engineering foundation and must use programming knowledge to deploy models, build data pipelines, and orchestrate data warehousing solutions. Python, Java, and Scala are three of the top programming languages most commonly used by data engineers. 
Data engineers must also be able to manipulate database management systems, which facilitate information storage and retrieval. Data engineers use SQL to build and manage relational database systems. 
Data engineers also need to understand the basics of distributed systems and demonstrate fluency in Hadoop, which is a framework that enables distributed processing of vast data sets. A strong understanding of data APIs is also a must. Software applications use APIs to access and retrieve data, and data engineers build APIs in databases so that data scientists can query the data. 
Finally, data engineers use cursory machine learning knowledge to understand the needs of data scientists, deploy models in production more efficiently, and build improved data pipelines. 
 
Key Data Science Skills
 
 
Data scientists have strong programming skills and a solid understanding of statistics. Python is known as the lingua franca of data science, and data scientists use this popular programming language to write code and use powerful Python-based tools. Data scientists also use R to manipulate data, implement machine learning algorithms, and conduct statistical analysis. Data scientists also use SQL to read, retrieve, and add data to databases.
Machine learning is also a key data science skill. Data scientists use algorithms to clean, categorize, and analyze large data sets. Machine learning combines computer science and statistics, and machine learning models help data scientists make data-driven predictions and recommendations. 
Data scientists must also be well-versed in data visualization, which uses charts, graphics, maps and more to represent data to stakeholders. Data scientists must also be able to create coherent narratives that show how their findings impact an organization’s business goals. 
 
Top Tools for Data Engineers
 
 
Data engineers need to be proficient with distributed processing technologies and tools used to work with data at scale. Top tools for data engineers include: 

Apache Hadoop and Apache Spark. Hadoop is a major big data tool that enables batch processing of vast datasets across servers. Spark is a data processing engine that enables stream processing. 
Amazon Web Services/Redshift. Data warehousing applications like AWS are built to show a long-range view of data over time. 
Microsoft Azure. Data engineers use this cloud technology to build data analytics systems at scale. 
C++. Data engineers use this programming language to rapidly compute large data sets quickly in the absence of predefined algorithms. 

 
Top Tools for Data Scientists 
 
 
Data scientists need a strong command of analytical and data visualization tools, including: 

Tableau. This data viz software allows data scientists to create interactive visualizations.  Tableau can manage large amounts of data and interface with multiple data sources. 
Jupyter. This interactive computational notebook can be used for writing live code, cleaning data, data, viz, and more.
Apache Hadoop. Hadoop can store large data sets and stream the data to applications like MapReduce, which handle data analytics. 
Scikit-learn. This predominantly Python-based machine learning library offers features like data classification, regression, clustering, preprocessing, and more. 

 
Ready to launch your data career? 
 
 
If you want to pivot into a career in data science or data engineering, Springboard’s Data Science Career Track or Data Engineering Career Track will give you the skills you need to get hired. Apply today"
https://www.kdnuggets.com/2021/08/demystifying-ai-prejudices.html,Demystifying AI: The prejudices of Artificial Intelligence (and human beings),"AI models are necessarily trained on historical data from the real-world--data that is generated from the daily goings on of society. If social-based biases are inherent in the training data, then will the AI predictions highlight these same biases? If so, what should we do (or not do) about making AI fair?","comments
By Manjesh Gupta, Associate Manager - AI/Machine Learning at Virtusa.

Our present human society is a product of millions of years of biological evolution and thousands of years of social evolution. Everything has a history. We make beliefs about people or things based on our accumulated knowledge. In such a scenario, it is quite natural that some of our beliefs are prejudiced because, at times, we do not have enough information. Gordon Allport defines “prejudice” as a “feeling, favorable or unfavorable, toward a person or thing, prior to, or not based on, actual experience.” It is often said that prejudices exist and will continue to exist. The real question is whether we as individuals or a society are willing to change our prejudiced beliefs when presented with counter-evidence. In 1953, Albert Einstein wrote in an essay, “Few people are capable of expressing with equanimity opinions which differ from the prejudices of their social environment. Most people are even incapable of forming such opinions.”
In a social setting, these prejudiced beliefs manifest themselves as attitude or behavior, favorable or unfavorable, towards an individual or a group, based on their sex, gender, social class, race, ethnicity, language, political affiliation, sexuality, religion, and other personal characteristics. In such cases, generally, the group identity of an individual or sub-group takes precedence over the individual identity. We know that we behave in a prejudiced manner (which may not even be necessarily wrong at times).
Do AI algorithms reproduce this human behavior?
Let us examine a few cases.
If you ask some of the natural language processing algorithms – “Man is to Computer Programmer as Woman is to ___________?” It may answer “Homemaker.” The word-embeddings used in such algorithms are known to reflect gender (and other biases) for quite some time now. This paper examined the “word2vec” embeddings to show the presence of gender stereotypes. The paper also suggests a method to neutralize the bias.
The Gender Shades project showed that Facial recognition systems from IBM, Microsoft, and Face++ are biased against women and “darker” subjects in terms of accuracy of recognition. These algorithms were, on average, around 15% less accurate for female and “darker” subjects. Recently, an algorithm designed to generate “high-definition” faces from pixelated images generated an output of a “white” person when a pixelated face of Barrack Obama was used as input.
In an extremely alarming use case of artificial intelligence, an algorithm that suggested the risk of a person to commit a crime again was found to be biased against “black” people. This algorithm was being used by judicial systems in the United States. Recently, researchers from Harrisburg University claim to have built a deep learning algorithm that can predict if a person is a criminal based solely on a picture of their face. This paper was supposed to be published by Springer Nature. Researchers and experts from various fields joined a petition to stop its publication, saying that predicting ""criminality"" is not an exact science. Later, Springer clarified that they had already rejected the paper before the petition started.
So, what causes AI algorithms to exhibit this behavior?
In a way, the results of these algorithms hold a mirror to human society. They reflect and perhaps even amplify the issues already present. We know that these algorithms need data to learn. Their predictions are only as good as the data they are trained on and the goal they are set to achieve.
The data needed to train these algorithms is huge (think millions and above). Suppose we are trying to develop an algorithm to identify cats and dogs from pictures. Not only do we need thousands of pictures of cats and dogs, but they should be labeled (say the cat is class 0 and dog is class 1) so that the algorithm can understand. We can download these images off the internet (the ethics of which is questionable), but still, they need to be labeled manually. Now, consider the complexity and effort required to correctly label a million images in one thousand classes. Often this labeling task is done by “cheap labor” who may or may not have the motivation to do it correctly, or they simply make mistakes.
Another problem in the data set is that of class imbalance. Let’s say we used ten thousand images of dogs and only one thousand images of cats to train our above algorithm. This may not be the actual proportion of the cat and dog populations in the real world, and so the class cat is underrepresented. These problems are generally referred to as “dataset bias.”
The second issue in AI algorithms is feature selection. They classify examples based on different features. Let us say we are making an algorithm to predict credit risk for incoming loan customers for a bank. We try to make an unbiased algorithm and not use features like race, gender, caste, ethnicity, etc., in the algorithm. However, we include ZIP code as a feature, which seems rather harmless. Many studies have shown that ZIP code can be used as a proxy for socioeconomic status. If we try to look carefully around us, we may find that certain ZIP codes have the majority of people of a particular class, color, ethnicity, caste, etc. In this case, even though we tried to develop an unbiased algorithm, we unintentionally introduced social bias by using ZIP code as a feature. At times, it may be difficult to identify which feature introduces unintentional social bias. Also, based on selected features, there may be a trade-off between algorithm accuracy and social bias introduced into the algorithm.
Third, human beings live in a time dimension. They do not necessarily stay the same their entire lives. A person born poor can become rich while another rich person may squander all their wealth. A convicted criminal may change to become a better person while another “role model” person may commit horrific crimes in the future. AI algorithms (perhaps, just like human beings) try to make sense of the future based upon the presumption that what happened in the past will somehow repeat. This may be true for some (or maybe a lot of) phenomena and people. However, it is certainly not true for all phenomena or all people. Sure it takes time for people and culture to change (for better or worse), but they never remain static.
These AI algorithms are not aware of the context, their predictions, or the consequences of their prediction. At present, they are mostly dependent on human beings for training data and what exactly to derive from that data. Here is a brief TED talk which explains how AI algorithms learn.
This social bias introduced in AI algorithms leads to loss of social and economic opportunity and dignity in many cases. Especially with the widespread use of such algorithms, it becomes critical to examine the problems with them. The debate on this topic often leads to much deeper questions. How do we define and create a “fair” and “balanced” dataset? How do we ensure that all people developing AI algorithms use fair and balanced datasets? When a human being makes an error in judgment or action, they can be held accountable for it (or maybe not, or maybe it depends on how rich they are). Can we make an AI algorithm accountable for its errors, and how? What problems should not be solved using AI and should better be left to human judgment for now? These questions, among others, are tackled in the field of study called Ethics of Artificial Intelligence.
Personally, I believe that it is impossible to completely eliminate our prejudices, but as human beings, we should reflect deeply on how we can minimize them in ourselves and our creations.
[* The words ""bias,"" ""social bias,"" and ""prejudice"" in this article are used in a social sense and should not be confused with the mathematical ""bias"" of a machine learning algorithm.]
Original. Reposted with permission.
 
Bio: Manjesh Gupta has nine years of experience across Artificial Intelligence, Machine Learning, Data Analytics, Research, IT Project Management, Stakeholder Management, Change Management, and Capacity Building, with technical ML/DL skills in NLP, Risk Analytics, Computer Vision, and Time Series Forecasting.
Related:

Towards a Responsible and Ethical AI
How to Create Unbiased Machine Learning Models
Ethics, Fairness, and Bias in AI"
https://www.kdnuggets.com/2021/05/vaex-pandas-1000x-faster.html,Vaex: Pandas but 1000x faster,"If you are working with big data, especially on your local machine, then learning the basics of Vaex, a Python library that enables the fast processing of large datasets, will provide you with a productive alternative to Pandas.","By Ahmad Anis, Machine learning and Data Science Student.
comments

Photo by Harley-Davidson on Unsplash.
While working with pandas, if you have encountered a large dataset, then you might have thought of an alternative, especially when your machine is not strong. Pandas is really good for small/average-sized datasets, but as data gets bigger, it does not perform as well as it performs on simple and smaller datasets.
Here you can see the comparison of Pandas with another library modin on reading the dataset from a CSV file.

Similarly, a very common problem pandas users often go through is the dead jupyter kernel due to out of memory. The computations are expensive, and the CPU is not strong enough to handle those.
In this article, you are going to learn about Vaex, a Python library that is similar to Pandas, how to install it, and some of its important functions that can help you in performing different tasks.
 
Introduction to Vaex
 
Vaex is a python library that is an out-of-core dataframe, which can handle up to 1 billion rows per second. 1 billion rows. Yes, you read it right, that too, in a second. It uses memory mapping, a zero-copy policy which means that it will not touch or make a copy of the dataset unless explicitly asked to. This makes it possible to work with datasets that are equal to the size of your hard drive. Vaex also uses lazy computations for the best performance and no memory wastage.
Installation
You can install vaex using pip:

pip install vaex



 
Or, if you use anaconda, you can use conda to install.

conda install -c conda-forge vaex



 
Getting Started
We will import both vaex and pandas to compare the performance.

import vaex
import pandas as pd



 
We will create an artificial dataset and read it both using Pandas and Vaex to see the difference in the performance.

import numpy as np
n_rows = 1000000
n_cols = 20
df = pd.DataFrame(np.random.randint(100000000, 1000000000, size=(n_rows, n_cols)), columns=[‘c%d’ % i for i in range(n_cols)])



 
This dataset contains 1000,000 rows and 20 columns.
Let’s save it in a csv file.

df.to_csv(‘dataset.csv’, index=False)



 
Vaex can work well with both CSV files and hdf5 files. To read a CSV file, we can use vaex.from_csv function. This function can read a CSV file and optionally convert it to HDF5 format.
If you are working with the jupyter notebook, you can use %%time magic command to check the execution time.

%%time
vaex_df = vaex.from_csv(‘dataset.csv’,convert=True, chunk_size=5_000)



 
You can check the execution time,  which is 15.8ms. If the CSV file is large, you can use chunk_size argument to read the file in chunks.

You can see that it is taking about 15.8 ms total to read the file, which is around 200 MBs.
This has created an hdf5 file too. Let us read that using vaex.

%%time

vaex_df = vaex.open(‘dataset.csv.hdf5’)




You can see that vaex has read the simple hdf5 file in 20.1 ms total with 19.4 ms CPU time.
Let us read the same file using pandas.

%%time

df_test = pd.read_csv('dataset.csv')




And you can see that it took pandas 2.19 s total to read the same file, which shows a big performance difference between vaex and pandas.
If we add the same chunk size to both vaex and pandas, we can see that vaex is still very fast as compared to pandas.

Here vaex read the data in 28.6 µs which is equal to 0.02 ms, whereas pandas read the same file in 4.41 ms total, which is a huge performance gap.
 
Important Vaex Functions
 
Functions related to Opening/Reading the dataset
(1) Open
Using the Open function, you can open a dataframe by giving its path. You can convert your dataframe to other formats such as hdf5 format using the Open function. This function can take a path or a list of paths to read the dataset. Some examples of it are given.

>>> df = vaex.open('table1.hdf5')
>>> df = vaex.open('datafile#*.csv', convert='bigdata.hdf5')



 
(2) Open Many Files
Often you have data distributed in many files, and you have to open them separately. Vaex provides open_many function, which can open the list of filenames, and return a DataFrame with all DataFrames concatenated at a fast speed.

import vaex

df_concat = vaex.open_many(['file1.csv', 'file2.csv'])



 
(3) Concat
Concat is a function in vaex that can concatenate the given list of data frames. For example:

new_df = vaex.concat([df1, df2, df3])



 
This will concatenate the list of data frames into a new dataframe.
(4) Creating a Dataframe using Pandas DataFrame
This function is used to create an in-memory dataframe using a pandas dataframe.

import vaex

import pandas as pd

df = pd.read_csv(‘test.csv’) #pandas dataframe

df_vaex = vaex.from_pandas(df) #in-memory vaex df



 
(5) DataFrame from Array
This function takes in a list or an array and converts it into a vaex dataframe.

text = [‘Testing’, ‘lowercase’, ‘UPPERCASE’, ‘test_example’]

df = vaex.from_arrays(text=text)




Important Functions related to Statistics and Aggregation
(1) Finding Correlation
Oftentimes, you need to find the correlation, vaex provides an easy way to calculate the correlation.

df.correlation(x, y=None, binby=[], limits=None, shape=128, sort=False, sort_key=<ufunc 'absolute'>, selection=False, delay=False, progress=None)



 
Here vaex calculates the correlation between x and y, possibly on a grid defined by binby. Here x and y are 2 expressions.
Let's say you have a DataFrame, with 5 columns as follows.

You want to calculate the correlation based on the following expressions.
x = ""c0**2+c1**2+c2**2+c3**2"" 
and
y =""-log(c4+c5)"",
then it will calculate the correlation coefficient using the following formula cov[x,y]/(std[x]*std[y]) between x and y.

vaex_df.correlation(x=""c0**2+c1**2+c2**2+c3**2"", y=""-log(c4+c5)"")



 
(2) Count
This function counts the number of Not Null values, or all values if the expression provided is * or None.

vaex_df.count('*') #counts all values





vaex_df.count() #counts not null values only




(3) Covariance
Creating a covariance matrix is another important trick to get statistical insights from a dataset. Using vaex, you can quickly create a covariance matrix by using covar function on a DataFrame.
You need to provide two expressions, x and y, that will calculate the covariance cov[x,y] possibly on-grid defined by binby. This function will return a scalar if no binby is provided.

vaex_df.covar(""c0**2+c1**2+c2**2"", '-log(c4)', binby='c4')




Covariance Matrix.
(4) Describe
Just like pandas, the describe function gives a statistical description of the dataset, including the count, mean, standard deviation, minimum and maximum value.

vaex_df.describe()




Similarly, there are functions related to min, max, std, median, mode, min-max, and other important statistical functions, which you can explore in the documentation here.
(5) Groupby
Vaex can also do fast group-by aggregations. The output of it is a vaex DataFrame. Let's see an example.

gender = [‘male’,’male’,’female’,’male’,’female’]

weight = [95, 87, 74, 79, 65]

vaex_df = vaex.from_arrays(gender=gender, weight=weight)




vaex_df
Now you can perform GroupBy operations on it.

vaex_df.groupby(by=’gender’).agg({‘weight’:‘mean’})



 
This will group by on gender column and calculate the aggregation function mean on the weight column.

You can pass in multiple aggregate functions in a list for a single column. It will automatically give appropriate names to the resulting columns.

vaex_df.groupby(by=’gender’).agg({‘weight’:[‘mean’,’std’]})




Important Functions related to Data Cleaning
(1) Filling Missing Data
Let's create a small dataset to demonstrate this function.

test = [1,2,3,4,5,np.nan,np.nan]

test_df = vaex.from_arrays(co=test)




You can pass in any value in fillna to fill nan values with that value.

test_df.fillna(-1)




Notice that it is not in-place filling, so you might need to work on it. Another important thing is that no copy of the underlying data is made, but only a view/reference is made. The original columns will be renamed, and by default, they will be hidden columns. No data is lost.
(2) Missing Data Information
You can use ismissing(), isna(), and isnan() functions for the information about missing data. The first function returns true where there are missing values (masked arrays), missing strings, or None. The second function returns a boolean expression indicating if the values are Not Available (missing or NaN). The third function returns an array where there are NaN values.

test_df.co.isna()





test_df.co.isnan()





test_df.co.ismissing()




(3) Dropping Missing Data
You can drop missing data using dropna(), dropnan(), dropmissing(), and dropinf(). All of these functions perform similar tasks but they drop by using different functions. For example, dropmissing() function drops values uses ismissing filter function, and dropna() drops using the isna() filter function.

test_df.dropna()





test_df.dropnan()





test_df.dropmissing()




Notice that it will return a shallow copy of the original dataframe.
(4) Drop Columns
You can drop the columns in a vaex dataframe using the drop function.

vaex_df.drop(‘c0’) 



 
You can pass in a single column or a list of the columns. It is also not in place, and you have to check the parameter. If the column is used in a virtual column, you can simply just True the check parameter, and it will hide it.
String Operations
Vaex has a separate class for string functions vaex.expression.StringOperations. To use these in your dataset, let's say you want to use it on a string column. All you have to do is to call df.str.reqFunction() this will apply that function on every row.
Let's create a new testing dataframe to demonstrate the string functions.

import vaex
text = [‘Testing’, ‘lowercase’, ‘UPPERCASE’, ‘test_example’]
df = vaex.from_arrays(text=text)



 
(1) Lower
This function converts all the uppercase characters to lowercase characters.

df.text.str.lower()




(2) Contains
This is an important function that takes in a keyword argument pattern, which can be a string to search in all rows or a regex pattern.

df.text.str.contains(pattern=’([A-Z])’, regex=True)




It has returned True for all the rows that contain the matched regex. Alternatively, you can search only a single string.

df.text.str.contains(pattern=’ing’, regex=False)




(3) Endswith
This function is used to check if the given strings end with the specific pattern or not. This pattern can be string or regex.

df.text.str.endswith(‘case’)




(4) Alpha Numeric Check
This function isalnum is used to check if all the characters in the string are alphanumeric or not. In our case, our last row has an underscore which is not an alphanumeric character.

df.text.str.isalnum()




There are a lot of other functions which you can explore in the official documentation.
Functions related to plotting
Vaex can quickly visualize a huge amount of dataset in no time. Most of the plots are done in 1 or 2 dimensions, and vaex can full fill most of the use cases.
The simplest 1D plot can be shown using

vaex_df.plot1d(vaex_df.c0)




When only given 1 argument i.e., column name, it will show the histogram of that column. We can do some complex visualization, that is to not plot the counts, but to use some other bin. In most cases, passing the what='<statistic>(<expression>) where <statistic> is from one of the statistics methods mentioned in the API docs.

vaex_df.plot1d(vaex_df.c0, what=’mean(c0)’)



Similarly for 2D plotting, you can use

vaex_df.plot(vaex_df.c0, vaex_df.c1



You can explore other graphs and graph types in the official documentation.
 
Virtual Columns
 
Another important feature of Vaex is the virtual columns. Sometimes, any expression is stored as a column, and it is convenient to do so. Vaex gives virtual columns, which do not take any memory and are computed on the fly when needed. This virtual column is treated as the normal column.
Let’s create a dummy dataset.

_rows = 10

n_cols = 2

data=np.random.randint(100000000, 1000000000, size=(n_rows, n_cols))


data numpy array
 

df = vaex.from_arrays(d1=data[:, 0], d2=data[:, 1])



Now let's say you want to create another column d3 that is the sum of d1 and d2 columns. You can do

df['d3'] = df.d1+df.d2



 
This will create a virtual column d3 that is calculated on the run time when needed and will not occupy any memory.

You can confirm it by calling the info function on the dataframe.

df.info()



 
Important Note
 
If you are using Google Colab to use vaex, you might end up with an old IPython version error, so you need to update the IPython via the following command in the cell.

!pip install ipython -U



 
Then you need to restart the runtime.
 
Ending Notes
 
In this article, you learned the basics of Vaex, which is a Python library used for the fast processing of big data and can be a good alternative to Pandas, especially for large datasets. Vaex has a lot of other functions and features, so I would definitely recommend checking out the official documentation.
 
Related:

The secret to analysing large, complex datasets quickly and productively?
Good-bye Big Data. Hello, Massive Data!
How to Speed Up Pandas with Modin"
https://www.kdnuggets.com/2021/08/learned-women-data-science-conferences.html,What I Learned From “Women in Data Science” Conferences,"Read the author's perspective after attending 3 ""Women in Data Science"" conferences.","comments
By Vidhi Chugh, Data Scientist

Source
 
Covid has changed our lives in more ways than we could ever imagine. Working from home became a norm, something which was looked down upon when women used to seek this 'LUXURY' in pre-covid times.
With the booming 'digital' world, networking events also found their way through online webinars. Specifically, in the context of women conferences, I often used to wonder why is there a need to call out women and organize separate events for them.
Whenever we talk about an all-inclusive world, why does women's participation in ""general conferences"" not speak for itself? What stops women from demonstrating their work or giving voice to their beliefs in front of a wider audience?
With all these thoughts lingering in my mind, I thought to attend these conferences and witness for myself.
I have heard the rationale of people from both sides, some I am sharing with you here. In the end, I will conclude with my opinion and takeaways after attending 3 such women-focused conferences.
 
Why it all started? The points in favor:
 
 
Voice it out: General conferences are broadly seen as places where ""men are talking business"" while women are presumably amateurish and inept when they do the same. Women-focused conferences relieve them of this impression and give them the voice and freedom to share a wide range of topics e.g. how to maintain work-life balance, managing kids during the pandemic, receiving support from colleagues, a new business idea, or collaboration on the current project.
Cutting edge skills: If the conference involves attendees from an industry that demands disruptive technical skills, often synonymously dominated by men, the talks also involve such skills that a lot of women are yet to pick up, making it a tough bet for their time (filled with family responsibilities).
 
Issues with women conferences:
 
 
Why try to be mother nature: The gender difference should only stick to the point that mother nature has distinguished it to. And it should just stop there. Anything afterward is purely based on merit, dedication, and hard work. Why is it important to bring together women in one conference and teach them how to open up and ask for help, being assertive and confident?
Why preaching? Above all, my question is why do we need to teach all grown-ups about what they already know? Why can't we behave agnostic to the gender? No one can teach the whole society and pivot the deep-rooted biased lens we have been seeing the world with. It calls for a bottom-up approach where every individual feels the responsibility to promote and support equality in fighting the prejudices prevalent in the society
Marginalization: It is more of a vicious circle where the marginalized group is intended to be supported with such events but ends up getting marginalized further. Bring women and men at par, let the other side understand, discuss, support each other, and grow together.
Sometimes, I wonder whether it is specific to a particular field or industry, but unfortunately, it is everywhere. Recently, I came across a post where a senior executive had to call out a finance conference with an all-male panel discussion.
There is a fancy word for it manel i.e., male panel.

Sourced from author’s LinkedIn network
 
Male Advocate: Men play a crucial role in promoting women's rights and opportunities. Improving the condition of women in society and the workplace cannot be done with the other half (as they say), the men.
LinkedIn has introduced courses on helping women advance their careers. There is one course in particular that stands out 'Becoming a Male Ally at Work'. The course helps the 'gender in the majority to better understand the stereotypes that are holding women back.
 
What did I find after attending 3 women conferences?
 
 
While I can go on talking about how we, the society, can come together in solidarity and make it reach an upright state, this all might sound just too theoretical for you.
So, let's see a real example of how the power of women supporting each other can do wonders.
This year, I attended 3 such conferences and where women presented remarkable work in Data Science. I found one thing consistent across these networks - leaders from the industry share their stories and tips to deal with difficult situations. Some of the advice that resonates with me:
Being a yes (wo)man: Boundaries are just something that exists in our minds. Once you are past these self-imposed boundaries, the world of opportunities will open up for you. This fearless attitude of being a 'Yes woman' has done wonders to me on both fronts, personally and professionally. I became more confident of doing my best to whatever came my way.
Network your way through: Make an ecosystem of women in data, connecting with like-minded people gives you a perspective of how to handle similar situations. It also helps you make wiser decisions without committing the same mistakes that others have done.
Share the same vocabulary: Understand the business requirements, concerns of the stakeholders, map it to how you can contribute, and voice it out. I believe that the organizations are appreciative of their associates that add value and do not let their gender, caste, or religion come in the way of retaining successful business contributors.
Stop fighting the unconscious: It is important to acknowledge that there might be a subtle bias present in the subconscious of people around you. But we have to stop letting it come in the way of our vision. I am a working mother and know for sure the value of each hour that slips out of my hand. I would rather prefer to spend my time productively by upskilling myself. People can take anything but your knowledge away from you. Keep working on your knowledge bank and it will pave the right path for you in the long run.
Ways to be productive: Women are good at multi-tasking (and we know that :)). So, it is important that we use it to our advantage. Listening to podcasts, or technical courses while cooking or taking nature's walk keeps me on my toes and lets me utilize my time judiciously. Another important aspect of networking comes from the right use of social media, I try to connect with the leaders in Data Science to keep myself adept with the latest technologies and algorithms.
No golden rule: In the end, I would say that there is no recipe for success. Working on the ""try fast, fail fast"" formula proves to be good sometimes. So, do not be a victim of self-doubt and give a shot at everything that you dream of. When anyone tries to get you down, do not let it affect you. 'Show-up to the adversities in the face'.
So, with all the confidence I gained from attending these conferences, I implemented what I had learned, i.e., showing up. Yes, it has helped me get past the fear of self-doubt.
Though attending women in data science conferences has done good to me, my heart wishes for a day where conferences do not get a group-specific tag or label. That would be the day when the purpose of such conferences being conducted today would manifest into an inclusive and diverse world.
Till then, let us work on ""each one, help one"".
 
Bio: Vidhi Chugh is inquisitive about the power of data, and is commonly referred to as a Data Scientist.
Related:

Towards a Responsible and Ethical AI
8 Women in AI Who Are Striving to Humanize the World
More Resources for Women in AI, Data Science, and Machine Learning"
https://www.kdnuggets.com/2021/06/shortage-data-science-jobs-5-years.html,Will There Be a Shortage of Data Science Jobs in the Next 5 Years?,The data science workflow is getting automated day by day.,"comments
By Pranjal Saxena, Data Scientist, Top Writer in Artificial Intelligence


Photo by Andrea Piacquadio from Pexels

 
I have been in the data science field for the last half-decade when python programming came into the trend. Back then, in 2016, neural networks and deep learning were just some buzzy words. At that time, there was a hype about Google self-driving cars and reinforcement learning. But, most of the data science enthusiasts were not even aware of the working of neural networks.
Today in 2021, most companies are adopting a data science strategy to make more revenue by automating different scenarios and replacing dozens of IT people with a single data scientist who can automate the task of those IT people using various automating tools like BluePrism, UI Path, Python and machine learning algorithms.
That’s why most of us are working hard to learn python, machine learning, analytics, deep learning. Why? Because there is an excellent value for the data scientist in the industries. And, also people are getting a good hike in their job data in the data science field.


But, do you know in today’s time, these “automation tasks are being automated using another automation strategy?” The whole data science pipeline is being automated using a single tool.


In 2019, data scientists used to spent days in data gathering, data cleaning, feature selection, but now we have many tools in the market that can do these tasks in a few minutes.
On the other hand, we were trying different machine learning libraries like logistic regression, random forest, boosting machines, naive Bayes, and other data science libraries to a better model.
But, today, we have tools like H2O, PyCaret, and many other cloud providers who can do the same model selection on the same data using the combination of other 30–50 machine learning libraries to give you the best machine learning algorithms for your data with least error.
Things are now getting change at a fast pace. And, we are anyhow losing our value because everyone will trust the tool that tries more than twenty machine learning algorithms to come up with better accuracy than us who tries only a couple of machine learning libraries to come up with less accuracy.
 
The tough reality part
 
Until now, we have discussed how some automation tools are doing well in the machine learning area. And these tools are doing well than us because where we are using limited machine learning algorithm knowledge. In contrast, these tools are using the combination of libraries to get more efficient results by automating the complete EDA process providing the best possible results in less time.
But, what about the deep learning area where we have less command than the machine learning area and having limited processing power. There also we have a good amount of tools in the market. These tools invest a good amount of money in having the best processors.


Deep learning is all about more data, processing power, and a complex neural network that needs more processing power to provide more accurate results.


When we talk about deep learning, that is famous for handling unstructured data. And, 95% time, we work with images and test data here. Object detection, image segmentation, building chatbots, sentiment analysis, document similarity are the famous use cases.
But, working on these use cases required knowledge of different deep learning algorithms like convolution neural network, recurrent neural network, U-Net, hourglass, YOLO, and many more models that need a good amount of processing power to process more data for better accuracy.
The catch here is that when in today’s time in 2021, companies are investing a good amount of money in automating these complete pipeline workflows. And, we are busy understanding basic machine learning and deep learning model irrespective of the fact that we can’t afford high-end machines without any investors.
Each company is aware of this fact, so after five years, when these cloud-enabled data science tools will become more efficient and will be able to provide better accuracy in much less amount time, then why companies will invest in hiring us and not buying the subscription of those tools?
 
The Ray of Hope
 
When all these things are going to automate, you might be thinking about the future of data science enthusiasts. Will, there be a shortage of jobs or will there be fewer hirings?
Well, things become easier when we think differently. It is true that companies will keep focusing on the automated workflow of machine learning. But, remember, no company wants to depend on another company for their work.
Each company aims to build their product so that instead of depending on others, they can build their automated system and then sell them in the market to earn more revenue. So, yes, there will be a need for data scientists who can help industries build automation systems that can automate the task of machine learning and deep learning.
At last, we can say that the role of data scientists will be to automate the pipeline with optimized results. So, in the end, we will be automating the pipeline of machine learning workflow and let the automation decide the best features in the data and derive the best possible result using the best-curated algorithm.
 
Final Thoughts
 
We have seen how there will be a lack of data science jobs in the next five years because companies will be adopting the automated pipelines of data science. But, there will also be high demands for data scientists who can automate data science pipelines.
As per my thought to automate those pipelines, we first need to understand machine learning algorithms to build a better automated system, which will eventually lead to more jobs.
Well, What are your thoughts? I would love to hear yours. I hope you liked the article. Stay connected for more related articles. I publish articles on real-time data science scenarios and their use cases.
Thanks for the reading!
10 Python Tricks That Will Wow You
Handy features to improve your Python programming skills
 
15 Ultimate Daily Hacks for Every Programmer
You don’t need to import TensorFlow to print “hello world”
 
Found this story interesting? Follow me (Pranjal) on Medium. If you want to reach out to me with private questions do connect me on Linkedin. And, If you want to get more exciting articles on data science and technology directly to your mail then here is my free newsletter: Pranjal’s Newsletter.
 
Original. Reposted with permission.
Related:

Data Scientist, Data Engineer & Other Data Careers, Explained
A Guide On How To Become A Data Scientist (Step By Step Approach)
These Soft Skills Can Make or Break Your Data Science Career"
https://www.kdnuggets.com/2021/04/awesome-tricks-best-practices-kaggle.html,Awesome Tricks And Best Practices From Kaggle,Easily learn what is only learned by hours of search and exploration.,"comments
By Bex T., Top Writer in AI


Weekly Awesome Tricks And Best Practices From Kaggle

 
About This Project
 
Kaggle is a wonderful place. It is a gold mine of knowledge for data scientists and ML engineers. There are not many platforms where you can find high-quality, efficient, reproducible, awesome codes brought by experts in the field all in the same place.
It has hosted 164+ competitions since its launch. These competitions attract experts and professionals from around the world to the platform. As a result, there are many high-quality notebooks and scripts on each competition and for the massive amount of open-source datasets Kaggle provides.
At the beginning of my data science journey, I would go to Kaggle to find datasets to practice my skills. Whenever I looked at other kernels I would be overwhelmed by the complexity of the code and immediately shy away.
But now, I find myself spending a considerable amount of time reading other’s notebooks and making submissions to competitions. Sometimes, there are pieces that are worth spending your entire weekend on. And sometimes, I find simple but deadly effective code tricks and best practices that can only be learned by watching other pros.
And the rest is simple, my OCD practically forces me to spill out every single piece of data science knowledge I have. So here I am, writing the first edition of my ‘Weekly Awesome Tricks And Best Practices From Kaggle’. Throughout the series, you will find me writing about anything that can be useful during a typical data science workflow including code shortcuts related to common libraries, best practices that are followed by top industry experts on Kaggle, and so on, all learned by me during the past week. Enjoy!
 
1. Plotting Only the Lower Part of Correlation Matrix
 
A good correlation matrix can say a lot about your dataset. It is common to plot it to see the pairwise correlation between your features and the target variable. According to your needs, you can decide which features to keep and feed into your ML algorithm.
But today, datasets contain so many features that it can be overwhelming to look at correlation matrices like this:


Weekly Awesome Tricks And Best Practices From Kaggle

 
However nice, there is just too much information to take in. Correlation matrices are mostly symmetrical along the main diagonal, so they contain duplicate data. Also, the diagonal itself is useless. Let’s see how we can plot only the useful half:




The resulting plot is much easier to interpret and free of distractions. First, we build the correlation matrix using the .corr method of the DataFrame. Then, we use np.ones_like function with dtype set to bool to create a matrix of True values with the same shape as our DataFrame:

Then, we pass it to Numpy’s .triu function which returns a 2D boolean mask that contains False values for the lower triangle of the matrix. Then, we can pass it to Seaborn’s heatmap function to subset the matrix according to this mask:

I also made a few additions to make the plot a bit nicer, like adding a custom color palette.
 
2. Include Missing Values in value_counts
 
A handy little trick with value_counts is that you can see the proportion of missing values in any column by setting dropna to False:

By determining the proportion of values that are missing, you can make a decision as to whether to drop or impute them. However, if you want to look at the proportion of missing values across all columns, value_counts is not the best option. Instead, you can do:

First, find the proportions by dividing the number of missing values by the length of the DataFrame. Then, you can filter out columns with 0%, i. e. only choose columns with missing values.
 
3. Using Pandas DataFrame Styler
 
Many of us never realize the vast, untapped potential of pandas. An underrated and often overlooked feature of pandas is its ability to style its DataFrames. Using the .style attribute of pandas DataFrames, you can apply conditional designs and styles to them. As a first example, let’s see how you can change the background color depending on the value of each cell:




It is almost a heatmap without using Seaborn’s heatmap function. Here, we are counting each combination of diamond cut and clarity using pd.crosstab Using the .style.background_gradient with a color palette, you can easily spot which combinations occur the most. From the above DataFrame only, we can see that the majority of diamonds are of ideal cut and the largest combination is with the ‘VS2’ type of clarity.
We can even take this further by finding the average price of each diamond cut and clarity combination in crosstab:




This time, we are aggregating diamond prices for each cut and clarity combination. From the styled DataFrame, we can see that the most expensive diamonds have ‘VS2’ clarity or premium cut. But it would be better if we could display the aggregated prices by rounding them. We can change that with .style too:




By chaining .format method with a format string {:.2f}, we are specifying a precision of 2 floating points.
With .style, your imagination is the limit. With a little bit of knowledge of CSS, you can build custom styling functions for your needs. Check out the official pandas guide for more information.
 
4. Configuring Global Plot Settings With Matplotlib
 
When doing EDA, you will find yourself keeping some settings of Matplotlib the same for all of your plots. For example, you might want to apply a custom palette for all plots, using bigger fonts for tick labels, changing the location of legends, using fixed figure sizes etc.
Specifying each custom change to plots can be a pretty boring, repetitive and time-consuming task. Fortunately, you can use Matplotlib’s rcParams to set global configs for your plots:

from matplotlib import rcParams


rcParams is just a plain-old Python dictionary containing default settings of Matplotlib:



You can tweak pretty much every possible aspect of each individual plot. What I usually do and have seen others doing is set a fixed figure size, tick label font size, and a few others:

You can avoid a lot of repetition by setting these right after you import Matplotlib. See all the other available settings by calling rcParams.keys().
 
5. Configuring Global Settings of Pandas
 
Just like Matplotlib, pandas has global settings you can play around with. Of course, most of them are related to displaying options. The official user guide says that the entire options system of pandas can be controlled with 5 functions available directly from pandas namespace:

get_option() / set_option() — get/set the value of a single option.
reset_option() — reset one or more options to their default value.
describe_option() — print the descriptions of one or more options.
option_context() — execute a code block with a set of options that revert to prior settings after execution.

All options have case-insensitive names and are found using regex under the hood. You can use pd.get_option to see what is the default behavior and change it to your liking using set_option:

>>> pd.get_option(‘display.max_columns’)
20


For example, the above option controls the number of columns that are to be shown when there are many columns in a DataFrame. Today, the majority of datasets contain more than 20 variables and whenever you call .head or other display functions, pandas annoyingly puts ellipsis to truncate the result:

>>> houses.head()





I would rather see all columns by scrolling through. Let’s change this behavior:

>>> pd.set_option(‘display.max_columns’, None)


Above, I completely remove the limit:

>>> houses.head()





You can revert back to the default setting with:

pd.reset_option(‘display.max_columns’)


Just like columns, you can tweak the number of default rows shown. If you set display.max_rows to 5, you won’t have to call .head() all the time:

>>> pd.set_option(‘display.max_rows’, 5)>>> houses





Nowadays, plotly is becoming vastly popular, so it would be nice to set it as default plotting backed for pandas. By doing so, you will get interactive plotly diagrams whenever you call .plot on pandas DataFrames:

pd.set_option(‘plotting.backend’, ‘plotly’)


Note that you need to have plotly installed to be able to do this.
 
If you don’t want to mess up default behavior or just want to change certain settings temporarily, you can use pd.option_context as a context manager. The temporary behavior change will only be applied to the code block that follows the statement. For example, if there are large numbers, pandas has an annoying habit of converting them to standard notation. You can avoid this temporarily by using:




You can see the list of available options in the official pandas user guide.
 
Bio: Bex T. is a Top Writer in AI, writing “I wish I found this earlier” posts about Data Science and Machine Learning.
Original. Reposted with permission.
Related:

8 Places for Data Professionals to Find Datasets
10 resources for data science self-study
10 Python Skills for Beginners"
https://www.kdnuggets.com/2021/07/best-sota-nlp-course-free.html,The Best SOTA NLP Course is Free!,"Hugging Face has recently released a course on using its libraries and ecosystem for practical NLP, and it appears to be very comprehensive. Have a look for yourself.","By Matthew Mayo, KDnuggets.
comments
I probably don't need to tell you that Hugging Face — and in particular its Transformers library — has become a major power player in the NLP space. Transformers is full of SOTA NLP models which can be used out of the box as-is, as well as fine-tuned for specific uses and high performance. Hugging Face NLP tools don't stop there, however; its ecosystem includes numerous additional libraries, such as Datasets, Tokenizers, and Accelerate, and the 🤗 Model Hub.
However, with all of the massive and relentless advancements of natural language processing recently, keeping up with research breakthroughs and SOTA practices can be fraught with challenges. Hugging Face truly does provide a cohesive set of tools for practical NLP, but how does one keep up with both the most recent best practices and approaches to implementing NLP solutions and implementing these solutions with Hugging Face libraries?
Well, Hugging Face has a solution to this as well. Welcome to the free and (ad-free) Hugging Face course.
 

 
This course, taught by Matthew Carrigan, Lysandre Debut, and Sylvain Gugger, promises to cover a lot of ground:

Introduction -  it will provide an introduction to the 🤗 Transformers library — how Transformer models work; using models from the 🤗 Hub; fine-tuning models on datasets; and sharing your results.
Diving in -  the course covers the basics of 🤗 Datasets and 🤗 Tokenizers, allowing students to plot their approach to common NLP problems on their own.
Advanced - more complex topics are covered — learn specialized architectures, for concerns such as memory efficiency and long sequences; gain exposure to more custom solutions; confidently solve complex NLP problems; and contribute to 🤗 Transformers.

 

A brief overview of the Hugging Face course (source)
 
What do you need to jump into this course? While prior PyTorch or TensorFlow knowledge is not necessary, though would certainly be helpful, course creators note 2 specific prerequisites:

A good knowledge of Python
An understanding of introductory deep learning, perhaps via a course such as those developed by fast.ai or deeplearning.ai

I have not gone through the course to much extent yet; however, I have been hoping for and expecting something like this from Hugging Face for some time, and as such I will definitely be going through this material as soon as I can. 
From what I have looked through, the course looks very well put together, with code, videos, notes, etc. I like the progression as well, getting students using the Transformers library right away via the Pipeline function, and then moving further down the layers from there. There is also setup help to get students up and running quickly with TensorFlow or PyTorch in either a local environment or using Colab, with all options fully supported throughout.

Output of the Transformer model is sent directly to the model head to be processed (source)
 
There is a lot going on in the vast field of NLP, and there is a lot to keep up with. Hugging Face first set out to centralize and make widely-available the increasing number of Transformers, and these early efforts have made its subsequent ecosystem a leader in the field. Why not see how you can master these tools yourself? This will undoubtedly help you stay relevant and up on the SOTA in the ever-evolving field of NLP.
 
Related:

The Essential Guide to Transformers, the Key to Modern SOTA AI
How to Create and Deploy a Simple Sentiment Analysis App via API
Great New Resource for Natural Language Processing Research and Applications"
https://www.kdnuggets.com/2020/12/industry-2021-predictions-ai-data-science-machine-learning.html,"Industry 2021 Predictions for AI, Analytics, Data Science, Machine Learning","We bring you industry predictions from 12 innovative companies - what key trends they expect in 2021 in AI, Analytics, Data Science, and Machine Learning?","By Gregory Piatetsky, KDnuggets.
comments
Earlier we published 

 AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021
 Main 2020 Developments and Key 2021 Trends in AI, Data Science, Machine Learning Technology

Here is last part in our 2021 Predictions series - the predictions from the industry.  We received many submissions, and to keep this article size manageable, we limited this to 12 companies: 
Alluxio, Alteryx, Diamanti, Dremio, 
Indicative, Lexalytics, Luminoso, MathWorks,
MobiDev, Qlik, SAS, and Splice Machine.



Haoyuan Li, Founder and CEO, Alluxio

""Containers"" everywhere for analytics and AI

 Containerized application deployments and Kubernetes have started to gain traction with enterprises increasingly moving away from traditional Hadoop based data lakes. While moving away, enterprises are realizing the benefit of abstracting the physical infrastructure while also adopting public clouds for agility. Vendor lock in is a concern but at the same time a uniform toolset across environments is a must to reduce spending on the expertise required to operate across environments, such as hybrid and multi-cloud. Container based deployments for compute abstraction alongside new abstraction services for storage anywhere, will be the solution of choice for enterprises moving off Hadoop.

Convergence of Machine Learning frameworks

 Companies of all sizes and at all stages are moving aggressively towards operationalizing machine learning efforts. There are several popular frameworks for model training, including TensorFlow and PyTorch, leading the game. Just like Apache Spark is considered a leader for data transformation jobs and Presto is emerging as the leading tech for interactive querying, 2021 will be the year we'll see a front-runner dominate the broader model training space with PyTorch or TensorFlow as leading contenders.

AI & Analytics provided by the same platform (team)

 AI and Analytics capabilities were provided by different platforms / teams in the past. Over the years, we are seeing the platform is converging and the AI team is more focused on the algorithmic side, while AI & Analytics platform teams merged to provide the software infrastructure for both analytics and AI use cases.



Alan Jacobson, Chief Data and Analytics Officer, Alteryx

Upskilling will play a bigger role in corporate boardrooms as well as in employees lives.

While it is always important for companies to offer training to employees, the fields of data science and digital transformation are challenging companies to break the mold and deliver new and constantly evolving  ways to upskill and deliver ROI. More and more, we're going to see upskilling programs that help people learn and apply skills in real time. Hackathons are one example of how this is happening currently in many companies. We're going to see an expansion of these and other on the job experiences that use real data and real problems with a goal of creating real value. Data science has evolved to the point where people don't need to go back to college to learn, they'll learn on the job or while at home by encountering new tools and technologies.  And with a huge shortage of those with analytic skills, many will start new jobs and careers based on the new skills.

The ""analytic divide"" is going to get worse.

Like the much-publicized ""digital divide"" we're also seeing the emergence of an ""analytic divide."" Many companies were driven to invest in analytics due to the pandemic, while others have been forced to cut anything they didn't view as critical to keep the lights on - and a proper investment in analytics was, for these organizations, analytics was on the chopping block. This means that the analytic divide will further widen in 2021, and this trend will continue for many years to come.  Without a doubt, winners and losers in every industry will continue to be defined by those that are leveraging analytics and those that are not.

Analytics platforms and processes will increasingly outperform ad-hoc, siloed solving.

Businesses are already starting to democratize data across the organization, arming more employees with real-time insights. I see this accelerating with both a cultural shift and a technology shift.  This trend will result in data gurus and citizen data scientists with deep domain knowledge increasingly joining forces as part of a more holistic and effective problem-solving process. 


Boris Kurktchiev, Field CTO, Diamanti

 Everyone thought that AI/ML was going to be the next big thing, but I think there is confusion around what AI/ML means. There's a lot of confusion to the end user about how and why AI matters to them. We'll see more advances in augmented technology to determine what the application of AI and ML means and how to use it to make technology better for the end user. 
 Everyone wants hybrid cloud, and hybrid cloud relies on one thing: federated Kubernetes. This idea has been the twinkle in the developer community's eye since 2015. 2021 is the year that we see a proper implementation of that to the point where organizations can truly have a hybrid cloud. Without federated Kubernetes, organizations must contend with disparate components living in different clouds but not able to truly integrate with one another.



Tomer Shiran, co-founder of Dremio

Separation of Compute and Data Becomes the Default Choice

The rise of cloud data lake storage (e.g., Amazon S3 and Azure Data Lake Storage) as the default bit bucket in the cloud, combined with the infinite supply and elasticity of cloud compute resources, has ushered in a new era in data analytics architectures. Just as applications have moved to microservice architectures, data itself is now able to fully exploit cloud capabilities. Data can be stored and managed in open source file and table formats such as Apache Parquet and Apache Iceberg, and accessed by decoupled and elastic compute engines such as Apache Spark (batch), Dremio (SQL) and Apache Kafka (streaming). With these advances data will, in essence, become its own tier, enabling us to rethink data architectures and leverage application design benefits for big data analytics.

The Shine of the Cloud Data Warehouse Wears Off

The cloud data warehouse vendors have leveraged the separation of storage from compute to deliver offerings with a lower cost of entry than traditional data warehouses, as well as improved scalability. However, the data itself isn't separated from compute-it must first be loaded into the data warehouse, and can only be accessed through the data warehouse. This includes paying the data warehouse vendor to get the data into AND out of their system. So, while upfront expenses for a cloud data warehouse may be less, the costs at the end of the year are likely significantly higher than expected. By leveraging modern cloud data lake engines and open source table formats like Apache Iceberg, however, companies can now query data in the data lake directly without any degradation of performance, resulting in an extreme reduction in complex and costly data copies and movement.

Data Privacy and Governance Kicks Into Another Gear in the United States

Users are increasingly concerned about their online privacy making it much more likely that the United States will adopt regulations similar to Europe's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). This will require companies to double down on privacy and data governance in their data analytics infrastructure. Furthermore, companies will realize that data privacy and governance cannot be achieved with separate standalone tools, and instead must be implemented as an integral part of the analytics infrastructure. Because of this, data version control will become standard in cloud data lakes and open source technologies such as Project Nessie will enable companies to securely manage and govern data in an enterprise-wide platform. 


Jeremy Levy, CEO of Indicative.

 As data professionals, we have a responsibility to the broader public. I think that within the next year we will see progress toward a code of ethics within the data analytics space, led by conscious companies who recognize the seriousness of potential abuses. Perhaps the US government will intervene and pass some version of its own GDPR, but I believe that technology companies will lead this charge. What Facebook has done with engagement data is not illegal, but we've seen that it can have deleterious effects on child development and on our personal habits. In the coming years, we will look back on the way companies used personal data in the 2010s and cringe in the way we do when we see people smoking on a plane in films from the 1960s.



Lexalytics CEO Jeff Catlin and Chief Scientist Paul Barba:

 Data Annotation will become the next big ""side hustle"" in 2021. It's already a common way to make an extra buck or two, but there's been a race to the bottom in pricing, where annotations are largely sourced well below minimum wage in industrialized nations. However, as AI sees successes in industries requiring expertise, like health care or law, the demand for specialist knowledge will see the development of infrastructure for matching more lucrative annotation contracts to professionals.
 There will be more consolidation in the ML platform space. As AI became the ""it"" technology over the last few years, a bunch of AI infrastructure companies popped up and began peddling AI platforms to ease the task of building models for companies looking to leverage AI. While it sounds good on the surface, there is no identified business task being solved here, it's simply more efficient use of technology, and that's hard to sell.  It's likely that the VCs who backed these plays will begin severing the cash lifelines in 2021.
 The improvements in deep learning models over the last 18 months means that NLP features that have been desired but unfulfilled will start showing results. These include better entity recognition which drives better normalization, which in turn drives generic relationship extraction. The advances in deep learning models make all of these possible.
 AI platforms will consolidate, but AI services will pick up the slack here.  Companies are becoming more accepting of 3rd party expertise in machine learning, and this is driving an increase in consulting services for ML.  This trend will continue and accelerate in 2021.
 Fake news detection will start showing dividends. Fake news detection is an incredibly hard problem, but a lot of very smart people are spending a lot of time working on it. The spread of misinformation will be notably lower by late 2021.


Robyn Speer, Chief Science Officer at Luminoso

Doing more to fight bias in AI

 In 2021, I really hope business will do more to fight AI bias in all its forms. If only it could be as simple as ""not training on biased data."" But where is unbiased data going to come from? Any data that you collect in quantity reflects the biases of the world we live in. I recently discussed this in this Twitter thread.
 I see four steps to fighting AI bias that happen at different stages of machine learning: Knowing the biases of our source data and how to account for them; applying de-biasing techniques, when appropriate, to counteract the ways that biases get baked into intermediate representations; ensuring that the results of machine learning are used in ways that are fair and transparent; and being responsive and accountable in cases where the system turns out to have flaws or unintended consequences.



Johanna Pingel and David Willingham, Deep Learning Project Managers at MathWorks

A note on COVID-19: Investment in AI has not decreased 

We'd be remiss if we didn't mention COVID-19, an unforeseen trend of 2020, which is expected to continue with us into 2021. Overall investment in AI-related projects has not decreased. While some heavily impacted industries have cut back in the near term, analysts report that these have been offset by those who increased their investment above what they had forecasted. Many are using this time to invest in upskilling remote learning, with AI themed courses amongst the top sought after by the engineering and scientific community, making them primed and ready to take on more AI projects in 2021.

AI aligns engineering, computer science, data science and IT direction

 Engineers will continue to work with data scientists using AI models to enhance existing applications or discover new innovative solutions to the projects they're working on. However, creating a successful AI-based system is more than just developing a model. It requires model lifecycle management, which includes training, deploying, monitoring and updating the model for the system in which it resides. To do this efficiently these processes need to be automated, robust and well maintained. In 2021, engineers will augment their workflows to include:

Model explainability will reduce the aversion to AI within safety critical systems

 AI has long been considered a black box approach to modelling systems, and with it a fear that how it operates is largely unknown. As more explainability methods are being produced by research and more software vendor tools offer them, industry practitioners will more readily adopt AI innovations within their workflows.

Engineers and scientists are beginning to understand why a model is making certain decisions and the limits at which a model can operate safely. They are running experiments to explain how a model operates in a variety of scenarios and using visualizations to understand the inner workings of a model when it doesn't behave as it should. It's driving innovation in the verification and validation of AI within safety critical systems, with automotive, aerospace and medical standards committees, such as EUROCAE and the FDA, working on the levels needed for certification.


Maksym Tatariants, Data Science Engineer at MobiDev

Growing adoption of Machine Learning in mainstream software, including mobile apps, defined 2020. Together with the hardware support, like Apple's M1 chip, or Nvidia's Ampere GPU architecture, the ""intelligent process automation"" will continue its growth in 2021.

Besides, Edge AI has made a strong contribution to hardware evolution. There is definitely a clear focus on optimizing the latest neural networks for smartphones and IoT devices. Acceleration techniques, such as Automated Mixed Precision or TensorRT, are becoming available and easy-to-use. Thereby, they will help to perform edge computing in a better manner. As a result it will improve privacy and security of the user's data.

3D dimension research is another key trend: 3D reconstruction, pose estimation, and scene understanding. Although there's still some lack of information, plenty of promising model architectures are appearing. The 2021 challenge is to learn how to work with 3D dimensions in real-time on consumer-grade hardware. 


Dan Sommer, Senior Director, Global Market Intelligence Lead at Qlik

According to Gartner, by the end of 2024, 75% of enterprises will shift from piloting to operationalizing AI, driving a 5x increase in streaming data and analytics infrastructures. Having up-to-date and business ready data are more important than ever.

Since the pandemic arrived, we've seen a surge in the need for real-time and up-to-date data. What is usually fairly stale - quarterly business forecasts, for example - is fleeting and mutable now. Alerts, data refreshes and forecasts will need to occur more often, with the freshest variables. On a macro level, we've seen disruptions to supply chains, with hospitals scrambling to procure PPE and consumers stockpiling toilet paper. In the case of PPE, we reacted to an actual shortage too slowly; with toilet paper, consumers broke the supply chain by assuming a shortage where none existed. Surges like these are accentuated in a crisis, and we have to build preparedness for them.


Kimberly Nevala, AI Strategic Advisor, SAS: 

 The Analytics ""Core"" Gets Reinforced. The pandemic upended expected business trajectories and exposed the weaknesses in machine learning systems dependent on large amounts of representative historical data, including well-bounded and reasonably predictable patterns. As a result, organizations will bolster investments in traditional analytics teams and techniques better suited to rapid data discovery and hypothesizing.
 Ethical AI Principles Cede to Responsible AI Practices. Organizations will move beyond ethics in principle to practical procedures to guide AI decision-making. This will include right-sizing AI governance and oversight to their specific industry, problem domain and level of maturity. Accountability for AI-enabled products and services will be placed with the product owner.  Bolstered by increased consumer awareness and agency, leading adopters will promote ""responsible AI"" practices as a value-added differentiator.

Sarah Gates, Analytics Strategist, SAS: 2. The Year of ModelOps

 Pressures created by COVID-19 have raised organizational awareness of and need for ModelOps - the holistic approach used to rapidly move mathematical models through the analytics lifecycle, delivering value and insights faster. For organizations wanting to accelerate their digital transformation and to rev up agility and competitiveness, ModelOps is the magic fairy dust that will make it possible.



Monte Zweben, CEO of Splice Machine.

 Feature Stores will be implemented as the #1 ML product in 2021 to operationalize Machine Learning
 Every commercial database will have ML features
 Cloud migrations accelerate 10x, causing a major land grab by AWS, Azure, and GCP
 Vendor lock-in becomes the #1 concern of cloud migrations as companies fear lock-in by the cloud provider equivalent to the control Oracle/IBM had
 Everyone will be talking about the democratization of machine learning and data science as companies break out of the model where there is a centralized data science silo holding everybody up - much like the ""web"" group was in the 2000's - now every development team has web skills. The same will happen with ML
 Data lakes finally die and the re-emergence of the curated SQL data warehouse for structured data with associated cloud storage for unstructured data makes the dream of Big Data finally real 


Related:

 AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021
 Main 2020 Developments and Key 2021 Trends in AI, Data Science, Machine Learning Technology
 Industry AI, Analytics, Machine Learning, Data Science Predictions for 2020"
https://www.kdnuggets.com/2021/05/vc-pitch-deck-open-source-elt-platform.html,"How to pitch to VCs, explained: The Deck We Used to Raise Capital For Our Open-Source ELT Platform","Winning seed funding from venture capitalists is a daunting task, and the pitch is key. Learn how one effective slide deck resulted in a successful early funding round for an open-source start-up, Airbyte.","comments
By John Lafleur, Co-Founder, Airbyte.io.
There are quite a few decks already available online, and we found them all useful, each in different ways. That’s why we thought the deck we used for our seed round could be helpful to some companies, especially those that are open source or developer tools. For context: on that seed deck, we started working on Airbyte in the end of July and raised our seed round with Accel in December, only 5 months in. We’re assuming the deck was acceptable, as we raised the seed after only 13 days in the fundraising process.
We will write a dedicated article about the process within a few days, so this article is really about the deck itself.
 
Our Deck
 
Our deck was pretty standard, though some might consider it a bit short. That’s why we always sent the appendixes along with the deck after our meetings.
The structure and our pitch on each slide
Here are more details on our slides - why we included them and what we were saying during our pitches.
 
0. Cover
 

Although you might only spend 10 seconds discussing the cover slide, it is important. It’s about showing that you have identified your positioning.
 
1. Industry Context
 

Thanks to Snowflake’s IPO and Segment’s acquisition, we knew that data infrastructure was hot from an investor's point of view. We decided to start with some context on the industry and how we fit into this ecosystem.
What we were saying: 
When you look at the data infrastructure industry, there is often a new category that emerges through a commercial product. Once the market matures, an open-source alternative gets created and ends up taking over the category. This behavior is often seen because data infrastructure requires privacy, security, and scale, which cloud-based solutions can’t offer as well as open-sourced ones. There are many examples, such as Kafka, Spark, and now DBT. We want to be the open-source solution for data integration.
You might wonder why an open-source approach would also win the format for data integration; sometimes, a closed-source cloud-based approach works. This last sentence is a transition to the next slide. 
 
2. Problem We’re Solving
 

What we were saying:
In June and July, we started reaching out to 250 of Fivetran’s, StitchData’s, and Matillion’s customers. We ultimately managed to talk to 45 of them. We wanted to know whether an open-source approach would make sense to address data integration. What we learned is that a cloud-based closed-source solution will never be able to fully address the data integration problem. It has several inherent issues.
100% of the companies we talked to were using Fivetran, StitchData, or other solutions, while also building and maintaining their own connectors. They did so because either (a) the ETL solution didn’t support the connector they wanted, or (b) the solution supported it, but not in the way they needed.
When you look at Fivetran, for instance, you’ll see that after 8 years, they only support 150 connectors. The hard part about ETL/ELT is not about building the connectors but maintaining them. It is costly, and any cloud-based closed-source solution will be restricted by an ROI (return on investment) consideration. It isn’t profitable for them to support the long tail of connectors, so they only focus on the most popular integrations.
During those 45 interactions, we also identified a third issue. Some of the companies were about to stop using Fivetran for some connectors because it started to become too pricey. The value of an ELT solution is about replacing a paid data engineer that builds and maintains a connector in-house. The amount of work required from an engineer is almost the same whether a low volume or a high volume of data is being moved. So with volume-based pricing, at some point, it just stops making sense to use an external solution.
And the last inherent issue with a cloud-based approach: although cloud data warehouses are winning the enterprise market, it is because they are considered part of the data infrastructure. All other solutions must go through a rigorous privacy compliance process that will take several months.
 
3. Our Solution
 

What we were saying: 
We’re building an open-source ELT platform that syncs data from SaaS apps, APIs, and databases to data warehouses, data lakes, and other databases. Our solution can fully integrate with your data infrastructure and stack if you are using Kubernetes or Airflow for orchestration or DBT for transformation. Our goal is to become the open-source standard for anything ELT by the end of 2021.
 
4. How it addresses the problem
 

What we were saying: 
By making it trivial to build and maintain connectors using Airbyte rather than doing it in-house, we will become the new standard for building connectors. This will help us support the long tail of connectors. And since connectors run as Docker containers, you can build them in the language of your choice.
As connectors are open-sourced, any team can edit a pre-built connector and tune it to their needs. If a connector breaks, anyone can jump on the code, submit a PR, and, once approved, the change can be propagated across all the existing users of that connector.
Airbyte focuses first on a self-hosted offer: pricing will be based on the feature and number of connectors used. The pricing will not be indexed on the volume of data. Being open sourced enables us to have a bottom-up approach with a frictionless adoption from data teams without going through privacy compliance, as we handle data security as a first-class citizen.
 
5. Who We’re Targeting / Our Go-To-Market
 

What we were saying: 
With Airbyte, we wanted to address 2 different audiences:

Data consumers, including data analysts and scientists.
Data engineers who were building and maintaining the connectors themselves, or managing the data infrastructure.

What made Fivetran successful is that they enabled data consumers to leverage the data without the help of data engineers—they made them autonomous (as much as Segment made product teams autonomous). Airbyte provides a UI that makes it very easy to start replicating data for non-technical users. It takes literally 2 minutes for a data analyst to replicate data from Salesforce to Snowflake, including the deployment through our Docker Compose.
In addition, Airbyte will offer data integration connectors through an API that data engineers can leverage to build their own workflow and applications. This is also a way for Airbyte to address SaaS businesses that want to offer their own integrations to their customers through Airbyte.
 
6. Our Team
 

In this slide, we review the expertise of the team. In our case, we have 4 people coming from Liveramp, where they built and maintained more than 1,000 connectors and were moving more than 100TB of data every day. Our goal is to make investors understand that we’ve already done it in the past, but now the goal is to enable every company to do it.
 
7. Our Velocity
 

This is possibly the most important slide, as it validates everything you’ve said before with facts. The number of companies using you is most likely the most important data point for investors.
What we were saying:
We actually started working on Airbyte at the end of July. We soft-launched an MVP 2 months later (at the end of September), with only 6 connectors. We wanted to have feedback as early as possible. It’s been 6-7 weeks since we soft-launched, and we are now used by X companies, we have Y contributors, and we’ve ramped up the number of connectors to 43.
 
8. Our Roadmap
 

Now that we’ve shown our velocity up to this point, it’s time to show what we can accomplish in the next few quarters. Investors want to see where you will be when it is time to raise the next round. This slide’s goal is to accomplish this.
 
9. Our Ask
 

This is the last slide of the pitch. The goal here is that you shouldn’t need to say much! It shouldn’t come as a surprise and is a direct consequence of the roadmap slide.
 
10. Appendix!
 

You need such a slide to separate the main deck from the appendix. No surprise here! We only showed the appendix slides if asked relevant questions.
 
11. Some growth metrics [1]
 

The point for us here was to insist that we hadn’t done a hard launch yet, only a soft launch on our social networks on 09/24. The rest of the growth is organic.
 
12. Some growth metrics [2]
 

Investors will always ask for the names of companies that are using you. So you should always have such a slide ready. The more recognizable the names of the companies are, the better.
 
13. The competitive landscape
 

What we were saying:
Here are a few choices we made that distinguish Airbyte from other open-source solutions:

Airbyte’s connectors are usable out of the box through a UI and an API, with monitoring, scheduling, and orchestration.
Airbyte runs connectors as Docker containers, so they can be built in the language of your choice.
Airbyte’s components are modular, and you can decide to use subsets of the features to better fit in with your data infrastructure (e.g., orchestration with Airflow or K8s or Airbyte’s…).
We intend to integrate with DBT for the transformation piece soon and let the community contribute normalization schemas for all connectors.
Unlike Singer, Airbyte uses one single open-source repo to standardize and consolidate all developments from the community, leading to higher-quality connectors. We built a compatibility layer with Singer so that Singer taps can run within Airbyte.

 
14. Our investors
 

This is probably the slide we showed the least often, as serious VCs will already know who your main investors are. But you should still have it ready just in case.
 
Original. Reposted with permission.
Related:

Crafting an Elevator Pitch for your Data Science Startup
How To Get Funding For AI Startups
Six Ways For Data Scientists to Succeed at a Startup"
https://www.kdnuggets.com/2020/11/data-professionals-add-variation-resumes.html,How Data Professionals Can Add More Variation to Their Resumes,This article presents seven ways data professionals can add variation to their resumes.,"By Devin Partida, Editor-in-Chief of ReHack.com.
comments
Data professionals remain in high demand as data plays an increasingly crucial role in business. Despite the data science labor shortage, though, this field can be a competitive one. If workers want to boost their chances of securing a desirable position, they can add some variation to their resumes.
Variety on a resume demonstrates flexibility, which is an essential skill for data professionals today. It can be an excellent resume-booster, but jumping from position to position to improve a resume isn’t practical. Thankfully, applicants can add variety without needing to take on many new jobs.
Here are seven ways data professionals can add variation to their resumes.
 

 
1. List Any Publications Under Your Name
 
Job titles aren’t the only things on a resume that showcase variety. If applicants have any publications like books, journal articles or whitepapers under their name, they should list them. These stand out to potential employers as an example that a worker is serious about their field.
Remember that everything should be relevant to the specific job at hand. Only data-related publications should appear on a resume for a data-related position. If workers don’t have any relevant published works to their name, they can look for opportunities to be part of one.
 
2. Mention Noteworthy Projects
 
Throughout their career, data professionals will work on various projects, either for work or as hobbies. Whether they’re an impressive personal pursuit or an event that saved their company money, these projects are examples of relevant skills. Mentioning specific projects instead of general work descriptions also adds variation to an otherwise repetitive resume.
Anything that showcases different approaches or skills, or is particularly impressive, is worth mentioning. Descriptions of these projects don’t need to be long and should focus on what makes them unique. It’s best to express these accomplishments in numbers and metrics, which stand out more than words.
 
3. Look for Unique Opportunities in Current Positions
 
An applicant’s current company may have projects available that would add variation to a resume. As data professionals work, they should look out for any opportunities to learn or employ new skills. Volunteering to be part of these tasks will help build a more impressive resume.
Data professionals can ask their managers about any such opportunities or keep an eye out for them. Whether it’s working on a special company project or pioneering a new process for the department, these appear more often than some may think.
 
4. Perform Freelance Work
 
Data professionals don’t have to confine themselves to the work available in their current position. Data scientists and analysts are in-demand workers, so they can perform freelance work to boost their resumes. Freelance projects enable professionals to take on tasks they wouldn’t otherwise perform, adding variation without another full-time job.
Since this type of work enables professionals to choose their own hours, it can fit around their current schedule. Workers don’t need to take on too much extra work. Just a few projects can add some needed variety.
 
5. Work for an International Company
 
If professionals want to find another full-time position that will add variety, they may consider working internationally. International companies are more likely to have different projects than those available in the U.S. Even if they don’t, working with different cultures showcases flexibility.
There are several ways to work internationally, so workers can find something no matter their current situation. Data professionals may even be able to find contract work for international companies, so they can do it on top of their other work.
 
6. Consider Their Soft Skills
 
Even through various positions, data professionals may find that their day-to-day work looks similar. These workers can still find chances to add variation to their resume by mentioning the soft skills they’ve accumulated. Even if the data-centric work looks the same from position to position, different work environments may develop various soft skills.
Crunching numbers isn’t the only skill that’s important to the work of a data professional. They also need to communicate results to various audiences, work in teams and be adaptable. Highlighting these soft skills instead of tasks that would look more similar adds variety to a resume.
 
7. Get Specific
 
Hiring managers typically only have around 30 seconds to look over an applicant’s resume. Data professionals need to showcase variety without taking up too much space as a result. Being specific about all descriptors also forces applicants to focus on what makes each entry unique.
A general, zoomed-out look at different data positions may make them all look the same. Getting specific about each one highlights what makes them different.
 
Become the Most Appealing Candidate Possible
 
There’s a lot of variety in data-related work, even if it may not be immediately apparent. Data professionals looking to improve their resumes can follow these steps to showcase this variation. They can then become an even more appealing candidate in an already in-demand field.
 
Bio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com.
Related:

How Data Scientists Can Avoid ‘Lost in Translation’ Syndrome When Communicating With Management
How Automation Is Improving the Role of Data Scientists
Is Data Science for Me? 14 Self-examination Questions to Consider"
https://www.kdnuggets.com/2020/12/mlops-why-required-what-is.html,MLOps – “Why is it required?” and “What it is”?,"Creating an model that works well is only a small aspect of delivering real machine learning solutions. Learn about the motivation behind MLOps, the framework and its components that will help you get your ML model into production, and its relation to DevOps from the world of traditional software development.","comments
By Arnab Bose, Chief Scientific Officer, and Aditya Aggarwal, Advanced Analytics Practice Lead, Abzooba
 
MLOps Motivation
 
Machine Learning (ML) models built by data scientists represent a small fraction of the components that comprise an enterprise production deployment workflow, as illustrated in Fig 1 [1] below.​​​​​​​ To operationalize ML models, data scientists are required to work closely with multiple other teams such as business, engineering, and operations. This represents organizational challenges in terms of communication, collaboration, and coordination. The goal of MLOps is to streamline such challenges with well-established practices. Additionally, MLOps brings about agility and speed that is a cornerstone in today's digital world.

Fig 1: Only a small fraction of real-world ML systems are composed of the ML code, as shown by the small box in the middle.​ The required surrounding infrastructure is vast and complex.
 
MLOps challenges similar to DevOps
 
The challenges of ML model operationalization have a lot in common with software productionisation where DevOps has proven itself.
Therefore, adopting the best practices from DevOps is a prudent approach to help data scientists overcome challenges common to software productionisation. For example, the use of agile methodology promoted by DevOps in contrast to waterfall methodology is an efficiency boost. Additional DevOps practices used in MLOps are listed in Table 1.​​​​​​​​​​​​​​​​​​​​​
Table 1: MLOps leveraging DevOps.



ML model operationalization Challenges
Solution from DevOps


1) Continuous integration and continuous delivery (CI/CD): To set up a pipeline such that the updates are continuously built and ready for production accurately, securely, and seamlessly.
Use a CI/CD framework to build, test, and deploy software. It offers the benefits of reproducibility, security, and code version control.


2) Longer development to deployment lifecycle: Data Scientists develop models/algorithms and hand them over to operations to deploy into production. Lack of coordination and improper handoff between the two parties lead to delays and errors.
3) Ineffective communication between teams leads to delays in the final solution: The evaluation of an ML solution usually comes towards the end of the project lifecycle. With the development teams usually working in silos, the solution becomes a black-box to other stakeholders. This is worsened by the lack of intermediate feedback. These pose significant challenges in terms of time, effort, and resources.​​​
Agile methodology solves this coordination problem by enforcing an end-to-end pipeline set up at the initial stage itself. Agile methodology divides the project into a sprint. In each sprint, developers deliver incremental features that are ready for deployment. The output from each sprint (made using pipelines) is visible to each and every member from a very early stage of the project. Therefore, the risk of last-minute surprises reduces, and early feedback becomes a common practice. In industry parlance, this does a ""shift left"" to coordination issues.



 
 MLOps challenges different from DevOps
 
According to industry parlance, MLOps is DevOps for ML. While it is true to some extent, there are challenges typical to ML that need to be addressed by MLOps platforms.
An example of such a challenge is the role of data. ​​​​​​​​​​​​​​​​​In traditional software engineering (i.e., software 1.0), developers write logic and rules (as code) that are well defined in the program space, as demonstrated in Fig 2 [2]. However, in machine learning (i.e., software 2.0), data scientists write code that defines how to use parameters to solve a business problem. The parameter values are found using data (with techniques such as gradient descent). These values may change with different versions of the data, thereby changing the code behavior. In other words, data plays an equally important role as the written code in defining the output. And both can change independently of each other. This adds a layer of data complexity in addition to the model code as an intrinsic part of the software that needs to be defined and tracked.​​​​​​​ ​​​​​​

Fig 2: Software 1.0 vs. Software 2.0.
The various challenges that need to be taken care of by an MLOps platform are listed in Table 2.
Table 2: ML specific challenges.



ML Specific challenges
Description


1) Data and hyper-parameters versioning 
In traditional software application, code versioning tools are used to track changes. Version control is a prerequisite for any continuous integration (CI) solution as it enables reproducibility in a fully automated fashion. Any change in source code triggers the CI/CD pipeline to build, test and deliver production-ready code.
In Machine Learning, output model can change if algorithm code or hyper-parameters or data change. While code and hyper-parameters are controlled by developers, change in data may not be. This warrants the concept of data and hyper-parameters versioning in addition to algorithm code. Note that data versioning is a challenge for unstructured data such as images and audio and that MLOps platforms adopt unique approaches to this challenge.


2) Iterative development and experimentations
ML algorithm and model development is iterative and experimental. It requires a lot of parameter tuning and feature engineering. ML pipelines work with data versions, algorithm code versions and/or hyper-parameters. Any change in these artifacts (independently) trigger new deployable model versions that warrant experimentation and metrics calculations. MLOps platforms tracks the complete lineage for these artifacts.


3) Testing
Machine Learning requires data and model testing to detect problems as early in the ML pipeline as possible.
a) Data validation - check that the data is clean with no anomalies and new data is conformant to prior distribution.
b) Data preprocessing - check that data is preprocessed efficiently and in a scalable manner and avoid any training-serving skew [3].
c) Algorithm validation - track classification / regression metrics based on business problem as well as ensure algorithm fairness.


4) Security
ML models in production are often part of a larger system where its output is consumed by applications that may or may not be known. This exposes multiple security risks. MLOps needs to provide security and access control to make sure outputs of ML models is used by known users only.


5) Production Monitoring 
Models in production requires continuous monitoring to make sure models are performing per expectation as they process new data. There are multiple dimensions of monitoring such as covariate shift, prior shift, among others


6) Infrastructure requirement
ML applications need scale and compute power that translates into complex infrastructure. For example, GPU may be necessary during experimentations and production scaling may be necessary dynamically. ​​​​​​​



 
MLOps Components
 
​​​​​​​With the background in MLOps and its similarity to as well difference from DevOps, the following describes the different components that comprise an MLOps framework, as shown in Fig 3. The workflow underlying them is through the agile methodology, as indicated in Section 2.

Fig 3: MLOps Framework.

Use case discovery: This stage involves collaboration between business and data scientists to define a business problem and translate that into a problem statement and objectives solvable by ML with associated relevant KPIs (Key Performance Indicator).
​​​​Data Engineering: This stage involves collaboration between a data engineer and a data scientist to acquire data from various sources and prepare the data (processing/validation) for modeling.
​​​​​​​Machine Learning pipeline: This stage is designing and deploying a pipeline integrated with CI/CD. Data scientists use pipelines for multiple experimentation and testing. The platform keeps track of data and model lineage and associated KPIs across the experiments.
Production deployment: This stage accounts for secure and seamless deployment into the production server of choice, be it public cloud, on-premise, or hybrid.
Production monitoring: This stage includes both model and infrastructure monitoring. Models are continuously monitored using configured KPIs like changes in input data distribution or changes in model performance. Triggers are set for more experimentations with new algorithms, data, and hyper-parameters that generate a new version of the ML pipeline. Infrastructure is monitored per memory and compute requirements and to scale as needed.

​​​​​
References

Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, M. Young, J. Crespo, and D. Dennison, “Hidden technical debt in machine learning systems”, in Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, 2015, pp. 2503–2511. [Online]. Available: http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems
​​​​​​​ Karpathy, ""Software 2.0"", November 12, 2017 [Online]. Available: https://medium.com/@karpathy/software-2-0-a64152b37c35
Breck, M. Zinkevich, N. Polyzotis, S. Whang and S. Roy, ""Data validation for machine learning"", in Proceedings of the 2nd SysML Conference, Palo Alto, CA, USA, 2019. Available: https://mlsys.org/Conferences/2019/doc/2019/167.pdf

 
Bios: Dr. Arnab Bose is Chief Scientific Officer at Abzooba, a data analytics company and an adjunct faculty at the University of Chicago where he teaches Machine Learning and Predictive Analytics, Machine Learning Operations, Time Series Analysis and Forecasting, and Health Analytics in the Master of Science in Analytics program. He is a 20-year predictive analytics industry veteran who enjoys using unstructured and structured data to forecast and influence behavioral outcomes in healthcare, retail, finance, and transportation. His current focus areas include health risk stratification and chronic disease management using machine learning, and production deployment and monitoring of machine learning models.
Aditya Aggarwal serves as Data Science – Practice Lead at Abzooba Inc. With more than 12+ years of experience in driving business goals through data-driven solutions, Aditya specializes in predictive analytics, machine learning, business intelligence & business strategy across a range of industries.
Related:

Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes
Taming Complexity in MLOps
What Does it Mean to Deploy a Machine Learning Model?"
https://www.kdnuggets.com/2021/06/machine-learning-model-interpretation.html,Machine Learning Model Interpretation,Read this overview of using Skater to build machine learning visualizations.,"comments
By Himanshu Sharma, Bioinformatics Data Analyst


Tree (Source: By Author)

 
Interpreting a machine learning model is a difficult task because we need to understand how a model works in the backend, what all parameters the model uses, and how the model is generating the prediction. There are different python libraries that we can use to create machine learning model visualizations and analyze who the model is working.
Machine Learning Model Dashboard
Creating dashboards to interpret machine learning model
 
Staker is an open-source python library that enables machine learning model interpretations for different types of black-box models. It helps us create different types of visualization, making it easier to understand how a model is working.
In this article, we will explore Skater and what are its different functionalities. Let’s get started…
 
Installing required libraries
 
We will start by installing a skater using pip installation. The command given below will install the skater using pip.

!pip install -U skater


 
Importing required libraries
 
The next step will be importing the required libraries. To interpret the model using Skater we first need to create a model.

%matplotlib inline 
import matplotlib.pyplot 
import matplotlib.pyplot as plt 
import numpy as np 
from sklearn.model_selection 
import train_test_split from sklearn.ensemble 
import RandomForestClassifier 
from sklearn import datasets 
from sklearn import svm
from skater.core.explanations import Interpretation
from skater.model import InMemoryModel
from skater.core.global_interpretation.tree_surrogate 
import TreeSurrogate
from skater.util.dataops import show_in_notebook


 
Creating Model
 
We will create a Random Forest Classifier and use the IRIS dataset.

iris = datasets.load_iris()
digits = datasets.load_digits()
X = iris.data
y = iris.target
clf = RandomForestClassifier(random_state=0, n_jobs=-1)

xtrain, xtest, ytrain, ytest = train_test_split(X,y,test_size=0.2, random_state=0) 
clf = clf.fit(xtrain, ytrain)

y_pred=clf.predict(xtest)
prob=clf.predict_proba(xtest)

from skater.core.explanations import Interpretation
from skater.model import InMemoryModel
from skater.core.global_interpretation.tree_surrogate import TreeSurrogate
from skater.util.dataops import show_in_notebook

interpreter = Interpretation(
        training_data=xtrain, training_labels=ytrain, feature_names=iris.feature_names
    )

pyint_model = InMemoryModel(
            clf.predict_proba,
            examples=xtrain,
            target_names=iris.target_names,
            unique_values=np.unique(ytrain).tolist(),
            feature_names=iris.feature_names,
        )


 
Creating Visualizations
 
We will start by creating different visualizations that will help us analyze how the model we have created is working.
1. Partial dependence plot
This plot shows us how a particular feature affects the model's prediction.

interpreter.partial_dependence.plot_partial_dependence(
   ['sepal length (cm)'] , pyint_model, n_jobs=-1, progressbar=False, grid_resolution=30, with_variance=True,figsize = (10, 5)
)




PDP Plot (Source: By Author)

 
2. Feature importance
In this graph, we will analyze the importance of features in the model that we have created.

plots = interpreter.feature_importance.plot_feature_importance(pyint_model, ascending=True, progressbar=True,
                                n_jobs=-1)




Feature Importance (Source: By Author)

 
3. Surrogate tree
It is a pictorial representation of the random forest model that we have created. At each step, it is showing the Gini index value, class, etc.

surrogate_explainer = interpreter.tree_surrogate(oracle=pyint_model, seed=5)
surrogate_explainer.fit(xtrain, ytrain)
surrogate_explainer.plot_global_decisions(show_img=True)




Surrogate Tree(Source: By Author)

 
This is how we can use Skater to create different graphs that help us analyze how a model is performing. Go ahead try this with different datasets and let me know your comments in the response section.
This article is in collaboration with Piyush Ingale
 
Before You Go
 
Thanks for reading! If you want to get in touch with me, feel free to reach me on hmix13@gmail.com or my LinkedIn Profile. You can view my Github profile for different data science projects and packages tutorials. Also, feel free to explore my profile and read different articles I have written related to Data Science.
 
Bio: Himanshu Sharma is a Bioinformatics Data Analyst at MEDGENOME. Himanshu is a Data Science Enthusiast with hands-on experience in analysing datasets, creating machine learning and deep learning models. He has worked on creating different data science projects and Poc's for different organisations. He has vast experience in creating CNN models for Image recognition and object detection along with RNN for time series prediction. Himanshu is an active blogger and have published around 100+ articles in the field of Data Science. 
Original. Reposted with permission.
Related:

Automating Machine Learning Model Optimization
The Explainable Boosting Machine
Interpretability, Explainability, and Machine Learning – What Data Scientists Need to Know"
https://www.kdnuggets.com/2021/04/easy-automl-python.html,Easy AutoML in Python,"We’re excited to announce that a new open-source project has joined the Alteryx open-source ecosystem. EvalML is a library for automated machine learning (AutoML) and model understanding, written in Python.","comments
By Dylan Sherry, EvalML Team Lead

Alteryx hosts two open-source projects for modeling.
Featuretools is a framework to perform automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning.
Compose is a tool for automated prediction engineering. It allows you to structure prediction problems and generate labels for supervised learning.
We’ve seen Featuretools and Compose enable users to easily combine multiple tables into transformed and aggregated features for machine learning, and to define time series supervised machine learning use-cases.
The question we then asked was: what happens next? How can users of Featuretools and Compose build machine learning models in a simple and flexible way?
We’re excited to announce that a new open-source project has joined the Alteryx open-source ecosystem. EvalML is a library for automated machine learning (AutoML) and model understanding, written in Python.

import evalml

# obtain features, a target and a problem type for that target
X, y = evalml.demos.load_breast_cancer()
problem_type = 'binary'
X_train, X_test, y_train, y_test = evalml.preprocessing.split_data(
    X, y, problem_type=problem_type, test_size=.2)

# perform a search across multiple pipelines and hyperparameters
automl = AutoMLSearch(X=x, y=y, problem_type=problem_type)
automl.search()

# the best pipeline is already refitted on the entire training data
best_pipeline = automl.best_pipeline
best_pipeline.predict(X_test)




EvalML's AutoML search in action

 
EvalML provides a simple, unified interface for building machine learning models, using those models to generate insights and to make accurate predictions. EvalML provides access to multiple modeling libraries under the same API. EvalML supports a variety of supervised machine learning problem types including regression, binary classification and multiclass classification. Custom objective functions let users phrase their search for a model directly in terms of what they value. Above all we’ve aimed to make EvalML stable and performant, with ML performance testing on every release.
 
What’s Cool about EvalML
 
1. Simple Unified Modeling API
EvalML reduces the amount of effort it takes to get to an accurate model, saving time and complexity.
EvalML pipelines produced by AutoML include preprocessing and feature engineering steps out of the box. Once users have identified the target column of the data which they’d like to model, EvalML’s AutoML will run a search algorithm to train and score a collection of models, will enable users to select one or more models from that collection, and to then use those models for insight-driven analysis or to generate predictions.
EvalML was designed to work well with Featuretools, which can integrate data from multiple tables and generate features to turbocharge ML models, and with Compose, a tool for label engineering and time series aggregation. EvalML users can easily control how EvalML will treat each inputted feature, as a numeric feature, a categorical feature, text, date-time, etc.


You can use Compose and Featuretools with EvalML to build machine learning models

 
EvalML models are represented using a pipeline data structure, composed of a graph of components. Every operation applied to your data by AutoML is recorded in the pipeline. This makes it easy to turn from selecting a model to deploying a model. It's also easy to define custom components, pipelines and objectives in EvalML, whether for use in AutoML or as standalone elements.
 
2. Domain-Specific Objective Functions
EvalML supports defining custom objective functions which you can tailor to match your data and your domain. This allows you to articulate what makes a model valuable in your domain, and to then use AutoML to find models which deliver that value.
The custom objectives are used to rank models on the AutoML leaderboard during and after the search process. Using a custom objective will help guide the AutoML search towards models which are the highest impact. Custom objectives will also be used by AutoML to tune the classification threshold of binary classification models.
The EvalML documentation provides examples of custom objectives and how to use them effectively.
 
3. Model Understanding
EvalML grants access to a variety of models and tools for model understanding. Currently supported are feature importance and permutation importance, partial dependence, precision-recall, confusion matrices, ROC curves, prediction explanations, and binary classifier threshold optimization.


An example of partial dependence from the EvalML documentation

 
 
4. Data Checks
EvalML's data checks can catch common problems with your data prior to modeling, before they cause model quality problems or mysterious bugs and stack traces. Current data checks include a simple approach to detecting target leakage, where the model is given access to information during training which won’t be available at prediction-time, detection of invalid datatypes, high class imbalance, highly null columns, constant columns, and columns which are likely an ID and not useful for modeling.

 
Getting Started Using EvalML
 
You can get started with EvalML by visiting our documentation page, where we have installation instructions as well as tutorials which show examples of how to use EvalML, a user guide which describes the components and core concepts of EvalML, API reference and more. The EvalML codebase lives at https://github.com/alteryx/evalml. To get in touch with the team, check out our open-source slack. We are actively contributing to the repository and will respond to any issues you post.
 
What’s Next?
 
EvalML has an active feature roadmap, including time series modeling, parallel evaluation of pipelines during AutoML, upgrades to the AutoML algorithm, new model types and preprocessing steps, tools for model debugging and model deployment, support for anomaly detection, and much more.
Want to hear more? If you’re interested in hearing about updates as the project continues, please take a moment to follow this blog, star our repo in GitHub, and stay tuned for more features and content on the way!
 
Bio: Dylan Sherry is a software engineer who leads the team building the EvalML AutoML package. Dylan has a decade of experience working on automated modeling technologies.
Original. Reposted with permission.
Related:

Uber Open Sources the Third Release of Ludwig, its Code-Free Machine Learning Platform
Algorithms for Advanced Hyper-Parameter Optimization/Tuning
Build Your Own AutoML Using PyCaret 2.0"
https://www.kdnuggets.com/2020/08/nlp-model-forge.html,The NLP Model Forge: Generate Model Code On Demand,You've seen their Big Bad NLP Database and The Super Duper NLP Repo. Now Quantum Stat is back with its most ambitious NLP product yet: The NLP Model Forge.,"By Matthew Mayo, KDnuggets.
comments
Quantum Stat first came through with The Big Bad NLP Database, a collection of freely-accessible NLP datasets, curated from around the internet. It then released The Super Duper NLP Repo, which, at the time of introduction, provided centralized access to 100 freely-accessible NLP notebooks, curated from around the internet, and ready to launch in Colab with a single click. Now Quantum Stat is back with arguably its most ambitious NLP clearinghouse product yet.
The NLP Model Forge is here to help you create NLP models quickly and easily. As conveyed to me by Quantum Stat CEO Ricky Costa:
 

[The NLP Model Forge] allows users to generate code snippets from 1,400 NLP models curated from top NLP research companies such as Hugging Face Facebook DeepPavlov and AI2.

 

 
The general idea of The NLP Model Forge is that you are able to browse and search example model code by task type, which include sequence and token classification, question answering, text summarization, text generation, translation, and more. Each model has a number of attributes, which include:

name (e.g. distilbert-base-uncased-finetuned-sst-2-english)
source language
type (e.g. DistilBERT)
tags(e.g. pytorch, tf, rust, distilbert, text-classification, sst-2 dataset)
labels (e.g. 0: NEGATIVE, 1: POSITIVE)

All of these model attributes, when taken together, provide a rich description and overview of realistic model expectations and implementation details. For example, it can be easily gleaned from the above example model's attributes that it is a DistilBERT text classification (sentiment analysis) model with implementations available in at least PyTorch and TensorFlow.
Once a desired model has been located, one can toggle the ""load"" switch in order to queue up the model code, after which the ""get code"" button can be used to load in the selected model code. See the image below for more details.

 
Once the code screen has been opened, the particular model's code can be viewed directly. Alternatively, a Colab notebook can be launched with a single click in order to have the model code snippet opened into an executable environment. See the image below for more details.

 
The NLP Model Forge seems best suited to individuals looking to bounce around and try out different models in their own projects, without needing to extensively research the ins and outs of each prior to giving them a test run. Once a suitable model has been settled upon, I could see the example code snippets being modified, extended, and personalized for maximum implementation effect. It would best be embraced as a fast iteration prototype assist than complete code for an end product, but, to be fair, nowhere is it stated that The NLP Model Forge is anything but this.
The NLP Model Forge is a promising project, and a worthy addition to Quantum Stat's NLP product repertoire. If you are looking for a centralized spot for browsing and searching model code snippets, or to get ideas for your next NLP project, it is definitely worth a visit.
 
Related:

The Big Bad NLP Database: Access Nearly 300 Datasets
The Super Duper NLP Repo: 100 Ready-to-Run Colab Notebooks
Natural Language Processing Recipes: Best Practices and Examples"
https://www.kdnuggets.com/2021/07/wht-simpler-fast-fourier-transform-fft.html,WHT: A Simpler Version of the fast Fourier Transform (FFT) you should know,The fast Walsh Hadamard transform is a simple and useful algorithm for machine learning that was popular in the 1960s and early 1970s. This useful approach should be more widely appreciated and applied for its efficiency.,"comments
By Sean O'Connor, a science and technology author and investigator.
The fast Walsh Hadamard transform (WHT) is a simplified version of the Fast Fourier Transform (FFT.)
The 2-point WHT of the sequence a, b is just the sum and difference of the 2 values:

WHT(a, b) = a+b, a-b.



 
It is self-inverse allowing for a fixed constant:

WHT(a+b, a-b) = 2a, 2b



 
Due to (a+b) + (a-b) = 2a and (a+b) - (a-b) = 2b.
The constant can be split between the two Walsh Hadamard transforms using a scaling factor of √2 to give a normalized WHTN:

WHTN(a, b) = (a+b)/√2, (a-b)/√2

WHTN((a+b)/√2, (a-b)/√2) = a, b



 
That particular constant results in the vector length of a, b being unchanged after transformation since a2+b2 =((a+b)/√2)2+ ((a-b)/√2)2 as you may easily calculate.
The 2-point transform can be extended to longer sequences by sequentially adding and subtracting pairs of similar terms, alike in the pattern of + and – symbols they contain.
To transform a 4-point sequence a, b, c, d first do two 2-point transforms:

WHT(a, b) = a+b, a-b

WHT(c, d) = c+d, c-d



 
Then add and subtract the alike terms a+b and c+d:

WHT(a+b, c+d) = a+b+c+d, a+b-c-d



 
and the alike terms a-b and c-d:

WHT(a-b, c-d) = a-b+c-d, a-b-c+d



 
The 4-point transform of a, b, c, d then is

WHT(a, b, c, d) = a+b+c+d,  a+b-c-d, a-b+c-d, a-b-c+d



 
When there are no more similar terms to add and subtract, that signals completion (after log2(n) stages, where n is 4 in this case.)  The computational cost of the algorithm is nlog2(n) add/subtract operations, where n, the size of the transform, is restricted to being a positive integer power of 2 in the general case.
If the transform was done using matrix operations, the cost would be much higher (n2 fused multiply-add operations.)

Figure 1.  The 4-point Walsh Hadamard transform calculated in matrix form.
The +1, -1 entries in Figure 1 are presented in a certain natural order which most of the actual algorithms for calculating the WHT result in, which is fortunate since then the matrix is symmetric, orthogonal and self-inverse.
You can also view the +1, -1 patterns of the WHT as waveforms.


Figure 2.  The waveforms of the 8-point WHT presented in natural order.
When you calculate the WHT of a sequence of numbers, you are really just determining how much of each waveform is embedded in the original sequence.  And that is complete and total information with which you can fully reconstruct any sequence from its transform.
The waveforms of the WHT typically correlate strongly with the patterns found in natural data like images, allowing the transform to be used for data compression.

Figure 3.  A 65536-pixel image compressed to 5000 points using a WHT.
In Figure 3, a 65536-pixel image was transformed with a WHT, the 5000 maximum magnitude embeddings were preserved, and then the inverse transform was applied (simply another WHT.)
The central limit theorem (CLT) tells you that adding a large quantity of random numbers results in the Normal distribution with its characteristic bell curve.  The CLT applies equally to sums and differences of a large quantity of random numbers.  As a result, C.M. Rader proposed (in 1969) using the WHT to quickly generate Normally distributed random numbers from conventional uniformly distributed random numbers.  You simply generate a sequence of uniform random numbers, say between –1 and 1, and then transform them using the WHT.
Similarly, you can disrupt the orderly waveform patterns of the WHT by choosing a fixed randomly chosen pattern of sign flips to apply to any input to the transform.  That is equivalent to multiplying the WHT matrix H with a diagonal matrix D of randomly chosen +1, -1 entries giving HD.  The disrupted waveform patterns in HD then fail to correlate with any of the patterns seen in natural data.  As a result, the output of HD has the Normal distribution and is actually a fast Random Projection of the natural data.  Random projections have a wide number of applications in machine learning, such as locality sensitive hashing, compressive sensing, random projection trees, neural network pre and post-processing etc.
 
References
Walsh (Hadamard) Transform:

https://archive.org/details/bbc-rd-reports-1974-07
https://www.jjj.de/fxt/

Normal Distribution:

https://archive.org/details/DTIC_AD0695042

Random Projections:

https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41466.pdf

Other Applications:

https://ai462qqq.blogspot.com/

Related:

Essential Machine Learning Algorithms: A Beginner’s Guide
All Machine Learning Algorithms You Should Know in 2021
An easy guide to choose the right Machine Learning algorithm"
https://www.kdnuggets.com/2021/06/fine-tune-bert-transformer-spacy.html,How to Fine-Tune BERT Transformer with spaCy 3,A step-by-step guide on how to create a knowledge graph using NER and Relation Extraction.,"comments
By Walid Amamou, Founder of UBIAI


Photo by Alina Grubnyak on Unsplash

 
Since the seminal paper “Attention is all you need” of Vaswani et al, Transformer models have become by far the state of the art in NLP technology. With applications ranging from NER, Text Classification, Question Answering or text generation, the applications of this amazing technology are limitless.
More specifically, BERT — which stands for Bidirectional Encoder Representations from Transformers— leverages the transformer architecture in a novel way. For example, BERT analyses both sides of the sentence with a randomly masked word to make a prediction. In addition to predicting the masked token, BERT predicts the sequence of the sentences by adding a classification token [CLS] at the beginning of the first sentence and tries to predict if the second sentence follows the first one by adding a separation token[SEP] between the two sentences.


BERT Architecture

 
In this tutorial, I will show you how to fine-tune a BERT model to predict entities such as skills, diploma, diploma major and experience in software job descriptions. If you are interested to go a step further and extract relations between entities, please read our article on how to perform joint entities and relation extraction using transformers.
Fine tuning transformers requires a powerful GPU with parallel processing. For this we use Google Colab since it provides freely available servers with GPUs.
For this tutorial, we will use the newly released spaCy 3 library to fine tune our transformer. Below is a step-by-step guide on how to fine-tune the BERT model on spaCy 3.
 
Data Labeling:
 
To fine-tune BERT using spaCy 3, we need to provide training and dev data in the spaCy 3 JSON format (see here) which will be then converted to a .spacy binary file. We will provide the data in IOB format contained in a TSV file then convert to spaCy JSON format.
I have only labeled 120 job descriptions with entities such as skills, diploma, diploma major, and experience for the training dataset and about 70 job descriptions for the dev dataset.
In this tutorial, I used the UBIAI annotation tool because it comes with extensive features such as:

ML auto-annotation
Dictionary, regex, and rule-based auto-annotation
Team collaboration to share annotation tasks
Direct annotation export to IOB format

Using the regular expression feature in UBIAI, I have pre-annotated all the experience mentions that follows the pattern “\d.*\+.*” such as “5 + years of experience in C++”. I then uploaded a csv dictionary containing all the software languages and assigned the entity skills. The pre-annotation saves a lot of time and will help you minimize manual annotation.


UBIAI Annotation Interface

 
For more information about UBIAI annotation tool, please visit the documentation page and my previous post “Introducing UBIAI: Easy-to-Use Text Annotation for NLP Applications”.
The exported annotation will look like this:

MS B-DIPLOMA
in O
electrical B-DIPLOMA_MAJOR
engineering I-DIPLOMA_MAJOR
or O
computer B-DIPLOMA_MAJOR
engineering I-DIPLOMA_MAJOR
. O
5+ B-EXPERIENCE
years I-EXPERIENCE
of I-EXPERIENCE
industry I-EXPERIENCE
experience I-EXPERIENCE
. I-EXPERIENCE
Familiar O
with O
storage B-SKILLS
server I-SKILLS
architectures I-SKILLS
with O
HDD B-SKILLS


In order to convert from IOB to JSON (see documentation here), we use spaCy 3 command:

!python -m spacy convert drive/MyDrive/train_set_bert.tsv ./ -t json -n 1 -c iob
!python -m spacy convert drive/MyDrive/dev_set_bert.tsv ./ -t json -n 1 -c iob


After conversion to spaCy 3 JSON, we need to convert both the training and dev JSON files to .spacy binary file using this command (update the file path with your own):

!python -m spacy convert drive/MyDrive/train_set_bert.json ./ -t spacy!python -m spacy convert drive/MyDrive/dev_set_bert.json ./ -t spacy


 
Model Training:

Open a new Google Colab project and make sure to select GPU as hardware accelerator in the notebook settings.
In order to accelerate the training process, we need to run parallel processing on our GPU. To this end we install the NVIDIA 9.2 cuda library:


!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604–9–2-local_9.2.88–1_amd64.deb!dpkg -i cuda-repo-ubuntu1604–9–2-local_9.2.88–1_amd64.deb!apt-key add /var/cuda-repo-9–2-local/7fa2af80.pub!apt-get update!apt-get install cuda-9.2


To check the correct cuda compiler is installed, run: !nvcc --version

Install the spacy library and spacy transformer pipeline:


pip install -U spacy
!python -m spacy download en_core_web_trf



Next, we install the pytorch machine learning library that is configured for cuda 9.2:


pip install torch==1.7.1+cu92 torchvision==0.8.2+cu92 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html



After pytorch install, we need to install spacy transformers tuned for cuda 9.2 and change the CUDA_PATH and LD_LIBRARY_PATH as below. Finally, install the cupy library which is the equivalent of numpy library but for GPU:


!pip install -U spacy[cuda92,transformers]
!export CUDA_PATH=”/usr/local/cuda-9.2""
!export LD_LIBRARY_PATH=$CUDA_PATH/lib64:$LD_LIBRARY_PATH
!pip install cupy



SpaCy 3 uses a config file config.cfg that contains all the model training components to train the model. In spaCy training page, you can select the language of the model (English in this tutorial), the component (NER) and hardware (GPU) to use and download the config file template.



Spacy 3 config file for training. Source

 
The only thing we need to do is to fill out the path for the train and dev .spacy files. Once done, we upload the file to Google Colab.

Now we need to auto-fill the config file with the rest of the parameters that the BERT model will need; all you have to do is run this command:


!python -m spacy init fill-config drive/MyDrive/config.cfg drive/MyDrive/config_spacy.cfg


I suggest to debug your config file in case there is an error:

!python -m spacy debug data drive/MyDrive/config.cfg



We are finally ready to train the BERT model! Just run this command and the training should start:


!python -m spacy train -g 0 drive/MyDrive/config.cfg — output ./


P.S: if you get the error cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_INVALID_PTX: a PTX JIT compilation failed, just uninstall cupy and install it again and it should fix the issue.
If everything went correctly, you should start seeing the model scores and losses being updated:


BERT training on google colab

 
At the end of the training, the model will be saved under folder model-best. The model scores are located in meta.json file inside the model-best folder:

“performance”:{
“ents_per_type”:{
“DIPLOMA”:{
“p”:0.5584415584,
“r”:0.6417910448,
“f”:0.5972222222
},
“SKILLS”:{
“p”:0.6796805679,
“r”:0.6742957746,
“f”:0.6769774635
},
“DIPLOMA_MAJOR”:{
“p”:0.8666666667,
“r”:0.7844827586,
“f”:0.8235294118
},
“EXPERIENCE”:{
“p”:0.4831460674,
“r”:0.3233082707,
“f”:0.3873873874
}
},
“ents_f”:0.661754386,
“ents_p”:0.6745350501,
“ents_r”:0.6494490358,
“transformer_loss”:1408.9692438675,
“ner_loss”:1269.1254348834
}


The scores are certainly well below a production model level because of the limited training dataset, but it’s worth checking its performance on a sample job description.
 
Entity Extraction with Transformers
 
To test the model on a sample text, we need to load the model and run it on our text:

nlp = spacy.load(“./model-best”)

text = [
'''Qualifications
- A thorough understanding of C# and .NET Core
- Knowledge of good database design and usage
- An understanding of NoSQL principles
- Excellent problem solving and critical thinking skills
- Curious about new technologies
- Experience building cloud hosted, scalable web services
- Azure experience is a plus
Requirements
- Bachelor's degree in Computer Science or related field
(Equivalent experience can substitute for earned educational qualifications)
- Minimum 4 years experience with C# and .NET
- Minimum 4 years overall experience in developing commercial software
'''
]

for doc in nlp.pipe(text, disable=[""tagger"", ""parser""]):
    print([(ent.text, ent.label_) for ent in doc.ents])


Below are the entities extracted from our sample job description:

[
(""C"", ""SKILLS""),
(""#"", ""SKILLS""),
("".NET Core"", ""SKILLS""),
(""database design"", ""SKILLS""),
(""usage"", ""SKILLS""),
(""NoSQL"", ""SKILLS""),
(""problem solving"", ""SKILLS""),
(""critical thinking"", ""SKILLS""),
(""Azure"", ""SKILLS""),
(""Bachelor"", ""DIPLOMA""),
(""'s"", ""DIPLOMA""),
(""Computer Science"", ""DIPLOMA_MAJOR""),
(""4 years experience with C# and .NET\n-"", ""EXPERIENCE""),
(""4 years overall experience in developing commercial software\n\n"", ""EXPERIENCE"")
]


Pretty impressive for only using 120 training documents! We were able to extract most of the skills, diploma, diploma major, and experience correctly.
With more training data, the model would certainly improve further and yield higher scores.
 
Conclusion:
 
With only a few lines of code, we have successfully trained a functional NER transformer model thanks to the amazing spaCy 3 library. Go ahead and try it out on your use case and please share your results. Note, you can use UBIAI annotation tool to label your data, we offer free 14 days trial.
As always, if you have any comment, please leave a note below or email at admin@ubiai.tools!
 
Bio: Walid Amamou is the Founder of UBIAI, an annotation tool for NLP applications, and holds a PhD in Physics.
Original. Reposted with permission.
Related:

How to Create and Deploy a Simple Sentiment Analysis App via API
How to Apply Transformers to Any Length of Text
Great New Resource for Natural Language Processing Research and Applications"
https://www.kdnuggets.com/2020/09/data-scientist-data-problem-wrong.html,Data Scientists think data is their #1 problem. Here’s why they’re wrong.,"We tend to think it's all about the data. However, for real data science projects at real organizations in real life, there are more fundamental aspects to consider to do data science right.","comments
By James Taylor, CEO and leading authority on Digital Decisioning and delivering business impact from AI and machine learning.

I often see articles or posts that identify data integration or preparation as the key issues facing data science projects. This always puzzles me as this is not our lived experience - not what we see when we work with Fortune 500 companies adopting predictive analytics, machine learning, or AI. But I think I have figured it out. The problem is as follows:
What data scientists think counts as a ""data science project"" is not, in fact, a data science project.
Let me illustrate this with some data from a great study. Back in 2016, the Economist Information Unit did a survey on ""Broken links: Why analytics investments have yet to pay off"" and below, you see how this data appears to support the argument that data problems are #1.

Wow - pretty clear that Data integration/preparation is the biggest problem, with nearly twice as many projects reporting it as a problem as the next one.
In fact, though, this is a subset of the data from the survey. Here's the full data set:

Data integration and preparation only ranks #4. Problem definition/framing, Solution approach/design, and Action/change management all rank higher. This is our experience.
In large, established ""grown-up"" companies, data science projects fail for one or both of two reasons:

They are solving the wrong problem. They are building an analytic that is not what the business need, that will not solve a true business problem, or that is poorly designed to fit into the business context.
Because they cannot action the model they build. They can't change the business decision making to take advantage of the analytic by changing the decisions made and actions taken.

And this illustrates the problem.
The problem is that data scientists THINK their project starts with data and ends with the communication of their analysis. If that's your focus, then data is your #1 problem.
But this is not where data science projects start nor where they end. They have to start and end with the business. That means starting with a business problem - a business decision that the business wants to improve - and ending with that problem being solved - the business behaves differently (better). If that's your focus, then your problem is not data but problem definition and operationalization - making the analytic work IRL.
Here's the difference shown in those phases. On the left, what many data scientists think their projects involved, and on the right, what it really involves.

Bottom line: If your data science team is telling you that data is their #1 problem, then they're doing it wrong.
I've written about this before - check out this article on the study itself and this one on adopting decision modeling as a better way to define the problems your data science team is trying to solve. You might also like our recent white paper and videos on Building an Analytic Enterprise.
Original. Reposted with permission.
Related:

Top 10 Statistics Mistakes Made by Data Scientists
Common mistakes when carrying out machine learning and data science
What is the most important question for Data Science (and Digital Transformation)"
https://www.kdnuggets.com/2021/01/data-engineering-troublesome.html,"Data Engineering — the Cousin of Data Science, is Troublesome","A Data Scientist must be a jack of many, many trades. Especially when working in broader teams, understanding the roles of others, such as data engineering, can help you validate progress and be aware of potential pitfalls. So, how can you convince your analysts to realize the importance of expanding their toolkit? Examples from real life often provide great insight.","comments
By Lissie Mei, Data Scientist at Visa.
We always deem data science as the “sexiest job of the 21st century.” When it comes to the transformation from a traditional company to an analytical company, either the company or the data scientists would expect to dive into the fancy world of analytics as soon as possible. But, is it always the case?
 
A troublesome start
 
Ever since we, a Practicum team from UC Davis, started the collaboration with Hilti, a leading manufacturing company in power tools and related services, we had provisioned several splendid blueprints: automation of pricing, propensity model…Working with such a great company was such a precious opportunity for us that we could barely wait to exploit our analytical skills to create business value. But when we started to tap into the data, we found that it is hard for us to directly acquire clean and structured data from a traditional company, as compared to from a data-driven company such as e-commerce companies.
As I was mainly responsible for the data cleaning and engineering of the project, I witnessed how we were hindered in the analytical progress due to the unready data.

I witnessed how we were hindered in the analytical progress due to the unready data.
We were directly working with the finance team, but the other team, pricing operations, was actually taking charge of the database. In the beginning, the process was heavily lagged because we could barely request and inquire about the data or people in time. Moreover, as the sales data of Hilti was sensitive and the company lacked a secure way to transfer data, a time-consuming masking process was needed upon every request of data. Thirdly, weak data engineering led to the inconsistency among several referring tables, and we could barely proceed with a solid model or conclusion. Finally, we have to deal with various data types: CSV, JSON, SQLite, etc…Indeed a good chance to learn, though.
After around two months, we got all the data ready, and every anomaly case was discussed and solved.
 
Diving time!
 
Our well-developed frames of visualizations and models couldn’t wait to have a taste of the fresh data. However, the most embarrassing thing happened when we were presenting the first proposal with actual figures.

Guess what? The big numbers didn’t seem to match. After a quick discussion, we realized that we didn’t receive the complete data at all. We were only focusing on the detail of data, such as anomalies and relationships among data sources, but we forgot to do the basic checks such as sum and count. This is a lesson that I would remember for a lifetime. Truly!
 
Why data engineering is so important
 
The most important thing that I learned from the data engineering drama is that the kind of roles working behind the scene, such as data engineers, are actually holding the gateway of innovation. When a traditional company considers exploiting their data, the most efficient and first-step action should be improving the data engineering process. With good data engineers, the company can build a healthy and scalable data pipeline making it much easier for data analysts to carry out data mining and finding business insights.

I also learned that why a lot of companies require their data analysts to have knowledge in programming related tools, such as Python and Scala, apart from analytics tools such as SQL and Excel. Usually, we cannot expect a “full-stack” analyst, but it is necessary that we have someone who can communicate with both engineering people and management people. Although a clear allocation of work is important for high efficiency, a guru of every data tool is indeed attractive.

Full-stack… makes sense!
What I am expecting myself to learn in the future is the knowledge of both the front side and the backside, such as Java, JavaScript, Kafka, Spark, and Hive, and I believe eventually, they would be the sparkling point in my experience.
Original. Reposted with permission.
Related:

How to Think Like a Data Scientist
How to Build Disruptive Data Science Teams: 10 Best Practices
On Building Effective Data Science Teams"
https://www.kdnuggets.com/2021/07/abstraction-data-science-not-great-combination.html,Abstraction and Data Science: Not a great combination,The article is about too much abstraction and how this programming concept when extended to Data Science makes Data Science non-intuitive.,"comments
By Venkat Raman, Co-Founder at Aryma Labs


Image by author (adapted from Francois Chollet's Prgramming vs ML paradigm diagram)

 
Abstraction - some succinct definitions.

“Abstraction is the technique of hiding implementation by providing a layer over the functionality.”
""Abstraction, as a process, denotes the extracting of the essential details about an item, or a group of items, while ignoring the inessential details""
""Abstraction - Its main goal is to handle complexity by hiding unnecessary details from the user""

Abstraction as a concept and implementation in software engineering is good. But when extended to Data Science and overdone, becomes dangerous.
Recently, the issue of sklearn’s default L2 penalty in its logistic regression algorithm came up again.
This issue was first discovered in 2019 by Zachary Lipton.

 
On the same issue, an excellent blog titled ‘Scikit-learn’s Defaults are wrong’ was written by W.D. Here is the link to that article.
This article IMHO is a must read for any serious Data Scientist.
While the author of the article has excellently captured the design pattern flaws, I would just like to build on it and add the problem of ‘Too much abstraction’.
In one of my recent posts, I had highlighted how abstracting away GLM in sklearn’s logistic regression makes large number of people believe that Regression in Logistic Regression is merely a misnomer and it has nothing to do with Regression!!
Below is an image from that article highlighting the issue.

 
So, why is ‘Too much abstraction’ a problem in Data Science?
 
I took the liberty of modifying Francois chollets famous diagram on difference between traditional programming and ML to drive home some important points regarding ‘too much abstraction’.

 
Firstly, in normal programming, if you do abstraction, you just abstract away the fixed rules. This works out fine in software development realm as you don’t want certain people to tinker around ‘fixed rules’ or they simply don’t care ‘how things work under the hood’.
But in Data science, if you do too much abstraction, you are also abstracting away the intuition of how the algorithm works and most importantly you are hiding away the knobs and levers necessary to tweak the model.
 
In Data science, if you do 'too much abstraction', you are also abstracting away the intuition of how the algorithm works and most importantly you are hiding away the knobs and levers necessary to tweak the model.

Let’s not forget that the role of data scientist is to develop intuition of how the algorithms works and then tweak the relevant knobs/ levers to make the model a right fit to solve business problem.
Taking this away from Data Scientists is just counter intuitive.
These aside, there are other pertinent questions on ‘too much abstraction’.
Let’s revisit one of the Abstraction definitions from above: “Abstraction - Its main goal is to handle complexity by hiding unnecessary details from the user.”
When it comes to data science libraries or low code solutions, the question arises who decides ‘what is unnecessary’? Who decides which knobs and levers a user can or can’t see and tweak?
Are the people making these decisions well trained in Statistics and machine learning concepts? or are they coming from a purely programming background?
In this regard I can’t help but loan some apt excerpts from W.D ‘s article ""One of the more common concerns you’ll hear–not only from formally trained statisticians, but also DS and ML practitioners–is that many people being churned through boot camps and other CS/DS programs respect neither statistics nor general good practices for data management"".
On the user side in Data Science, here are the perils of using libraries or low code solutions with ‘too much abstraction’.

Nobody knows the statistical/ML knowledge level of the user or the training they may or may not have had.
At the hands of a person with poor stats/ML knowledge these are just 'execute the lines with closed eyes' and see the magic happen.

The dangers of doing Data Science wrongly just becomes that much exacerbated. Not to mention ‘You don’t need math for ML’ and ‘Try all models’ kind of articles encouraging people to do data science without much diligence. Any guesses for what could go wrong ?
Data science is not some poem that it can be interpreted in any which way. There is a definitive right and wrong way to do data science and implement data science solutions.
Also, Data Science is just not about predictions. How these predictions are made and what ingredients led to those predictions also matter a lot. ‘Too much abstraction’ abstracts out these important parts too.
 
Read the Documentation
 
Coming to defense of these ‘too much abstracted’ libraries and solutions, some remark the user should ‘Read the documentation carefully and in detail’.
Well not many have the time and most importantly some low code solutions and libraries are sold on the idea of ‘Perform ML in 2-3 lines of code’ or ‘Do modelling faster’.
So again, referencing W.D, ‘read the doc is a cop-out’. Especially if it comes from low code solution providers.
 
A Bigger Problem to Ponder Upon
 
Having said all this, Sklearn is still by and large a good library for Machine Learning. The problem of L2 default might be one of the very few flaws.
However, I would urge the readers to ponder over this:
If abstracting away some details in one ML algorithm could cause so much issues, imagine what abstracting away details from dozen or so ML algorithms in a single line of code could result in. Some low code libraries do exactly that.

If abstracting away some details in one ML algorithm could cause so much issues, imagine what abstracting away details from dozen or so ML algorithms in a single line of code could result in!

I am not against abstraction or automation per say. My concern is only with ‘too much abstraction’ . And I don’t have a concrete answer for how to tackle ‘too much abstraction’ in data science libraries. One can only wonder if there is even a middle ground.
But one thing is very clear. The issues of ‘too much abstraction’ in Data Science are real.
The more one abstracts away, the more is the chance of doing data science wrongly.
Perhaps all we can do is, be wary of low code solutions and libraries. Caveat emptor. 
 
Bio: Venkat Raman is Co-Founder at Aryma Labs.
Original. Reposted with permission.
Related:

5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist
Managing Your Reusable Python Code as a Data Scientist
Unleashing the Power of MLOps and DataOps in Data Science"
https://www.kdnuggets.com/2020/10/ethics-ai-qa-farzindar.html,The Ethics of AI,Marketing scientist Kevin Gray asks Dr. Anna Farzindar of the University of Southern California about a very important subject - the ethics of AI.,"comments
By Kevin Gray and Dr. Anna Farzindar


Original art by Dr. Anna Farzindar

 
Kevin Gray: AI has become part of our daily lives, hasn’t it!
Dr. Anna Farzindar: I was working on my laptop when my college daughter said “Mom please don’t do anything wrong with AI!” Then two days later during our family dinner, my younger freshman high school daughter told a story about a video on social media showing a small home care robot that tricked the owner and lied. She asked me “Mom, aren’t you afraid of robots?”
These short conversations made me think about how the new generation is a big consumer of technology but, at the same time, they are concerned and worried about the future AI.
 
KG: Getting back to basics, what is AI?
AF: From talking to your virtual assistance on smartphone (like SIRI), watching a recommended movie on Netflix, searching on Google, following the suggested Instagram posts, using the sophisticated methods of an auto trading stock market, applying the decision making systems for your loan approval, or (soon) sitting in a self-driving car, AI algorithms are so embedded in our daily life that is hard to imagine living a single day without them! AI is like our closest friend who serves us. But is AI our best friend who looking our interests, or could it turn to an enemy?
Artificial intelligence (AI) is the field of computer science which creates human-like intelligence, even with the capacity of predicting the future. AI algorithms give machines the ability of performing tasks by learning from experience and data then refining their learning from new input to adapt to new situations.
Intelligent robots and simulations of the human brain have been the topic of fiction for decades. Is it true that AI will take over the human race and become a danger to humanity? How many people will lose their jobs because of robots? Will smart phones keep children occupied for hours and even replace parenthood? Is AI an imminent threat to humankind? Who actually controls AI and what are their goals? 
It is time for everyone to be aware of the impact and ethics of artificial intelligence no matter if we are a user of technology or a programmer who creates the algorithms. The question is where we are heading as humans in this AI world.
 
KG: Can AI really become as intelligent as humans? 
AF: Based on the Oxford dictionary the definition of intelligence is: the ability to learn, understand and think in a logical way about things; the ability to do this well. Considering this definition of intelligence if a machine can learn and perform the tasks in a logical way then it could be intelligent!
In 1950 Alan Turing, an English mathematician and computer scientist, proposed a method to determine if a machine can think like a human. In this test one person as a questioner communicates in natural language with two separate rooms to get an answer, one with a computer and one with a human. After a number of questions, if the human evaluator can’t decide whether the answers came from a human or AI for half of the test runs, then the machine is said to have passed the test. 
Some researchers showed that AI can pass Turing test in many complex natural language processing (NLP) tasks like machine translation and automatic summarization, simulating a human in 60% of the cases.
To assess the quality of AI performance we need to define the evaluation metrics.
Intrinsic evaluations directly judge the quality of output of a machine by direct analyses in terms of some set of norms. For example, in stock market prediction we can train the model on historical financial datasets (e.g. for the past ten years) and make a prediction by reading a window of data from the last 50 days to determine next day’s opening price. The intrinsic evaluation will measure how the predicted price is different from actual price.
Extrinsic evaluations assess the performance of AI system components based on how they affect the completion of some other tasks. In our stock market prediction example, in addition to predicting the stock price, if we analyze several aspects such as the positive and negative sentiment from tweets, impact of the news and measure other financial technical indicators, then all of these components could be evaluated to determine the performance of the whole system. 
In some applications AI performance is good enough for specific tasks that have been requested. But in other applications, AI execution is largely superior to humans because of access to the big data, power of computing machines, speed of processing, powerful algorithms, contribution of team of engineers and billions of dollars of investment to create AI systems.
 
KG: What is emotional intelligence? 
AF: Emotional intelligence is the ability to perceive, understand emotions and integrate emotion to facilitate thought and responses to a task. Some AI applications are designed for interpretation of human emotion. For example, AI can help the company to understand their customers better by measuring the consumer’s emotion from the reviews or social media. Also, AI can analyze spoken expressions from calls to customer service and find voice patterns. Some systems have the ability for automatic recognition of the facial expressions from videos and capturing the emotional reactions in real time.
But there are some biases in emotional AI technologies used to interpret the human emotion. For example, the system could assign more negative emotions to people of certain ethnicities or cultures that could mislead an organization in understanding customer satisfaction and, consequently, make wrong decisions.
 
KG: How dangerous really is AI? What are some of the ways it now is and potentially could be seriously abused? 
AF: Most of the AI algorithms are developed with poorly specified goals but are rapidly deployed in a large-scale environment. The impact of ""super intelligent"" machines is unknown and it could be difficult or impossible for humans to control it. From the past experiences such as the atomic scientists leading to a catastrophic event, there is an urgent need to take responsibility for AI before humans are surprised by their creation.
Therefore, there is a necessity to develop specific legal and ethical guidelines for everyone: for technology developers to be aware of the algorithms and biases, for industry on to how use AI and connect the data in their various applications with different objectives, for governments to assist them better in planning for AI, and for consumers to be aware of AI and its decision-making in transparent and explainable processes.
We could create unconscious and implicit biases in the algorithms. Imagine a brilliant young computer scientist from the Middle East who applies for a job and sends her resume to companies but gets no answers from HR. This is one of the current inequalities in the field of AI, which is the use of machine learning in recruitment for candidate assessment and preselection. 
If there are no records of past Middle Eastern female engineers in the company, then the model will never be able to select a candidate with this profile! In this case, AI made an unfair decision by rejecting a qualified candidate. “Fairness” is the behavior of AI models without privileging or discriminating against an individual or group of users, for example, based on their gender or race. 
 
KG: What can we do to prevent abuse of AI? Are there things ordinary citizens can do?
AF: The spreading of false information and fake news is an important case of abuse of AI. Recently “DeepFake” apps raised concerns regarding candidates for the 2020 US election. DeepFake, a combination for “deep learning” and “fake,” refers to AI software that can merge a digital face onto an existing video and audio of a person. 
The manipulation of behavior of AI could affect the vulnerable groups like teenagers, specific races or ethnic minorities by unintentionally promoting hate by AI systems. For example, recently “Black Lives Matter” drew attention to the problem of racism, but some people against this movement posted the racist videos on social media. When a video gets a certain number of views, the AI model promotes it as interesting content for more exposure to viewers, resulting in more hate in society. Also, fake news and misinformation about COVID-19 are another example of abuse of AI. Social media allows people across the globe to spread false information and conspiracy theories. AI is not intelligent enough to distinguish what are good values and bad values for human beings, but people can.
Also, it is important to study the long-term effect of AI, for example the impact on the next generation. Some concerns are the possibility of addiction to Instagram or social media, raising pampered children with physical and mental problems, and lost connections between the parents and children. 
AI ethics must go beyond the news headlines and theoretical discussions. It is imperative to develop the capacity to learn about the consequences of our work as developers of AI systems, or as consumers of such technologies in influencing the wider world. 
 
KG: Could you elaborate more on the ethics of AI? 
AF: The ethics of AI concern the moral obligation of tasks performed by machines and how it impacts on humans. For example, many companies integrate AI to improve their business by collecting data on users’ behaviors and analyzing patterns. Do they use the data or sell them to third parties in a responsible manner? What are the guidelines for making ethical choices by industry?
The ethics includes different aspects of AI such as fairness, bias in AI, explainable artificial intelligence, manipulation of behavior, human-robot interaction, AI safety, adversarial attack, and data privacy. 

Fairness is the behavior of AI without privileging one arbitrary group of users over others, e.g. based on their age, gender or race. For example, in the hiring process many AI systems fail to give the equal opportunity to some candidates.
Bias in AI is the errors in a system with unfair outcomes. For example, some facial recognition algorithms falsely identified African-American and Asian faces more than white faces due to bias and lack of data in training the machine learning.
Explainable and Transparent AI means how easily the results of a machine can be understood by humans. Especially with recent AI technologies and deep learning techniques, it is very hard to understand how the system predicts the future using the millions of patterns from the past experience and makes an automatic decision based on it. For example, applications for a credit card or loan approval by AI is hard to track in a transparent way.
Manipulation of Behavior could happen in business or gaming for example. Most social media platforms and games are designed based on human psychology to leave users to “feel” in control. In gambling the system gives the “illusion of control” to users.
Ethics of Human-Robot Interaction means AI can be used to manipulate humans into believing and doing things. For example, elderly care must clarify the purpose the robot will serve.
AI safety can be defined as the effort to ensure that AI is deployed in ways that do not harm and humans can control it.  For example, in designing and deploying self-driving cars human safety must be the primary objective.
Ethical Adversaries - Adversaries are inputs to AI models that the attackers have intentionally designed to push AI decision-making towards making mistakes. Then AI will perform unexpected behaviors and make unfair decisions. For example, by adding some noise and wrong data into the input that would cause error, similar to optical illusions for machines. 
Data Privacy is responsibly collecting, using and storing data pertaining to an individual or group. Data ethics is doing the right thing with data for human and society. For example, when an organization processes personal data and sells these data to a third party, they must be responsible for data protection when the third party is vulnerable to security or violations of privacy.

 
KG: I found Stuart Russell’s Human Compatible: Artificial Intelligence and the Problem of Control very informative. Are there other books, articles or podcasts you can recommend to those who’d like to learn more about this topic?
AF: I am giving a webinar on Ethics of AI on October 21st, 2020 and the video will be available on Youtube. 
There are some resources for better understanding of AI such as: Artificial intelligence and life in 2030 
There are several interesting TED talks about Ethics of AI, such as: Stuart Russell: 3 principles for creating safer AI
More details about the Ethics of Artificial Intelligence and Robotics could be found at: https://plato.stanford.edu/entries/ethics-ai/
Some information about Bias in AI could be eye-opener such as: The Truth About Algorithms by Cathy O'Neil
I think it is time to create a new scientific field to investigate Artificial Intelligence (AI) and its influence on people, their communities, society, and humanity. A discipline to study short-term and long-term goals and impacts of AI which could be called AIology!
KG: Thank you, Anna!
 
Kevin Gray is President of Cannon Gray, a marketing science and analytics consultancy.
Anna Farzindar, Ph.D. is a faculty member of the Department of Computer Science, Viterbi School of Engineering, University of Southern California. Her Instagram art page is https://www.instagram.com/annafarzindar and her personal website is www.farzindar.com.
About the painting: 
Title:  Ethics of Technology
Technique: watercolor on rice paper
Size: 37in x 25in
Year:2020
Artist: Anna Farzindar
 
Related:

Free From Stanford: Ethical and Social Issues in Natural Language Processing
Automatic Text Summarization in a Nutshell
Recommender Systems in a Nutshell"
https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html,A Beginner’s Guide to the CLIP Model,"CLIP is a bridge between computer vision and natural language processing. I'm here to break CLIP down for you in an accessible and fun read! In this post, I'll cover what CLIP is, how CLIP works, and why CLIP is cool.","comments
By Matthew Brems, Growth Manager @ Roboflow
You may have heard about OpenAI's CLIP model. If you looked it up, you read that CLIP stands for ""Contrastive Language-Image Pre-training."" That doesn't immediately make much sense to me, so I read the paper where they develop the CLIP model – and the corresponding blog post.
I'm here to break CLIP down for you in a – hopefully – accessible and fun read! In this post, I'll cover:

what CLIP is,
how CLIP works, and
why CLIP is cool.

 
What is CLIP?
 
CLIP is the first multimodal (in this case, vision and text) model tackling computer vision and was recently released by OpenAI on January 5, 2021. From the OpenAI CLIP repository, ""CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3.""
Depending on your background, this may make sense -- but there's a lot in here that may be unfamiliar to you. Let's unpack it.

CLIP is a neural network model.
It is trained on 400,000,000 (image, text) pairs. An (image, text) pair might be a picture and its caption. So this means that there are 400,000,000 pictures and their captions that are matched up, and this is the data that is used in training the CLIP model.
""It can predict the most relevant text snippet, given an image."" You can input an image into the CLIP model, and it will return for you the likeliest caption or summary of that image.
""without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3."" Most machine learning models learn a specific task. For example, an image classifier trained on classifying dogs and cats is expected to do well on the task we've given it: classifying dogs and cats. We generally would not expect a machine learning model trained on dogs and cats to be very good at detecting raccoons. However, some models -- including CLIP, GPT-2, and GPT-3 -- tend to perform well on tasks they aren't directly trained to do, which is called ""zero-shot learning.""
""Zero-shot learning"" is when a model attempts to predict a class it saw zero times in the training data. So, using a model trained on exclusively cats and dogs to then detect raccoons. A model like CLIP, because of how it uses the text information in the (image, text) pairs, tends to do really well with zero-shot learning -- even if the image you're looking at is really different from the training images, your CLIP model will likely be able to give a good guess for the caption for that image.

To put this all together, the CLIP model is:

a neural network model built on hundreds of millions of images and captions,
can return the best caption given an image, and
has impressive ""zero-shot"" capabilities, making it able to accurately predict entire classes it's never seen before!

When I wrote my Introduction to Computer Vision post, I described computer vision as ""the ability for a computer to see and understand what it sees in manner similar to humans.""
When I've taught natural language processing, I described NLP in a similar way: ""the ability for a computer to understand language in manner similar to humans.""
CLIP is a bridge between computer vision and natural language processing.
It's not just a bridge between computer vision and natural language processing -- it's a very powerful bridge between the two that has a lot of flexibility and a lot of applications.
 
How does CLIP work?
 
In order for images and text to be connected to one another, they must both be embedded. You've worked with embeddings before, even if you haven't thought of it that way. Let's go through an example. Suppose you have one cat and two dogs. You could represent that as a dot on a graph, like below:


Embedding of ""1 cat, 2 dogs."" (Source.)

 
It may not seem very exciting, but we just embedded that information on the X-Y grid that you probably learned about in middle school (formally called Euclidean space). You could also embed your friends' pet information on the same graph and/or you could have chosen plenty of different ways to represent that information (e.g. put dogs before cats, or add a third dimension for raccoons).
I like to think of embedding as a way to smash information into mathematical space. We just took information about dogs and cats and smashed it into mathematical space. We can do the same thing with text and with images!
The CLIP model consists of two sub-models called encoders:

a text encoder that will embed (smash) text into mathematical space.
an image encoder that will embed (smash) images into mathematical space.

Whenever you fit a supervised learning model, you have to find some way to measure the ""goodness"" or the ""badness"" of that model – the goal is to fit a model that is as ""most good"" and ""least bad"" as possible.
The CLIP model is no different: the text encoder and image encoder are fit to maximize goodness and minimize badness.
So, how do we measure ""goodness"" and ""badness?""
In the image below, you'll see a set of purple text cards going into the text encoder. The output for each card would be a series of numbers. For example, the top card, pepper the aussie pup would enter the text encoder – the thing smashing it into mathematical space – and come out as a series of numbers like (0, 0.2, 0.8).
The exact same thing will happen for the images: each image will go into the image encoder and the output for each image will also be a series of numbers. The picture of, presumably Pepper the Aussie pup, will come out like (0.05, 0.25, 0.7).


The pre-training phase. (Source.)

 
""Goodness"" of our model
In an ideal world, the series of numbers for the text ""pepper the aussie pup"" will be very close (identical) to the series of numbers for the corresponding image. In fact, this should be the case everywhere: the series of numbers for the text should be very close to the series of numbers for the corresponding image. One way for us to measure ""goodness"" of our model is how close the embedded representation (series of numbers) for each text is to the embedded representation for each image.
There is a convenient way to calculate the similarity between two series of numbers: the cosine similarity. We won't get into the inner workings of that formula here, but rest assured that it's a tried and true method of seeing how similar two vectors, or series of numbers, are. (Though it isn't the only way!)
In the image above, the light blue squares represent where the text and image coincide. For example, T1 is the embedded representation of the first text; I1 is the embedded representation of the first image. We want the cosine similarity for I1 and T1 to be as high as possible. We want the same for I2 and T2, and so on for all of the light blue squares. The higher these cosine similarities are, the more ""goodness"" our model has!
""Badness"" of our model
At the same time as wanting to maximize the cosine similarity for each of those blue squares, there are a lot of grey squares that indicate where the text and image don't align. For example, T1 is the text ""pepper the aussie pup"" but perhaps I2 is an image of a raccoon.


Picture of a raccoon with bounding box annotation. (Source.)

 
Cute though this raccoon is, we want the cosine similarity between this image (I2) and the text pepper the aussie pup to be pretty small, because this isn't Pepper the Aussie pup!
While we wanted the blue squares to all have high cosine similarities (as that measured ""goodness""), we want all of the grey squares to have low cosine similarities, because that measures ""badness.""


Maximize cosine similarity of the blue squares; minimize cosine similarity of the grey squares. (Source.)

 
How do the text and image encoders get fit?
The text encoder and image encoder get fit at the same time by simultaneously maximizing the cosine similarity of those blue squares and minimizing the cosine similarity of the grey squares, across all of our text+image pairs.

Note: this can take a very long time depending on the size of your data. The CLIP model trained on 400,000,000 labeled images. The training process took 30 days across 592 V100 GPUs. This would have cost $1,000,000 to train on AWS on-demand instances!

Once the model is fit, you can pass an image into the image encoder to retrieve the text description that best fits the image – or, vice versa, you can pass a text description into the model to retrieve an image, as you'll see in some of the applications below!
CLIP is a bridge between computer vision and natural language processing.
 
Why is CLIP cool?
 
With this bridge between computer vision and natural language processing, CLIP has a ton of advantages and cool applications. We'll focus on the applications, but a few advantages to call out:

Generalizability: Models are usually super brittle, capable of knowing only the very specific thing you trained them to do. CLIP expands knowledge of classification models to a wider array of things by leveraging semantic information in text. Standard classification models completely discard the semantic meaning of the class labels and simply  enumerated numeric classes behind the scenes; CLIP works by understanding the meaning of the classes.
Connecting text / images better than ever before: CLIP may quite literally be the ""world's best caption writer"" when considering speed and accuracy together.
Already-labeled data: CLIP is built on images and captions that were already created; other state-of-the-art computer vision algorithms required significant additional human time spent labeling.



Why does @OpenAI's CLIP model matter?https://t.co/X7bnSgZ0or
— Joseph Nelson (@josephofiowa) January 6, 2021
 
 
Some of the uses of CLIP so far:

CLIP has been used to index photos on sites like Unsplash.
One Twitter user took celebrities including Elvis Presley, Beyoncé, and Billie Eilish, and used CLIP and StyleGAN to generate portraits in the style of ""My Little Pony.""
Have you played Pictionary? Now you can play online at paint.wtf, where you'll be judged by CLIP.
CLIP could be used to easily improve NSFW filters!
Find photos matching a mood – for example, via a poetry passage.
OpenAI has also created DALL-E, which creates images from text.

We hope you'll check out some of the above – or create your own! We've got a CLIP tutorial for you to follow. If you do something with it, let us know so we can add it to the above list!
It's important to note that CLIP is a bridge between computer vision and natural language processing. CLIP is not the only bridge between them. You could build those text and image encoders very differently or find other ways of connecting the two. However, CLIP has so far been an exceptionally innovative technique that has promoted significant additional innovation.
We're eager to see what you build with CLIP and to see the advancements that are built on top of it!
 
Bio: Matthew Brems is Growth Manager @ Roboflow.
Original. Reposted with permission.
Related:

OpenAI Releases Two Transformer Models that Magically Link Language and Computer Vision
Evaluating Object Detection Models Using Mean Average Precision
Reducing the High Cost of Training NLP Models With SRU++"
https://www.kdnuggets.com/2021/03/pandas-big-data-better-options.html,Are You Still Using Pandas to Process Big Data in 2021? Here are two better options,"When its time to handle a lot of data -- so much that you are in the realm of Big Data -- what tools can you use to wrangle the data, especially in a notebook environment? Pandas doesn’t handle really Big Data very well, but two other libraries do. So, which one is better and faster?","comments
By Roman Orac, Data Scientist.

Photo by NASA on Unsplash.
I recently wrote two introductory articles about processing Big Data with Dask and Vaex — libraries for processing bigger than memory datasets. While writing, a question popped up in my mind:
Can these libraries really process bigger than memory datasets, or is it all just a sales slogan?
This intrigued me to do a practical experiment with Dask and Vaex and try to process a bigger than memory dataset. The dataset was so big that you cannot even open it with pandas.
 
What do I mean by Big Data?
 

Photo by ev on Unsplash.
Big Data is a loosely defined term, which has as many definitions as there are hits on Google. In this article, I use the term to describe a dataset that is so big that we need specialized software to process it. With Big, I am referring to “bigger than the main memory on a single machine.”
Definition from Wikipedia:

Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software.

 
What are Dask and Vaex?
 

Photo by JESHOOTS.COM on Unsplash.
Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. This includes numpy, pandas, and sklearn. It is open-source and freely available. It uses existing Python APIs and data structures to make it easy to switch between Dask-powered equivalents.
Vaex is a high-performance Python library for lazy Out-of-Core DataFrames (similar to Pandas) to visualize and explore big tabular datasets. It can calculate basic statistics for more than a billion rows per second. It supports multiple visualizations allowing interactive exploration of big data.
Dask and Vaex Dataframes are not fully compatible with Pandas Dataframes, but some most common “data wrangling” operations are supported by both tools. Dask is more focused on scaling the code to compute clusters, while Vaex makes it easier to work with large datasets on a single machine.
 
The Experiment
 

Photo by Louis Reed on Unsplash.
I generated two CSV files with 1 million rows and 1000 columns. The size of a file was 18.18 GB, which is 36.36 GB combined. Files have random numbers from a Uniform distribution between 0 and 100.

Two CSV files with random data. Photo made by the author.
 

import pandas as pd
import numpy as np
from os import path
n_rows = 1_000_000
n_cols = 1000
for i in range(1, 3):
    filename = 'analysis_%d.csv' % i
    file_path = path.join('csv_files', filename)
    df = pd.DataFrame(np.random.uniform(0, 100, size=(n_rows, n_cols)), columns=['col%d' % i for i in range(n_cols)])
    print('Saving', file_path)
    df.to_csv(file_path, index=False)
df.head()



 

Head of a file. Photo made by the author.
The experiment was run on a MacBook Pro with 32 GB of main memory — quite a beast. When testing the limits of a pandas Dataframe, I surprisingly found that reaching a Memory Error on such a machine is quite a challenge!
macOS starts dumping data from the main memory to SSD when the memory is running near its capacity. The upper limit for pandas Dataframe was 100 GB of free disk space on the machine.

When your Mac needs memory, it will push something that isn’t currently being used into a swapfile for temporary storage. When it needs access again, it will read the data from the swap file and back into memory.

I’ve spent some time thinking about how I should address this issue so that the experiment would be fair. The first idea that came to my mind was to disable swapping so that each library would have only the main memory available — good luck with that on macOS. After spending a few hours, I wasn’t able to disable swapping.
The second idea was to use a brute force approach. I’ve filled the SSD to its full capacity so that the operating system couldn’t use swap as there was no free space left on the device.

Your disk is almost full notification during the experiment. Photo made by the author.
This worked! pandas couldn’t read two 18 GB files, and Jupyter Kernel crashed.
If I performed this experiment again, I would create a virtual machine with less memory. That way, it would be easier to show the limits of these tools.
Can Dask or Vaex help us and process these large files? Which one is faster? Let’s find out.
 
Vaex vs. Dask
 

Photo by Frida Bredesen on Unsplash.
When designing the experiment, I thought about basic operations when performing Data Analysis, like grouping, filtering, and visualizing data. I came up with the following operations:

calculating 10th quantile of a column,
adding a new column,
filtering by column,
grouping by column and aggregating,
visualizing a column.

All of the above operations perform a calculation using a single column, e.g.:

# filtering with a single column
df[df.col2 > 10]



 
So I was intrigued to try an operation, which requires all data to be processed:

calculate the sum of all of the columns.

This can be achieved by breaking down the calculation into smaller chunks. E.g., reading each column separately and calculating the sum, and in the last step calculating the overall sum. These types of computational problems are known as Embarrassingly parallel — no effort is required to separate the problem into separate tasks.
 
Vaex
 

Photo by Photos by Lanty on Unsplash.
Let’s start with Vaex. The experiment was designed in a way that follows best practices for each tool — this is using binary format HDF5 for Vaex. So we need to convert CSV files to HDF5 format (The Hierarchical Data Format version 5).

import glob
import vaex
csv_files = glob.glob('csv_files/*.csv')
for i, csv_file in enumerate(csv_files, 1):
    for j, dv in enumerate(vaex.from_csv(csv_file, chunk_size=5_000_000), 1):
        print('Exporting %d %s to hdf5 part %d' % (i, csv_file, j))
        dv.export_hdf5(f'hdf5_files/analysis_{i:02}_{j:02}.hdf5')



 
Vaex needed 405 seconds to covert two CSV files (36.36 GB) to two HDF5 files, which have 16 GB combined. Conversion from text to binary format reduced the file size.
Open HDF5 dataset with Vaex:

dv = vaex.open('hdf5_files/*.hdf5')



 
Vaex needed 1218 seconds to read the HDF5 files. I expected it to be faster as Vaex claims near-instant opening of files in binary format.
From Vaex documentation:

Opening such data is instantenous regardless of the file size on disk: Vaex will just memory-map the data instead of reading it in memory. This is the optimal way of working with large datasets that are larger than available RAM.

Display head with Vaex:

dv.head()



 
Vaex needed 1189 seconds to display head. I am not sure why displaying the first 5 rows of each column took so long.
Calculate 10th quantile with Vaex:
Note, Vaex has percentile_approx function, which calculates an approximation of quantile.

quantile = dv.percentile_approx('col1', 10)



 
Vaex needed 0 seconds to calculate the approximation of the 10th quantile for the col1 column.
Add a new column with Vaex:

dv[‘col1_binary’] = dv.col1 > dv.percentile_approx(‘col1’, 10)



 
Vaex has a concept of virtual columns, which stores an expression as a column. It does not take up any memory and is computed on the fly when needed. A virtual column is treated just like a normal column. As expected, Vaex needed 0 seconds to execute the command above.
Filter data with Vaex:
Vaex has a concept of selections, which I didn’t use as Dask doesn’t support selections, which would make the experiment unfair. The filter below is similar to filtering with pandas, except that Vaex does not copy the data.

dv = dv[dv.col2 > 10]



 
Vaex needed 0 seconds to execute the filter above.
Grouping and aggregating data with Vaex:
The command below is slightly different from pandas as it combines grouping and aggregation. The command groups the data by col1_binary and calculate the mean for col3:

group_res = dv.groupby(by=dv.col1_binary, agg={'col3_mean': vaex.agg.mean('col3')})



 

Calculating mean with Vaex. Photo made by the author.
Vaex needed 0 seconds to execute the command above.
Visualize the histogram:
Visualization with bigger datasets is problematic as traditional tools for data analysis are not optimized to handle them. Let’s try if we can make a histogram of col3 with Vaex.

plot = dv.plot1d(dv.col3, what='count(*)', limits=[0, 100])



 

Visualizing data with Vaex. Photo made by the author.
Vaex needed 0 seconds to display the plot, which was surprisingly fast.
Calculate the sum of all columns
Memory is not an issue when processing a single column at a time. Let’s try to calculate the sum of all the numbers in the dataset with Vaex.

suma = np.sum(dv.sum(dv.column_names))



 
Vaex needed 40 seconds to calculate the sum of all columns.
 
Dask
 

Photo by Kelly Sikkema on Unsplash.
Now, let’s repeat the operations above but with Dask. The Jupyter Kernel was restarted before running Dask commands.
Instead of reading CSV files directly with Dask’s read_csv function, we convert the CSV files to HDF5 to make the experiment fair.

import dask.dataframe as dd
ds = dd.read_csv('csv_files/*.csv')
ds.to_hdf('hdf5_files_dask/analysis_01_01.hdf5', key='table')



 
Dask needed 763 seconds for conversion. Let me know in the comments if there is a faster way to convert the data with Dask. I tried to read the HDF5 files that were converted with Vaex with no luck.
Best practices with Dask:

HDF5 is a popular choice for Pandas users with high performance needs. We encourage Dask DataFrame users to store and load data using Parquet instead.

Open HDF5 dataset with Dask:

import dask.dataframe as dd

ds = dd.read_csv('csv_files/*.csv')



 
Dask needed 0 seconds to open the HDF5 file. This is because I didn’t explicitly run the compute command, which would actually read the file.
Display head with Dask:

ds.head()



 
Dask needed 9 seconds to output the first 5 rows of the file.
Calculate the 10th quantile with Dask:
Dask has a quantile function, which calculates actual quantile, not an approximation.

quantile = ds.col1.quantile(0.1).compute()



 
Dask wasn’t able to calculate quantile as Juptyter Kernel crashed.
Define a new column with Dask:
The function below uses the quantile function to define a new binary column. Dask wasn’t able to calculate it because it uses quantile.

ds['col1_binary'] = ds.col1 > ds.col1.quantile(0.1)



 
Filter data with Dask:

ds = ds[(ds.col2 > 10)]



 
The command above needed 0 seconds to execute as Dask uses the delayed execution paradigm.
Grouping and aggregating data with Dask:

group_res = ds.groupby('col1_binary').col3.mean().compute()



 
Dask wasn’t able to group and aggregate the data.
Visualize the histogram of col3:

plot = ds.col3.compute().plot.hist(bins=64, ylim=(13900, 14400))



 
Dask wasn’t able to visualize the data.
Calculate the sum of all columns:

suma = ds.sum().sum().compute()



 
Dask wasn’t able to sum all the data.
 
Results
 
The table below shows the execution times of the Vaex vs. Dask experiment. NA means that the tool couldn’t process the data, and Jupyter Kernel crashed.

Summary of execution times in the experiment. Photo made by the author.
 
Conclusion
 

Photo by Joshua Golde on Unsplash.
Vaex requires conversion of CSV to HDF5 format, which doesn’t bother me as you can go to lunch, come back, and the data will be converted. I also understand that in harsh conditions (like in the experiment) with little or no main memory reading data will take longer.
What I don’t understand is the time that Vaex needed to display the head of the file (1189 seconds for the first 5 rows!). Other operations in Vaex are heavily optimized, which enables us to do interactive data analysis on bigger than main memory datasets.
I kinda expected the problems with Dask as it is more optimized for compute clusters instead of a single machine. Dask is built on top of pandas, which means that operations that are slow in pandas, stay slow in Dask.
The winner of the experiment is clear. Vaex was able to process bigger than the main memory file on a laptop while Dask couldn’t. This experiment is specific as I am testing performance on a single machine, not a compute cluster.
Original. Reposted with permission.
 
Related:

Pandas on Steroids: End to End Data Science in Python with Dask
Why and How to Use Dask with Big Data
Good-bye Big Data. Hello, Massive Data!"
https://www.kdnuggets.com/2021/05/dont-need-data-engineers-need-better-tools-data-scientists.html,"We Don’t Need Data Engineers, We Need Better Tools for Data Scientists","In today's data science jobs landscape, a variety of roles are being filled from specialized engineering positions to the more generalized data scientist. However, is it possible that some of these job types are duplicative or misdirected, such as that of the Data Engineer, which might exist as we know it because of a lack of adequate tooling for Data Scientists?","comments
By Devin Petersohn, PhD Student at UC Berkeley.

In most companies, Data Engineers support Data Scientists in various ways. Often this means translating or productionizing the notebooks and scripts that a Data Scientist has written. A large portion of the Data Engineer’s role could be replaced with better tooling for Data Scientists, freeing Data Engineers to do more impactful (and scalable) work.
 
Why does this matter?
 
There’s a sentiment making its way around the internet (again): We don’t need Data Scientists, we need Data Engineers.

(source)

(source)
These articles focus on the number of available job positions for the title of “Data Engineer” vs. “Data Scientist.” Let’s put aside the fact that the hiring managers who post these positions often don’t know the difference between the two jobs and use them interchangeably (or use whatever is in style at the moment). For the sake of this article, we can take the existence of the positions at face value. The question then becomes: Is the surplus of available Data Engineer positions solely a personnel problem?
 
Data Science is messy because it reflects the real world
 
Data Scientists are domain experts (on top of knowing statistics), and they don’t often have a strong background in programming. I’ve seen this expertise discounted in multiple Twitter and forum threads, with software engineers and other “technical people” asking questions like “Why don’t they just learn Spark?” This type of mentality completely misses the fact that Data Scientists can already do what they want to do at smaller scales with their existing tools. Data Scientists want to gain insights, not worry about building elegant pipelines. Companies want something actionable, not beautiful.

Insights are more important than elegant pipelines.

Popular Data Science tools are also criticized by more technical people and academics: “Why would anyone use pandas?” pandas must be the most popular tool to hate by people who have no use for it. It is loved (or at least appreciated) by the Data Scientists who use it daily, however.
pandas, among other tools, was built to handle the messiness of the real world. Just look at how many parameters read_csv has:

(read_csv reference)
If pandas is so bad, why has nothing unseated it as the standard dataframe for Python Data Science? Why does it continue to grow in adoption year after year? It’s not the fastest, it’s not the most robust, so why?
 
Data Engineers have to handle the messiness that scalable tools can’t
 
The scalable systems (e.g., Apache Spark) that are robust enough for production use can’t handle the messiness of the real world as-is. It’s difficult to scale without clean and simple assumptions, and the messier the problem, the harder it is to scale. Data Engineers handle the messiness because scalable tools can’t.
Messiness, in this case, can mean:

Group/Join Key Skew
Partitioning
Debugging Distributed Systems
Cluster configuration and resource provisioning

None of these are things that you have to worry about with smaller-scale systems. Outside of the Bay Area, most Data Engineers spend time debugging and translating to a distributed system, usually Spark.

Multiple rewrites are necessary to turn one-time insights into production jobs.
We can’t really fault anyone here. The people who built the scalable tools in use today were building for highly technical users like themselves. Highly technical people don’t need their tooling to handle messiness for them, and often they want knobs to tune. Dogfooding is a popular concept in system engineering: “those that built it also use it.” I think worrying so heavily about dogfooding can in part cause the landscape we are seeing in data science today: “only people as technical as those that built the system can use it.”
 
What, then, should Data Engineers do?
 
The Data Science ecosystem needs systems that don’t only focus on the problems of those building it. Data Scientists have been mostly stuck using the same or similar tools for the last 10+ years. The explanation for this is twofold: (1) Data Scientists love using their existing tools because they understand them, and (2) those who are capable of building large-scale systems have largely (unintentionally) overlooked the problems of those less technical than they.
We need Data Engineers to help build scalable tools that empower Data Scientists, not translate pandas to Spark. Who better to help build the next generation of Data Science tools than today’s Data Engineers?
Original. Reposted with permission.
 
Related:

How to Build an Impressive Data Science Resume
Why You Should Consider Being a Data Engineer Instead of a Data Scientist
Data careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space"
https://www.kdnuggets.com/2021/03/top-stories-2021-feb.html,"Top February Stories: We Don’t Need Data Scientists, We Need Data Engineers; How to create stunning visualizations using python from scratch",Also: How to Get Your First Job in Data Science without Any Work Experience; Telling a Great Data Story: A Visualization Decision Tree,"Here are the most popular KDnuggets posts in February:

Most Viewed - Platinum Badge (>30,000 UPV)

 We Don't Need Data Scientists, We Need Data Engineers, by Mihail Eric (*)
 How to create stunning visualizations using python from scratch, by Sharan Kumar R (*)
 How to Get Your First Job in Data Science without Any Work Experience, by Madison Hunter (*)
 Telling a Great Data Story: A Visualization Decision Tree, by Stan Pugsley (*)
 Data Science vs Business Intelligence, Explained, by Stan Pugsley (*)


Most Viewed - Gold Badge (>18,000 UPV)

 Powerful Exploratory Data Analysis in just two lines of code, by Francois Bertrand (*)
 Data Science Learning Roadmap for 2021, by Harshit Tyagi
 The Best Data Science Project to Have in Your Portfolio, by Soner Yildirim (*)
 Machine Learning Systems Design: A Free Stanford Course, by Matthew Mayo (*)
 Cartoon: Data Scientist vs Data Engineer, by Gregory Piatetsky (*)


Most Viewed - Silver Badge (> 7,500 UPV)

 How Reading Papers Helps You Be a More Effective Data Scientist, by Eugene Yan
 Build Your First Data Science Application, by Naser Tamimi
 Deep learning doesn't need to be a black box, by Ben Dickson
 Approaching (Almost) Any Machine Learning Problem, by Matthew Mayo
 How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals, by Emma Ding
 3 Ways Understanding Bayes Theorem Will Improve Your Data Science, by Nicole Janeway Bills




Most Shared - Platinum Badge (>1,000 shares)

 We Don't Need Data Scientists, We Need Data Engineers, by Mihail Eric


Most Shared - Gold Badge (>500 shares)

 How to Get Your First Job in Data Science without Any Work Experience, by Madison Hunter
 How to create stunning visualizations using python from scratch, by Sharan Kumar R
 Telling a Great Data Story: A Visualization Decision Tree, by Stan Pugsley
 Data Science vs Business Intelligence, Explained, by Stan Pugsley
 Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall, by Ahmed Gad (*)
 6 NLP Techniques Every Data Scientist Should Know, by Sara Metwalli (*)


Most Shared - Silver Badge (>300 shares)

 Data Science Learning Roadmap for 2021, by Harshit Tyagi
 Essential Math for Data Science: Introduction to Matrices and the Matrix Product, by Hadrien Jean (*)
 Deep learning doesn't need to be a black box, by Ben Dickson
 Machine Learning Systems Design: A Free Stanford Course, by Matthew Mayo
 Getting Started with 5 Essential Natural Language Processing Libraries, by Matthew Mayo (*)
 Build Your First Data Science Application, by Naser Tamimi


(*) indicates that badge added or upgraded based on these monthly results.

Most Shareable (Viral) Blogs
Among the top blogs, here are the blogs with the highest ratio of shares/unique views, which suggests that people who read it really liked it. 

 6 NLP Techniques Every Data Scientist Should Know, by Sara Metwalli
 Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall, by Ahmed Gad
 Getting Started with 5 Essential Natural Language Processing Libraries, by Matthew Mayo
 Essential Math for Data Science: Introduction to Matrices and the Matrix Product, by Hadrien Jean
 10 resources for data science self-study, by Benjamin Obi Tayo"
https://www.kdnuggets.com/2021/03/8-women-ai-striving-humanize-world.html,8 Women in AI Who Are Striving to Humanize the World,"Some exceptional female researchers and engineers are working on projects to make the world a better place with the help of AI, data science, and machine learning.","comments
By Liudmyla Taranenko, Data Scientist at MobiDev
 
Can artificial intelligence positively change the world?
 
Wired reports a gender bias exists in AI and, in 2018, found that only 12% of AI researchers are women. When I started my career as a Data Analyst, a Data Science engineer position was not widely available in Ukraine. And mostly, it wasn’t available for female Maths graduates without special skills and experience. Self-education and getting acquainted with ML algorithms took me some time and a lot of effort. Nowadays, I work as an AI engineer at MobiDev, and the more experience I get, the more willing I am to share my experiences with people in my articles and webinars. 
Today, I want to talk about some exceptional women working in AI who inspire me in my everyday work. 
Two of them, Joy Buolamwini and Fei-Fei Li, along with others, were honored for International Women’s Day in 2019 by KDnuggets.
 
Joy Buolamwini - Founder, Algorithmic Justice League
 
Algorithmic Justice League’s mission is to raise public awareness about AI and its impact on the reduction of AI-bias and harm. Her pioneering research, published in 2018/2019, showed how facial recognition software used by Amazon, Microsoft, IBM, and other companies was not “machine neutral.” It had a distinctly-lower performance when trying to recognize darker female faces as accurately as those of white men. Fast Company reported that the software was taken off the market for re-configuration based on Buolamwini’s research.
 
Fei-Fei Li - Professor of Computer Science, Stanford University
 
Fei-Fei Li is a Stanford professor and the founder of Stanford’s Human-Centered AI Institute. She developed ImageNet, first published in 2009, as a training tool to teach AI how to identify objects. ImageNet was considered the birth of AI-dataset training, with the first tests achieving 71.8% accuracy. Since then, the annual ImageNet challenge is a contest to see which algorithm identifies objects with the lowest error rate. The last competition was held in 2017 when the winning algorithm hit 97.3% accuracy, which is better than human abilities.
 
But here are eight other women, not yet honored by KDNuggets, who use AI in positive ways and whose contributions to make a better world are significant. The list is in alphabetical order by last name.

 
1. Monica Abarca, Co-Founder & CEO, qAIRa  – Making the Planet Cleaner
 
As the co-founder and CEO of qAIRa, Monica Abarca develops technological solutions to deal with air contamination. Dangerous levels of air contamination are responsible for over four million deaths each year. The qAIRa company combines drone technology with air-quality monitoring, using data analytics to identify air contamination, and produces real-time maps showing critical areas.
Government officials use this information to protect the public from the dangers of air-borne contaminants in an emergency, such as an industrial gas leak, natural disaster, or chemical spill. Another use for these AI-driven systems is to improve overall air quality by effectively monitoring targeted areas subject to air-pollution abatement initiatives.
 
2. Regina Barzilay, Faculty Lead & Professor, MIT  – Helping to Prevent Breast Cancer
 
Working as the faculty lead and a professor at MIT, Regina Barzilay has long been a thought leader in this AI space. She was the first person to receive the $1 million prize for the AAAI Squirrel AI Award for Artificial Intelligence for the Benefit of Humanity.
Professor Barzilay is a strong advocate for having standards that ensure equity and fairness in applying AI technology, especially in medicine. As an example, she points to the Tyrer-Cuzick model used to determine a patient’s risk of getting breast cancer by analyzing imaging data. This system has a modest accuracy with white women but fails miserably with women of African or Asian descent.
This failure happens because the software has not been adequately machine-trained on enough sets of images from racially-diverse groups. This problem is due to undesirable researcher-bias. It is readily proven and unconscionable. Professor Barzilay insists that software developers eliminate bias by validating AI software on different groups of people or by allowing it to be open source to compare system accuracy using diverse models.
 
3. Hulya Emir-Farinas, Director of Data Science, FitBit  – Researching People’s Motivation to Lead a Healthier Life
 
As the Director of Data Science at Fitbit R&D, Dr. Emir-Farinas works with AI applications using machine learning combined with behavioral science and healthcare. She is working to answer questions that include:

What motivates an individual to pursue a healthier lifestyle?
What are real and imagined barriers that prevent making a positive change?
How does Fitbit enhance its users’ ability to make lifestyle changes?

Answering such questions necessitates a multi-disciplinary approach involving behavioral economics, behavior science, health science, and machine learning to deliver the appropriate interventions, and encouragement, and to make the solution “smart” with more personalization.
 
4. Dina Machuve, Lecturer and Researcher at the Nelson Mandela African Institution of Science and Technology – Improving Agriculture
 
At the Nelson Mandela African Institution of Science and Technology, Dina Machuve is a lecturer and researcher. She focuses on creating data-driven solutions to improve agriculture. One application is a diagnostics system that helps identify diseases in poultry using bioinformatics and computer vision technology.
There are over 380 million household farms that create food for 70% of the people living in developing countries. Manchuve’s solution, now deployed in Tanzania, uses systematic data collection and analysis in small to medium sized farms. The project demonstrates the value of using AI methods of deep learning for disease diagnostics to improve livestock health. It works by collecting data for analysis in low-resourced settings from the 3.7 million households that raise chickens.
 
5. Deborah Raji, Fellow, Mozilla – Preventing Demographic Bias
 
Deborah Raji was given the 2020 Barlow award at the Electronic Frontier Foundation Pioneer Award Ceremony for her work on racial-bias in AI. Her concentration is the negative impact AI has on minorities when dealing with the American justice system. She advocates eradicating and replacing the seriously-flawed facial recognition and surveillance systems used by law enforcement in many American cities.
 
6. Tempest van Schaik, Senior Machine Learning Engineer, Microsoft – Helping Children with Cystic Fibrosis
 
As a Senior Machine Learning Engineer at Microsoft, Tempest van Schaik is on the Commercial Software Engineering team in the Data Science division for Microsoft’s Azure cloud-based services. This team is responsible for coding high-level AI projects for the cloud.
One exciting project is Fizzyo, a device that improves physiotherapy for patients with Cystic Fibrosis. It turns breathing exercises into controls for video games. This device makes the experience more fun for children undergoing therapy and provides data collection of every breath while the child plays, which leads to better treatment outcomes.
 
7. Lucy Vasserman, Staff Software Engineer, Google – Helping Animal Conservation
 
At Google, Lucy Vasserman works as a staff software engineer on innovative AI projects. Google partnered with conservation groups to use AI to study wildlife. Since the 1990s, the groups have collected 4.5 million photos of animals using camera traps. That database of images formed the initial image library for Wildlife Insights. Others can add camera trap images to help map wildlife globally and to build the growing image database. Anyone can access the database online to explore the photo and the location maps of the camera traps.
Vasserman worked on the program to train it to identify around 100 species. It can process and categorize 3,600 photos per hour. The machine learning software analyses the images to discover trends such as the population size of individual species, predator/prey interactions, and how wild animals respond to human hunting and encroachment on their habitat.
 
8. Fernanda Viégas, Principal Scientist, Google – Making Attractive Visualizations of Complex Data
 
At Google, Fernanda Viégas works as a data visualization expert. Examples of the visualization systems created by her with other Big Picture team members, can be experienced at fernandaviegas.com. She is the co-leader of Google’s PAIR (People + AI Research).
Some of her projects show wind currents, collaboration patterns of Wikipedia editorial activities, and dynamic maps of world-news events. Dr. Viégas produces highly-visual, data-driven artwork that is part of a permanent collection at the Museum of Modern Art in New York.
As a Latina who came from a non-technical background, she obtained her doctorate at the MIT Media Lab. She makes an effort to increase diversity initiatives in technological fields to reduce the underrepresentation of women and minorities in data science.
 
Conclusion
 
Women are encouraged to get involved in AI due to its world-changing qualities and to reduce the gender-bias that exists. Take inspiration from the late Justice Ruth Bader Ginsburg of the U.S. Supreme Court. When asked, “How many justices would she like to be women on the Supreme Court?” She said, “All of them.”
 
Bio: Liudmyla Taranenko is a Data Scientist at MobiDev.
Related:

19 Inspiring Women in AI, Big Data, Data Science, Machine Learning
Resources for Women in AI, Data Science, and Machine Learning
Gender Diversity in AI Research"
https://www.kdnuggets.com/2021/04/how-organize-your-data-science-project-2021.html,How to organize your data science project in 2021,"Maintaining proper organization of all your data science projects will increase your productivity, minimize errors, and increase your development efficiency. This tutorial will guide you through a framework on how to keep everything in order on your local machine and in the cloud.","By Benjamin Obi Tayo, Ph.D., DataScienceHub.
comments

It's always good idea to maintain two versions of your project, one locally and the other on Github.
This article will discuss some useful tips that will enable you to better organize your data science projects. Before delving into some tips for data science project management, let’s first discuss why it is important to organize your project.
 
4 Reasons why it is important to organize your project
 

Organization increases productivity. If a project is well organized, with everything placed in one directory, it makes it easier to avoid wasting time searching for project files such as datasets, codes, output files, and so on.
A well-organized project helps you to keep and maintain a record of your ongoing and completed data science projects.
Completed data science projects could be used for building future models. If you have to solve a similar problem in the future, you can use the same code with slight modifications.
A well-organized project can easily be understood by other data science professionals when shared on platforms such as Github.

For illustrative purposes, we will use the cruise ship data set. We assume that we would like to build a machine learning model for recommending cruise ship crew size based on predictor variables such as age, tonnage, passengers, length, cabins, etc. In section I, we describe how the project should be organized locally. Then in section I, we describe how to create a Github repository for the project. It is always recommended that you maintain two versions of your project, one locally and the other on Github. The advantage of this is that you can access the Github version of your project from anywhere in the world and at any time, as long as you have an internet connection. Another advantage is that if something were to happen with your local computer that could impact your computer adversely, such as viruses in the computer, then you can always be confident that you still have your project files on Github that can serve as a backup.
 
I. Local Project Directory
 
It is good to have a project directory for each project that you are working on.
Directory Name
When creating a project directory for your project, it’s good to select a directory name that reflects your project, for example, for the machine learning model for recommending crew size, one may choose a directory name such as ML_Model_for_Predicting_Ships_Crew_Size.
Directory content
Your project directory should contain the following:
(1) Project plan: This could be a world document where you describe what your project is all about. You may start by providing a brief synopsis followed by step by step plan of what you would like to accomplish. For example, before building a model, you may ask yourself:

What are the predictor variables?
What is the target variable? Is my target variable discrete or continuous?
Should I use classification or regression analysis?
How do I handle missing values in my dataset?
Should I use normalization or standardization when bringing variables to the same scale?
Should I use Principal Component Analysis or not?
How do I tune hyperparameters in my model?
How do I evaluate my model to detect biases in the dataset?
Should I use ensemble methods where I train using different models then perform an ensemble average, e.g. using classifiers such as SVM, KNN, Logistic Regression, then average over 3 models?
How do I select the final model?

(2) Project datasets: You should include the comma separated value (CSV) files for all the datasets to be used for the project. In this example, there is just one CSV file: cruise_ship_info.csv.
(3) Project Codes: Once you’ve figured out what your project plans and objectives are, it is time to start coding. Depending on the type of problem you are solving, you may decide to use a jupyter notebook or an R script for writing your code. Let’s just assume we are going to be using a jupyter notebook.
On your jupyter notebook, start by adding a project heading or title, for example:

Machine Learning Model for Predicting a Ship’s Crew Size



 
Then you may provide a brief synopsis of your project, followed by the author’s name, and date, for example:

We build a simple model using the cruise_ship_info.csv data set for predicting a ship’s crew size. 
This project is organized as follows: 
(a) data preprocessing and variable selection; 
(b) basic regression model; 
(c) hyper-parameters tuning; and 
(d) techniques for dimensionality reduction.

Author: Benjamin O. Tayo
Date: 4/8/2019



 
As you develop the code, you want to make sure that the jupyter notebook is well organized into sections that highlight the machine learning model building workflow, such as:

Importation of necessary python libraries

Importation of dataset

Exploratory data analysis

Feature selection and dimensionality reduction

Feature scaling and data partitioning into train and test sets

Model building, testing, and evaluation



 
For sample project jupyter notebook and R script files, please see the following Github reps, bot13956/ML_Model_for_Predicting_Ships_Crew_Size and bot13956/weather_pattern.
(4) Project Outputs: You may also store key project outputs in your local directory. Some key project outputs could be data visualizations, graphs illustrating model error as a function of different parameters, or tables containing key outputs such as R2 values, mean square errors, or regression coefficients. Project outputs are very handy because they can be used to prepare project reports or PowerPoint presentation slides to be presented to your data science team or to the business administrators in your company.
(5) Project report: In some cases, you may have to put together a project report to describe project accomplishments and provide prescribed actions to be taken based on the findings and insights from your model. In this case, you need to put together a project report using MS word. When writing a project report, you can make good use of some visualizations produced from your main code. You want to add these to the report. Your main code may be added as an appendix to the project report.
An example of a project report file can be found at bot13956/Monte_Carlo_Simulation_Loan_Status.
 
II. Github Project Directory
 
Once you’ve solved the problem of interest, you then have to create a project repository on GitHub and upload project files such as datasets, jupyter notebooks, R program scripts, and sample outputs. Creating a GitHub repository for any data science project is extremely important. It enables you to have access to your code at all times. You get to share your code with a community of programmers and other data scientists. Also, it is a means for you to showcase your data science skills.
Tips for creating a Github repository: Make sure you choose a suitable title for your repository. For example:

Repository Name: bot13956/ML_Model_for_Predicting_Ships_Crew_Size



 
Then include a README file to provide a synopsis of what your project is all about.

Author: Benjamin O. Tayo
Date: 4/8/2019

We build a simple model using the cruise_ship_info.csv data set for predicting a ship's crew size. 
This project is organized as follows: 
(a) data preprocessing and variable selection; 
(b) basic regression model; 
(c) hyper-parameters tuning; and 
(d) techniques for dimensionality reduction.

cruise_ship_info.csv: dataset used for model building.

Ship_Crew_Size_ML_Model.ipynb: the jupyter notebook containing code.



 
Then you may upload your project files, including the dataset, jupyter notebook, and sample outputs.
Here is an example of a Github repository for a machine learning project:
Repository URL: https://github.com/bot13956/ML_Model_for_Predicting_Ships_Crew_Size.
In summary, we’ve discussed how a data science project has to be organized. Good organization leads to better productivity and efficiency. When next you have to work on a new project, please take the time to organize your project. This will not only help with increasing efficiency and productivity, but it will also help to minimize errors. Also, keeping good records of all your current and completed projects enables you to create a repository where you can save all your projects for future use.
Original. Reposted with permission.
 
Related:

Learn to build an end to end data science project
Automatic Version Control for Data Scientists
Software engineering fundamentals for Data Scientists"
https://www.kdnuggets.com/2021/04/time-series-using-sql.html,Working With Time Series Using SQL,This article is an overview of using SQL to manipulate time series data.,"comments
By Michael Grogan, Data Science Consultant


Source: Photo by Tumisu from Pixabay

 
Tools such as Python or R are most often used to conduct deep time series analysis.
However, knowledge of how to work with time series data using SQL is essential, particularly when working with very large datasets or data that is constantly being updated.
Here are some useful commands that can be invoked in SQL to better work with time series data within the data table itself.
 
Background
 
In this example, we are going to work with weather data collected across a range of different times and locations.
The data types in the table of the PostgreSQL database are as below:

weather=# SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'weatherdata';
 column_name |          data_type          
-------------+-----------------------------
 date        | timestamp without time zone
 mbhpa       | integer
 temp        | numeric
 humidity    | integer
 place       | character varying
 country     | character varying
 realfeel    | integer
 timezone    | integer
(8 rows)


As you can see, date is defined as a timestamp data type without time zone (which we will also look at in this article).
The variable of interest is temp (temperature) — we are going to look at ways to analyse this variable more intuitively using SQL.
 
Calculating Moving Averages
 
Here is a snippet of some of the columns in the data table:

        date         | mbhpa | temp  | humidity 
---------------------+-------+-------+----------
 2020-10-12 18:33:24 |  1010 | 13.30 |       74
 2020-10-15 02:12:54 |  1017 |  7.70 |       75
 2020-10-14 23:53:42 |  1016 |  8.80 |       75    
 2020-10-15 11:03:25 |  1016 |  9.70 |       75      
 2020-10-15 13:05:23 |  1017 | 12.30 |       74    
 2020-10-15 18:47:25 |  1015 | 12.10 |       74     
 2020-10-16 23:23:23 |  1011 |  9.10 |       75   
 2020-10-20 10:25:15 |   967 | 13.80 |       83   
 2020-10-27 16:30:30 |   980 | 12.00 |       75   
 2020-10-29 15:12:07 |   988 | 11.70 |       75   
 2020-10-28 18:42:52 |   990 |  8.80 |       77


Suppose we wish to calculate a moving average of temperature across different time periods.
To do this, we firstly need to make sure that the data is ordered by date, and decide on how many periods should be included in the averaging window.
To start with, a 7-period moving average is used, with all temperature values ordered by date.

>>> select date, avg(temp) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) FROM weatherdata where place='Place Name';        date         |         avg         
---------------------+---------------------
 2020-11-12 16:36:40 |  8.8285714285714286
 2020-11-14 15:45:08 |  9.8142857142857143
 2020-11-15 08:53:26 | 10.3857142857142857
 2020-11-17 10:50:32 | 11.2285714285714286
 2020-11-18 14:18:58 | 11.8000000000000000
 2020-11-25 14:54:11 | 11.6285714285714286
 2020-11-25 19:00:21 | 10.9142857142857143
 2020-11-25 19:05:31 | 10.2000000000000000
 2020-11-25 23:41:34 |  9.2857142857142857
 2020-11-26 15:03:10 |  9.4857142857142857
 2020-11-26 17:18:33 |  8.3428571428571429
 2020-11-26 21:30:39 |  7.9142857142857143
 2020-11-26 22:29:17 |  7.6142857142857143


Now, let’s add a 30 and 60-period moving average. We will store these averages along with the 7-period moving average in the one table.

>>> select date, temp, avg(temp) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW), avg(temp) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW), avg(temp) OVER (ORDER BY date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW), avg(temp) OVER (ORDER BY date ROWS BETWEEN 59 PRECEDING AND CURRENT ROW) FROM weatherdata where place='Place Name';




Source: Output Created by Author

 
More information on how to calculate moving averages within SQL can be found at the following resource by sqltrainingonline.com.
 
Working with time zones
 
You will notice that the timestamp contains a date and time. While this is fine when storing just one location in the table, things can get quite tricky when working with locations across multiple time zones.
Note that an integer variable named timezone was created within the table.
Suppose that we are analysing weather patterns across different places with a range of time zones ahead of the inputted time — in this case, all data points were inputted at GMT time.

       date          | timezone 
---------------------+----------
 2020-05-09 15:29:00 |       11
 2020-05-09 17:05:00 |       11
 2020-05-09 17:24:00 |       11
 2020-05-10 13:02:00 |       11
 2020-05-13 19:13:00 |       11
 2020-05-10 13:04:00 |       11
 2020-05-10 15:47:00 |       11
 2020-05-13 19:10:00 |       11
 2020-05-14 17:17:00 |       11
 2020-05-09 15:20:00 |        5
 2020-05-09 17:04:00 |        5
 2020-05-09 17:25:00 |        5
 2020-05-09 18:12:00 |        5
 2020-05-10 13:02:00 |        5
 2020-05-10 15:50:00 |        5
 2020-05-10 20:32:00 |        5
 2020-05-11 17:31:00 |        5
 2020-05-13 19:11:00 |        5
 2020-05-17 21:41:00 |       11
 2020-05-15 14:08:00 |       11
 2020-05-14 16:55:00 |        5
 2020-05-15 14:10:00 |        5
(22 rows)


The new times can be calculated as follows:

weather=# select date + interval '1h' * timezone from weatherdata;
      ?column?       
---------------------
 2020-05-10 02:29:00
 2020-05-10 04:05:00
 2020-05-10 04:24:00
 2020-05-11 00:02:00
 2020-05-14 06:13:00
 2020-05-11 00:04:00
 2020-05-11 02:47:00
 2020-05-14 06:10:00
 2020-05-15 04:17:00
 2020-05-09 20:20:00
 2020-05-09 22:04:00
 2020-05-09 22:25:00
 2020-05-09 23:12:00
 2020-05-10 18:02:00
 2020-05-10 20:50:00
 2020-05-11 01:32:00
 2020-05-11 22:31:00
 2020-05-14 00:11:00
 2020-05-18 08:41:00
 2020-05-16 01:08:00
 2020-05-14 21:55:00
 2020-05-15 19:10:00
(22 rows)


We can now store the new times as an updated variable, which we will name as newdate.

>>> select date + interval '1h' * (timezone+1) as newdate, temp, mbhpa from weatherdata;      newdate       | temp | mbhpa
--------------------+------+-------
2020-05-10 03:29:00 |  4.2 |  1010
2020-05-10 05:05:00 |  4.1 |  1009
2020-05-10 05:24:00 |  3.8 |  1009


This clause allows us to generate the updated times (which would reflect the actual time in each specific place when variables such as temperature, barometric pressure, etc, were recorded.
 
Inner Join and Having Clauses
 
You will notice in the above table that temperature values are included across a range of places.
Suppose that wind speed is also calculated for each place in a separate table.
In this case, we wish to calculate the average temperature across each listed place where the wind speed is higher than 20.
This can be accomplished using the inner join and having clauses as follows:

>>> select t1.place, avg(t1.temp), avg(t2.gust) from weatherdata as t1 inner join wind as t2 on t1.place=t2.place group by t1.place having avg(t2.gust)>'20';      place      |         avg          |         avg          
-----------------+----------------------+----------------------
 Place 1         |        17.3          |        22.4
 Place 2         |        14.3          |        26.8
 Place 3         |        7.1           |        27.1


 
Conclusion
 
In this article, you have been introduced to some introductory examples of using SQL to work with time series data.
In particular, you saw how to:

Calculate moving averages
Work with different time zones
Calculate averages across different subsets of data

Many thanks for your time and any questions or feedback are greatly appreciated.
Disclaimer: This article is written on an “as is” basis and without warranty. It was written with the intention of providing an overview of data science concepts, and should not be interpreted as professional advice. The findings and interpretations in this article are those of the author and are not endorsed by or affiliated with any third-party mentioned in this article.
 
Bio: Michael Grogan is a Data Science Consultant. He posesses expertise in time series analysis, statistics, Bayesian modeling, and machine learning with TensorFlow.
Original. Reposted with permission.
Related:

Multidimensional multi-sensor time-series data analysis framework
Rejection Sampling with Python
Deep Learning Is Becoming Overused"
https://www.kdnuggets.com/2021/09/a-breakdown-deep-learning-frameworks.html,A Breakdown of Deep Learning Frameworks,"Deep Learning continues to evolve as one of the most powerful techniques in the AI toolbox. Many software packages exist today to support the development of models, and we highlight important options available with key qualities and differentiators to help you select the most appropriate for your needs.","By Kevin Vu, Exxact Corp.
comments

What is a Deep Learning Framework?
A deep learning framework is a software package used by researchers and data scientists to design and train deep learning models. The idea with these frameworks is to allow people to train their models without digging into the algorithms underlying deep learning, neural networks, and machine learning.
These frameworks offer building blocks for designing, training, and validating models through a high-level programming interface. Widely used deep learning frameworks such as PyTorch, TensorFlow, MXNet, and others can also use GPU-accelerated libraries such as cuDNN and NCCL to deliver high-performance multi-GPU accelerated training.
Why Use a Deep Learning Framework?

They supply readily available libraries for defining layers, network types (CNNs, RNNs), and common model architectures
They can support computer vision applications; image, speech, and natural language processing
They have familiar interfaces via popular programming languages such as Python, C, C++, and Scala
Many deep learning frameworks are accelerated by NVIDIA deep learning libraries such as cuDNN, NCCl, and cuBLAS for GPU accelerated deep learning training

 
Example Frameworks
 



Framework
Qualities
Differentiators




TensorFlow


Easy to use - well defined APIs, documentation
Flexible - ideal for researching and prototyping new ideas
Multiple tools for building on top of TensorFlow such as TensorFlow Slim, Scikit Flow, PrettyTensor, Keras, and TFLearn
TensorFlow Lite allows for deployment on mobile and embedded devices
JavaScript library can deploy models via the web browser and Node.js




Great community engagement and support
Large body of existing TensorFlow samples and code, accelerates research
Computational graph visualizations via TensorBoard
Python interface




Aesara (successor to Theano)


Automatic differentiation as a symbolic expression
Computation graph optimizations and on-the-fly code generation for speed, numerical stability, and memory usage




Low-level and flexible for research of new ideas
Python-based, with NumPy integration
No multi GPU




Caffe


Designed for computer vision framework problems
Too rigid for researching new algorithms
Caffe is in maintenance mode




NVIDIA fork of Caffe is maintained and updated by NVIDIA
Delivers optimized performance on the latest GPU hardware




Caffe2


Caffe2 is now a part of PyTorch, and the APIs are being deprecated





PyTorch


PyTorch is based on Python. It is the successor of Torch, which was based on the Lua programming language
Primary audience is researchers
Supports dynamic computational graphs
PyTorch 1.0 is a new iteration that includes PyTorch merged with Caffe2 (current stable version is 1.9.0 from June 2021)




Now the primary framework used at Facebook/Used by Facebook FAIR for research
Extremely flexible for research
Shares the same backend as the popular Torch framework it was based on




Chainer


Models that are fast to prototype and easier to debug
CuPy: NumPy-equivalent multi-dimensional CUDA® array-library
Extensions & Tools: ChainerRL, ChainerMN, for computer vision
Is now in maintenance mode since the company behind it is changing their primary framework to PyTorch




Dynamic computation graphs with a Python API are strengths of Chainer and PyTorch




Apache MXNet


Primary focus is training and deploying deep neural networks
Matlab support
Portable and scalable, including multi-GPU and multi-node support




Supports 8 language bindings, including C++, Python, Julia, Java, Clojure, R, Scala, and Perl
Allows for a mix of symbolic and imperative programming




Matlab


Beyond just DL classifiers: combine image/vision processing with DL
Automates distributed training and deployment to a data center and embedded without manual coding




High productivity desktop IDE (integrated development environment) that makes research, prototyping, and debugging easy
MATLAB expert technical support
Easily integrates into existing MATLAB and simulink workflows





 
More Details on TensorFlow
 
An open-source software library created by Google, TensorFlow is a popular tool for machine learning, especially for training deep neural networks. TensorFlow’s API primarily supports Python and C, but there are also APIs for C++, JavaScript, and Java; however, only the Python API is guaranteed to be stable.
TensorFlow’s community has also developed support for a number of other languages, including C#, Haskell, Julia, R, Ruby, Rust, and Scala.
The advantage of TensorFlow is that it has so many entry points. Beyond languages, there is a wide range of tools that integrate with or are built on top of TensorFlow.
TensorFlow also has a very large community of users where you can get help, and it’s well documented.
 
More Details on Keras
 
Keras is an open-source library that’s focused on providing a simple Python API for neural networks with features such as scalability for GPU clusters. It’s built on top of TensorFlow 2.0 and can also run on Theano.
Keras has the same portability as TensorFlow, meaning you can run it in a browser, as well as mobile and embedded devices. Keras is used by a number of major organizations, including CERN and NASA.
 
More Details on PyTorch
 
PyTorch is another product of the FAANG ecosystem coming from Facebook’s AI Research lab (FAIR). PyTorch is largely focused on computer vision and natural language processing (NLP) tasks. Like TensorFlow, PyTorch’s primary interface language is Python, but there is also C++ support.
PyTorch’s community has a number of tools that integrate with the library, such as Skorch for scikit-learn compatibility, extBrewer for NLP, NeMo toolkit for conversational AI, and PyTorch Lightning which is similar in idea to TensorFlow and Keras in that it’s focused on simplifying the coding required to get a model working.
PyTorch is also a good stand-in for NumPy (a popular tool in machine learning and data science) with tensors, which are like NumPy arrays but optimized to run on CPUs or GPUs. PyTorch has an experimental deployment method for mobile devices but is optimized to run on cloud computing platforms, including Amazon Web Services, Google Cloud, and Microsoft Azure.
There are a wide number of deep learning frameworks to choose from. If one of the options listed here doesn’t suit your needs, there are others, including Amazon’s Gluon (based on MXNet), DL4J, and Sonnet.
 
Original. Reposted with permission.
Related:

Geometric foundations of Deep Learning
High Performance Deep Learning, Part 1
Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall"
https://www.kdnuggets.com/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html,Algorithms for Advanced Hyper-Parameter Optimization/Tuning,"In informed search, each iteration learns from the last, whereas in Grid and Random, modelling is all done at once and then the best is picked. In case for small datasets, GridSearch or RandomSearch would be fast and sufficient. AutoML approaches provide a neat solution to properly select the required hyperparameters that improve the model’s performance.","comments
By Gowrisankar JG, Software Developer at Hexaware
Most Professional Machine Learning practitioners follow the ML Pipeline as a standard, to keep their work efficient and to keep the flow of work. A pipeline is created to allow data flow from its raw format to some useful information. All sub-fields in this pipeline’s modules are equally important for us to produce quality results, and one of them is Hyper-Parameter Tuning.


A Generalized Machine Learning Pipeline

 
Most of us know the best way to proceed with Hyper-Parameter Tuning is to use the GridSearchCV or RandomSearchCV from the sklearn module. But apart from these algorithms, there are many other Advanced methods for Hyper-Parameter Tuning. This is what the article is all about, Introduction to Advanced Hyper-Parameter Optimization, Transfer Learning and when & how to use these algorithms to make out the best of them.
Both of the algorithms, Grid-Search and Random-Search are instances of Uninformed Search. Now, let’s dive deep !!
 
Uninformed search
 
Here in these algorithms, each iteration of the Hyper-parameter tuning does not learn from the previous iterations. This is what allows us to parallelize our work. But, this isn’t very efficient and costs a lot of computational power.
Random search tries out a bunch of hyperparameters from a uniform distribution randomly over the preset list/hyperparameter search space (the number iterations is defined). It is good in testing a wide range of values and normally reaches to a very good combination very fastly, but the problem is that, it doesn’t guarantee to give the best parameter’s combination.
On the other hand, Grid search will give the best combination, but it can takes a lot of time and the computational cost is high.


Searching Pattern of Grid and Random Search

 
It may look like grid search is the better option, compared to the random one, but bare in mind that when the dimensionality is high, the number of combinations we have to search is enormous. For example, to grid-search ten boolean (yes/no) parameters you will have to test 1024 (2¹⁰) different combinations. This is the reason, why random search is sometimes combined with clever heuristics, is often used.
 
Why bring Randomness in Grid Search? [Mathematical Explanation]
 
Random search is more of a stochastic search/optimization perspective — the reason we introduce noise (or some form of stochasticity) into the process is to potentially bounce out of poor local minima. While this is more typically used to explain the intuition in general optimization (like stochastic gradient descent for updating parameters, or learning temperature-based models), we can think of humans looking through the meta-parameter space as simply a higher-level optimization problem. Since most would agree that these dimensional spaces (reasonably high) lead to a non-convex form of optimization, we humans, armed even with some clever heuristics from the previous research, can get stuck in the local optima.
Therefore, Randomly exploring the search space might give us better coverage, and more importantly, it might help us find better local optima.
Sofar in Grid and Random Search Algorithms, we have been creating all the models at once and combining their scores before deciding the best model at the end.
An alternative approach would be to build models sequentially, learning from each iteration. This approach is termed as Informed Search.
 
Informed Method: Coarse to Fine Tuning
 
A basic informed search methodology.
The process follows:

Random search
Find promising areas in the search space
Grid search in the smaller area
Continue until optimal score is obtained

You could substitute (3) with random searches before the grid search.
 
Why Coarse to Fine?
 
Coarse to Fine tuning optimizes and uses the advantages of both grid and random search.

Wide searching capabilities of random search
Deeper search once you know where a good spot is likely to be

No need to waste time on search spaces that are not giving good results !! Therefore, this better utilizes the spending of time and computational efforts, i.e we can iterate quickly, also there is boost in the performance.
 
Informed Method: Bayesian Statistics
 
The most popular informed search method is Bayesian Optimization. Bayesian Optimization was originally designed to optimize black-box functions.
This is a basic theorem or rule from Probability Theory and Statistics, in case if you want to brush up and get refreshed with the terms used here, refer this.

Bayes Rule | Theorem
A statistical method of using new evidence to iteratively update our beliefs about some outcome. In simpler words, it is used to calculate the probability of an event based on its association with another event.



Source: Bayes Theorem in Data Science

 

LHS is the probability of A, given B has occurred. B is some new evidence. This is known as the ‘posterior’.
RHS is how we calculate this.
P(A) is the ‘prior’. The initial hypothesis about the event. It is different to P(A|B), the P(A|B) is the probability given new evidence.
P(B) is the ‘marginal likelihood’ and it is the probability of observing this new evidence.
P(B|A) is the ‘likelihood’ which is the probability of observing the evidence, given the event we care about.

Applying the logic of Bayes rule to hyperparameter tuning:

Pick a hyperparameter combination
Build a model
Get new evidence (i.e the score of the model)
Update our beliefs and chose better hyperparameters next round


Bayesian hyperparameter tuning is quite new but is very popular for larger and more complex hyperparameter tuning tasks as they work well to find optimal hyperparameter combinations in these kinds of situations.

 
Note
 
For more complex cases you might want to dig a bit deeper and explore all the details about Bayesian optimization. Bayesian optimization can only work on continuous hyper-parameters, and not categorical ones.
 
Bayesian Hyper-parameter Tuning with HyperOpt
 
HyperOpt package, uses a form of Bayesian optimization for parameter tuning that allows us to get the best parameters for a given model. It can optimize a model with hundreds of parameters on a very large scale.


HyperOpt: Distributed Hyper-parameter Optimization

 
To know more about this library and the parameters of HyperOpt library feel free to visit here. And visit here for a quick tutorial with adequate explanation on how to use HyperOpt for Regression and Classification.
 
Introducing the HyperOpt package.
 
To undertake Bayesian hyperparameter tuning we need to:

Set the Domain: Our Grid i.e. search space (with a bit of a twist)
Set the Optimization algorithm (default: TPE)
Objective function to minimize: we use “1-Accuracy”


Know more about the Optimization Algorithm used, Original Paper of TPE (Tree of Parzen Estimators)

 
Sample Code for using HyperOpt [ Random Forest ]
 
HyperOpt does not use point values on the grid but instead, each point represents probabilities for each hyperparameter value. Here, simple uniform distribution is used, but there are many more if you check the documentation.

HyperOpt implemented on Random Forest

To really see this in action !! try on a larger search space, with more trials, more CVs and a larger dataset size.


 
For practical implementation of HyperOpt refer:
 
[1] Hyperopt Bayesian Optimization for Xgboost and Neural network
[2] Tuning using HyperOpt in python

Curious to know why XGBoost has high potential in winning competitions ?? Read the below article to expand your knowledge !!

XGBoost — Queen of Boosting Algorithms?
Kaggler’s Favo Algorithm | Understanding How & Why XGBoost is used to win Kaggle competitions
 
 
Informed Method: Genetic Algorithms
 
Why does this work well?

It allows us to learn from previous iterations, just like Bayesian hyperparameter tuning.
It has the additional advantage of some randomness
TPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.

 
A useful library for genetic hyperparameter tuning: TPOT
 

TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.
Consider TPOT your Data Science Assistant for advanced optimization.

Pipelines not only include the model (or multiple models) but also work on features and other aspects of the process. Plus it returns the Python code of the pipeline for you! TPOT is designed to run for many hours to find the best model. You should have a much larger population and offspring size as well as hundreds of more generations to find a good model.
 
TPOT Components ( Key Arguments )
 

generations — Iterations to run training for
population_size — The number of models to keep after each iteration
offspring_size — Number of models to produce in each iteration
mutation_rate — The proportion of pipelines to apply randomness to
crossover_rate — The proportion of pipelines to breed each iteration
scoring — The function to determine the best models
cv — Cross-validation strategy to use



TPOT Classifier

 
We will keep default values for mutation_rate and crossover_rate as they are best left to the default without deeper knowledge on genetic programming.
 
Notice: No algorithm-specific hyperparameters?
 
Since TPOT is an open-source library for performing AutoML in Python.
AutoML ??
Automated Machine Learning (AutoML) refers to techniques for automatically discovering well-performing models for predictive modeling tasks with very little user involvement.


Output for the above code snippet

 
TPOT is quite unstable when not run for a reasonable amount of time. The below code snippets shows the instability of TPOT. Here, only the random state has been changed in the below three codes, but the Output shows major differences in choosing the pipeline, i.e. model and it’s hyperparameters.



You can see in the output the score produced by the chosen model (in this case a version of Naive Bayes) over each generation, and then the final accuracy score with the hyperparameters chosen for the final model. This is a great first example of using TPOT for automated hyperparameter tuning. You can now extend this on your own and build great machine learning models!
To understand more about TPOT:
[1] TPOT for Automated Machine Learning in Python
[2] For more information in using TPOT, visit the documentation.
 
Summary
 
In informed search, each iteration learns from the last, whereas in Grid and Random, modelling is all done at once and then the best is picked. In case for small datasets, GridSearch or RandomSearch would be fast and sufficient.
AutoML approaches provide a neat solution to properly select the required hyperparameters that improve the model’s performance.
Informed methods explored were:

‘Coarse to Fine’ (Iterative random then grid search).
Bayesian hyperparameter tuning, updating beliefs using evidence on model performance (HyperOpt).
Genetic algorithms, evolving your models over generations (TPOT).

I hope you’ve learned some useful methodologies for your future work undertaking hyperparameter tuning in Python!
Create REST API in Minutes With Go / Golang
In this article, there is a short comparison between different routers and then the walk-through to create a REST API…
 
If you are curious to know about Golang’s Routers and want to try out a simple web development project using Go, I suggest to read the above article.
For more informative articles from me, follow me on medium.
And if you’re passionate about Data Science/Machine Learning, feel free to add me on LinkedIn.

 
References
[1] Bayesian Hyperparameter Optimization — A Primer
[2] How To Make Deep Learning Models That Don’t Suck
[3] Algorithms for Hyper-Parameter Optimization
[4] Grid Search and Bayesian Hyperparameter Optimization
[5] Tree-structured Parzen Estimator
[6] Informed Search - Hyperparameter Tuning
 
Bio: Gowrisankar JG (@jg_gowrisankar) is passionate about Data Science, Data Analytics, Machine Learning and #NLP, and is a Software Developer at Hexaware. 
Original. Reposted with permission.
Related:

Automated Machine Learning: The Free eBook
Top Python Libraries for Data Science, Data Visualization & Machine Learning
Build Your Own AutoML Using PyCaret 2.0"
https://www.kdnuggets.com/2021/06/pycaret-101-introduction-beginners.html,PyCaret 101: An introduction for beginners,This article is a great overview of how to get started with PyCaret for all your machine learning projects.,"comments
By Moez Ali, Founder & Author of PyCaret


PyCaret — An open-source, low-code machine learning library in Python

 
PyCaret
 
PyCaret is an open-source, low-code machine learning library and end-to-end model management tool built-in Python for automating machine learning workflows. Its ease of use, simplicity, and ability to quickly and efficiently build and deploy end-to-end machine learning pipelines will amaze you.
PyCaret is an alternate low-code library that can replace hundreds of lines of code with few lines only. This makes the experiment cycle exponentially fast and efficient.
PyCaret is simple and easy to use. All the operations performed in PyCaret are sequentially stored in a Pipeline that is fully automated for deployment. Whether it’s imputing missing values, one-hot-encoding, transforming categorical data, feature engineering, or even hyperparameter tuning, PyCaret automates all of it. To learn more about PyCaret, watch this 1-minute video.


PyCaret — An open-source, low-code machine learning library in Python

 
 
Features of PyCaret
 


Image by Author

 
 
Modules in PyCaret
 
PyCaret is a modular library arranged into modules and each module representing a machine learning use-case. As of the writing of this story, the following modules are supported:


Image by Author — Machine Learning use-case supported in PyCaret

 
* Time Series module is in making and will be available in the next major release.
 
Installing PyCaret
 
Installing PyCaret is very easy and takes only a few minutes. We strongly recommend using a virtual environment to avoid potential conflicts with other libraries.
PyCaret’s default installation is a slim version of pycaret that only installs hard dependencies listed here.

# install slim version (default)
pip install pycaret# install the full version
pip install pycaret[full]


When you install the full version of pycaret, all the optional dependencies as listed here are also installed.


PyCaret by numbers — Image by author

 
 
👉 Let’s get started
 
Before I show you how easy it is to do machine learning with PyCaret, let’s talk a little bit about the machine learning lifecycle at a high level:


Machine Learning Life Cycle — Image by Author (Read from left-to-right)

 

Business Problem — This is the first step of the machine learning workflow. It may take from few days to a few weeks to complete, depending on the use case and complexity of the problem. It is at this stage, data scientists meet with subject matter experts (SME’s) to gain an understanding of the problem, interview key stakeholders, collect information, and set the overall expectations of the project.
Data Sourcing & ETL — Once the problem understanding is achieved, it then comes to using the information gained during interviews to source the data from the enterprise database.
Exploratory Data Analysis (EDA) — Modeling hasn’t started yet. EDA is where you analyze the raw data. Your goal is to explore the data and assess the quality of the data, missing values, feature distribution, correlation, etc.
Data Preparation — Now it’s time to prepare the data model training. This includes things like dividing data into a train and test set, imputing missing values, one-hot-encoding, target encoding, feature engineering, feature selection, etc.
Model Training & Selection — This is the step everyone is excited about. This involves training a bunch of models, tuning hyperparameters, model ensembling, evaluating performance metrics, model analysis such as AUC, Confusion Matrix, Residuals, etc, and finally selecting one best model to be deployed in production for business use.
Deployment & Monitoring — This is the final step which is mostly about MLOps. This includes things like packaging your final model, creating a docker image, writing the scoring script, and then making it all work together, and finally publish it as an API that can be used to obtain predictions on the new data coming through the pipeline.

The old way of doing all this is pretty cumbersome, long, and requires a lot of technical know-how and I possibly cannot cover it in one tutorial. However, in this tutorial, I will use PyCaret to demonstrate how easy it has become for a data scientist to do all this very efficiently.
 
👉 Business Problem
 
For this tutorial, I will be using a very popular case study by Darden School of Business, published in Harvard Business. The case is regarding the story of two people who are going to be married in the future. The guy named Greg wanted to buy a ring to propose to a girl named Sarah. The problem is to find the ring Sarah will like, but after a suggestion from his close friend, Greg decides to buy a diamond stone instead so that Sarah can decide her choice. Greg then collects data of 6000 diamonds with their price and attributes like cut, color, shape, etc.
 
👉 Data
 
In this tutorial, I will be using a dataset from a very popular case study by the Darden School of Business, published in Harvard Business. The goal of this tutorial is to predict the diamond price based on its attributes like carat weight, cut, color, etc. You can download the dataset from PyCaret’s repository.

# load the dataset from pycaret
from pycaret.datasets import get_data
data = get_data('diamond')




Sample rows from data

 
 
👉 Exploratory Data Analysis
 
Let’s do some quick visualization to assess the relationship of independent features (weight, cut, color, clarity, etc.) with the target variable i.e. Price

# plot scatter carat_weight and Price
import plotly.express as px
fig = px.scatter(x=data['Carat Weight'], y=data['Price'], 
                 facet_col = data['Cut'], opacity = 0.25, template = 'plotly_dark', trendline='ols',
                 trendline_color_override = 'red', title = 'SARAH GETS A DIAMOND - A CASE STUDY')
fig.show()





Let’s check the distribution of the target variable.

# plot histogram
fig = px.histogram(data, x=[""Price""], template = 'plotly_dark', title = 'Histogram of Price')
fig.show()





Notice that distribution of Price is right-skewed, we can quickly check to see if log transformation can make Price approximately normal to give fighting chance to algorithms that assume normality.

import numpy as np# create a copy of data
data_copy = data.copy()# create a new feature Log_Price
data_copy['Log_Price'] = np.log(data['Price'])# plot histogram
fig = px.histogram(data_copy, x=[""Log_Price""], title = 'Histgram of Log Price', template = 'plotly_dark')
fig.show()





This confirms our hypothesis. The transformation will help us to get away with skewness and make the target variable approximately normal. Based on this, we will transform the Price variable before training our models.
 
👉 Data Preparation
 
Common to all modules in PyCaret, the setup is the first and the only mandatory step in any machine learning experiment using PyCaret. This function takes care of all the data preparation required before training models. Besides performing some basic default processing tasks, PyCaret also offers a wide array of pre-processing features. To learn more about all the preprocessing functionalities in PyCaret, you can see this link.

# initialize setup
from pycaret.regression import *
s = setup(data, target = 'Price', transform_target = True, log_experiment = True, experiment_name = 'diamond')




setup function in pycaret.regression module

 
When you initialize the setup function in PyCaret, it profiles the dataset and infers the data types for all input features. If all data types are correctly inferred, you can press enter to continue.
Notice that:

I have passed log_experiment = True and experiment_name = 'diamond' , this will tell PyCaret to automatically log all the metrics, hyperparameters, and model artifacts behind the scene as you progress through the modeling phase. This is possible due to integration with MLflow.
Also, I have used transform_target = True inside the setup. PyCaret will transform the Price variable behind the scene using box-cox transformation. It affects the distribution of data in a similar way as log transformation (technically different). If you would like to learn more about box-cox transformations, you can refer to this link.



Output from setup — truncated for display

 
 
👉 Model Training & Selection
 
Now that data is ready for modeling, let’s start the training process by using compare_models function. It will train all the algorithms available in the model library and evaluates multiple performance metrics using k-fold cross-validation.

# compare all models
best = compare_models()




Output from compare_models

 

# check the residuals of trained model
plot_model(best, plot = 'residuals_interactive')




Residuals and QQ-Plot of the best model

 

# check feature importance
plot_model(best, plot = 'feature')





 
Finalize and Save Pipeline
 
Let’s now finalize the best model i.e. train the best model on the entire dataset including the test set and then save the pipeline as a pickle file.

# finalize the model
final_best = finalize_model(best)# save model to disk
save_model(final_best, 'diamond-pipeline')


save_model function will save the entire pipeline (including the model) as a pickle file on your local disk. By default, it will save the file in the same folder as your Notebook or script is in but you can pass the complete path as well if you would like:

save_model(final_best, 'c:/users/moez/models/diamond-pipeline'


 
👉 Deployment
 
Remember we passed log_experiment = True in the setup function along with experiment_name = 'diamond' . Let’s see the magic PyCaret has done with the help of MLflow behind the scene. To see the magic let’s initiate the MLflow server:

# within notebook (notice ! sign infront)
!mlflow ui# on command line in the same folder
mlflow ui


Now open your browser and type “https://localhost:5000”. It will open a UI like this:


https://localhost:5000

 
Each entry in the table above represents a training run resulting in a trained Pipeline and a bunch of metadata such as DateTime of a run, performance metrics, model hyperparameters, tags, etc. Let’s click on one of the models:


Part I — CatBoost Regressor

 


Part II — CatBoost Regressor (continued)

 


Part III — CatBoost Regressor

 
Notice that you have an address path for the logged_model. This is the trained Pipeline with Catboost Regressor. You can read this Pipeline using the load_model function.

# load model
from pycaret.regression import load_model
pipeline = load_model('C:/Users/moezs/mlruns/1/b8c10d259b294b28a3e233a9d2c209c0/artifacts/model/model')# print pipeline
print(pipeline)




Output from print(pipeline)

 
Let’s now use this Pipeline to generate predictions on the new data

# create a copy of data and drop Price
data2 = data.copy()
data2.drop('Price', axis=1, inplace=True)# generate predictions
from pycaret.regression import predict_model
predictions = predict_model(pipeline, data=data2)
predictions.head()




Predictions generated from Pipeline

 
Woohoo! We now have inference from our trained Pipeline. Congrats, if this is your first one. Notice that all the transformations such as target transformation, one-hot-encoding, missing value imputation, etc. happened behind the scene automatically. You get a data frame with prediction in actual scale, and this is what you care about.


Image by Author

 


Image by Author

 
There is no limit to what you can achieve using this lightweight workflow automation library in Python. If you find this useful, please do not forget to give us ⭐️ on our GitHub repository.
To hear more about PyCaret follow us on LinkedIn and Youtube.
Join us on our slack channel. Invite link here.
 
You may also be interested in:
 
Build your own AutoML in Power BI using PyCaret 2.0
Deploy Machine Learning Pipeline on Azure using Docker
Deploy Machine Learning Pipeline on Google Kubernetes Engine
Deploy Machine Learning Pipeline on AWS Fargate
Build and deploy your first machine learning web app
Deploy PyCaret and Streamlit app using AWS Fargate serverless
Build and deploy machine learning web app using PyCaret and Streamlit
Deploy Machine Learning App built using Streamlit and PyCaret on GKE
 
Important Links
 
Documentation
Blog
GitHub
StackOverflow
Install PyCaret
Notebook Tutorials
Contribute in PyCaret
 
Want to learn about a specific module?
 
Click on the links below to see the documentation and working examples.
Classification
Regression
Clustering
Anomaly Detection
Natural Language Processing
Association Rule Mining
 
THE END
 
Bio: Moez Ali is a Data Scientist, and is Founder & Author of PyCaret.
Original. Reposted with permission.
Related:

Easy MLOps with PyCaret + MLflow
Write and train your own custom machine learning models using PyCaret
5 Things You Don’t Know About PyCaret"
https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html,The Most Complete Guide to PyTorch for Data Scientists,All the PyTorch functionality you will ever need while doing Deep Learning. From an Experimentation/Research Perspective.,"By Rahul Agarwal, MLE @ FB | Ex-Walmart DS | MLWhiz.
comments

 
PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.
I remember picking PyTorch up only after some extensive experimentation a couple of years back. To tell you the truth, it took me a lot of time to pick it up but am I glad that I moved from Keras to PyTorch. With its high customizability and pythonic syntax, PyTorch is just a joy to work with, and I would recommend it to anyone who wants to do some heavy lifting with Deep Learning.
So, in this PyTorch guide, I will try to ease some of the pain with PyTorch for starters and go through some of the most important classes and modules that you will require while creating any Neural Network with Pytorch.
But, that is not to say that this is aimed at beginners only as I will also talk about the high customizability PyTorch provides and will talk about custom Layers, Datasets, Dataloaders, and Loss functions.
So let’s get some coffee ☕ ️and start it up.
 
Tensors
Tensors are the basic building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU. In this part, I will list down some of the most used operations we can use while working with Tensors. This is by no means an exhaustive list of operations you can do with Tensors, but it is helpful to understand what tensors are before going towards the more exciting parts.
 
1. Create a Tensor
We can create a PyTorch tensor in multiple ways. This includes converting to tensor from a NumPy array. Below is just a small gist with some examples to start with, but you can do a whole lot of more things with tensors just like you can do with NumPy arrays.


 
2. Tensor Operations
Again, there are a lot of operations you can do on these tensors. The full list of functions can be found here.


Note: What are PyTorch Variables? In the previous versions of Pytorch, Tensor and Variables used to be different and provided different functionality, but now the Variable API is deprecated, and all methods for variables work with Tensors. So, if you don’t know about them, it’s fine as they re not needed, and if you know them, you can forget about them.
 
The nn.Module
 

Photo by Fernand De Canne on Unsplash
 

 
Here comes the fun part as we are now going to talk about some of the most used constructs in Pytorch while creating deep learning projects. nn.Module lets you create your Deep Learning models as a class. You can inherit from nn.Module to define any model as a class. Every model class necessarily contains an __init__ procedure block and a block for the forward pass.

In the __init__ part, the user can define all the layers the network is going to have but doesn't yet define how those layers would be connected to each other.
In the forward pass block, the user defines how data flows from one layer to another inside the network.

So, put simply, any network we define will look like:

Here we have defined a very simple Network that takes an input of size 784 and passes it through two linear layers in a sequential manner. But the thing to note is that we can define any sort of calculation while defining the forward pass, and that makes PyTorch highly customizable for research purposes. For example, in our crazy experimentation mode, we might have used the below network where we arbitrarily attach our layers. Here we send back the output from the second linear layer back again to the first one after adding the input to it(skip connection) back again(I honestly don’t know what that will do).

We can also check if the neural network forward pass works. I usually do that by first creating some random input and just passing that through the network I have created.

x = torch.randn((100,784))
model = myCrazyNeuralNet()
model(x).size()
--------------------------
torch.Size([100, 10])

 
 
A word about Layers
Pytorch is pretty powerful, and you can actually create any new experimental layer by yourself using nn.Module. For example, rather than using the predefined Linear Layer nn.Linear from Pytorch above, we could have created our custom linear layer.

You can see how we wrap our weights tensor in nn.Parameter. This is done to make the tensor to be considered as a model parameter. From PyTorch docs:
Parameters are Tensor subclasses, that have a very special property when used with Module - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear in parameters() iterator
As you will later see, the model.parameters() iterator will be an input to the optimizer. But more on that later.
Right now, we can now use this custom layer in any PyTorch network, just like any other layer.

But then again, Pytorch would not be so widely used if it didn’t provide a lot of ready to made layers used very frequently in wide varieties of Neural Network architectures. Some examples are: nn.Linear, nn.Conv2d, nn.MaxPool2d, nn.ReLU, nn.BatchNorm2d, nn.Dropout, nn.Embedding, nn.GRU/nn.LSTM, nn.Softmax, nn.LogSoftmax, nn.MultiheadAttention, nn.TransformerEncoder, nn.TransformerDecoder
I have linked all the layers to their source where you could read all about them, but to show how I usually try to understand a layer and read the docs, I would try to look at a very simple convolutional layer here.

So, a Conv2d Layer needs as input an Image of height H and width W, with Cin channels. Now, for the first layer in a convnet, the number of in_channels would be 3 (RGB), and the number of out_channels can be defined by the user. The kernel_size mostly used is 3x3, and the stride normally used is 1.
To check a new layer which I don’t know much about, I usually try to see the input as well as output for the layer like below where I would first initialize the layer:

conv_layer = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = (3,3), stride = 1, padding=1)

 
And then pass some random input through it. Here 100 is the batch size.

x = torch.randn((100,3,24,24))
conv_layer(x).size()
--------------------------------
torch.Size([100, 64, 24, 24])

 
So, we get the output from the convolution operation as required, and I have sufficient information on how to use this layer in any Neural Network I design.
 
Datasets and DataLoaders
How would we pass data to our Neural nets while training or while testing? We can definitely pass tensors as we have done above, but Pytorch also provides us with pre-built Datasets to make it easier for us to pass data to our neural nets. You can check out the complete list of datasets provided at torchvision.datasets and torchtext.datasets. But, to give a concrete example for datasets, let’s say we had to pass images to an Image Neural net using a folder which has images in this structure:

data
    train
        sailboat
        kayak
        .
        .

 
We can use torchvision.datasets.ImageFolder dataset to get an example image like below:


This dataset has 847 images, and we can get an image and its label using an index. Now we can pass images one by one to any image neural network using a for loop:

for i in range(0,len(train_dataset)):
    image ,label = train_dataset[i]
    pred = model(image)

 
But that is not optimal. We want to do batching. We can actually write some more code to append images and labels in a batch and then pass it to the Neural network. But Pytorch provides us with a utility iterator torch.utils.data.DataLoader to do precisely that. Now we can simply wrap our train_dataset in the Dataloader, and we will get batches instead of individual examples.

train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10)

 
We can simply iterate with batches using:

for image_batch, label_batch in train_dataloader:
    print(image_batch.size(),label_batch.size())
    break
------------------------------------------------------------------
torch.Size([64, 3, 224, 224]) torch.Size([64])

 
So actually, the whole process of using datasets and Dataloaders becomes:

You can look at this particular example in action in my previous blogpost on Image classification using Deep Learning here.
This is great, and Pytorch does provide a lot of functionality out of the box. But the main power of Pytorch comes with its immense customization. We can also create our own custom datasets if the datasets provided by PyTorch don’t fit our use case.
 
Understanding Custom Datasets
To write our custom datasets, we can make use of the abstract class torch.utils.data.Dataset provided by Pytorch. We need to inherit this Dataset class and need to define two methods to create a custom Dataset.

__len__ : a function that returns the size of the dataset. This one is pretty simple to write in most cases.
__getitem__: a function that takes as input an index i and returns the sample at index i.

For example, we can create a simple custom dataset that returns an image and a label from a folder. See that most of the tasks are happening in __init__ part where we use glob.glob to get image names and do some general preprocessing.

Also, note that we open our images one at a time in the __getitem__ method and not while initializing. This is not done in __init__ because we don't want to load all our images in the memory and just need to load the required ones.
We can now use this dataset with the utility Dataloader just like before. It works just like the previous dataset provided by PyTorch but without some utility functions.

 
Understanding Custom DataLoaders
This particular section is a little advanced and can be skipped going through this post as it will not be needed in a lot of situations. But I am adding it for completeness here.
So let’s say you are looking to provide batches to a network that processes text input, and the network could take sequences with any sequence size as long as the size remains constant in the batch. For example, we can have a BiLSTM network that can process sequences of any length. It’s alright if you don’t understand the layers used in it right now; just know that it can process sequences with variable sizes.

This network expects its input to be of shape (batch_size, seq_length) and works with any seq_length. We can check this by passing our model two random batches with different sequence lengths(10 and 25).

model = BiLSTM()
input_batch_1 = torch.randint(low = 0,high = 10000, size = (100,10))
input_batch_2 = torch.randint(low = 0,high = 10000, size = (100,25))
print(model(input_batch_1).size())
print(model(input_batch_2).size())
------------------------------------------------------------------
torch.Size([100, 1])
torch.Size([100, 1])

 
Now, we want to provide tight batches to this model, such that each batch has the same sequence length based on the max sequence length in the batch to minimize padding. This has an added benefit of making the neural net run faster. It was, in fact, one of the methods used in the winning submission of the Quora Insincere challenge in Kaggle, where running time was of utmost importance.
So, how do we do this? Let’s write a very simple custom dataset class first.

Also, let’s generate some random data which we will use with this custom Dataset.


Example of one random sequence and label. Each integer in the sequence corresponds to a word in the sentence.
 

 
We can use the custom dataset now using:

train_dataset = CustomTextDataset(X,y)

 
If we now try to use the Dataloader on this dataset with batch_size>1, we will get an error. Why is that?

train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10)
for xb,yb in train_dataloader:
    print(xb.size(),yb.size())

 

This happens because the sequences have different lengths, and our data loader expects our sequences of the same length. Remember that in the previous image example, we resized all images to size 224 using the transforms, so we didn’t face this error.
So, how do we iterate through this dataset so that each batch has sequences with the same length, but different batches may have different sequence lengths?
We can use collate_fn parameter in the DataLoader that lets us define how to stack sequences in a particular batch. To use this, we need to define a function that takes as input a batch and returns (x_batch, y_batch ) with padded sequence lengths based on max_sequence_length in the batch. The functions I have used in the below function are simple NumPy operations. Also, the function is properly commented so you can understand what is happening.

We can now use this collate_fn with our Dataloader as:

train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10,collate_fn = collate_text)for xb,yb in train_dataloader:
    print(xb.size(),yb.size())

 

See that the batches have different sequence lengths now
 

 
It will work this time as we have provided a custom collate_fn. And see that the batches have different sequence lengths now. Thus we would be able to train our BiLSTM using variable input sizes just like we wanted.
 
Training a Neural Network
We know how to create a neural network using nn.Module. But how to train it? Any neural network that has to be trained will have a training loop that will look something similar to below:

In the above code, we are running five epochs and in each epoch:

We iterate through the dataset using a data loader.
In each iteration, we do a forward pass using model(x_batch)
We calculate the Loss using a loss_criterion
We back-propagate that loss using loss.backward() call. We don't have to worry about the calculation of the gradients at all, as this simple call does it all for us.
Take an optimizer step to change the weights in the whole network using optimizer.step(). This is where weights of the network get modified using the gradients calculated in loss.backward() call.
We go through the validation data loader to check the validation score/metrics. Before doing validation, we set the model to eval mode using model.eval().Please note we don't back-propagate losses in eval mode.

Till now, we have talked about how to use nn.Module to create networks and how to use Custom Datasets and Dataloaders with Pytorch. So let's talk about the various options available for Loss Functions and Optimizers.
 
Loss functions
Pytorch provides us with a variety of loss functions for our most common tasks, like Classification and Regression. Some most used examples are nn.CrossEntropyLoss, nn.NLLLoss, nn.KLDivLoss and nn.MSELoss. You can read the documentation of each loss function, but to explain how to use these loss functions, I will go through the example of nn.NLLLoss

The documentation for NLLLoss is pretty succinct. As in, this loss function is used for Multiclass classification, and based on the documentation:

the input expected needs to be of size (batch_size x Num_Classes ) — These are the predictions from the Neural Network we have created.
We need to have the log-probabilities of each class in the input — To get log-probabilities from a Neural Network, we can add a LogSoftmax Layer as the last layer of our network.
The target needs to be a tensor of classes with class numbers in the range(0, C-1) where C is the number of classes.

So, we can try to use this Loss function for a simple classification network. Please note the LogSoftmax layer after the final linear layer. If you don't want to use this LogSoftmax layer, you could have just used nn.CrossEntropyLoss

Let’s define a random input to pass to our network to test it:

# some random input:X = torch.randn(100,784)
y = torch.randint(low = 0,high = 10,size = (100,))

 
And pass it through the model to get predictions:

model = myClassificationNet()
preds = model(X)

 
We can now get the loss as:

criterion = nn.NLLLoss()
loss = criterion(preds,y)
loss
------------------------------------------
tensor(2.4852, grad_fn=<NllLossBackward>)

 
 
Custom Loss Function
Defining your custom loss functions is again a piece of cake, and you should be okay as long as you use tensor operations in your loss function. For example, here is the customMseLoss

def customMseLoss(output,target):
    loss = torch.mean((output - target)**2)     
    return loss

 
You can use this custom loss just like before. But note that we don’t instantiate the loss using criterion this time as we have defined it as a function.

output = model(x)
loss = customMseLoss(output, target)
loss.backward()

 
If we wanted, we could have also written it as a class using nn.Module , and then we would have been able to use it as an object. Here is an NLLLoss custom example:

 
Optimizers
Once we get gradients using the loss.backward() call, we need to take an optimizer step to change the weights in the whole network. Pytorch provides a variety of different ready to use optimizers using the torch.optim module. For example: torch.optim.Adadelta, torch.optim.Adagrad, torch.optim.RMSprop and the most widely used torch.optim.Adam. To use the most used Adam optimizer from PyTorch, we can simply instantiate it with:

optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))

 
And then use optimizer.zero_grad() and optimizer.step() while training the model.
I am not discussing how to write custom optimizers as it is an infrequent use case, but if you want to have more optimizers, do check out the pytorch-optimizer library, which provides a lot of other optimizers used in research papers. Also, if you anyhow want to create your own optimizers, you can take inspiration using the source code of implemented optimizers in PyTorch or pytorch-optimizers.

Other optimizers from pytorch-optimizer library
 

 
 
Using GPU/Multiple GPUs
Till now, whatever we have done is on the CPU. If you want to use a GPU, you can put your model to GPU using model.to('cuda'). Or if you want to use multiple GPUs, you can use nn.DataParallel. Here is a utility function that checks the number of GPUs in the machine and sets up parallel training automatically using DataParallel if needed.

The only thing that we will need to change is that we will load our data to GPU while training if we have GPUs. It’s as simple as adding a few lines of code to our training loop.

 
Conclusion
Pytorch provides a lot of customizability with minimal code. While at first, it might be hard to understand how the whole ecosystem is structured with classes, in the end, it is simple Python. In this post, I have tried to break down most of the parts you might need while using Pytorch, and I hope it makes a little more sense for you after reading this.
You can find the code for this post here on my GitHub repo, where I keep codes for all my blogs.
If you want to learn more about Pytorch using a course based structure, take a look at the Deep Neural Networks with PyTorch course by IBM on Coursera. Also, if you want to know more about Deep Learning, I would like to recommend this excellent course on Deep Learning in Computer Vision in the Advanced machine learning specialization.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
Bio: Rahul Agarwal is Senior Statistical Analyst at WalmartLabs. Follow him on Twitter @mlwhiz.
Original. Reposted with permission.
Related:

6 bits of advice for Data Scientists
The Hitchhiker’s Guide to Feature Extraction
The 5 Classification Evaluation Metrics Every Data Scientist Must Know"
https://www.kdnuggets.com/2021/04/top-3-statistical-paradoxes-data-science.html,Top 3 Statistical Paradoxes in Data Science,Observation bias and sub-group differences generate statistical paradoxes.,"comments
By Francesco Casalegno, Project Manager and Machine Learning Engineer


Paradoxes contradict our expectations. Photo by Greg & Lois Nunes on Unsplash.

 
Observation bias and sub-group differences can easily produce statistical paradoxes in any data science application. Ignoring these elements can therefore completely undermine the conclusions of our analysis.
It is indeed not unusual to observe surprising phenomena such as sub-groups trends that are completely reverted in the aggregated data. In this article we look at the 3 most common kinds of statistical paradoxes encountered in Data Science.
 
1. Berkson’s Paradox
 
A first striking example is the observed negative association between COVID-19 severity and smoking cigarettes (see e.g. the European Commission review by Wenzel 2020). Smoking cigarettes is a well-known risk factor for respiratory diseases, so how do we explain this contradiction?
The work of Griffith 2020 recently published on Nature suggests that this can be a case of Collider Bias, also called Berkson’s Paradox. To understand this paradox, let us consider the following graphical model, where we include a third random variable: “being hospitalized”.


Berkson’s Paradox: “hospitalization” is a collider variable for both “smoking cigarettes” and “COVID-19 severity”. (Image by author)

 
This third variable “being hospitalized” is a collider of the first two. This means that both smoking cigarettes and having severe COVID-19 increase chances of being ill in a hospital. Berkson’s Paradox precisely arises when we condition on a collider, i.e. when we only observe data from hospitalized people rather than considering the whole population.
Let’s consider the following example dataset. In the left figure we have observations from the whole population, while on the right figure we only consider a subset of hospitalized people (i.e. we condition on the collider variable).


Berkson’s Paradox: If we condition on the collider “hospitalization”, we observe a reversal in the relation between smoking and COVID-19! (Image by author)

 
In the left figure we can observe the positive correlation between COVID-19 severity and smoking cigarettes that we expected as we know that smoking is a risk factor for respiratory diseases.
But in the right figure—where we only consider hospital patients—we see the opposite trend! To understand this, consider the following points.

Having high severity of COVID-19 increases chances of being hospitalized. In particular, if severity > 1 hospitalization is required.
Smoking several cigarettes a day is a major risk factor for a variety of diseases (heart attacks, cancer, diabetes), which increase the chances of being hospitalized for some reason.
Hence, if a hospital patient has lower COVID-19 severity, they have higher chances of smoking cigarettes! Indeed, they must have some disease different from COVID-19 (e.g. heart attacks, cancer, diabetes) to justify their hospitalization, and this disease may very well be caused by their smoking cigarettes.

This example is very similar to the original work of Berkson 1946, where the author noticed a negative correlation between cholecystitis and diabetes in hospital patients, despite diabetes being a risk factor for cholecystitis.
 
2. Latent Variables
 
The presence of a latent variable may also produce an apparently inverted correlation between two variables. While Berkson’s Paradox arises because of the conditioning on a collider variable (which should therefore be avoided), this other kind of paradox can be fixed by conditioning on the latent variable.
Let’s consider, for instance, the relation between number of firefighter deployed to extinguish a fire and the number people that are injured in the fire. We would expect that having more firefighters would improve the outcome (to some extent—see Brooks’s Law), yet a positive correlation is observed in aggregated data: the more firefighters are deployed, the higher the number of injured!
To understand this paradox, let us consider the following graphical model. The key is to consider again a third random variable: “fire severity”.


Latent Variable Paradox: “fire severity” is a latent variable for both “n of firefighters deployed” and “n of injured”. (Image by author)

 
This third latent variable positively correlates with the other two. Indeed, more severe fires tend to cause more injuries, and at the same time they require more firefighters to be extinguished.
Let’s consider the following example dataset. In the left figure we have aggregated observations from all kinds of fires, while on the right figure we only consider observations corresponding to three fixed degrees of fire severity (i.e. we condition our observations on the latent variable).


Latent Variables: If we condition on the latent variable “fire severity”, we observe a reversal in the relation between number of firefighters deployed and number of injured people! (Image by author)

 
In the right figure, where we condition observations on the degrees of fire severity, we can see the negative correlation we would have expected.

For a fire of given severity we can indeed observe that the more the firefighters deployed, the fewer the injured people.
If the we look at fires with higher severity, we observe the same trend even though both number of firefighters deployed and number of injured people are higher.

 
3. Simpson’s Paradox
 
Simpson’s Paradox is a surprising phenomenon arising when a trend that is consistently observed in sub-groups, but the trend is inverted if sub-groups are merged. It is often related to the class imbalance in data sub-groups.
A notorious occurrence of this paradox is from Bickel 1975, where acceptance rates to the University of California weer analysed to find evidence of sex discrimination, and two apparently contradicting facts were revealed.

On the one hand, in every department he observed that female applicants had higher acceptance rates than male applicants.
On the other hands, aggregate numbers showed that female applicants had lower acceptance rates than male applicants.

To see how this is possible, let’s consider the following dataset with the two departments Dept. A and Dept. B.

Out of 100 male applicants: 80 applied to Dept. A and 68 were accepted (85%), while 20 applied to Dept. B and 12 were accepted (60%).
Out of 100 female applicants: 30 applied to Dept. A and 28 were accepted (93%), while 70 applied to Dept. B and 46 were accepted (66%).



Simpson’s Paradox: female applicants are more likely to be accepted in each department, but the overall female acceptance rate is inferior to the male one! (Image by author)

 
The paradox is expressed by the following inequalities.


Simpson’s Paradox: The inequalities behind the apparent contradiction. (Image by author)

 
We can now understand the origin of our seemingly contradictory observations. The point is that there is a significant class imbalance in the sex of applicants in each of the two departments (Dept. A: 80–30, Dept. B: 20–70). Indeed, most female students applied to the more competitive Dept. B (which has low rates of admission), while most male students applied to the less competitive Dept. A (which has higher rates of admission). This causes the contradictory observations we had.
 
Conclusions
 
Latent variables, collider variables, and class imbalance can easily produce statistical paradoxes in many data science applications. A particular attention to these key points is therefore essential to correctly derive trends and analyse the results.
 
Bio: Francesco Casalegno is a Project Manager and Machine Learning Engineer, with a passion for solving any sort of problems related in general to Data Science. His background is in Software Engineering and Applied Mathematics. Francesco is always looking forward to new challenges, and he strongly believes in continuously improving himself and the team working with him.
Original. Reposted with permission.
Related:

10 Statistical Concepts You Should Know For Data Science Interviews
Rejection Sampling with Python
The Inferential Statistics Data Scientists Should Know"
https://www.kdnuggets.com/2021/07/github-copilot-open-source-alternatives-code-generation.html,GitHub Copilot Open Source Alternatives,GitHub's Copilot code generation tool is currently only available via approved request. Here are 4 Copilot alternatives that you can use in your programming today.,"By Matthew Mayo, KDnuggets.
comments
Recently, GitHub publicly unveiled Copilot, the preview of its ""AI pair programmer,"" a code completion style tool designed to provide line or function suggestions in your IDE. It has certainly made waves in the world of programming and beyond, and you have likely heard at least something about it.

But Copilot is more than simple autocomplete and is more context aware than other code assistants. Powered by OpenAI's Codex AI system, Copilot contextualizes a situation using docstrings, function names, comments, and preceding code to best generate and suggest what it determines to be the most appropriate code. Copilot is designed to improve over time, ""learning"" from how developers use it.

Trained on billions of lines of public code, GitHub Copilot puts the knowledge you need at your fingertips, saving you time and helping you stay focused.

 
Currently available for Visual Studio Code and platforms powered by a VS Code backend — such as GitHub's Codespaces — Copilot ""understands"" dozens of languages, with the technical preview being noted as doing ""especially well for Python, JavaScript, TypeScript, Ruby, and Go."" You can accept default code suggestions, cycle through additional proposals, modify the code you accept, or ignore Copilot suggestions at a particular point in your code altogether.
Presently, Copilot is only available via approved request. But fret not; open source alternatives of varying specification exist, and are available for you to try out right now.
Let's have a look at 4 code-generation and -suggestion alternatives to GitHub Copilot that you can use in your programming today. Though my investigation into these was prompted by the discovery of Second Mate (below), I have listed the options in descending order by number of GitHub stars, as it seemed as good a way to do so as any.
 
Captain Stack
 
 
We will start this off with a code suggestion tool, as opposed to code generation, but do so as with 514 stars at publication time, this seems to be the most popular of our alternatives.

This feature is somewhat similar to Github Copilot's code suggestion. But instead of using AI, it sends your search query to Google, then retrieves StackOverflow answers and autocompletes them for you.

 

 
Captain Stack only works with VSCode, making it an especially Copilot analog, and is installed as a VSCode extension.
Use Captain Stack to automate your Stack Overflow code copying! :)
 
GPT-Code-Clippy (GPT-CC)
 
 
GPT-CC is a code-generation tool which employs a GPT-3 model for generation.

GPT-Code-Clippy (GPT-CC) is an open source version of GitHub Copilot, a language model -- based on GPT-3, called GPT-Codex -- that is fine-tuned on publicly available code from GitHub.

 

 
The VSCode extension of GPT-CC is available here. Somewhat curiously, from this extension repo, is the following reference to sitting atop the aforementioned Captain Stack:

This extension also sits completely atop this other clone of Github Copilot aptly named Captain Stack, since instead of synthesizing the answers using deep learning, it extracts them from StackOverflow posts.

 
The main model repo has 74 stars as of publication time.
 
Second Mate
 
 
Second Mate is a code-generation tool for Emacs, leveraging a GPT model.

An open-source, mini imitation of GitHub Copilot using EleutherAI GPT-Neo-2.7B (via Huggingface Model Hub) for Emacs.
This is a much smaller model so will likely not be as effective as Copilot, but can still be interesting to play around with!

 

 
Setup for Second Mate includes running a Flask app as a backend, and configuring the Emacs plugin to point at said backend server URL for submitting requests. Second Mate has 46 stars as of publication time.
 
Clara-Copilot VSCode
 
 
Lastly, Clara-Copilot is a sparsely-documented VSCode Copilot alternative that doesn't explain upfront what mechanism it's using to accomplish its goals.

A alternative to Github Copilot for vscode until you get the access to github copilot.

 
(Edit: eagle-eyed reader FIREHAWK has noted, in the comments below, that Clara-Copilot makes use of Code Grepper, making it a code search and recommendation solution as opposed to code generation).

It does, however, provide an example of how to use the extension, and boasts that it ""[s]upports around ~ 50 Programming aproximately LOL !,"" and ""[g]ives you snippets at an instant."" The repo does have 28 stars at publication. Try at your own caution :)
 
Hopefully this small collection of alternatives provides you with something to hold you over until Copilot is released to the masses. You might even find that one of these are useful enough to work for you long term. Thanks to the respective authors of these tools.
 
Related:

GitHub Copilot: Your AI pair programmer – what is all the fuss about?
Managing Your Reusable Python Code as a Data Scientist
Data Scientists, You Need to Know How to Code"
https://www.kdnuggets.com/2020/11/microsoft-google-open-sourced-frameworks-scaling-deep-learning-training.html,Microsoft and Google Open Sourced These Frameworks Based on Their Work Scaling Deep Learning Training,Google and Microsoft have recently released new frameworks for distributed deep learning training.,"By Jesus Rodriguez, Intotheblock.
comments


Source: https://neurohive.io/en/news/google-introduced-gpipe-new-library-for-efficiently-training-large-scale-neural-networks/

 

I recently started a new newsletter focus on AI education. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:


 
Microsoft and Google have been actively working on new models for training deep neural networks. The result of that work has been the release of two new frameworks: Microsoft’s PipeDream and Google’s GPipe that follow similar principles to scale the training of deep learning models. Both projects have been detailed in respective research papers(PipeDream, GPipe) which I would try to summarize today.
Training is one of those areas of the lifecycle of deep learning programs that we don’t think of as challenging until the model’s hit certain scale. While training basic models during experimentation is relatively trivial, the complexity scales linearly with the quality and size of the model. For example, the winner of the 2014 ImageNet visual recognition challenge was GoogleNet, which achieved 74.8% top-1 accuracy with 4 million parameters, while just three years later, the winner of the 2017 ImageNet challenge went to Squeeze-and-Excitation Networks, which achieved 82.7% top-1 accuracy with 145.8 million (36x more) parameters. However, in the same period, GPU memory has only increased by a factor of ~3.

As models scale in order to achieve higher levels of accuracy, the training of those models becomes increasingly challenging. The previous example demonstrates that is unsustainable to rely on GPU infrastructure improvements to achieve better training. Instead, we need distributed computing methods that parallelize training workloads across different nodes in order to scale training. That concept of parallelizable training might sound trivial but it results extremely complicated in practice. If you think about it, we are talking about partitioning the knowledge acquisition aspects of a model across different nodes and recombining the pieces into a cohesive model after. However, training parallelism is a must in order to scale deep learning models. To address those challenges, Microsoft and Google have devoted months of research and engineering that resulted in the release of GPipe and PipeDream respectively.
 
Google’s GPipe
 
GPipe focuses on scaling training workloads for deep learning programs. The complexity of training processes from an infrastructure standpoint is an often-overlooked aspect of deep learning models. Training datasets are getting larger and more complex. For instance, in the health care space, is not uncommon to encounter models that need to be trained using millions of high resolution images. As a result, training processes often take a long time to complete and result incredibly expensive from the memory and CPU consumption.
An effective way to think about the parallelism of deep learning models is to divide it between data and model parallelism. The data parallelism approach employs large clusters of machines to split the input data across them. Model parallelism attempts to move the model to accelerators, such as GPUs or TPUs, which have special hardware to accelerate model training. At a high level, almost all training datasets can be parallelized following certain logic but the same can’t be said about models. For instance, some deep learning models are composed of parallel branches which can be trained independently. In that case, a classic strategy is to divide the computation into partitions and assign different partitions to different branches. However, that strategy falls short in deep learning models that stack layers sequentially, presenting a challenge to parallelize computation efficiently.
GPipe combines both data and model parallelism by leveraging aa technique called pipelining. Conceptually, GPipe is a distributed machine learning library that uses synchronous stochastic gradient descent and pipeline parallelism for training, applicable to any DNN that consists of multiple sequential layers. GPipe partitions a model across different accelerators and automatically splits a mini-batch of training examples into smaller micro-batches. This model allows GPipe’s accelerators to operate in parallel maximizing the scalability of the training process.
The following figure illustrates the GPipe model with a neural network with sequential layers is partitioned across four accelerators. Fk is the composite forward computation function of kth partition. Bk is the corresponding backpropagation function. Bk depends on both Bk+1 from upper layer and the intermediate activations of Fk. In the top model, we can see how the sequential nature of the network leads to the underutilization of resources. The bottom figure shows the GPipe approach in which the input mini-batch is divided into smaller macro-batches which can be processed by the accelerators at the same time.


Source: https://arxiv.org/pdf/1811.06965.pdf

 
Microsoft’s PipeDream
 
A few months ago, Microsoft Research announced the creation of Project Fiddle, a series of research projects to streamline distributed deep learning. PipeDreams is one of the first released from Project Fiddle focusing on the parallelization of the training of deep learning models.
PipeDream takes a different approach from other methods to scale the training of deep learning models leveraging a technique known as pipeline parallelism. This approach tries to address some of the challenges of data and model parallelism techniques such as the ones used in GPipe. Typically, data parallelism methods suffer from high communication costs at scale when training on cloud infrastructure and can increase GPU compute speed over time. Similarly, model parallelism techniques often leverage hardware resources inefficiently and places an undue burden on programmers to determine how to split their specific model given a hardware deployment.


Source: https://www.microsoft.com/en-us/research/uploads/prod/2019/08/fiddle_pipedream_sosp19.pdf

 
PipeDream tries to overcome some of the challenges of data-model parallelism methods by using a technique called pipeline parallelism. Conceptually, Pipeline-parallel computation involves partitioning the layers of a DNN model into multiple stages, where each stage consists of a consecutive set of layers in the model. Each stage is mapped to a separate GPU that performs the forward pass (and backward pass) for all layers in that stage.
Given a specific deep neural network, PipeDream automatically determines how to partition the operators of the DNN based on a short profiling run performed on a single GPU, balancing computational load among the different stages while minimizing communication for the target platform. PipeDream effectively load balances even in the presence of model diversity (computation and communication) and platform diversity (interconnect topologies and hierarchical bandwidths). PipeDream’s approach to training parallelism its principles offer several advantages over data-model parallelism methods. For starters, PipeDream requires less communications between the worker nodes as each worker in a pipeline execution has to communicate only subsets of the gradients and output activations, to only a single other worker. Also, PipeDream separates computation and communication in a way that leads to easier parallelism.


Source: https://www.microsoft.com/en-us/research/uploads/prod/2019/08/fiddle_pipedream_sosp19.pdf

 
Training parallelism is one of the key challenges for building larger and more accurate deep learning models. An active area of research within the deep learning community, training parallelism methods needs to combine effective concurrent programming techniques with the nature of deep learning models. While still in early stages, Google’s GPipe and Microsoft’s PipeDream stand on its own merits as two of the most creative approaches to training parallelism available to deep learning developers.
 
Original. Reposted with permission.
Related:

Uber Open Sources the Third Release of Ludwig, its Code-Free Machine Learning Platform
What I learned from looking at 200 machine learning tools
Netflix’s Polynote is a New Open Source Framework to Build Better Data Science Notebooks"
https://www.kdnuggets.com/2021/01/deepmind-muzero-important-deep-learning-systems.html,DeepMind’s MuZero is One of the Most Important Deep Learning Systems Ever Created,MuZero takes a unique approach to solve the problem of planning in deep learning models.,"By Jesus Rodriguez, Intotheblock.
comments
This year I am going to make an attempt to summarize important papers in the AI space, in very short and simple terms that can be easily understood by anyone.

I recently started a new newsletter focus on AI education and already has over 50,000 subscribers. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:


 
“I’ve seen the future of artificial intelligence(AI) and it’s called MuZero”. Those were the words used by one of my mentors when he read the first preliminary research paper about MuZero published by DeepMind in 2019. A single deep learning model that can master games like Atari, Go, Chess or Shogi without even knowing the rules. That seems like something out of a sci-fi book. Well, that’s the essence of MuZero as described by DeepMind in a new research paper published in Nature a few weeks ago.
Conceptually, MuZero presents a solution to one of the toughest challenges in the deep learning space: planning. Since the early days of machine learning, researchers have looked at techniques that can both effectively learn a model given an environment and also plan the best course of action. Think about a self-driving car or a stock market scenario in which the rules of the environment are constantly changing. Typically, those environment has resulted incredibly challenging for planning in deep learning models. At a high level, most efforts related to planning in deep neural network fit into the following categories:
1) Lookahead Search Systems: This type of systems rely on knowledge of the environment for its planning. AlphaZero is a prominent example of models in this group. However, lookahead search techniques struggled when applied to messy environments.
2) Model-Based Systems: This type of systems try to learn a representation of the environment in order to plan. Systems such as Agent57 have been successful in this area but they can be incredibly expensive to implement.
MuZero combines ideas from both approaches but using an incredibly simple principle. Instead of trying to model the entire environment, MuZero solely focuses on its most important aspects that can drive the most useful planning decisions. Specifically, MuZero decomposes the problem in three elements critical to planning:
1) The value: how good is the current position?
2) The policy: which action is the best to take?
3) The reward: how good was the last action?
For instance, using the given position in a game, MuZero uses a representation function H to map the observations to an input embedding used by the model. Planned actions are described by a dynamic function G and a prediction function F.


Source: https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules

 
The experience collected is used to train a neural network. It is important to notice that the experience includes both observations and rewards as well as the results of searches.


Source: https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules

 
Using this simple idea DeepMind was able to evolve MuZero into a model able to achieve super-human performance in complex planning problems ranging from Chess to Atari. In all benchmarks, MuZero outperformed state-of-the-art reinforcement learning algorithms.
The impact of methods such as MuZero in deep learning planning is likely to be relevant for years to come. Certainly, we should keep an eye into what DeepMind is going to do next in this area.
 
Original. Reposted with permission.
Related:

Facebook Open Sourced New Frameworks to Advance Deep Learning Research
How LinkedIn Uses Machine Learning in its Recruiter Recommendation Systems
Learning by Forgetting: Deep Neural Networks and the Jennifer Aniston Neuron"
https://www.kdnuggets.com/2021/07/11-important-probability-distributions-explained.html,11 Important Probability Distributions Explained,"There are many distribution functions considered in statistics and machine learning, which can seem daunting to understand at first. Many are actually closely related, and with these intuitive explanations of the most important probability distributions, you can begin to appreciate the observations of data these distributions communicate.","By Terence Shin, Data Scientist | MSc Analytics & MBA student.
comments

Image Created by Author.
Probability distributions were so daunting when I first saw them, partly because there are so many of them, and they all have such unfamiliar names.
Fast forward to today, and I realized that they’re actually very simple concepts to understand when you strip away all of the math behind them, and that’s exactly what we’re going to do today.
Rather than getting into the mathematical side of things, I’m going to conceptually go over what I believe are the most fundamental and essential probability distributions.
By the end of this article, you’ll not only learn about several probability distributions, but you’ll also realize how closely related many of these are to each other!
First, you need to know a couple of terms:

A probability distribution simply shows the probabilities of getting different outcomes. For example, the distribution of flipping heads or tails is 0.5 and 0.5, respectively.
A discrete distribution is a distribution in which the values that the data can take on are countable.
A continuous distribution, on the other hand, is a distribution in which the values that the data can take on are not countable.

 
1. Normal Distribution
 

Image created by Author.
The normal distribution is arguably the most important distribution to know because many phenomena fit this distribution. IQs, heights of people, shoe size, birth weight are all examples that have a normal distribution.
The normal distribution has a bell-shaped curve and has the following properties:

It has a symmetric bell shape.
The mean and median are equal and are both located at the center of the distribution.
≈68% of the data falls within 1 standard deviation of the mean, ≈95% of the data falls within 2 standard deviations of the mean, and ≈99.7% of the data falls within 3 standard deviations of the mean.

The normal distribution is also an integral part of statistics, as it is the basis of several statistical inference techniques, including linear regression, confidence intervals, and hypothesis testing.
 
2. T-distribution
 
The t-distribution is similar to the normal distribution but is generally shorter and has fatter tails. It is used instead of the normal distribution when the sample sizes are small.
One thing to note is that as the sample size increases, the t-distribution converges to the normal distribution.
 
3. Gamma Distribution
 

Image created by Author.
The Gamma distribution is used to predict the wait time until a future event occurs. It is useful when something has a natural minimum of 0.
It’s also generalized distribution of the chi-squared distribution and the exponential distribution (which we’ll talk about later).
 
4. Chi-Squared Distribution
 

Image created by Author.
As said above, the chi-squared distribution is a particular case of the gamma distribution. As there’s a lot to the chi-squared distribution, I won’t go into too much detail, but there are several uses for it:

It allows you to estimate confidence intervals for a population standard deviation.
It is the distribution of sample variances when the underlying distribution is normal.
You can test deviances of differences between expected and observed values.
You can conduct a chi-squared test.

Note: Don’t worry so much if this one confused you because the following distributions are much simpler to understand and get a grasp of!
 
5. Uniform Distribution
 
The uniform distribution is really simple — each outcome has an equal probability. An example of this is rolling a dye.
The image above shows a distribution that is approximately uniformly distributed.
 
6. Bernoulli Distribution
 

Image created by Author.
In order to understand the Bernoulli Distribution, you first need to know what a Bernoulli trial is. A Bernoulli trial is a random experiment with only two possible outcomes, success or failure, where the probability of success is the same every time.
Therefore, the Bernoulli distribution is a discrete distribution for one Bernoulli trial.
For example, flipping a coin can be represented by a Bernoulli distribution, as well as rolling an odd number on a dye.
 
7. Binomial Distribution
 

Image created by Author.
Now that you understand the Bernoulli distribution, the binomial distribution simply represents multiple Bernoulli trials. Specifically, the binomial distribution is a discrete distribution that represents the probability of getting x successes out of n independent Bernoulli trials.
Here are some examples that use the binomial distribution:

What is the probability of getting 5 heads out of 10 coin flips?
What is the probability of getting 10 conversions out of 100 emails (assuming the probability of converting is the same)?
What is the probability of getting 20 responses from 500 customer feedback surveys (assuming the probability of getting a response is the same)?

One interesting thing about the binomial distribution is that it converges to a normal distribution as n (# of Bernoulli trials) gets large.
 
8. Geometric Distribution
 
The geometric distribution is also related to the Bernoulli distribution, like the binomial distribution, except that it answers a slightly different question. The geometric distribution represents the probability of having x Bernoulli (p) failures until first success? In other words, it answers, “how many trials are needed until your first success?”
An example of this is, “how many lottery tickets do I need to buy until I buy a winning ticket?”
You can also use the geometric distribution to find the probability of the number of Bernoulli (1-p) successes until failure. The geometric can also be used to check if an event is i.i.d if it fits the distribution.
 
9. Weibull Distribution
 

Image created by Author.
The Weibull distribution is like the geometric distribution, except it is a continuous distribution. Therefore, the Weibull distribution models the amount of time it takes for something to fail or the time between failures.
The Weibull distribution can answer questions like:

How long until a particular lightbulb dies?
How long until a customer churns?

 
10. Poisson Distribution
 

Image created by Author.
The Poisson distribution is a discrete distribution that represents how many times an event is likely to occur within a specific time period.
The Poisson distribution is most commonly used in queuing theory, which answers questions along the lines of “how many customers are likely to come (queue) within a given period of time?”.
 
11. Exponential Distribution
 

Image created by Author.
The exponential distribution is closely related to the Poisson distribution. If arrivals are distributed Poisson, then the time between arrivals (aka inter-arrival times) has the exponential distribution.
Original. Reposted with permission.
 
Related:

10 Must-Know Statistical Concepts for Data Scientists
Data Science 101: Normalization, Standardization, and Regularization
Essential Math for Data Science: Probability Density and Probability Mass Functions"
https://www.kdnuggets.com/2021/09/8-deep-learning-project-ideas-beginners.html,8 Deep Learning Project Ideas for Beginners,"Have you studied Deep Learning techniques, but never worked on a useful project? Here, we highlight eight deep learning project ideas for beginners that will help you sharpen your skills and boost your resume.","comments
By Aqsa Zafar, Ph.D. Scholar in Machine Learning | Founder at MLTUT | Solopreneur | Blogger.

 
1. Dog’s Breed Identification
 
There are various dog breeds, and most of them are similar to each other. As a beginner, you can build a Dog’s breed identification model to identify the dog’s breed.
For this project, you can use the dog breeds dataset to classify various dog breeds from an image. You can download the dog breeds dataset from Kaggle.
I also found this complete tutorial for Dog Breed Classification using Deep Learning by Kirill Panarin.
 
2. Face Detection
 
This is also a good deep learning project for beginners. In this project, you have to build a deep learning model that detects the human faces from the image.
Face recognition is computer vision technology. In face detection, you have to locate and visualize the human faces in any digital image.
You can build this project in Python using OpenCV. For the complete tutorial, check this article, Real-time Face Recognition with Python & OpenCV.
 
3. Crop Disease Detection
 
In this project, you have to build a model that predicts diseases in crops using RGB images. For building a Crop disease detection model, Convolutional Neural Networks (CNN) are used.
CNN takes an image to identify the disease and detect it. There are various steps in Convolutional Neural Network. These steps are:

Convolution Operation.
ReLU Layer.
Pooling.
Flattening.
Full Connection.

You can download the Agriculture crop images dataset from Kaggle.
 
4. Image Classification with CIFAR-10 Dataset
 
Image classification is the best project for beginners. In an image classification project, you have to classify the images into various classes.
For this project, you can use CIFAR-10 Dataset, which contains 60,000 color images. These images are categorized into 10 classes, such as cars, birds, dogs, horses, ships, trucks, etc.

Source: CIFAR-10 dataset.
For training data, there are 50,000 images, and for test data, 10,000 images are used. Image classification is one of the most used applications of deep learning. You can download the CIFAR-10 dataset here.
 
5. Handwritten Digit Recognition
 
To explore and test your deep learning skills, I think this is the best project to consider. In this project, you will build a recognition system that recognizes human handwritten digits.
You can check this tutorial for Handwritten Digit Recognition using Python.
This tutorial uses the MNIST dataset and a special type of deep neural network that is Convolutional Neural Networks.
 
6. Color Detection
 
This is a beginner-level project where you have to build an interactive app. This app will identify the selected color from any image. There are 16 million colors based on the different RGB color values, but we only know a few colors.
To implement this project, you need to have a labeled dataset of all the colors that we know, and then you need to calculate which color resembles the most with the selected color value.
In order to implement this project, you should be familiar with Computer Vision Python libraries OpenCV and Pandas.
You can check all the details regarding this project here.
 
7. Real-time Image Animation
 
This is an open-source project on computer vision. In this project, you have to perform image animation in real-time using OpenCV. I have taken this image from the project’s GitHub repository.

Source: GitHub.
As you can see in the image, the model mimics the expression of the person in front of the camera and changes the image expression accordingly.
This project is useful, especially if you are planning to enter into the fashion, retail, or advertising industry. You can check the code of this project at GitHub and Colab notebook too.
 
8. Driver Drowsiness Detection
 
Road Accident is a serious problem, and the major reason is the sleepy drivers. But you can prevent this problem by creating a driver drowsiness detection system.
Driver Drowsiness Detection system detects the drowsiness of the driver by constantly assessing the driver’s eyes and alerting him with alarms.
For this project, a webcam is necessary to monitor the driver’s eyes. Python, OpenCV, and Keras are used to alert the driver when he feels sleepy.
You can check this complete project tutorial here, Driver Drowsiness Detection System with OpenCV & Keras.
Original. Reposted with permission.
 
Bio: Aqsa Zafar, Ph.D. scholar in Data Mining researches ""Depression Detection from Social Media via Data Mining,"" and writes about Data Science and machine learning at MLTUT to share knowledge and experience in the field.
Related:

Top 10 Data Science Projects for Beginners
4 Tips for Dataset Curation for NLP Projects
21 Machine Learning Projects – Datasets Included"
https://www.kdnuggets.com/2021/07/python-tips-snippets-data-processing.html,5 Python Data Processing Tips & Code Snippets,This is a small collection of Python code snippets that a beginner might find useful for data processing.,"By Matthew Mayo, KDnuggets.
comments

Photo by Hitesh Choudhary on Unsplash
 
This article contains 5 useful Python code snippets that a beginner might find helpful for data processing.
Python is a flexible, general purpose programming language, providing for many ways to approach and achieve the same task. These snippets shed light on one such approach for a given situation; you might find them useful, or find that you have come across another approach that makes more sense to you.
 
1. Concatenate Multiple Text Files
 
Let's start with concatenating multiple text files. Should you have a number of text files in a single directory you need concatenated into a single file, this Python code will do so.
First we get a list of all the txt files in the path; then we read in each file and write out its contents to the new output file; finally, we read the new file back in and print its contents to screen to verify.

import glob

# Load all txt files in path
files = glob.glob('/path/to/files/*.txt')

# Concatenate files to new file
with open('2020_output.txt', 'w') as out_file:
    for file_name in files:
        with open(file_name) as in_file:
            out_file.write(in_file.read())

# Read file and print
with open('2020_output.txt', 'r') as new_file:
    lines = [line.strip() for line in new_file]
for line in lines: print(line)



file 1 line 1
file 1 line 2
file 1 line 3
file 2 line 1
file 2 line 2
file 2 line 3
file 3 line 1
file 3 line 2
file 3 line 3


 
2. Concatenate Multiple CSV Files Into a DataFrame
 
Staying with the theme of file concatenation, this time let's tackle concatenating a number of comma separated value files into a single Pandas dataframe.
We first get a list of the CSV files in our path; then, for each file in the path, we read the contents into its own dataframe; afterwards, we combine all dataframes into a single frame; finally, we print out the results to inspect.

import pandas as pd
import glob

# Load all csv files in path
files = glob.glob('/path/to/files/*.csv')

# Create a list of dataframe, one series per CSV
fruit_list = []
for file_name in files:
    df = pd.read_csv(file_name, index_col=None, header=None)
    fruit_list.append(df)

# Create combined frame out of list of individual frames
fruit_frame = pd.concat(fruit_list, axis=0, ignore_index=True)

print(fruit_frame)



            0   1    2
0      grapes   3  5.5
1      banana   7  6.8
2       apple   2  2.3
3      orange   9  7.2
4  blackberry  12  4.3
5   starfruit  13  8.9
6  strawberry   9  8.3
7        kiwi   7  2.7
8   blueberry   2  7.6


 
3. Zip & Unzip Files to Pandas
 
Let's say you are working with a Pandas dataframe, such as the resulting frame in the above snippet, and want to compress the frame directly to file for storage. This snippet will do so.
First we will create a dataframe to use with our example; then we will compress and save the dataframe directly to file; finally, we will read the frame back into a new frame directly from compressed file and print out for verificaiton.

import pandas as pd

# Create a dataframe to use
df = pd.DataFrame({'col_A': ['kiwi', 'banana', 'apple'],
	           'col_B': ['pineapple', 'grapes', 'grapefruit'],
		   'col_C': ['blueberry', 'grapefruit', 'orange']})

# Compress and save dataframe to file
df.to_csv('sample_dataframe.csv.zip', index=False, compression='zip')
print('Dataframe compressed and saved to file')

# Read compressed zip file into dataframe
df = pd.read_csv('sample_dataframe.csv.zip',)
print(df)



Dataframe compressed and saved to file

    col_A       col_B       col_C
0    kiwi   pineapple   blueberry
1  banana      grapes  grapefruit
2   apple  grapefruit      orange


 
4. Flatten Lists
 
Perhaps you have a situation where you are working with a list of lists, that is, a list in which all of its elements are also lists. This snippet will take this list of embedded lists and flatten it out to one linear list. 
First we will create a list of lists to use in our example; then we will use list comprehensions to flatten the list in a Pythonic manner; finally, we print the resulting list to screen for verification.

# Create of list of lists (a list where all of its elements are lists)
list_of_lists = [['apple', 'pear', 'banana', 'grapes'], 
                 ['zebra', 'donkey', 'elephant', 'cow'],
	         ['vanilla', 'chocolate'], 
                 ['princess', 'prince']]

# Flatten the list of lists into a single list
flat_list = [element for sub_list in list_of_lists for element in sub_list]

# Print both to compare
print(f'List of lists:\n{list_of_lists}')
print(f'Flattened list:\n{flat_list}')



List of lists:
[['apple', 'pear', 'banana', 'grapes'], ['zebra', 'donkey', 'elephant', 'cow'], ['vanilla', 'chocolate'], ['princess', 'prince']]

Flattened list:
['apple', 'pear', 'banana', 'grapes', 'zebra', 'donkey', 'elephant', 'cow', 'vanilla', 'chocolate', 'princess', 'prince']


 
5. Sort List of Tuples
 
This snippet will entertain the idea of sorting tuples based on specified element. Tuples are an often overlooked Python data structure, and are a great way to store related pieces of data without using a more complex structure type.
In this example, we will first create a list of tuples of size 2, and fill them with numeric data; next we will sort the pairs, separately by both first and second elements, printing the results of both sorting processes to inspect the results; finally, we will extend this sorting to mixed alphanumeric data elements.

# Some paired data
pairs = [(1, 10.5), (5, 7.), (2, 12.7), (3, 9.2), (7, 11.6)]

# Sort pairs by first entry
sorted_pairs  = sorted(pairs, key=lambda x: x[0])
print(f'Sorted by element 0 (first element):\n{sorted_pairs}')

# Sort pairs by second entry
sorted_pairs  = sorted(pairs, key=lambda x: x[1])
print(f'Sorted by element 1 (second element):\n{sorted_pairs}')

# Extend this to tuples of size n and non-numeric entries
pairs = [('banana', 3), ('apple', 11), ('pear', 1), ('watermelon', 4), ('strawberry', 2), ('kiwi', 12)]
sorted_pairs  = sorted(pairs, key=lambda x: x[0])
print(f'Alphanumeric pairs sorted by element 0 (first element):\n{sorted_pairs}')



Sorted by element 0 (first element):
[(1, 10.5), (2, 12.7), (3, 9.2), (5, 7.0), (7, 11.6)]

Sorted by element 1 (second element):
[(5, 7.0), (3, 9.2), (1, 10.5), (7, 11.6), (2, 12.7)]

Alphanumeric pairs sorted by element 0 (first element):
[('apple', 11), ('banana', 3), ('kiwi', 12), ('pear', 1), ('strawberry', 2), ('watermelon', 4)]



And there you have 5 Python snippets which may be helpful to beginners for a few different data processing tasks.
 
Related:

Data Preparation in SQL, with Cheat Sheet!
How to Clean Text Data at the Command Line
Top Python Libraries for Data Science, Data Visualization & Machine Learning"
https://www.kdnuggets.com/2020/07/ethical-social-issues-natural-language-processing.html,Free From Stanford: Ethical and Social Issues in Natural Language Processing,"Perhaps it's time to take a look at this relatively new offering from Stanford, Ethical and Social Issues in Natural Language Processing (CS384), an advanced seminar course covering ethical and social issues in NLP.","By Matthew Mayo, KDnuggets.
comments
Maybe you have dipped your toe in the waters of natural language processing by auditing Stanford's From Languages to Information course. Perhaps you have used the course material from Stanford's Natural Language Processing with Deep Learning to hone this additional particular set of skills. You may have learned from one of these many other freely-available top-notch natural language processing courses.
If you have previously toured some of these other courses, perhaps it's time to take a look at this relatively new offering from Stanford, Ethical and Social Issues in Natural Language Processing (CS384), an advanced seminar course covering ""issues in natural language processing related to ethical and social issues and the overall impact of these algorithms on people and society.""

 
Like a number of other Stanford CS courses (including those mentioned above), the learning materials for this course — including a few class slides, but mostly an impressive collection of relevant academic papers (sadly, videos for this course are not publicly available) — are freely-available to anyone interested in using them. Taught by professor Dan Jurafsky, along with teaching assistants Peter Henderson and Hang Jiang, one can expect the following from the course (taken directly from the course's website):

Topics include: bias in NLP data and models, privacy and computational profiling, measuring civility and toxicity online, computational propaganda, manipulation and framing, fairness/equity, power, and various applications to social good. We've drawn heavily on related classes like Yulia Tsvetkov and Alan Black's Computational Ethics for NLP and Emily Bender's Ethics in NLP.

Rather cleverly, the class schedule's individual weeks are classified as either covering ""ways to avoid ethical or social problems in doing NLP research,"" which are coded as ""red weeks"" signifying that ""(NLP Should) Do No Harm,"" or covering ""ways to apply NLP to help solve social or ethical problems,"" or ""blue weeks"" expressing that ""(NLP Should) Do Good."" 
Broad weekly topics, supported by the aforementioned comprehensive collection of academic papers on the subjects, include:

Gender Bias in NLP Models and Data
Racial Bias or Disparity in NLP Models
NLP as a tool for detecting stereotypes or bias
NLP for identifying toxicity/hate/abuse
NLP for Studying Propaganda and Political Misinformation
NLP for Fact-Checking/Fake News Detection
NLP for Studying Framing and its Biases



The duality of NLP. From Stanford's Ethical and Social Issues in Natural Language Processing (CS384) course slides.

 
This does not cover all of the subjects either, and lessons and reading materials are incredibly up to date. For example, there is a section on ""Issues in NLP related to COVID,"" which is obviously a timely and bleeding edge theme. 
Given the importance of ethics and social issues to so much of what data science and machine learning touches, and relatedly (and by extension) natural language processing, a course like this one should be perceived by all as being on equal footing as courses which transmit the technical know-how needed to practice and perform research in these areas. Numerous resources, including books and other courses, are now raising ethics and social issues to equal status, but devoted courses such as Stanford's Ethical and Social Issues in Natural Language Processing (CS384) are worthy of independent study. 
Do yourself a favor and have a look at the material freely-available in this course's materials. You will either have your current understanding of such an important array of topics reinforced, or you will be startled at the vast concerns you have overlooked to this point. Either way, the result will be your takeaway of some additional knowledge on genuinely pressing and far-reaching issues.
 
Related:

The Best NLP with Deep Learning Course is Free
From Languages to Information: Another Great NLP Course from Stanford
10 Free Top Notch Natural Language Processing Courses"
https://www.kdnuggets.com/2021/09/data-scientists-compete-global-job-market.html,How Data Scientists Can Compete in the Global Job Market,Data scientists wanting to stay competitive or break into the field will need the right approach. These techniques will help them search for and secure a new position.,"By Devin Partida, Editor-in-Chief of ReHack.com.
comments

The job market for data scientists is more active than ever and on track for rapid growth over the next few years. The U.S. Bureau of Labor Statistics predicts that the number of available positions will rise about 28% through 2026.
Companies are investing significant amounts of money into market research and business analysis, creating new opportunities for long-time data scientists and those new to the field. At the same time, the job market is also becoming more competitive. The average compensation for data science positions is rising as these jobs become more important to businesses, encouraging hiring managers to more carefully vet new hires.
Data scientists wanting to stay competitive or break into the field will need the right approach. These techniques will help them search for and secure a new position.
 
 The State of the Global Data Science Job Market
 
 
People are generating more information than ever — experts believe worldwide data is on track to be in excess of 175 zettabytes by 2025. At the same time, innovations in AI and big data analysis have made large data sets more valuable than ever for businesses — but only if they work with trained scientists who can uncover the necessary insights.
Half of all surveyed businesses have used AI in one way or another, and more say they plan to invest further in data-driven solutions in the near future.
Right now, it’s not unusual for a single data science job posting to receive hundreds of applications. Higher demand also means rising compensation, and businesses are being more careful in who they hire for these positions.
In response, many hiring managers are inflating the job requirements of new data science openings — demanding stronger credentials, more experience and additional keywords. Even data scientists with good qualifications or strong academic track records aren’t guaranteed a position right now.
 
 Best Practices for Becoming Competitive in the Global Job Market
 
Data scientists who want to break into the field or secure a new position will need the right strategy to succeed. These six tips will help established professionals and those new to the industry secure work.
 
1. Know the Right Words to Use
 
Familiarity with popular industry keywords — like Python, SQL, AI and data analytics — can help you write a CV and resume that will more effectively communicate your skill set and get past the resume filters often used by hiring managers.
Keeping up with changing industry demands can also help keep you competitive. While Python remains an essential skill, more businesses expect familiarity with deep learning, gradient boosting machines and big data analytics. Many companies also expect applicants to have used a wide variety of approaches for data mining and analysis in the past.
 
2. Communicate Familiarity With Industry-Standard Tools
 
When applying for positions that expect knowledge of artificial intelligence, emphasizing knowledge in data science and machine learning may help you secure an interview.
At the same time, keyword stuffing, the act of unnaturally filling a resume with keywords to beat resume scanners or attract the attention of hiring managers, should be avoided. Try to only employ them in a resume or CV when they are relevant and help you explain your unique background and data science skill set.
 
3. Learn How Big Businesses Look for Data Science Professionals
 
Examining how major companies hire data scientists can also help you improve your resume and CV. AI and ML company Daitaku was recently featured in a case study about how it finds data scientists internationally. The report emphasizes how skill sets matter more than geography.
 
4. Take Advantage of General Job-Seeking Best Practices
 
Job application best practices typically also help data scientists looking for a new position. Tailoring your CV and cover letter to each job you apply for will take some extra effort. Still, it can help you communicate your particular skill set before an interview and illustrate how you’re a good fit for a certain position.
 
5. Network With Other Data Scientists
 
Actively networking with other data scientists and recruiters looking for professionals can help you expand your network and more easily find openings that match your skills and experience level.
While waiting to hear back from hiring managers, you may also look for short-term work that can help you further develop your skills and add a bullet point or two to your resume.
 
6. Consider Freelance Work
 
Businesses needing data scientists but struggling to fill new positions might offer temporary and freelance work to qualified applicants. Platforms like UpWork and freelance job search boards can provide you with leads on these positions.
 
 Looking Forward: How Data Scientists Can Stay Competitive
 
 
There are more openings for data scientists than ever, but that doesn’t mean the market is becoming less competitive. The growing value of data science and the lack of skilled candidates has companies hiring very carefully.
Data scientists wanting to find a new position or break into the market should keep on top of industry trends and become familiar with various mining and analysis techniques. Best practices for job searching — like customized CVs and careful use of keywords — can also help them secure an interview.
You can stand out amid a sea of competitors and land your ideal data science job by employing these techniques.
 
Bio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com
Related:

How to Succeed in Becoming a Freelance Data Scientist
How Data Professionals Can Add More Variation to Their Resumes
How Automation Is Improving the Role of Data Scientists"
https://www.kdnuggets.com/2020/12/14-data-science-projects-improve-skills.html,14 Data Science projects to improve your skills,There's a lot of data out there and so many data science techniques to master or review. Check out these great project ideas from easy to advanced difficulty levels to develop new skills and strengthen your portfolio.,"By Terence Shin, Data Scientist | MSc Analytics & MBA student.
comments

Photo by Austin Distel on Unsplash.
First of all, I wanted to give a huge shoutout to all of the nurses, doctors, grocery clerks, public administrators, and anyone else that’s putting their lives at risk to serve their communities.
Let’s not take this for granted. Take this time in isolation to learn new skills, read books, and improve yourself. For those interested in data, data analytics, or data science, I’m providing a list of fourteen data science projects that you can do during your spare time!
There are three types of projects:

Visualization projects
Exploratory data analysis (EDA) projects
Prediction modeling

 
Visualization Projects
 
Perhaps the quickest projects to complete are data visualizations! Below are three interesting datasets that you can use to create some intriguing visualizations to add to your portfolio.

Coronavirus visualizations

Difficulty: Easy
Link to dataset here.

Learn how to build dynamic visualizations using Plotly to show how the coronavirus has spread globally over time like the one above! Plotly is an amazing library that makes data visualizations dynamic, appealing, and simple.
If you want to learn how to build a visualization like the one above, check out my tutorial here.
My friend, Jack, also wrote an article on predicting the coronavirus recovery here!

Australian Wildfire Visualizations

Difficulty: Easy
Link to dataset here.

Taken from Vox.
The 2019–2020 bushfire season, also known as the black summer, consisted of several extreme wildfires starting in June 2019. The fires burnt an estimated 18.6 million hectares and over 5,900 buildings, according to Wikipedia.
This makes for an interesting project! Leverage your data visualization skills using Plotly or Matplotlib to show the magnitude and geographical impact of the wildfires.
See how my friend, Jack, predicted Brazil’s wildfire patterns here!

Earth Surface Temperature Visualization

Difficulty: Easy-Medium
Link to dataset here.

Photo by William Bossen on Unsplash.
Have any climate change deniers? Create some data visualizations to show how the Earth’s surface temperatures have changed over time. You can do this by creating a line graph or another animated Choropleth map!
Bonus: create a prediction model that shows what Earth’s temperatures are expected to be in fifty years.
 
Exploratory Data Analysis Projects
 
Exploratory Data Analysis (EDA), also known as Data Exploration, is a step in the Data Analysis Process, where a number of techniques are used to better understand the dataset being used.
If you want to learn more about EDA, check out my guide here!

New York Airbnb Data Exploration

Difficulty: Medium
Link to dataset here.

Photo by Oliver Niblett on Unsplash.
Since 2008, guests and hosts have used Airbnb to expand on traveling possibilities and present more personalized ways of experiencing the world. This dataset contains information on 2019 listings in New York and its geographical information, prices, number of reviews, and more.
Some questions that you can try to answer are as follows:

Which hosts are the busiest and why?
What areas have more traffic than others, and why is that the case?
Are there any relationships between prices, number of reviews, and the number of days that a given listing is booked?

 

Most Important Factors related to Employee Attrition and Performance

Difficulty: Easy
Link to dataset here.

Photo by Campaign Creators on Unsplash.
IBM created a synthetic dataset that you can use to understand how various factors affect employee attrition and satisfaction. Some of the variables include education, job involvement, performance rating, and work-life balance.
Explore this dataset and see if there are any significant variables that indeed affect employee satisfaction. Take it a step further and see if you can rank the variables from most important to the least.

World University Rankings

Difficulty: Easy
Link to dataset here.

Photo by Vasily Koloda on Unsplash.
Do you think your country has the best university in the world? What does it mean to be the ‘best’ university to start with? This dataset contains three global university rankings. Using this data, see if you can answer the following questions:

What countries are the top universities in?
What are the main factors that determine one’s world ranking?

 

Alcohol and school success

Difficulty: Easy
Link to dataset here.

Photo by Kevin Kelly on Unsplash.
Does alcohol affect students’ grades? If not, what does? This data was obtained in a survey from students in math and Portuguese language courses in secondary school. It contains several variables like alcohol consumption, family size, involvement in extracurriculars.
Using this, explore the relationship between school performance and various factors. As a bonus, see if you can predict a student’s final grade based on other variables!

Pokemon Data Exploration

Difficulty: Easy
Link to dataset here.

Taken from Pokemon.com.
For all of you gamers out there, here’s a dataset that contains information on all 802 Pokemon from all seven generations. Here are several questions that you can try to answer!

Which generation has the strongest Pokemon? Which has the weakest?
What Pokemon type is the strongest? The weakest?
Is it possible to build a classifier to identify a legendary Pokemon?
Are there any correlations between physical traits and strength stats (attack, defense, speed, etc.)?

 

Exploring Factors of Life Expectancy

Difficulty: Easy
Link to dataset here.
WHO created a dataset of the health status of all countries over time and includes statistics on life expectancy, adult mortality, and more. Using this dataset, explore the relationships between various variables. What has the biggest impact on life expectancy?
This dataset was created to answer the following questions:

Do various predicting factors that have been chosen initially really affect Life expectancy? What are the predicting variables actually affecting life expectancy?
Should a country having a lower life expectancy value(<65) increase its healthcare expenditure in order to improve its average lifespan?
How do Infant and Adult mortality rates affect life expectancy?
Does Life Expectancy have a positive or negative correlation with eating habits, lifestyle, exercise, smoking, drinking alcohol, etc.
What is the impact of schooling on the lifespan of humans?
Does Life Expectancy have a positive or negative relationship with drinking alcohol?
Do densely populated countries tend to have a lower life expectancy?
What is the impact of Immunization coverage on life Expectancy?

Check out my article on Predicting Life Expectancy with Regression for inspiration!
 
Prediction Modeling
 

Time Series Forecast on Energy Consumption

Difficulty: Medium-Advanced
Link to dataset here.

Photo by Matthew Henry on Unsplash.
This dataset is composed of power consumption data from PJM’s website. PJM is a regional transmission organization in the United States. Using this dataset, see if you can build a time series model to predict energy consumption. In addition to that, see if you can find trends around hours of the day, holiday energy usage, and long term trends!
 

Loan Prediction Forecast

Difficulty: Easy
Link to dataset here.

Photo by Dmitry Demidko on Unsplash.
Taken from Analytics Vidhya, this dataset has 615 rows and 13 columns on past loans that have and haven’t been approved. See if you can create a model that predicts whether a loan will get approved or not.
 

Used Car Price Estimator

Difficulty: Medium
Link to dataset here.

Photo by Parker Gibbs on Unsplash.
Craigslist is the world’s largest collection of used vehicles for sale. This dataset is composed of scraped data from Craigslist and is updated every few months. Using this data set, see if you can create a dataset that predicts whether a car listing is over or underpriced.
Check out my model that predicts used car prices here!

Detecting Credit Card Fraud

Difficulty: Medium-Advanced
Link to dataset here.

Photo by rupixen.com on Unsplash.
This dataset presents transactions that occurred in two days, with 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, with the positive class (frauds) account for 0.172% of all transactions. Learn how to work with unbalanced datasets and build a credit card fraud detection model.
 

Skin Cancer Image Detection

Difficulty: Advanced
Link to dataset here.

Photo by Allie Smith on Unsplash.
With over 10,000 images, see if you can build a neural network to detect skin cancer. This definitely the hardest project and requires extensive knowledge of neural networks and image recognition. Tip: refer to kernels created by other users if you’re stuck!
Original. Reposted with permission.
Related:

How to Acquire the Most Wanted Data Science Skills
How I Levelled Up My Data Science Skills In 8 Months
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science"
https://www.kdnuggets.com/2020/09/performance-machine-learning-model.html,How to Evaluate the Performance of Your Machine Learning Model,"You can train your supervised machine learning models all day long, but unless you evaluate its performance, you can never know if your model is useful. This detailed discussion reviews the various performance metrics you must consider, and offers intuitive explanations for what they mean and how they work.","comments
By Saurabh Raj, IIT Jammu.
 
Why evaluation is necessary?
 
Let me start with a very simple example.
Robin and Sam both started preparing for an entrance exam for engineering college. They both shared a room and put an equal amount of hard work while solving numerical problems. They both studied almost the same hours for the entire year and appeared in the final exam. Surprisingly, Robin cleared, but Sam did not. When asked, we got to know that there was one difference in their strategy of preparation, “test series.” Robin had joined a test series, and he used to test his knowledge and understanding by giving those exams and then further evaluating where is he lagging. But Sam was confident, and he just kept training himself.
In the same fashion, as discussed above, a machine learning model can be trained extensively with many parameters and new techniques, but as long as you are skipping its evaluation, you cannot trust it.
How to read the Confusion Matrix?
A confusion matrix is a correlation between the predictions of a model and the actual class labels of the data points.

Confusion Matrix for a Binary Classification.
Let’s say you are building a model that detects whether a person has diabetes or not. After the train-test split, you got a test set of length 100, out of which 70 data points are labeled positive (1), and 30 data points are labelled negative (0). Now let me draw the matrix for your test prediction:

Out of 70 actual positive data points, your model predicted 64 points as positive and 6 as negative. Out of 30 actual negative points, it predicted 3 as positive and 27 as negative.
Note: In the notations, True Positive, True Negative, False Positive, & False Negative, notice that the second term (Positive or Negative) is denoting your prediction, and the first term denotes whether you predicted right or wrong.
Based on the above matrix, we can define some very important ratios:

TPR (True Positive Rate) = ( True Positive / Actual Positive )
TNR (True Negative Rate) = ( True Negative/ Actual Negative)
FPR (False Positive Rate) = ( False Positive / Actual Negative )
FNR (False Negative Rate) = ( False Negative / Actual Positive )

For our case of diabetes detection model, we can calculate these ratios:
TPR = 91.4%
TNR = 90%
FPR = 10%
FNR = 8.6%
If you want your model to be smart, then your model has to predict correctly. This means your True Positives and True Negatives should be as high as possible, and at the same time, you need to minimize your mistakes for which your False Positives and False Negatives should be as low as possible. Also in terms of ratios, your TPR & TNR should be very high whereas FPR & FNR should be very low,
A smart model: TPR ↑ , TNR ↑, FPR ↓, FNR ↓
A dumb model: Any other combination of TPR, TNR, FPR, FNR
One may argue that it is not possible to take care of all four ratios equally because, at the end of the day, no model is perfect. Then what should we do?
Yes, it is true. So that is why we build a model keeping the domain in our mind. There are certain domains that demand us to keep a specific ratio as the main priority, even at the cost of other ratios being poor. For example, in cancer diagnosis, we cannot miss any positive patient at any cost. So we are supposed to keep TPR at the maximum and FNR close to 0. Even if we predict any healthy patient as diagnosed, it is still okay as he can go for further check-ups.
 
Accuracy
 
Accuracy is what its literal meaning says, a measure of how accurate your model is.
Accuracy = Correct Predictions / Total Predictions
By using confusion matrix, Accuracy = (TP + TN)/(TP+TN+FP+FN)
Accuracy is one of the simplest performance metrics we can use. But let me warn you, accuracy can sometimes lead you to false illusions about your model, and hence you should first know your data set and algorithm used then only decide whether to use accuracy or not.
Before going to the failure cases of accuracy, let me introduce you with two types of data sets:

Balanced:A data set that contains almost equal entries for all labels/classes. E.g., out of 1000 data points, 600 are positive, and 400 are negative.
Imbalanced:A data set that contains a biased distribution of entries towards a particular label/class. E.g., out of 1000 entries, 990 are positive class, 10 are negative class.

Very Important: Never use accuracy as a measure when dealing with imbalanced test set.
Why?
Suppose you have an imbalanced test set of 1000 entries with 990 (+ve) and 10 (-ve). And somehow, you ended up creating a poor model which always predicts “+ve” due to the imbalanced train set. Now when you predict your test set labels, it will always predict “+ve.” So out of 1000 test set points, you get 1000 “+ve” predictions. Then your accuracy would come,
990/1000 = 99%
Whoa! Amazing! You are happy to see such an awesome accuracy score.
But, you should know that your model is really poor because it always predicts “+ve” label.
Very Important: Also, we cannot compare two models that return probability scores and have the same accuracy.
There are certain models that give the probability of each data point for belonging to a particular class like that in Logistic Regression. Let us take this case:

Table 1.
As you can see, If P(Y=1) > 0.5, it predicts class 1. When we calculate accuracy for both M1 and M2, it comes out the same, but it is quite evident that M1 is a much better model than M2 by taking a look at the probability scores.
This issue is beautifully dealt with by Log Loss, which I explain later in the blog.
 
Precision & Recall
 
Precision: It is the ratio of True Positives (TP) and the total positive predictions. Basically, it tells us how many times your positive prediction was actually positive.

Recall : It is nothing but TPR (True Positive Rate explained above). It tells us about out of all the positive points how many were predicted positive.

F-Measure: Harmonic mean of precision and recall.

To understand this, let’s see this example: When you ask a query in google, it returns 40 pages, but only 30 were relevant. But your friend, who is an employee at Google, told you that there were 100 total relevant pages for that query. So it’s precision is 30/40 = 3/4 = 75% while it’s recall is 30/100 = 30%. So, in this case, precision is “how useful the search results are,” and recall is “how complete the results are.”
 
ROC & AUC
 
Receiver Operating Characteristic Curve (ROC):
It is a plot between TPR (True Positive Rate) and FPR (False Positive Rate) calculated by taking multiple threshold values from the reverse sorted list of probability scores given by a model.

A typical ROC curve.
Now, how do we plot ROC?
To answer this, let me take you back to Table 1 above. Just consider the M1 model. You see, for all x values, we have a probability score. In that table, we have assigned the data points that have a score of more than 0.5 as class 1. Now sort all the values in descending order of probability scores and one by one take threshold values equal to all the probability scores. Then we will have threshold values = [0.96,0.94,0.92,0.14,0.11,0.08]. Corresponding to each threshold value, predict the classes, and calculate TPR and FPR. You will get 6 pairs of TPR & FPR. Just plot them, and you will get the ROC curve.
Note: Since the maximum TPR and FPR value is 1, the area under the curve (AUC) of ROC lies between 0 and 1.
The area under the blue dashed line is 0.5. AUC = 0 means very poor model, AUC = 1 means perfect model. As long as your model’s AUC score is more than 0.5. your model is making sense because even a random model can score 0.5 AUC.
Very Important: You can get very high AUC even in a case of a dumb model generated from an imbalanced data set. So always be careful while dealing with imbalanced data set.
Note: AUC had nothing to do with the numerical values probability scores as long as the order is maintained. AUC for all the models will be the same as long as all the models give the same order of data points after sorting based on probability scores.
 
Log Loss
 
This performance metric checks the deviation of probability scores of the data points from the cut-off score and assigns a penalty proportional to the deviation.
For each data point in a binary classification, we calculate it’s log loss using the formula below,

Log Loss formula for a Binary Classification.
where p = probability of the data point to belong to class 1 and y is the class label (0 or 1).
Suppose if p_1 for some x_1 is 0.95 and p_2 for some x_2 is 0.55 and cut off probability for qualifying for class 1 is 0.5. Then both qualify for class 1, but the log loss of p_2 will be much more than the log loss of p_1.

As you can see from the curve, the range of log loss is [0, infinity).
For each data point in multi-class classification, we calculate it’s log loss using the formula below,

Log Loss formula for multi-class classification.
where y(o,c) = 1 if x(o,c) belongs to class 1. The rest of the concept is the same.
 
Coefficient of Determination
 
It is denoted by R². While predicting target values of the test set, we encounter a few errors (e_i), which is the difference between the predicted value and actual value.
Let’s say we have a test set with n entries. As we know, all the data points will have a target value, say [y1,y2,y3…….yn]. Let us take the predicted values of the test data be [f1,f2,f3,……fn].
Calculate the Residual Sum of Squares, which is the sum of all the errors (e_i) squared, by using this formula where fi is the predicted target value by a model for i’th data point.

Total Sum of Squares.
Take the mean of all the actual target values:

Then calculate the Total Sum of Squares, which is proportional to the variance of the test set target values:

If you observe both the formulas of the sum of squares, you can see that the only difference is the 2nd term, i.e., y_bar and fi. The total sum of squares somewhat gives us an intuition that it is the same as the residual sum of squares only but with predicted values as [ȳ, ȳ, ȳ,…….ȳ ,n times]. Yes, your intuition is right. Let’s say there is a very simple mean model that gives the prediction of the average of the target values every time irrespective of the input data.
Now we formulate R² as:

As you can see now, R² is a metric to compare your model with a very simple mean model that returns the average of the target values every time irrespective of input data. The comparison has 4 cases:
case 1: SS_R = 0
(R² = 1) Perfect model with no errors at all.
case 2: SS_R > SS_T
(R² < 0) Model is even worse than the simple mean model.
case 3: SS_R = SS_T
(R² = 0) Model is same as the simple mean model.
case 4: SS_R < SS_T
(0< R² <1) Model is okay.
 
Summary
 
So, in a nutshell, you should know your data set and problem very well, and then you can always create a confusion matrix and check for its accuracy, precision, recall, and plot the ROC curve and find out AUC as per your needs. But if your data set is imbalanced, never use accuracy as a measure. If you want to evaluate your model even more deeply so that your probability scores are also given weight, then go for Log Loss.
Remember, always evaluate your training!
Original. Reposted with permission.
 
Related:

Idiot’s Guide to Precision, Recall, and Confusion Matrix
Using Confusion Matrices to Quantify the Cost of Being Wrong
Achieving Accuracy with your Training Dataset"
https://www.kdnuggets.com/2021/02/deploy-flask-api-kubernetes-connect-micro-services.html,How to Deploy a Flask API in Kubernetes and Connect it with Other Micro-services,A hands-on tutorial on how to implement your micro-service architecture using the powerful container orchestration tool Kubernetes.,"comments
By Rik Kraan, Vantage AI
Kubernetes is a powerful container orchestration tool that automates deployment and management of containers. If you have a simple lightweight application that exists of one service, don’t bother using Kubernetes. Kubernetes’ benefits emerge if your application has a micro-service architecture with several components working together. It is a ‘open-source system for automating deployment, scaling and management of containerized applications and comes with several benefits including:

Easy (auto-)scaling based on demand
Ways to make your application fault tolerant by distributing workloads in a way that it will remain functional in case of partial failure
Automated health-checks and self-healing processes
Taking care of the communication between your micro-services and balancing incoming traffic evenly over all your resources

Starting with Kubernetes may seem daunting at first, but if you grasp the main concepts of it and play around with the excellent tutorials on the official website, you can get started fairly easily.
In this blog I will:

Provide a quick overview of the main concepts of Kubernetes
Demonstrate how to start your own local cluster
Deploy a MySQL database on your cluster
Set up an Flask app that functions as REST API to communicate with the database



Network. Photo by Alina Grubnyak on Unsplash

 
Kubernetes basics
 
In this section I will cover the basics of Kubernetes without too many details; feel free to dive deeper by reading the official documentation.
A Kubernetes cluster consists of a master and one or more worker nodes. This architecture is one of the main features of Kubernetes. As you will see, your micro-services are distributed over different nodes so they will remain healthy if one of the worker nodes fails. The master is responsible for managing the cluster and exposes the API via which you can communicate with your cluster. By default, worker nodes come with a few components including some pre-installed software that enables running containers of popular container services as Docker and containerd.
Three main concepts are essential to deploy your own applications on a Kubernetes cluster: Deployments, Pods and Services.

A Deployment is a set of instructions provided to the master on how to create and update your application. With these instructions the master will schedule and run your application on individual worker nodes. The deployment is continuously monitored by the master. If one of the instances of your applications goes down (e.g. if a worker node goes down), it will be automatically replaced by a new instance.



Kubernetes cluster with a deployment (source: https://kubernetes.io/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/)

 

A Pod is the atomic unit within the Kubernetes platform. It represents a group of one or more containers and some shared resources for those containers (shared storage, a unique cluster IP address etc.). If you create a deployment, this deployment will create pods with containers inside them. Each pod is bound to a worker node. It is essential to understand that a worker node can have multiple pods, and all these pods will be rebuild on a different available worker node if the current worker node fails.



Overview of a worker node with several pods (source: https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/)

 

A service basically defines a logical set of pods and defines a policy on how to access them. This is necessary as pods can go down and be restarted (e.g. if a worker node is deleted or crashes). A service routes traffic across a set of pods and allow pods to die and replicate without impacting your application. When defining a service, you can specify the type of the service. By default Kubernetes creates a ClusterIP service, which makes your service only accessible from inside the cluster. You may want to expose some of your services (e.g. frontends) to the outside world. In this case you can create a LoadBalancer service, which creates an external load balancer and assigns a fixed external IP to it, so it can be accessed from outside the cluster (for example in your browser).



A cluster with 3 worker nodes, several pods and two services (A & B) tying pods together (source: https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)

 
Getting started with your own cluster
 
If you want to get your cluster to work quickly: all the code in this blog (and an explanatory Readme) can be found here. The application we will build consists of two micro-services:

a MySQL database
a Flask app that implements an API to access and perform CRUD (create, read, update delete) operations on the database.


Prerequisites: Have kubectland minikube installed (https://kubernetes.io/docs/tasks/tools/). And make sure your Docker CLI uses the Docker deamon in your cluster via the command eval $(minikube docker-env). No worries: if you restart your terminal you will automatically use your own Docker daemon again. Finally start your local cluster via the command minikube start.

 
First things first: when setting up a MySQL database we need to take into account two things. 1) To access the database we need some credentials configured and 2) we will need a persistent volume for the database so we will not lose all our data if the nodes will accidentally be taken down.
 
Creating secrets
 
Kubernetes has it’s own method of dealing with your sensitive information by configuring Kubernetes Secrets. This can be done with a simple YAML file. These secrets can be accessed by any pod in your cluster by specifying environment variables (which we will see later on). Secrets should be specified as base64-encoded strings. So first we have to get the encoded version of your password via your terminal: echo -n <super-secret-passwod> | base64. Copy the output and embed it in the following secrets.yml file at the db_root_password field. The metadata.name field is important as we have to specify this in a later stage, so be sure to remember it

You can now add the secrets to your cluster via your terminal: kubectl apply -f secrets.yml . And see if it worked by checking the secrets via kubectl get secrets.
 
Persistent volume
 
A persistent volume is a storage resource with a lifecycle independent of a Pod. This means that the storage will persist if a pod goes down. As Kubernetes has the permission to restart pods at any time, it is a good practice to set your database storage to a persistent volume. A persistent volume can be a directory on your local filesystem, but also a storage service of a cloud provider (for example AWS Elastic Block Storage or Azure Disk). The type of the persistent volume can be specified when creating the persistent volume. For this tutorial you will use a hostPath type, which will create a volume on your minikube node. However, make sure to use another type (see the documentation) in a production environment as your data will be lost if you delete your minikube node when using a hostPath type.
Making your application use a persistent volume exists of two parts:

Specifying the actual storage type, location, size and properties of the volume.
Specify a persistent volume claim that requests a specific size and access modes of the persistent volume for your deployments.

Create a persistent-volume.yml file and specify the size (in this example 2GB), access modes and the path the files will be stored. The spec.persistentVolumeReclaimPolicy specifies what should be done if the persistent volume claim is deleted. In the case of a stateful application like the MySQL database, you want to retain the data if the claim is deleted, so you can manually retrieve or backup the data. The default reclaim policy is inherited from the type of persistent volume, so it is good practice to always specify it in the yml file.

Again you can add the storage via kubectl apply -f persistent-volume.yml . And see if the details of your created resources via kubectl describe pv mysql-pv-volume and kubectl describe pvc mysql-pv-claim. As you made a hostPath type persistent volume, you can find the data by logging into the minikube node minikube ssh and navigate to the spcified path (/mnt/data).
 
Deploy the MySQL server
 
With our secrets and persistent volume (claim) in place, we can start building our application. First we will deploy a MySQL server. Pull the latest mysql imagedocker pull mysql and create the mysql-deployment.yml file. There are several things worth mentioning about this file. We specify that we only spin-up one pod (spec.replicas: 1). The deployment will manage all pods with a label db specified by spec.selector.matchLabels.app: db . The templatefield and all it’s subfields specify the characteristics of the pod. It will run the image mysql, will be named mysql as well and looks for the db_root_password field in the flaskapi-secrets secret and will set the value to the MYSQL_ROOT_PASSWORD environment variable. Furthermore we specify a port that the container exposes and which path should be mounted to the persistent volume spec.selector.template.spec.containers.volumeMounts.mountPath: /var/lib/mysql. At the bottom we also specify a service also called mysql of the LoadBalancertype so we can access our database via this service.

You can now deploy the MySQL server with kubectl apply -f mysql-deployment.yml. And see if a pod is running via kubectl get pods.
 
Create database and table
 
The last thing we have to do before implementing the API is initializing a database and schema on our MySQL server. We can do this using multiple methods, but for the sake of simplicity let’s access the MySQL server via the newly created service. As the pod running the MySQL service is only accessible from inside the cluster, you will start up a temporary pod that serves as mysql-client:

Set up the mysql-client via the terminal: kubectl run -it --rm --image=mysql --restart=Never mysql-client -- mysql --host mysql --password=<your_password>. Fill in the (decoded) password that you specified in the secrets.yml file.
Create the database, table and schema. You can do whatever you like, but to make sure the sample Flask app will work do as follows:


CREATE DATABASE flaskapi;
USE flaskapi;

CREATE TABLE users(user_id INT PRIMARY KEY AUTO_INCREMENT, user_name VARCHAR(255), user_email VARCHAR(255), user_password VARCHAR(255));


 
Deploying the API
 
Finally it is time to deploy your REST API. The following gist demonstrates an example of a Flask app that implements the API with only two endpoints. One for checking if the API functions and one for creating users in our database. In the GitHub repo you can find the python file that has endpoints for reading, updating and deleting entries in the database as well. The password for connecting to the database API is retrieved from the environment variables that were set by creating secrets. The rest of the environment variables (e.g MYSQL_DATABASE_HOST) is retrieved from the MySQL service that was implemented before (further on I will explain how to make sure the Flask app has access to this information).

To deploy this app in your Kubernetes cluster you have to make an image of this Flask app by creating a simple Dockerfile. Nothing special, preparing your container, installing requirements, copying the folder content and running the Flask app. Go to the GitHub repo to find the Dockerfile and the requirements.txt file that is required for building the image. Before you can deploy the Flask app in the Kubernetes cluster, you first have to build the image and name it flask-api via docker build . -t flask-api.

Now it is time to define the deployment and service for the Flask app that implements a RESTful API. The deployment will start up 3 pods (specified in the flaskapp-deployment.yml at the spec.replicas: 3 field) Within each of these pods a container is created from the flask-api image you just build. To make sure Kubernetes uses the locally built image (instead of downloading an image from an external repo like Dockerhub) make sure to set the imagePullPolicy to never. To make sure the Flask app can communicate with the database a few environment variables should be set. The db_root_password is retrieved from your created secrets. Each container that starts up inherits environmental variables with information of all running services, including IP and port addresses. So you don’t have to worry about having to specify the host and port of the MySQL database to the Flask app. Finally, you will define a service of the LoadBalancer type to divide the incoming traffic between the three pods.

 
Making requests to the API
 
You are now ready to use our API and interact with your database. The last step is to expose the API service to the outside world via your terminal: minikube service flask-service. You will now see something like

Go to the provided URL and you will see the Hello World message, to make sure your API is running correctly. You can now interact with the API using your favorite request service like Postman or curl in your terminal. To create a user provide a json file with a name, email and pwd field. for example:curl -H ""Content-Type: application/json"" -d '{""name"": ""<user_name>"", ""email"": ""<user_email>"", ""pwd"": ""<user_password>""}' <flask-service_URL>/create. If you implemented the other methods of the API (as defined in the GitHub repo) as well, you may now be able to query all users in the database via: curl <flask-service_URL>/users.
 
Conclusion
 
curl in your terminal. To create a user provide a json file with a name, email and pwd field. for example:curl -H ""Content-Type: application/json"" -d '{""name"": ""<user_name>"", ""email"": ""<user_email>"", ""pwd"": ""<user_password>""}' <flask-service_URL>/create. If you implemented the other methods of the API (as defined in the GitHub repo) as well, you may now be able to query all users in the database via: curl <flask-service_URL>/users.
 
In this hands-on tutorial you set up deployments, services and pods, implemented a RESTful API by deploying a Flask app and connected it with other micro-services (a MySQL database in this case). You can keep running this locally, or implement it on a remote server for example in the cloud and get it to production. Feel free to clone the repo and adjust the API as you like, or add additional micro-services.
Feel free to reach out to me if you have any additional questions, remarks or suggestions!
 
Bio: Rik Kraan is a medical doctor with a PhD in radiology, working as a data scientist at Vantage AI, a data science consultancy company in the Netherlands. Get in touch via rik.kraan@vantage-ai.com
Original. Reposted with permission.
Related:

Kubernetes vs. Amazon ECS for Data Scientists
Create and Deploy your First Flask App using Python and Heroku
Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes"
https://www.kdnuggets.com/2021/03/cmu-ms-business-analytics.html,Read This Before You Apply to a Business Analytics Master’s Program,Considering a master’s in business analytics? Here are four things to know before you apply.,"Sponsored Post.

 
1. You must be interested in the business side.
There’s a reason “business analytics” is called just that—it’s centered on how data insights can be used to solve business problems. Degrees like the online Master of Science in Business Analytics from Carnegie Mellon University build business knowledge and communication skills at the same time as they teach advanced analytical skills.
“You’ll learn multivariate regression models in one class, and in another learn to use those skills to model customer preferences, create sales-forecasting models, and use the information as the underpinning for strategy,” explained David Lamont, associate teaching professor in the Tepper School of Business.
2. Problem-solving is as important as programming skills. 
MSBA applicants usually need to know basic mathematics, statistics, and have some programming capabilities. But those who excel in analytics have a relentless curiosity and are unafraid of tackling difficult problems.
If you are someone who wants to learn how to apply your analytical skills to make a real impact within an organization, then you are well-suited for a business analytics degree.
3. It really is possible to complete a master’s while working full time. 
Online programs like Tepper’s are built with professionals in mind, offering asynchronous coursework and live sessions that meet in evenings.
“Life doesn't stop just because you're in school,” said Andrew Kwiatkowski, an MSBA student at Tepper. “CMU is famously rigorous but the professors and staff have provided terrific support and guidance.”
4. Make sure the degree will give you a good return on your investment.
Going back to school is an investment of your money and your time. That’s why it’s important to choose a program that will

give you long-term skills for your entire career arc,
provide a professional network that you’ll actually want to access, and
add cachet to your resume.

“The Tepper MSBA degree carries a network and it carries an exceptional brand. But it’s not just these benefits: we don’t let people out the door unless they have real skills. That’s part of the DNA of CMU as well as Tepper,” said Lamont.
The online Master of Science in Business Analytics from Carnegie Mellon University’s Tepper School of Business features a comprehensive curriculum that encompasses data visualization, machine learning, large-scale data management and more.
Begin your application or talk to an admissions counselor at 888-876-8959."
https://www.kdnuggets.com/2020/12/manning-deep-learning-design-patterns.html,Deep Learning Design Patterns!,"New book, ""Deep Learning Design Patterns"" presents deep learning models in a unique-but-familiar new way: as extendable design patterns you can easily plug-and-play into your software projects. Use code kdmath50 to save 50% off.","Sponsored Post.

It’s no secret that deep learning is amazing. For the past eight years, the deep learning revolution has taken hold of computing and made technologies we thought might never be possible a daily reality.
But isn’t actualising deep learning beyond the capabilities of anyone without big company backing, a research team, or a statistics PhD? How can regular machine learning engineers and developers take those cutting edge algorithms from the R&D labs and put them into production?
Deep Learning Design Patterns is here to help. It presents deep learning models in a unique-but-familiar new way: as extendable design patterns you can easily plug-and-play into your software projects.
This accessible, jargon-lite book is written by Andrew Ferlitsch, an expert on computer vision and deep learning at Google Cloud AI Developer Relations. Andrew’s whole career is based around making deep learning accessible to everyday developers, and that’s just what his book does.
Using diagrams, code samples, and easy-to-understand language, Andre shares insights from Google’s state-of-the-art neural networks and AI research papers. You'll quickly learn to incorporate the very latest models and techniques into your apps as idiomatic, composable, and reusable design patterns.
Deep Learning Design Patterns is published by Manning Publications—so you know its quality is assured. You can save 50% off Deep Learning Design Patterns, as well as all other Manning books and videos!
Just enter the code kdmath50 at checkout when you buy from manning.com.

Find out more."
https://www.kdnuggets.com/2021/04/data-science-101-normalization-standardization-regularization.html,"Data Science 101: Normalization, Standardization, and Regularization","Normalization, standardization, and regularization all sound similar. However, each plays a unique role in your data preparation and model building process, so you must know when and how to use these important procedures.","comments
By Susan Sivek, Data Science Journalist for Alteryx.
""Normal,"" ""standard,"" ""regular"": These are all fairly similar. Let's just put -ization on the end of each one, too. That won't ever be confusing, right?
If we could go back to the beginnings of statistics and data science, maybe we could advocate for choosing more distinctive words for these concepts. Alas, we're stuck with these terms for now.
Each of these three -izations plays a unique role in your data preparation and analysis process. Let's get some clarity on each, so you know when and how to use them.
Image via GIPHY.
 
Feature Scaling: Normalization and Standardization
 
One use of ""normalization"" is text normalization, the process by which text is prepared for analysis with natural language processing tools. The term is also used in describing database structure and organization.
However, there's yet another commonly used (but still somewhat variable) meaning of normalization: methods for scaling your data.
Let's talk first about what ""scaling your data"" means with the fictional library dataset below. Say you have a variable (aka feature) that has a wide range of values (and hence variance), like the ""Library Checkouts"" field below — especially as compared to the variance of ""Average Rating"":



Title
Average Rating (1 to 5)
Library Checkouts


Uncanny Valley
3.0
45


Quantum
3.4
1,301


The Lady Tasting Tea
3.8
2,122


The Midnight Library
4.1
12,310



 
This variation in variance (oof) can cause issues for machine learning. To address it, feature scaling in some form, such as the methods described below, is generally recommended. Neural networks and support vector machines are sensitive to scaling, along with algorithms that use the distances between points in their calculations, like clustering and PCA.
Image via GIPHY.
A feature with wide-ranging values can have a disproportionate influence on these models' predictions when compared to other features. Therefore, it's typically better to constrain all the features' values to a narrower range, so they are all integrated equally into the model. ""Scaling"" encompasses a variety of procedures that make the variables more comparable.
Min-Max Normalization
Let's dive into one form of normalization, which is one variety of feature scaling. ""Min-max normalization"" or ""min-max scaling"" recalculates all the values of your variables so that they fall within the range [0, 1] or [-1, 1]. (Check out an equation for this process.) The [0, 1] range is typically required for neural networks.
Our dataset above, if scaled so that values fall within [0, 1], would look like this:



Title
Average Rating (1 to 5)
Library Checkouts


Uncanny Valley
0
0


Quantum
0.364
0.102


The Lady Tasting Tea
0.727
0.169


The Midnight Library
1.000
1.000



 
As you can see, the minimum values and maximum values for each variable end up at the top and bottom of the [0, 1] range; the other values lie in between. Most importantly, all the values across the features are more comparable and may contribute to a better-performing model. However, as you can imagine, this method is not as effective with outliers, which can pull the minimum and/or maximum values strongly in one direction.
If you want to use this approach in Python and are using scikit-learn (one of the libraries included in Designer's Python Tool), you can use MinMaxScaler, for which the [0, 1] range is the default. MaxAbsScaler is another option and may be better for sparse datasets, as it preserves the data's inherent structure. The scikit-learn User Guide has an excellent section on these techniques. In Alteryx Designer, you can try out the user-created FeatureScaler macro. This macro can also convert your data (for example, a model's predictions on your normalized data) from their normalized form back to their original units.
Standardization
Just to be extra confusing, standardization is sometimes used to cover all these forms of scaling. However, one popular use of the term is a scaling method that can be more specifically called z-score standardization. This approach takes your features' values and scales them so that they end up being normally distributed (fitting that familiar old bell curve). The values are transformed, so their mean is 0, and their standard deviation is 1. This method is also sensitive to outliers' influence.
Standardization is especially important for machine learning algorithms that use distance measures (e.g., k-nearest neighbors, k-means clustering, principal component analysis) and for those that are built on the assumption that your data are normally distributed. These will likely perform better if you provide data that fit that assumption.
As above, one option is to use Python and scikit-learn, where StandardScaler will tackle this job. If you want to standardize your data in Designer, you can locate and use this macro that's installed to support the predictive analytics tools.
Which Method and When?
As in the recent posts on model evaluation metrics, there's no one right answer for all situations. You can try multiple methods of normalization and see which one helps your model perform better.
If your data has outliers that could be problematic for the approaches described above, you may want to try RobustScaler in scikit-learn, which uses the median and interquartile range to scale the data and retains the outliers. Here's a helpful tutorial for RobustScaler, and you can also check out this great visual comparison of what data with outliers look like when handled with each of these approaches.
Finally, remember that you usually will want to apply these methods to your training dataset only, not to your entire dataset. Scaling your entire dataset and then splitting it for training/testing allows some information about the distribution of the entire dataset to be available during training. If you split after scaling, your test dataset's scaled values would be determined by ""knowledge"" of the entire dataset. However, that information will not be available when the model is actually used in production. This problem is one form of what's called data leakage. Instead, split your dataset, train your model, preprocess your test data according to the same parameters used for the training data, and then assess your model's performance.
 
Regularization: Addressing a Different Issue
 
This term seems like it should be sorted into the same category with normalization and standardization. Just looking at the word itself — it sounds like a similar concept, right?
Regularization is actually a strategy used to build better-performing models by reducing the odds of overfitting, or when your model does such a good job of matching your training data that it performs badly on new data. In other words, regularization is a way to help your model generalize better by preventing it from becoming too complex.
However, regularization is not part of data preprocessing, unlike normalization and standardization. Instead, it is an optional component in the model-building process. Regularization is often discussed in the context of regression models. In Designer, you can optionally use ridge regression, LASSO, or elastic net regularization when building linear and logistic regression models. However, regularization is definitely also relevant for other algorithms, including neural networks and support vector machines.
In the simplest terms, depending on the method used, regularization for regression models may reduce the number of variables included in a model and/or may try to bring their coefficients closer to zero or a combination of both. For neural networks, regularization could also include weight decay; dropout, where some layers' output is ignored; and early stopping when a model's training ends early because it is generalizing less well as training proceeds (among other approaches).
As you can tell, regularization is in a whole different zone of the machine learning process from normalization and standardization, so don't let its deceptively similar sound trip you up!
Additional Resources

About Feature Scaling and Normalization and the effect of standardization for machine learning algorithms
Scikit-learn documentation for scaling data during preprocessing
Standardization in Cluster Analysis
What is Regularization?
Simple is Best: Occam's Razor in Data Science
How to Avoid Overfitting in Deep Learning Neural Networks

Original. Reposted with permission.
 
Bio: Susan Currie Sivek, Ph.D. is the data science journalist for the Alteryx Community where she explores data science concepts with a global audience. She is also the host of the Data Science Mixer podcast. Her background in academia and social science informs her approach to investigating data and communicating complex ideas — with a dash of creativity from her training in journalism.
Related:

Easy Guide To Data Preprocessing In Python
Data Transformation: Standardization vs Normalization
4 Tips for Advanced Feature Engineering and Preprocessing"
https://www.kdnuggets.com/2020/11/mastering-tensorflow-tensors-5-easy-steps.html,Mastering TensorFlow Tensors in 5 Easy Steps,Discover how the building blocks of TensorFlow works at the lower level and learn how to make the most of Tensor objects.,"comments
By Orhan G. Yalçın, AI Researcher
If you are reading this article, I am sure that we share similar interests and are/will be in similar industries. So let’s connect via Linkedin! Please do not hesitate to send a contact request! Orhan G. Yalçın — Linkedin

Photo by Esther Jiao on Unsplash
 

 
In this post, we will dive into the details of TensorFlow Tensors. We will cover all the topics related to Tensors in Tensorflow in these five simple steps:

Step I: Definition of Tensors → What is a Tensor?
Step II: Creation of Tensors → Functions to Create Tensor Objects
Step III: Qualifications of Tensors → Characteristics and Features of Tensor Objects
Step IV: Operations with Tensors → Indexing, Basic Tensor Operations, Shape Manipulation, and Broadcasting
Step V: Special Types of Tensors → Special Tensor Types Other than Regular Tensors

Let’s start!
 
Definition of Tensors: What is a Tensor?
 

Figure 1. A Visualization of Rank-3 Tensors (Figure by Author)
 

 
Tensors are TensorFlow’s multi-dimensional arrays with uniform type. They are very similar to NumPy arrays, and they are immutable, which means that they cannot be altered once created. You can only create a new copy with the edits.
Let’s see how Tensors work with code example. But first, to work with TensorFlow objects, we need to import the TensorFlow library. We often use NumPy with TensorFlow, so let’s also import NumPy with the following lines:

 
Creation of Tensors: Creating Tensor Objects
There are several ways to create a tf.Tensor object. Let’s start with a few examples. You can create Tensor objects with several TensorFlow functions, as shown in the below examples:

tf.constant, tf.ones, tf.zeros, and tf.range are some of the functions you can use to create Tensor objects

Output:
tf.Tensor([[1 2 3 4 5]], shape=(1, 5), dtype=int32)
tf.Tensor([[1. 1. 1. 1. 1.]], shape=(1, 5), dtype=float32) 
tf.Tensor([[0. 0. 0. 0. 0.]], shape=(1, 5), dtype=float32) 
tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)

 
As you can see, we created Tensor objects with the shape (1, 5) with three different functions and a fourth Tensor object with the shape (5, )using tf.range() function. Note that tf.ones and tf.zeros accepts the shape as the required argument since their element values are pre-determined.
 
Qualifications of Tensors: Characteristics and Features of Tensor Objects
TensorFlow Tensors are created as tf.Tensor objects, and they have several characteristic features. First of all, they have a rank based on the number of dimensions they have. Secondly, they have a shape, a list that consists of the lengths of all their dimensions. All tensors have a size, which is the total number of elements within a Tensor. Finally, their elements are all recorded in a uniform Dtype (data type). Let’s take a closer look at each of these features.
 
Rank System and Dimension
Tensors are categorized based on the number of dimensions they have:

Rank-0 (Scalar) Tensor: A tensor containing a single value and no axes (0-dimension);
Rank-1 Tensor: A tensor containing a list of values in a single axis (1-dimension);
Rank-2 Tensor: A tensor containing 2-axes (2-dimensions); and
Rank-N Tensor: A tensor containing N-axis (N-dimensions).


Figure 2. Rank-1 Tensor | Rank-2 Tensor| Rank-3 Tensor (Figure by Author)
 

 
For example, we can create a Rank-3 tensor by passing a three-level nested list object to the tf.constant function. For this example, we can split the numbers into a 3-level nested list with three-element at each level:

The code to create a Rank-3 Tensor object

Output:
tf.Tensor( [[[ 0  1  2]   
             [ 3  4  5]]   
             
            [[ 6  7  8]   
             [ 9 10 11]]],
  shape=(2, 2, 3), dtype=int32)

 
We can view the number of dimensions that our `rank_3_tensor` object currently has with the `.ndim` attribute.


Output:
The number of dimensions in our Tensor object is 3

 
 
Shape
The shape feature is another attribute that every Tensor has. It shows the size of each dimension in the form of a list. We can view the shape of the rank_3_tensor object we created with the .shape attribute, as shown below:


Output:
The shape of our Tensor object is (2, 2, 3)

 
As you can see, our tensor has 2 elements at the first level, 2 elements in the second level, and 3 elements in the third level.
 
Size
Size is another feature that Tensors have, and it means the total number of elements a Tensor has. We cannot measure the size with an attribute of the Tensor object. Instead, we need to use tf.size() function. Finally, we will convert the output to NumPy with the instance function .numpy() to get a more readable result:


Output:
The size of our Tensor object is 12

 
 
Dtypes
Tensors often contain numerical data types such as floats and ints, but may contain many other data types such as complex numbers and strings.
Each Tensor object, however, must store all its elements in a single uniform data type. Therefore, we can also view the type of data selected for a particular Tensor object with the .dtype attribute, as shown below:


Output:
The data type selected for this Tensor object is <dtype: 'int32'>

 
 
Operations with Tensors
 
Indexing
An index is a numerical representation of an item’s position in a sequence. This sequence can refer to many things: a list, a string of characters, or any arbitrary sequence of values.
TensorFlow also follows standard Python indexing rules, which is similar to list indexing or NumPy array indexing.
A few rules about indexing:

Indices start at zero (0).
Negative index (“-n”) value means backward counting from the end.
Colons (“:”) are used for slicing: start:stop:step.
Commas (“,”) are used to reach deeper levels.

Let’s create a rank_1_tensor with the following lines:


Output: 
tf.Tensor([ 0  1  2  3  4  5  6  7  8  9 10 11], 
  shape=(12,), dtype=int32)

 
and test out our rules no.1, no.2, and no.3:


Output: 
First element is: 0 
Last element is: 11 
Elements in between the 1st and the last are: [ 1  2  3  4  5  6  7  8  9 10]

 
Now, let’s create our rank_2_tensor object with the following code:


Output:
tf.Tensor( [[ 0  1  2  3  4  5]  
            [ 6  7  8  9 10 11]], shape=(2, 6), dtype=int32)

 
and test the 4th rule with several examples:


Output: 
The first element of the first level is: [0 1 2 3 4 5] 
The second element of the first level is: [ 6  7  8  9 10 11] 
The first element of the second level is: 0 
The third element of the second level is: 2

 
Now, we covered the basics of indexing, so let’s take a look at the basic operations we can conduct on Tensors.
 
Basic Operations with Tensors
You can easily do basic math operations on tensors such as:

Addition
Element-wise Multiplication
Matrix Multiplication
Finding the Maximum or Minimum
Finding the Index of the Max Element
Computing Softmax Value

Let’s see these operations in action. We will create two Tensor objects and apply these operations.

We can start with addition.


Output:
tf.Tensor( [[ 3.  7.]  
            [11. 15.]], shape=(2, 2), dtype=float32)

 
Let’s continue with the element-wise multiplication.


Output:
tf.Tensor( [[ 2. 12.]  
            [30. 56.]], shape=(2, 2), dtype=float32)

 
We can also do matrix multiplication:


Output:
tf.Tensor( [[22. 34.]  
            [46. 74.]], shape=(2, 2), dtype=float32)

 
NOTE: Matmul operations lays in the heart of deep learning algorithms. Therefore, although you will not use matmul directly, it is crucial to be aware of these operations.
Examples of other operations we listed above:


Output:
The Max value of the tensor object b is: 7.0 
The index position of the Max of the tensor object b is: [1 1] 
The softmax computation result of the tensor object b is: [[0.11920291 0.880797  ]  [0.11920291 0.880797  ]]

 
 
Manipulating Shapes
Just as in NumPy arrays and pandas DataFrames, you can reshape Tensor objects as well.
The tf.reshape operations are very fast since the underlying data does not need to be duplicated. For the reshape operation, we can use thetf.reshape() function. Let's use the tf.reshape function in code:


Output:
The shape of our initial Tensor object is: (1, 6) 
The shape of our initial Tensor object is: (6, 1) 
The shape of our initial Tensor object is: (3, 2) 
The shape of our flattened Tensor object is: tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32)

 
As you can see, we can easily reshape our Tensor objects. But beware that when doing reshape operations, a developer must be reasonable. Otherwise, the Tensor might get mixed up or can even raise an error. So, look out for that 😀.
 
Broadcasting
When we try to do combined operations using multiple Tensor objects, the smaller Tensors can stretch out automatically to fit larger tensors, just as NumPy arrays can. For example, when you attempt to multiply a scalar Tensor with a Rank-2 Tensor, the scalar is stretched to multiply every Rank-2 Tensor element. See the example below:


Output:
tf.Tensor( [[ 5 10]  
            [15 20]], shape=(2, 2), dtype=int32)

 
Thanks to broadcasting, you don’t have to worry about matching sizes when doing math operations on Tensors.
 
Special Types of Tensors
We tend to generate Tensors in a rectangular shape and store numerical values as elements. However, TensorFlow also supports irregular, or specialized, Tensor types, which are:

Ragged Tensors
String Tensors
Sparse Tensors


Figure 3. Ragged Tensor | String Tensor| Sparse Tensor (Figure by Author)
 

 
Let's take a closer look at what each of them is.
 
Ragged Tensors
Ragged tensors are tensors with different numbers of elements along the size axis, as shown in Figure X.
You can build a Ragged Tensor, as shown below:


Output:
<tf.RaggedTensor [[1, 2, 3], 
                  [4, 5], 
                  [6]]>

 
 
String Tensors
String Tensors are tensors, which stores string objects. We can build a String Tensor just as you create a regular Tensor object. But, we pass string objects as elements instead of numerical objects, as shown below:


Output:
tf.Tensor([b'With this' 
           b'code, I am' 
           b'creating a String Tensor'],
  shape=(3,), dtype=string)

 
 
Sparse tensors
Finally, Sparse Tensors are rectangular Tensors for sparse data. When you have holes (i.e., Null values) in your data, Sparse Tensors are to-go objects. Creating a sparse Tensor is a bit time consuming and should be more mainstream. But, here is an example:


Output:
tf.Tensor( [[ 25   0   0   0   0]
            [  0   0   0   0   0]
            [  0   0  50   0   0]
            [  0   0   0   0   0]
            [  0   0   0   0 100]], shape=(5, 5), dtype=int32)

 
 
Congratulations
We have successfully covered the basics of TensorFlow’s Tensor objects.
Give yourself a pat on the back!
This should give you a lot of confidence since you are now much more informed about the building blocks of the TensorFlow framework.
Check Part 1 of this tutorial series:
Beginner's Guide to TensorFlow 2.x for Deep Learning Applications
Understanding the TensorFlow Platform and What it has to Offer to a Machine Learning Expert
Continue with Part 3 of the series:
Mastering TensorFlow “Variables” in 5 Easy Step
Learn how to use TensorFlow Variables, their differences from plain Tensor objects, and when they are preferred over…
 
Subscribe to the Mailing List for the Full Code
If you would like to have access to full code on Google Colab and the rest of my latest content, consider subscribing to the mailing list:

Slide to Subscribe to My Newsletter
Finally, if you are interested in applied deep learning tutorials, check out some of my articles:
Image Classification in 10 Minutes with MNIST Dataset
Using Convolutional Neural Networks to Classify Handwritten Digits with TensorFlow and Keras | Supervised Deep Learning
Image Generation in 10 Minutes with Generative Adversarial Networks
Using Unsupervised Deep Learning to Generate Handwritten Digits with Deep Convolutional GANs using TensorFlow and the…
Image Noise Reduction in 10 Minutes with Convolutional Autoencoders
Using Deep Convolutional Autoencoders to Clean (or Denoise) Noisy Images with the help of Fashion MNIST | Unsupervised…
Using Recurrent Neural Networks to Predict Bitcoin (BTC) Prices
Wouldn’t it be awesome if you were, somehow, able to predict tomorrow’s Bitcoin (BTC) price? Cryptocurrency market has…
Bio: Orhan G. Yalçın is an AI Researcher in the legal domain. He is a qualified lawyer with business development and data science skills, and has previously worked as a legal trainee for Allen & Overy on capital markets, competition, and corporate law matters.
Original. Reposted with permission.
Related:

WTF is a Tensor?!?
Getting Started with TensorFlow 2
The Most Important Fundamentals of PyTorch you Should Know"
https://www.kdnuggets.com/2021/02/multidimensional-multi-sensor-time-series-data-analysis-framework.html,Multidimensional multi-sensor time-series data analysis framework,This blog post provides an overview of the package “msda” useful for time-series sensor data analysis. A quick introduction about time-series data is also provided.,"comments
By Ajay Arunachalam, Orebro University
Hello, friends. In this blog post, I will take you through my package “msda” useful for time-series sensor data analysis. A quick introduction about time-series data is also provided. The demo notebook can be found on here
One of the specific use case applications focused on “Unsupervised Feature Selection” using the package can be found in the blog post here.
 
What is Time Series Data?
 
Time series data is information taken at a particular duration. For instance, having a set of sensor data observed at particular equal paces, each sensor can be classified as time series. If the data is collected without any order in time, or at once, it is not time series data.
There are two types of time series data:
1- Stock Series (Measure of attribute, in particular point of time)
2- Flow Series (Measure of activity, in a time interval)
 
Components of Time Series Data
 
To analyze time series data, we need to know the different pattern types. These patterns will together create the set of observations on time series.
1) Trend: A long pattern present in the time series. It represents the variations of low, medium and high frequency filtered out from the time series.
If there is no increasing or decreasing pattern in the time series data, it is taken as stationary in the mean.
There are two types of trend pattern:

Deterministic: In this case, the effects of shocks present in the time series are eliminated.
Stochastic: It is the process in which the effects of shocks are never eliminated as they have permanently changed the level of the time series.

2) Cyclic: The pattern exhibit up and down movements around a specified trend. The period of time is not fixed and usually composed of at least 2 months in duration.
3) Seasonal: Pattern that reflects regular fluctuations. These short-term movements occur due to the seasonal and custom factors of people. The data faces regular and predictable changes which occurs on regular intervals of calendar. It always consist of fixed and known period.
The main sources of seasonality:

Climate
Institutions
Social habits and practices
Calendar etc.

Models to create a seasonal component in time series:

Additive Model — It is the model in which the seasonal component is added with the trend component.
Multiplicative Model — In this model seasonal component is multiplied with the intercept if trend component is not present in the time series.

4) Irregular: It is an unpredictable component of time series.
 
Time Series Data vs Cross-Section Data
 
Time Series Data is composed of collection of data of one specific variable at particular interval of time. On the other hand, Cross-Section Data is consist of collection of data on multiple variables from different sources at a particular interval of time. Collection of company’s stock market data at regular interval of year is an example of time series data. But when the collection of company’s sales revenue, sales volume is collected for the past 3 months then it is taken as an example of cross-section data. Time series data is mainly used for obtaining results over an extended period of time, but cross-section data focuses on the information received from surveys at a particular time.
 
What is Time Series Analysis?
 
Analysis is performed in order to understand the structure and functions produced by the time series.
Two approaches are used for analyzing time series data are -

In the time domain
In the frequency domain

Time series analysis is mainly used for -

Decomposing the time series
Identifying and modeling the time-based dependencies
Forecasting
Identifying and model the system variation

 
Need of Time Series Analysis
 
In order to model successfully, the time series is important in machine learning and deep learning. Time series analysis is used to understand the internal structure and functions that are used for producing the observations. Time Series analysis is used for -

Descriptive — Patterns are identified in correlated data. In other words, the variations in trends and seasonality in the time series are identified.
Explanation — Understanding and modeling of data is performed.
Forecasting — The prediction from previous observations are performed for short term trends.
Invention Analysis — Effect performed by any event in time series data, is analyzed.
Quality Control — When the specific size deviates, it provides an alert.

 
Applications of Time Series Analysis
 


Few Time-Series Application Area Examples

 
Now, that we have seen through the basics of time-series, let’s dwell into the MSDA package & its details.
 
What is MDSA?
 
MSDA is an open source low-code Multi-Sensor Data Analysis library in Python that aims to reduce the hypothesis to insights cycle time in a time-series multi-sensor data analysis & experiments. It enables users to perform end-to-end proof-of-concept experiments quickly and efficiently. The module identifies events in the multidimensional time series by capturing the variation and trend to establish relationships aimed towards identifying the correlated features helping in feature selection from raw sensor signals.
The package includes:-

Time series analysis.
The variation of each sensor column wrt time (increasing, decreasing, equal).
How each column values varies wrt other column, and the maximum variation ratio between each column wrt other column.
Relationship establishment with trend array to identify most appropriate sensor.
User can select window length and then check average value and standard deviation across each window for each sensor column.
It provides count of growth/decay value for each sensor column values above or below a threshold value.
Feature Engineering

a) Features involving trend of values across various aggregation windows: change and rate of change in average, std. deviation across window.
b) Ratio of changes, growth rate with std. deviation.
c) Change over time.
d) Rate of change over time.
e) Growth or decay.
f) Rate of growth or decay.
g) Count of values above or below a threshold value.
 
Overview:-
 
Prototype for feature/sensor selection from multi-dimensional heterogeneous/homogeneous time series multi-sensor data. The intuitive representation of the framework is as shown below.


Pictorial representation of multi-dimensional time series data feature selection

 
Features Include:-
 


Core Functionalities in MSDA

 
MSDA Workflow:-
 


MSDA algorithm workflow

 
Terminal Installation:-
 
The easiest way to install msda is using pip.

pip install msda


or

$ git clone https://github.com/ajayarunachalam/msda
$ cd msda
$ python setup.py install


 
Install in Jupyter Notebook:-
 

!pip install msda


Follow the rest as demonstrated in the demo example [here] — https://github.com/ajayarunachalam/msda/tree/master/demo.ipynb
 
Who should use MSDA?
 
MSDA is an open source library that anybody can use. In my view, the ideal target audience of MSDA is:

Students.
Researchers for quick poc testing.
Experienced Data Scientists who want to increase productivity.
Citizen Data Scientists who prefer a low code solution.
Data Science Professionals and Consultants involved in building Proof of Concept projects.

 
Contact
 
You can reach me at ajay.arunachalam08@gmail.com
Thank you for reading. Happy Learning :)
 
References
 
Time series
A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is…
 
Introduction to Time Series Analysis
Hi folks,Thanks a lot for reading my blog posts and motivating me for writing.You can read all my blogs Here.
 
Bio: Ajay Arunachalam (personal website) is a Postdoctoral Researcher (Artificial Intelligence) at Centre for Applied Autonomous Sensor Systems, Orebro University, Sweden. Prior to this, he was working as a Data Scientist at True Corporation, a Communications Conglomerate, working with Petabytes of data, building & deploying deep models in production. He truly believes that Opacity in AI systems is need of the hour, before we fully accept the power of AI. With this in mind, he has always strived to democratize AI, and be more inclined towards building Interpretable Models. His interest is in Applied Artificial Intelligence, Machine Learning, Deep Learning, Deep RL, and Natural Language Processing, specifically learning good representations. From his experience working on real-world problems, he fully acknowledges that finding good representations is the key in designing the system that can solve interesting challenging real-world problems, that go beyond human-level intelligence, and ultimately explain complicated data for us that we don't understand. In order to achieve this, he envisions learning algorithms that can learn feature representations from both unlabelled and labelled data, be guided with and/or without human interaction, and that are on different levels of abstractions in order to bridge the gap between low-level data and high-level abstract concepts.
Original. Reposted with permission.
Related:

Building AI Models for High-Frequency Streaming Data
Simple & Intuitive Ensemble Learning in R
Simple Python Package for Comparing, Plotting & Evaluating Regression Models"
https://www.kdnuggets.com/2021/09/sas-popular-certifications-data-analytics-skills.html,Popular Certifications to validate your data and analytics skills,"Check out the most popular certifications from SAS to see what certification you want to pursue next. Now through the end of 2021, you can save 55% on your exam!","Sponsored Post.

There’s no doubt that IT certifications provide value to individuals and employers in today’s competitive job market. SAS recently surveyed SAS Coursera learners and 84% of certified individuals said a SAS certification helped improve their performance and advance their careers. This is similar to findings from the 2021 Pearson Vue Value of IT Certification Report, which highlights the benefits of increased confidence, greater determination to professionally succeed, increased job satisfaction, and greater work autonomy and independence. 
Check out the most popular certifications from SAS to see what certification you want to pursue next. Now through the end of 2021, you can save 55% on your exam!
 
1. SAS Certified Specialist: Base Programming
The Base Programming Specialist certification is a performance-based exam that tests your skills and validates your knowledge in accessing and creating data structures, managing data, error handling and generating reports and output. Frequently on Certification Magazine’s Annual IT Salary survey, the average base salary for this certification is $99, 920 – a 15% increase in only two years. 
 
2. SAS Certified Professional: Advanced Programming
The Advanced Programming Professional certification is another performance-based exam that tests your skills and validates your knowledge in accessing data using SQL, macro processing, advanced SAS programming techniques such as hash objects and dimensional arrays. Performance-based exams mean that you will access SAS to write and execute SAS code during the exam. If you’re not sure which programming exam is right for you, look at the comparison chart to help guide you down the right path. 
 
3. SAS Certified Professional: Artificial Intelligence & Machine Learning
This certification is designed for those who want to demonstrate their AI and analytics expertise using open source and SAS tools to garner insight from data. This certification validates your knowledge in Machine Learning, Natural Language Processing, Computer Vision and Model Forecasting and Optimization. As the demand for AI and ML skills continues to peak, so will the need to acquire certifications like this. 
 
4. SAS Certified Statistical Business Analyst
If you use SAS Statistical Procedures or SAS/STAT® software to conduct and interpret complex statistical data analysis, then consider taking this certification. You’ll be tested on ANOVA, Linear and Logistic Regression, Measure Model Performance and preparing Inputs for Predictive Model Performance. 
 
5. SAS® Viya® Programming Specialist
With the continued acceleration of cloud adoption, comes the continued need to evolve IT skills and acquire certifications. The SAS Viya Programming Specialist certification is new, so although this is not our most popular certification (yet!) it deserves a worthy mention. Especially for those who want to demonstrate a cloud-ready mentality and expertise in working with CAS tables and data sources, using CAS-enabled procedures and user-defined formats, modifying DATA step and SQL programs to run in CAS and using the CAS programming language, CASL. Check out this SAS Users YouTube video to hear more about our current SAS Viya certifications.
 
Each certification has aligned training, exam prep resources and practice exams so that you feel confident and prepared heading into the certification exam. The global SAS® certification program offers more than just the certifications discussed here to ensure you can diversify your professional toolkit and exemplify the skills you have."
https://www.kdnuggets.com/2021/04/data-science-predict-prevent-real-world-problems.html,Using Data Science to Predict and Prevent Real World Problems,"Do you have an interest in data science but lack an understanding of what, exactly, it can be used to accomplish in the real world? Read this article for a few examples of just how helpful data science can be for predicting and preventing real world problems.","By Devin Partida, Editor-in-Chief of ReHack.com.
comments
People in various industries are increasingly interested in using data science to learn more about notable trends that improve their decision-making. It’s also instrumental in predicting events and preventing unwanted consequences. Here are some thought-provoking examples.

Image by Gerd Altmann from Pixabay
 
1. Thwarting Cyberattacks
 
Many industries now have an extensive online presence. However, targeted cyberattacks can quickly hinder entire companies, forcing employees to temporarily switch to pen-and-paper recordkeeping or deal with the loss of access to files or portals.
However, data science can help company IT teams stay on top of and stop threats. For example, associate rule learning (ARL) gauges risk based on certain characteristics. It allows managers to predict possible issues based on whether they share commonalities with past problems.
Prevention also becomes possible when data science helps spot outliers. Suppose a system sees that an unknown person with an IP address in a foreign country tried to access a workplace network in the middle of the night. It could automatically deny that individual until a human takes a closer look.
 
2. Improving Law Enforcement Policies 
 
Ongoing police violence issues against people of color have led to a call for drastic change in the law enforcement sector. However, most people don’t know the best ways to go about making progress.
An organization called Campaign Zero uses a data-driven approach to suggest new policy frameworks for the future of policing. Researchers there reviewed 600 police union contracts. The results showed that 84% of the documents contained at least one stipulation that made it difficult to hold police officers accountable.
Data also shows which locations have police departments associated with the deaths of Black men that are higher than the national homicide rate. Another way to apply data analytics is to assess which officers have above-average incidents of misconduct or complaints lodged against them. Police chiefs could then make departmental decisions about interventions that could predict and prevent unnecessary violence.
 
3. Keeping Output Levels High
 
People are particularly interested in how data science could assist industries when success almost entirely depends on achieving consistent output. In such cases, unexpected events or misguided judgments could lead to plunging profits and dissatisfied clients.
However, data science could support preventive maintenance programs used in facilities such as manufacturing plants. Research suggests that 80% of maintenance technicians would rather prevent issues than react to them. When plant leaders use connected sensors that feed into a data analysis platform, they can predict when parts will fail or prevent problems by alerting people to concerning trends.
In Australia, government officials believe data science could be the key to improving results in the mining industry. Difficulties in finding new, high-quality mineral deposits mean companies often engage in costly exploration efforts that fall short of expectations. Data science could help with that, as well as monitor ventilation systems at established mines. Workers would stay safer while companies avoid excessive costs.
 
4. Helping Fitness Facilities Engage Customers 
 
Many people can relate to the scenario of joining a gym and making good on promises to get fit but becoming less enthusiastic over time. Data science can predict what causes a loss of motivation and prevent it from happening. It can also assist in making personalized recommendations for members.
A venture-backed technology company called CIPIO helps gym owners convert data into actionable strategies. Records may indicate that a particular member has only attended yoga sessions, and their attendance has gradually become less consistent. The system could recommend that gym staff inform that person about a class that combines yoga with brief periods of intensive cardio. That suggestion could raise interest by presenting a different opportunity.
Such a tool might also show broader trends, such as which months most members end their subscriptions. If the data indicates it usually happens in March, a club could offer extra incentives to reduce the chances of people leaving. It could also predict the likely interest in a new, specific type of class, such as chair workouts for older or injured people.
 
5. Avoiding Out-of-Stock Instances 
 
Consumers understandably get upset when they intend to buy something and find empty shelves. That’s why many retailers now use big data to predict demand and prevent stock shortages. Data analytics systems can look at societal trends, weather patterns, regional preferences and other specifics to assess people’s likelihood of buying.
That approach can also send the appropriate quantities of merchandise to the right locations, such as chain stores with national outlets. The data may show a rapid increase in people wanting workout clothes in Colorado, while such sales decline or remain flat in Arkansas. Retailers could use that information to keep stores adequately stocked.
Some large-scale festivals, such as the United Kingdom’s Glastonbury, have also relied on data science to predict outcomes and prevent disappointment. Researchers examined data to see whether a specific year was likely to have more or less rainfall than usual. That information helped retailers know what kind of merchandise to bring with them. The same approach enabled vendors to determine the most popular food and beverages so they could be better prepared.
 
Data Science Has Abundant Potential 
 
Most business leaders know the importance of understanding the overall likelihood of undesirable events and doing what they can to stop them from happening. Data science can make those aims more manageable, within and outside of the examples here.
It’s impossible to predict the future with certainty, but data science allows tracking trends and extracting meaningful insights from them. Regardless of the industry or audience served, it can help business decision-makers operate their industries smoothly while minimizing adverse outcomes.
 
Bio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com.
Related:

The Potential of Predictive Analytics in Labor Industries
Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions
How Automation Is Improving the Role of Data Scientists"
https://www.kdnuggets.com/2020/12/resampling-imbalanced-data-limits.html,Resampling Imbalanced Data and Its Limits,Can resampling tackle the problem of too few fraudulent transactions in credit card fraud detection?,"comments
By Maarit Widmann, Data Scientist at KNIME
Introduction
 
Car parking ticket machines used to only accept coins. A self-service vegetable stand used to only accept cash. And not such a long time ago I could buy a bus ticket from the bus driver! These days, however, you can (and you’re often encouraged to) pay for these and many other products and services by credit card. This leads to more and more transactions and also to types of transactions that didn’t exist before. Some time back, a credit card transaction for a vegetable stand would have looked suspicious!
With the increasing variety and volume of credit card usage, fraud is evolving too [1]. This is a huge challenge! For automatic fraud detection and prevention, a number of supervised and unsupervised fraud detection models have been suggested. The unsupervised methods, such as a neural autoencoder, are anomaly detection models and don’t require labeled data. The supervised methods, such as a decision tree or a logistic regression model, require labeled data, which are often not available. Imagine someone manually recognizing and labeling the transactions as “fraudulent” or “legitimate”! Another problem is that the fraudulent transactions are very few compared to the large amounts of legitimate transactions. This imbalance of the target classes decreases the performance of the decision tree algorithm and of other classification algorithms [2]. 
In this article, we will work with labeled, highly imbalanced transactions data: For each fraudulent transaction we have 579 legitimate transactions. We’ll check if we can improve the performance of a decision tree model by resampling; that is, by artificially creating more data about fraudulent transactions. Along the way, we’ll explain three different resampling methods and evaluate their effects on the fraud prevention application. At the end, we’ll provide a link to a KNIME workflow - an example implementation of the different resampling methods. 
 
Building a Classification Model for Fraud Detection 
 
In our demonstration we use the creditcard.csv data available on Kaggle. The data consist of 284807 credit card transactions, performed by EU cardholders in September 2013. 492 (0.2%) of the credit card transactions are fraudulent, and the remaining 284315 (99.8%) transactions are legitimate. The data contain a target class column with possible values fraudulent / legitimate, the time and amount of each transaction, and 28 principal components generated from the confidential features of the transactions. 
The workflow in Figure 1 shows the steps for accessing, preprocessing, resampling, and modeling the transactions data. Inside the yellow box, we access the transactions data, encode the target column from 0/1 to legitimate/fraudulent, and partition the data into training and test sets using 80/20 split and stratified sampling on the target column. Inside the orange boxes, we build four different versions of a decision tree model for fraud detection: a baseline model trained on the original training data plus three models trained on 

SMOTE oversampled data, 
Bootstrap oversampled data, and 
Bootstrap undersampled data.



Figure 1. A workflow that accesses and preprocesses transactions data and implements four different decision tree models for fraud detection: one trained on the original data and three models trained on resampled data.

 
 
Resampling Techniques
 
Table 1 summarizes the resampling methods that we include in our demonstration: 

oversampling (SMOTE) 
oversampling (Bootstrap)
undersampling (Bootstrap)

 


Table 1. Overview of three resampling methods: SMOTE, oversampling (Bootstrap), and undersampling (Bootstrap)

 
Effects of Resampling on Fraud Detection Performance
 
Resampling has two drawbacks, especially when the target class is as highly imbalanced as in our case. Firstly, oversampling the minority class might lead to overfitting, i.e. the model learns patterns that only exist in the particular sample that has been oversampled. Secondly, undersampling the majority class might lead to underfitting, i.e. the model fails to capture the general pattern in the data [3].
We compare the performances of the baseline model and the models trained on resampled data in terms of two scoring metrics: recall and precision (Figure 2). The metrics are explained in detail in the From Modeling to Scoring: Confusion Matrix and Class Statistics blog post. 

Recall is the proportion of correctly predicted fraudulent transactions. The higher the recall, the more fraudulent transactions are prevented by the model. 
Precision is the proportion of actual fraudulent transactions among those predicted as fraudulent. The higher the precision, the fewer false alarms are raised by the model.



Figure 2. Recall and precision statistics obtained by four decision tree models for fraud detection, each one trained on a different training set

  
The very low precision value 2% for the undersampled model in the bottom right corner in Figure 2 indicates underfitting: The undersampled model failed to learn the patterns underlying the  legitimate transactions. This seems reasonable considering that we discarded 99.8% of the legitimate transactions in the undersampling phase! Indeed, with so few examples in the fraudulent class, the only effect of undersampling has been to damage the representation of the legitimate class.
If you take a look at the performances obtained via oversampling in the two middle rows, you can see from their precision value that these models are raising more false alarms than the model trained on the full original data, while at the same time not improving the recognition of  the pattern underlying the fraudulent transactions. All of this indicates that the model has overfitted the data. 
As you can see, our fraud detection model is over-/underfitting when trained on resampled data. What is actually happening under the hood? 
 
Demonstrating Over- and Underfitting in Fraud Detection
 
Transactions data are confidential, and we can therefore only work with principal components as predictors of the target class fraudulent/legitimate. In order to better understand how a resampled model leads to over- or underfitting, let’s imagine we had some of the following columns in our data, since they often characterize fraudulent transactions [4]:

Shipping address equals billing address: yes/no
Urgent delivery: yes/no
Number of different items in the order
Number of orders of the same item
Number of credit cards associated with the shipping address

Since our training data only contain 394 fraudulent transactions, it could be, for example, that the majority of them are characterized by exceptionally many orders of the same item: one for 20 toasters, another for 50 smartphones, yet another for 25 winter coats, and so on. In reality, the fraudulent transactions are much more varying and continuously evolving. On the contrary, the 227451 legitimate transactions in the training data represent a huge variety of ways of using the credit card: for a banana, hotel booking, toaster, car parking, and more! 
In the following, we explain how the different resampling methods skew the transactions data, and how this leads to a deterioration in model performance, as we saw before.
 
Oversampling (SMOTE)
 
The corresponding model has a low precision value (44%) and it therefore raises many false alarms. The training set contains the original fraudulent transactions plus the synthetically generated fraudulent transactions within the feature space of the fraudulent transactions. For example, if we had one fraudulent transaction that purchases 20 toasters, SMOTE resampling might produce thousands of slightly different fraudulent transactions that all purchase a toaster. Eventually, all legitimate transactions that include a toaster would be predicted as fraudulent. This kind of model is overfitting the training data (Figure 3).


Figure 3. Example class predictions generated by an overfitting fraud detection model trained on SMOTE oversampled data: An arbitrary feature, for example, product=toaster incorrectly determines fraudulent transactions in new data.

 
Oversampling (Bootstrap)
 
This model performs worse than the baseline model in terms of both recall and precision. The training set contains thousands of exact copies of the original fraudulent transactions. For example, if we had a fraudulent transaction ordering 20 toasters, all legitimate transactions that ordered 20 items of the same product or a toaster would be suspicious, because these two features would have characterized so many fraudulent transactions in the oversampled data. At the same time, the model would have failed to generalize on large amounts of the same item as suspicious, emphasizing instead precisely 20 toasters as suspicious. Also this model is overfitting the training data (Figure 4). 


Figure 4. Example class predictions generated by an overfitting fraud detection model trained on Bootstrap oversampled data: an arbitrary set of features, for example, product=toaster and # of products=20 incorrectly determines fraudulent transactions in new data.

 
 
Undersampling (Bootstrap)
 
The model performs almost perfectly in terms of recall (92%), yet the worst in terms of precision (2%). Since more than 99% of the original transactions are discarded in the undersampling phase, our training data might only consist of credit card transactions for food and leave out hotel bookings, car parking, and many others. This kind of training data is not representative of the real data. Therefore, almost all transactions are predicted as fraudulent; the model is underfitting (Figure 5). 


Figure 5. Example class predictions generated by an underfitting fraud detection model trained on Bootstrap undersampled data: the model predicts all transactions as fraudulent in new data.

 
Diagnosing Problems of Resampling in Fraud Detection
 
As our example shows, in this case resampling can’t solve the problem of having too few fraudulent transactions in the dataset. However, resampling is shown to lead to performance gain when the a priori distribution is less skewed, for example, in disease detection [5]. Why is resampling failing then, for this credit card transaction dataset with too few fraudulent transactions in it?
Frauds are perpetrated in a large variety of patterns, and we have only a few fraudulent transactions in our training data. So fraud patterns are definitely under-represented in our training set. Resampling doesn’t solve the problem, because it does not increase the variety of the representation of fraudulent transactions, it just replicates in some form the fraud patterns represented in the dataset. Thus, the models trained on resampled data can only perform well in detecting some types of fraud, the types represented in the training data. 
In summary, the class of fraudulent transactions is too under-represented (just 0.2% of the whole dataset!) to represent a meaningful description of all the fraud patterns that are out there. Even introducing new similar synthetic fraudulent transactions cannot significantly change the range of the represented fraudulent transactions.
 
Conclusions
 
Transactions data are produced in huge volumes every day. In order to build a supervised model for fraud detection, they would need to be labeled. The labeling process, however, in this case, is tricky.
Firstly, even if we had the knowledge to appropriately label fraudulent transactions, the process would be very resource-intensive. Skillful experts at detecting frauds are rare and expensive and usually do not spend their time labeling datasets. Even with reliable and sufficient resources, manual labeling would take a prohibitively long time before a sufficiently large amount of data would be available. 
Secondly, expertise in fraud detection is so scarce, because criminals creatively devise ever newer fraud schemes and it is hard to keep up the pace with the newly introduced patterns. An expert might recognize all types of frauds known till then and still fail at recognizing the new fraud schemes, most recently created.
Finally, and luckily, there are generally fewer fraudulent transactions than legitimate transactions. Even after all this manual effort by extremely skillful people, we might still end up with an insufficient number of data for the fraudulent class.
Those are all reasons why fraud detection is often treated as a rare class problem, rather than an imbalanced class problem.
Yet, we can try. With this dataset, can we artificially increase the sample size of fraudulent transactions by resampling the training data? Not really. Resampling can improve the model performance if the target classes are imbalanced and yet sufficiently represented. In this case, the problem is really the lack of data. Resampling is subsequently leading to over- or underfitting rather than to a better model performance. 
This article just aims at giving you an idea of why, in some cases, resampling cannot work. Of course, better results could be obtained with more sophisticated resampling methods than those we have introduced in this article, like for example, a combination of under- and oversampling [6]. Better results could also be obtained with supervised algorithms other than the decision tree. Some machine learning supervised algorithms, such as logistic regression, are less sensitive to class imbalance than the decision tree, while other algorithms, such as ensemble models, are more robust to overfitting. Even better results could be obtained with the decision tree, for example by applying pruning techniques to avoid the overfitting effect or controlling the tree growth.
However, sometimes we must accept that the data is just not sufficient to describe the minority class. In this case, we must proceed with unlabeled data and try to isolate the events of the rare class via unsupervised algorithms, such as neural autoencoders, isolation forest, and clustering algorithms.
The Resampling in Supervised Fraud Detection Models workflow, used in this article to show the limitations of resampling, can be downloaded for free from the KNIME Hub. 
 
References
 
[1]""The Nilson Report."" HSN Consultants, Inc., November 2019,  Issue 1164, https://nilsonreport.com/publication_newsletter_archive_issue.php?issue=1164. Accessed 14 Oct. 2020.
[2]""Random Oversampling and Undersampling for Imbalanced Classification"" Machine Learning Mastery Pty. Ltd., January 2020, https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/. Accessed 14 Oct. 2020.
[3]""Oversampling and Undersampling"" Medium, September 2010, https://towardsdatascience.com/oversampling-and-undersampling-5e2bbaf56dcf. Accessed 14 Oct. 2020.
[4]https://www.bluefin.com/support/identifying-fraudulent-transactions/
[5]""Machine Learning Resampling Techniques for Class Imbalances"" Medium, January 2011, https://towardsdatascience.com/machine-learning-resampling-techniques-for-class-imbalances-30cbe2415867. Accessed 14 Oct. 2020.
[6]""How to Combine Oversampling and Undersampling for Imbalanced Classification"" Machine Learning Mastery Pty. Ltd., January 2020, https://machinelearningmastery.com/combine-oversampling-and-undersampling-for-imbalanced-classification/. Accessed 14 Oct. 2020.
 
Bio: Maarit Widmann is a Data Scientist at KNIME.
Related:

Undersampling Will Change the Base Rates of Your Model’s Predictions
The 5 Most Useful Techniques to Handle Imbalanced Datasets
Pro Tips: How to deal with Class Imbalance and Missing Labels"
https://www.kdnuggets.com/2020/11/essential-math-data-science-integrals-area-under-curve.html,Essential Math for Data Science: Integrals And Area Under The Curve,"In this article, you’ll learn about integrals and the area under the curve using the practical data science example of the area under the ROC curve used to compare the performances of two machine learning models.","By Hadrien Jean, Machine Learning Scientist.
comments

 
Calculus is a branch of mathematics that gives tools to study the rate of change of functions through two main areas: derivatives and integrals. In the context of machine learning and data science, you might use integrals to calculate the area under the curve (for instance, to evaluate the performance of a model with the ROC curve, or to calculate probability from densities.
In this article, you’ll learn about integrals and the area under the curve using the practical data science example of the area under the ROC curve used to compare the performances of two machine learning models. Building from this example, you’ll see the notion of the area under the curve and integrals from a mathematical point of view (from my book Essential Math for Data Science).
 
Practical Project
 
Let’s say that you would like to predict the quality of wines from various of their chemical properties. You want to do a binary classification of the quality (distinguishing very good wines from not very good ones). You’ll develop methods allowing you to evaluate your models considering imbalanced data with the area under the Receiver Operating Characteristics (ROC) curve.
Dataset
To do this, we’ll use a dataset showing various chemical properties of red wines and ratings of their quality. The dataset comes from here: https://archive.ics.uci.edu/ml/datasets/wine+quality. The related paper is Cortez, Paulo, et al. ”Modeling wine preferences by data mining from physicochemical properties.” Decision Support Systems 47.4 (2009): 547-553.


Figure 1: Illustration of wine quality modeling.

 
As illustrated in Figure 1, the dataset represents chemical analyses of wines (the features) and ratings of their quality. This rating is the target: this is what you’ll try to estimate.
First, let’s load the data and have a look at the features:

wine_quality = pd.read_csv(""https://raw.githubusercontent.com/hadrienj/essential_math_for_data_science/master/data/winequality-red.csv"",
                           sep="";"")
wine_quality.columns




Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
       'pH', 'sulphates', 'alcohol', 'quality'],
      dtype='object')



The last column quality is important as you’ll use it as the target of your classification. The quality is described by ratings from 3 to 8:

wine_quality[""quality""].unique()




array([5, 6, 7, 4, 8, 3])



Since the goal is to classify red wines of very good quality, let’s decide that the wines are very good when ratings are 7 or 8 and not very good otherwise.
Let’s create the dataset with y being the quality (the dependent variable, 0 for ratings less than 7 and 1 for ratings greater than or equal 7) and X containing all the other features.

X = wine_quality.drop(""quality"", axis=1).values
y = wine_quality[""quality""] >= 7



The first thing to do, before looking at the data, is to split it in a part for training your algorithms (the training set) and a part for testing them (the test set). This will allow you to evaluate the performance of your model on data unseen during the training.

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)



Preprocessing
As a first step, let’s standardize the data to help the convergence of the algorithm. You can use the class StandardScaler from Sklearn.
Note that you don’t want to consider the data from the test set to do the standardization. The method fit_transform() calculates the parameters needed for the standardization and apply it at the same time. Then, you can apply the same standardization to the test set without fitting again.

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_stand = scaler.fit_transform(X_train)
X_test_stand = scaler.transform(X_test)



First Model
As a first model, let’s train a logistic regression on the training set and calculate the classification accuracy (the percentage of correct classifications) on the test set:

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

log_reg = LogisticRegression(random_state=123, penalty=""none"")
log_reg.fit(X_train_stand, y_train)
y_pred = log_reg.predict(X_test_stand)
accuracy_score(y_test, y_pred)




0.8729166666666667



The accuracy is about 0.87, meaning that 87% of the test examples have been correctly classified. Should you be happy with this result?
 
Metrics for Imbalanced Datasets
 
Imbalanced Dataset
Since we separated the data into very good wines and not very good wines, the dataset is imbalanced: there are different quantities of data corresponding to each target class.
Let’s check how many observations you have in the negative (not very good wines) and positive classes (very good wines):

(y_train == 0).sum() / y_train.shape[0]




0.8650580875781948




(y_train == 1).sum() / y_train.shape[0]




0.13494191242180517



It shows that there are around 86.5% of the examples corresponding to class 0 and 13.5% to class 1.
Simple Model
To illustrate this point about accuracy and imbalanced datasets, let’s creates a model as a baseline and look at its performance. It will help you to see the advantages to use other metrics than accuracy.
A very simple model using the fact that the dataset is imbalanced would always estimate the class with the largest number of observations. In your case, such a model would always estimate that all wines are bad and get a decent accuracy doing that.
Let’s simulate this model by creating random probabilities below 0.5 (for instance, a probability of 0.15 means that there is a 15% chance that the class is positive). We need these probabilities to calculate both the accuracy and other metrics.

np.random.seed(1)
y_pred_random_proba = np.random.uniform(0, 0.5, y_test.shape[0])
y_pred_random_proba




array([2.08511002e-01, 3.60162247e-01, 5.71874087e-05, ...,
       4.45509477e-01, 1.36436118e-02, 2.61025624e-01])



Let’s say that if the probability is above 0.5, the class is estimated as positive:

def binarize(y_hat, threshold):
    return (y_hat > threshold).astype(int)

y_pred_random = binarize(y_pred_random_proba, threshold=0.5)
y_pred_random




array([0, 0, 0, ..., 0, 0, 0])



The variable y_pred_random contains only zeros. Let’s evaluate the accuracy of this random model:

accuracy_score(y_test, y_pred_random)




0.8625



This shows that, even with a random model, the accuracy is not bad at all: it doesn’t mean that the model is good.
To summarize, having a different number of observations corresponding to each class, you can’t rely on the accuracy to evaluate your model’s performance. In our example, the model could output only zeros and you would get around 86% accuracy.
You need other metrics to assess the performance of models with imbalanced datasets.
ROC Curves
A good alternative to the accuracy is the Receiver Operating Characteristics (ROC) curve. You can check the very good explanations of Aurélien Géron about ROC curves in Géron, Aurélien. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. O’Reilly Media, 2019.
The main idea is to separate the estimations from the model into four categories:

The true positives (TP): the prediction is 1 and the true class is 1.
The false positives (FP): the prediction is 1 but the true class is 0.
The true negatives (TN): the prediction is 0 and the true class is 0.
The false negatives (FN): the prediction is 0 but the true class is 1.

Let’s calculate these values for your first logistic regression model. You can use the function confusion_matrix from Sklearn. It presents a table organized as follows:


Figure 2: Illustration of a confusion matrix.

 

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred_random)




array([[414,   0],
       [ 66,   0]])



You can see that there is no positive observation that has been correctly classified (TP) with the random model.
Decision Threshold
In classification tasks, you want to estimate the class of data samples. For models like logistic regression which outputs probabilities between 0 and 1, you need to convert this score to the class 0 or 1 using a decision threshold, or just threshold. A probability above the threshold is considered as a positive class. For instance, using the default choice of the decision threshold at 0.5, you consider that the estimated class is 1 when the model outputs a score above 0.5.
However, you can choose other thresholds, and the metrics you use to evaluate the performance of your model will depend on this threshold.
With the ROC curve, you consider multiple thresholds between 0 and 1 and calculate the true positive rate as a function of the false positive rate for each of them.
You can use the function roc_curve from Sklearn to calculate the false positive rate (fpr) and the true positive rate (tpr). The function outputs also the corresponding thresholds. Let’s try it with our simulated random model where outputs are only values bellow 0.5 (y_pred_random_proba).

from sklearn.metrics import roc_curve
fpr_random, tpr_random, thresholds_random = roc_curve(y_test, y_pred_random_proba)



Let’s have a look at the outputs:

fpr_random




array([0.        , 0.        , 0.07246377, ..., 0.96859903, 0.96859903,
       1.        ])




tpr_random




array([0.        , 0.01515152, 0.01515152, ..., 0.98484848, 1.        ,
       1.        ])




thresholds_random




array([1.49866143e+00, 4.98661425e-01, 4.69443239e-01, ...,
       9.68347894e-03, 9.32364469e-03, 5.71874087e-05])



You can now plot the ROC curve from these values:

plt.plot(fpr_random, tpr_random)
# [...] Add axes, labels etc.





Figure 3: ROC curve corresponding to the random model.

 
Figure 3 shows the ROC curve corresponding to the random model. It gives you the true positive rate as a function of the false positive rate for each threshold.
However, be careful, the thresholds are from 1 to 0. For instance, the point at the bottom left corresponds to a threshold of 1: there is 0 true positive and 0 false positive because it is not possible to have a probability above 1, so with a threshold of 1, no observation can be categorized as positive. At the top right, the threshold is 0, so all observations are categorized as positive, leading to 100% of true positive but also 100% of false positive.
A ROC curve around the diagonal means that the model is not better than random which is the case here. A perfect model would be associated with a ROC curve with a true positive rate of 1 for all values of false positive rate.
Let’s now look at the ROC curve corresponding to the logistic regression model you trained earlier. You’ll need probabilities from the model, that you can get using predict_proba() instead of predict:

y_pred_proba = log_reg.predict_proba(X_test_stand)
y_pred_proba




array([[0.50649705, 0.49350295],
       [0.94461852, 0.05538148],
       [0.97427601, 0.02572399],
       ...,
       [0.82742897, 0.17257103],
       [0.48688505, 0.51311495],
       [0.8809794 , 0.1190206 ]])



The first column is the score for the class 0 and the second column for the score 1 (thus, the total of each row is 1), so you can keep the second column only.

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])
plt.plot(fpr, tpr)
# [...] Add axes, labels etc.





Figure 4: ROC curve corresponding to the logistic model.

 
You can see in Figure 4 that your model is actually better than a random model, which is not something you were able to know from the model accuracies (they were equivalent: around 0.86 for the random model and 0.87 for your model).
Visual inspection is good, but it would also be crucial to have a single numerical metric to compare your models. This is usually provided by the area under the ROC curve. You’ll see what is the area under the curve and how you can calculate in the next sections.
 
Integrals
 
Integration is the inverse operation of differentiation. Take a function f(x) and calculate its derivative f′(x), the indefinite integral (also called antiderivative) of f′(x) gives you back f(x) (up to a constant, as you’ll soon see).
You can use integration to calculate the area under the curve, which is the area of the shape delimited by the function, as shown in Figure 5.


Figure 5: Area under the curve.

 
A definite integral is the integral over a specific interval. It corresponds to the area under the curve in this interval.
 
Example
 
You’ll see through this example how to understand the relationship between the integral of a function and the area under the curve. To illustrate the process, you’ll approximate the integral of the function g(x)=2x using a discretization of the area under the curve.
Example Description
Let’s take again the example of the moving train. You saw that speed as a function of time was the derivative of distance as a function of time. These functions are represented in Figure 6.


Figure 6: The left panel shows f(x) which is the distance as a function of time, and the right panel its derivative g(x), which is the speed as a function of time.

 
The function shown in the left panel of Figure 6 is defined as f(x)=x2. Its derivative is defined as g(x)=2x.
In this example, you’ll learn how to find an approximation of the area under the curve of g(x).
Slicing the Function
To approximate the area of a shape, you can use the slicing method: you cut the shape into small slices with an easy shape like rectangles, calculate the area of each of these slices and sum them.
You’ll do exactly that to find an approximation of the area under the curve of g(x).


Figure 7: Approximation of the area under the curve by discretizing the area under the curve of speed as a function of time.

 
Figure 7 shows the area under the curve of f′(x) sliced as one-second rectangles (let’s call this difference Δx). Note that we underestimate the area (look at the missing triangles), but we’ll fix that later.
Let’s try to understand the meaning of the slices. Take the first one: its area is defined as 2⋅12⋅1. The height of the slice is the speed at one second (the value is 2). So there are two units of speed by one unit of time for this first slice. The area corresponds to a multiplication between speed and time: this is a distance.
For instance, if you drive at 50 miles per hour (speed) for two hours (time), you traveled 50⋅2=100 miles (distance). This is because the unit of speed corresponds to a ratio between distance and time (like miles per hour). You get:

 
To summarize, the derivative of the distance by time function is the speed by time function, and the area under the curve of the speed by time function (its integral) gives you a distance. This is how derivatives and integrals are related.
Implementation
Let’s use slicing to approximate the integral of the function g(x)=2x. First, let’s define the function g(x):

def g_2x(x):
    return 2 * x



As illustrated in Figure 7, you’ll consider that the function is discrete and take a step of Δx=1. You can create an x-axis with values from zero to six, and apply the function g_2x() for each of these values. You can use the Numpy method arange(start, stop, step) to create an array filled with values from start to stop (not included):

delta_x = 1
x = np.arange(0, 7, delta_x)
x




array([0, 1, 2, 3, 4, 5, 6])




y = g_2x(x)
y




array([ 0,  2,  4,  6,  8, 10, 12])



You can then calculate the slice’s areas by iterating and multiplying the width (Δx) by the height (the value of y at this point). of the slice. As you saw, this area (delta_x * y[i-1] in the code below) corresponds to a distance (the distance of the moving train traveled during the ith slice). You can finally append the results to an array (slice_area_all in the code below).
Note that the index of y is i-1 because the rectangle is on the left of the x value we estimate. For instance, the area is zero for x=0 and x=1.

slice_area_all = np.zeros(y.shape[0])
for i in range(1, len(x)):
    slice_area_all[i] = delta_x * y[i-1]
slice_area_all




array([ 0.,  0.,  2.,  4.,  6.,  8., 10.])



These values are the slice’s areas.
To calculate the distance traveled from the beginning to the corresponding time point (and not corresponding to each slice), you can calculate the cumulative sum of slice_area_all with the Numpy function cumsum():

slice_area_all = slice_area_all.cumsum()
slice_area_all




array([ 0.,  0.,  2.,  6., 12., 20., 30.])



This is the estimated values of the area under the curve of g(x) as a function of x. We know that the function g(x) is the derivative of f(x)=x2, so we should get back f(x) by the integration of g(x).
Let’s plot our estimation and f(x)f(x), which we’ll call the “true function”, to compare them:

plt.plot(x, x ** 2, label='True')
plt.plot(x, slice_area_all, label='Estimated')





Figure 8: Comparison of estimated and original function.

 
The estimation represented in Figure 8 shows that the estimation is not bad, but could be improved. This is because we missed all these triangles represented in red in Figure

One way to reduce the error is to take a smaller value for ΔxΔx, as illustrated in the right panel in Figure 9.



Figure 9: Missing parts in slices of the speed function (in red). The error is smaller with a smaller Δx.

 
Let’s estimate the integral function with Δx=0.1:

delta_x = 0.1
x = np.arange(0, 7, delta_x)
y = g_2x(x)
#  [...] Calculate and plot slice_area_all





Figure 10: Smaller slice widths lead to a better estimation of the original function.

 
As shown in Figure 10, we recovered (at least, up to an additive constant) the original function whose derivative we integrated.
Extension
In our previous example, you integrated the function 2x, which is a linear function, but the principle is the same for any continuous function (see Figure 11 for instance).


Figure 11: The slicing method can be used with many linear or nonlinear function, including all continuous functions.

 
 
Riemann Sum
 
Approximating an integral using this slicing method is called a Riemann sum. Riemann sums can be calculated in different ways, as you can see in Figure 12.


Figure 12: Four kinds of Riemann sums for integral approximation.

 
As pictured in Figure 12, with the left Riemann sum, the curve is aligned with the left corner of the rectangle. With the right Riemann sum, the curve is aligned with the right corner of the rectangle. With the midpoint rule, the curve is aligned with the center of the rectangle. With the trapezoidal rule, a trapezoidal shape is used instead of a rectangle. The curve crosses both top corners of the trapezoid.
 
Mathematical Definition
 
In the last section, you saw the relationship between the area under the curve and integration (you got back the original function from the derivative). Let’s see now the mathematical definition of integrals.
The integrals of the function f(x) with respect to x is denoted as following:
∫f(x)dx
The symbol dx is called the differential of x and refers to the idea of an infinitesimal change of x. It is a difference in x that approaches 0. The main idea of integrals is to sum an infinite number of slices which have an infinitely small width.
The symbol ∫ is the integral sign and refers to the sum of an infinite number of slices.
The height of each slice is the value f(x). The multiplication of f(x) and dx is thus the area of each slice. Finally, ∫f(x):dx is the sum of the slice areas over an infinite number of slices (the width of the slices tending to zero). This is the area under the curve.
You saw in the last section how to approximate function integrals. But if you know the derivative of a function, you can retrieve the integral knowing that it is the inverse operation. For example, if you know that:
d(x2)dx=2x
You can conclude that the integral of 2x is x2. However, there is a problem. If you add a constant to our function the derivative is the same because the derivative of a constant is zero. For instance,
d(x2+3)dx=2x
It is impossible to know the value of the constant. For this reason, you need to add an unknown constant to the expression, as follows:
∫2xdx=x2+c
with cc being a constant.
Definite Integrals
In the case of definite integrals, you denote the interval of integration with numbers below and above the integral symbol, as following:
∫baf(x)dx
It corresponds to the area under the curve of the function f(x) between x=a and x=b, as illustrated in Figure 13.
Figure 13: Area under the curve between x=a and x=b.
 
Area Under the ROC Curve
 
Now that you know how the area under the curve relates to integration, let’s see how to calculate it to compare numerically your models.
Remember that you had the ROC curves represented in Figure 14:

plt.plot(fpr_random, tpr_random, label=""Random"")
plt.plot(fpr, tpr, label=""Logistic regression"")
# [...] Add axes, labels etc.





Figure 14: ROC curves of the random model (blue) and the logistic regression model (green).

 
Let’s start with the random model. You want to sum each value of true positive rate multiplied by the width on the x-axis that is the difference between the corresponding value of false positive rate and the one before. You can obtain these differences with:

fpr_random[1:] - fpr_random[:-1]




array([0.00241546, 0.01207729, 0.        , ..., 0.01207729, 0.        ,
       0.06038647])



So the area under the ROC curve of the random model is:

(tpr_random[1:] * (fpr_random[1:] - fpr_random[:-1])).sum()




0.5743302591128678



Or you can simply use the function roc_auc_score() from Sklearn using the true target values and the probabilities as input:

from sklearn.metrics import roc_auc_score

roc_auc_score(y_test, y_pred_random_proba)




0.5743302591128678



An area under the ROC curve of 0.5 corresponds to a model that is not better than random and an area of 1 corresponds to perfect predictions.
Now, let’s compare this value to the area under the ROC curve of your model:

roc_auc_score(y_test, y_pred_proba[:, 1])




0.8752378861074513



This shows that your model is actually not bad and your predictions of the quality of the wine are not random.
In machine learning, you can use a few lines of code to train complex algorithms. However, as you saw here, a bit of math can help you to make the most of it and speed up your work. It will give you more ease in various aspects of your discipline, even, for instance, understanding the documentation of machine learning libraries like Sklearn.
 
Bio: Hadrien Jean is a machine learning scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog (hadrienj.github.io).
Original. Reposted with permission.
Related:

Boost your data science skills. Learn linear algebra.
Preprocessing for Deep Learning: From covariance matrix to image whitening
Essential Math for Data Science:  ‘Why’ and ‘How’"
https://www.kdnuggets.com/2021/07/7-open-source-libraries-deep-learning-graphs.html,7 Open Source Libraries for Deep Learning Graphs,"In this article we’ll go through 7 up-and-coming open source libraries for graph deep learning, ranked in order of increasing popularity.","By Kevin Vu, Exxact Corp.
comments

 
Introducing Deep Learning on Graphs
 
If you’re a deep learning enthusiast you’re probably already familiar with some of the basic mathematical primitives that have been driving the impressive capabilities of what we call deep neural networks. Although we like to think of a basic artificial neural network as some nodes with some weighted connections, it’s more efficient computationally to think of neural networks as matrix multiplication all the way down. We might draw a cartoon of an artificial neural network like the figure below, with information traveling in from left to right from inputs to outputs (ignoring recurrent networks for now).

Multilayer perceptron cartoon in the public domain, Source
 
This type of neural network is a feed-forward multilayer perceptron (MLP). If we want a computer to compute the forward pass for this model, it’s going to use a string of matrix multiplies and some sort of non-linearity (here represented by the Greek letter sigma) in the hidden layer:

MLP matrix multiplication cartoon in the public domain, Source
 
MLPs are well-suited for data that can be naturally shaped as 1D vectors. While neat and all, MLPs use an awful lot of parameters when data samples are large, and this isn’t a very efficient way to treat higher dimensional data like 2D images or 3D volumes. 2D data like images instead naturally lend themselves to the operation of convolution, wherein weights are applied in local neighborhoods across the entire image, instead of granting each point to point connection between layers it’s own weight. This type of weight sharing has a number of advantages, including translation equivariance, regularization, and parameter efficiency.
Convolution can be visualized like so:

Convolution cartoon in the public domain, Source
 
Of course we’re not going to sit down with pen and paper and perform these operations by hand, we want an algorithm that can quickly perform convolution across each image channel in a computer-friendly way.
In principle, computers perform convolutions something like the following:

Convolution as multiplication of matrices in the Fourier domain cartoon in the public domain, Source
 
That’s right, convolution operations are again implemented as the multiplication of matrices, although this time it is element-wise. This is thanks to the convolution theorem of the Fourier transform, which states that multiplication in the Fourier domain relates to convolution in the spatial domain. But what happens when our data of interest isn’t particularly well-suited to representation as a 1D vector or a 2D/3D image, and is instead naturally represented as a graph?

Graph cartoon in the public domain, Source
 
For our purposes, a graph is a collection of nodes connected by edges, as shown in the cartoon. The edges can have their own properties such as weights and/or directionality, and the nodes typically have some sort of states or features, just like the node activations in a feed-forward MLP.
In a graph neural network, each “layer” is just a snapshot of the node states of the graph, and these are connected by operational updates related to each node and its neighbors, such as neural networks operating as the edges between nodes.
If we want to use graph neural networks to achieve impressive results on graph-structured data, like what convolutional neural networks did for deep learning on images, we need an efficient way to implement these models on computers. That almost always means we need a way to convert the conceptual graph neural network framework to something that works on a modern deep learning GPU.
 
How can we possibly convert the complicated idea of graph neural networks to another form of matrix multiplication?
 

Source
 
A convenient way to represent the connections in a graph is with something called an adjacency matrix. As the name suggests, an adjacency matrix describes which nodes are next to each other (i.e. connected to each other by edges) in the graph.
But a graph neural network needs to operate on graphs with arbitrary structure (much like the convolutional kernels of a conv-net can work on images of different height and width), so we can’t expect the input data to have the same adjacency matrix each time or even for each adjacency matrix to have the same dimensions. We can deal with this by combining the adjacency matrices for several samples diagonally into a larger matrix describing all the connections in a batch.

Source
 
This allows us to deal with multiple graphs with different structures in a single batch, and you’ll notice that this formulation also results in weight sharing between nodes. There are a few more details to this: the adjacency matrix should be normalized so that feature scales don’t completely change, and there are other approaches to convolution on graphs than the graph convolution network approach (GCN) we are talking about here, but it’s a good starting point in understanding the GNN forward pass.
It’s enough to give us an appreciation for the data preparation and mathematical operations needed to implement deep learning on graphs. Luckily, the interest in deep learning for graph-structured data has motivated the development of a number of open source libraries for graph deep learning, leaving more cognitive room for researchers and engineers to concentrate on architectures, experiments, and applications.
In this article we’ll go through 7 up-and-coming open source libraries for graph deep learning, ranked in order of increasing popularity.
 
7 Open Source Libraries for Deep Learning on Graphs
 
7. GeometricFlux.jl
 

Source
 
Reflecting the dominance of the language for graph deep learning, and for deep learning in general, most of the entries on this list use Python and are built on top of TensorFlow, PyTorch, or JAX. This first entry, however, is an open source library for graph neural networks built on the Flux deep learning framework in the Julia programming language.
One may be tempted to write off GeometricFlux.jl, and even the whole idea of using the Julia language for deep learning due to the relatively small number of practitioners, but it is a language with a growing community and offers a number of technical advantages over Python. One would have hardly predicted DeepMind would start ditching TensorFlow in favor of JAX just a few years ago (see entry number 5 on this list), and likewise in just a few short years we may see the Julia language start to supplant Python as the standard language for machine learning.
The Julia programming language was designed from the start to be both highly productive (like Python), and fast like compiled languages including C. Julia language uses just-in-time compilation to achieve fast execution speed, while it’s read-execute-print loop (REPL) makes interactive and iterative programming reasonably productive. You will notice a slight delay when you run code for the first time, especially if you’re used to using Python in a particularly interactive way (like in Jupyter notebooks), but over time the speed-ups for a given workflow can be significant.
Julia is designed as a scientific programming language, and there has been significant development of automatic differentiation packages over the last five years or so. The end result is functionality that can combine research-centered libraries like the DifferentialEquations.jl package with machine learning capabilities as we see in the neural differential equations package DiffEqFlux.jl. The same goes for GeometricFlux.jl, which is built to be compatible with the graph theory research JuliaGraphs ecosystem as well as other parts of Flux.
If you’re using graph deep learning for work, it may be most efficient to stick with a library that’s built on PyTorch or the standard working framework for deep learning used for other projects. If you’re starting from scratch or doing research, however, GeometricFlux.jl makes a compelling entry point for graph deep learning and differentiable programming with Julia. The library’s friendly MIT License also makes it easy to build and contribute the tools you need, or to tackle some of the open issues from the project’s GitHub repository.
 
6. PyTorch GNN
 
The PyTorch Graph Neural Network library is a graph deep learning library from Microsoft, still under active development at version ~0.9.x after being made public in May of 2020. PTGNN is made to be readily familiar for users familiar with building models based on the torch.nn.Module class, and handles the workflow tasks of dataloaders and turning graphs into PyTorch-ready tensors.
PTGNN is based on an interesting architecture called the AbstractNeuralModel. This class encapsulates the entire process of training a graph neural network including tensorizing and pre-proccessing raw data, and includes the TNeuralModule that is the actual neural model sub-classed from PyTorch’s nn.Module class. The neural modules can be used independently of the AbstractNeuralModel object, and in fact can be combined with other types of PyTorch modules/layers if desired.
PTGNN is slightly younger than GeometricFlux.jl and has a less active commit history, but ekes out slightly more GitHub stars and forks. It has the same permissive and open source MIT License, but if you’re looking for a project to contribute to, you’ll need to be fairly self-directed. The “Issues” tab on GitHub provides little to no direction of what needs to be fixed or implemented. PTGNN has a few interesting design elements in its construction that may be of interest to work with or on, but if you’re a graph neural network enthusiast looking for a PyTorch-based graph deep learning library you may be better served by using PyTorch Geometric (number 1 on our list). PyTorch Geometric is more mature, having been in development for about 4 years now, and has an established and growing community of users and developers.
 
5. Jraph
 

Source
 
Late last year you may have noticed a blog post from DeepMind with a little less pomp and circumstance than their usual headline-grabbing landmarks. In December 2020 Deepmind described their ongoing efforts in developing and using a capable ecosystem of deep learning research libraries based on the functional differentiable programming library JAX. JAX is the conceptual progeny of what started as an academic project for simple but nigh-universal automatic differentiation in Python (especially NumPy) called Autograd.
After Google scooped up several of the research programmers responsible for the original Autograd, they developed a new library and now we have JAX. JAX is an interesting package due in no small part to its emphasis on composable functional programming paradigms. It also pays attention to the more general concept of “differentiable programming” rather than focusing primarily on neural networks like TensorFlow or PyTorch. Although PyTorch and TensorFlow can both be used to build, say, differentiable physics models instead of neural networks, JAX is more readily amenable to flexible differentiable programming for scientific and other programming tasks from the start. The JAX offering is compelling enough, at least, to induce DeepMind to embark on a substantial adoption and development track, despite having previously spent significant time building TensorFlow-based tools like Sonnet.
As part of DeepMind’s efforts to develop a JAX-based ecosystem for deep learning research they’ve developed a graph learning library called Jraph.
 
Original image in the public domain from Wikimedia contributor Scambelo

 
Unlike some of the other libraries on this list, Jraph is a lightweight and minimalistic graph learning library that doesn’t in general prescribe a specific way for working with itself. Jraph inherited some design patterns from a spiritual predecessor, Graph Nets, built with TensorFlow and Sonnet. Namely, Jraph uses the same GraphsTuple concept as Graph Nets, which is a data structure containing infromation describing nodes, edges, and edge directions. Another feature handled by Jraph, makes special accommodations for dealing with variable-structured graphs using masks and padding. That’s not a concern for most of the other Python libraries on this list, but it’s necessary due to the use of just-in-time compilation in JAX. This ensures that working with graphs in JAX doesn’t mean giving up the execution speedups JAX provides on both GPU and CPU hardware.
 
4. Spektral
 
Spektral logos used under the MIT License, from the Spektral documentation.

 
Spektral is a graph deep learning library based on Tensorflow 2 and Keras, and with a logo clearly inspired by the Pac-Man ghost villains. If you are set on using a TensorFlow-based library for your graph deep learning needs, Spektral may be your best option (although DGL, number 2 on our list, can support both PyTorch or TensorFlow back-ends). It’s designed to be easy-to-use and flexible, while retaining usage that is as close as possible to the familiar Keras API. This means that you can even train a model using the convenient model.fit() method, so long as you provide a Spetkral dataloader to handle the formation of TensorFlow friendly sparse matrices defining the graph. Unfortunately there is a trade-off for the ease-of-use of Spektral, and this comes in the form of noticeably slower training speeds for most tasks compared to the other major libraries DGL and PyTorch Geometric.
Spektral has significant adoption and it may be an appealing option should you want to build graph models with TensorFlow. It’s likely to be better supported than the Graph Nets library by Deepmind, which is next on our list but for all appearances is being phased out in favor of the JAX-based Jraph. Spektral is released under the Apache 2.0 open source license and has an active issues board with pull requests being integrated on a regular basis, making this an appealing library for someone wishing to not only use a good deep learning library, but contribute to one as well.
 
3. Graph Nets
 

Source
 
Graph Nets is another graph deep learning from Deepmind. Built on TensorFlow and Sonnet (another DeepMind library), it may soon be largely superseded by the JAX-based Jraph described earlier. Graph Nets requires TensorFlow 1, and as a result it feels somewhat dated despite being only about 3 years old. As of this writing, It has an impressive 737 forks and nearly 5,000 stars on GitHub, and, like most other libraries from Google/DeepMind, is licensed under Apache 2.0. Graph Nets originated the GraphsTuple data structure used by Jraph.
While Graph Nets seems to be quite popular on GitHub, it is probably a less attractive option than the other libraries on this list, unless you are working on a pre-existing code base that already makes heavy use of the library. For new projects with TensforFlow, Spektral and DGL are probably a better bet, as they’re built with more up-to-date technology and likely to continue to receive decent support for a few years.
 
2. Deep Graph Library (DGL)
 

Source
 
Rather than being associated with a major tech company like Microsoft’s PTGNN or Google/DeepMind’s Jraph and Graph Nets, DGL is the product of a group of deep learning enthusiasts called the Distributed Deep Machine Learning Community. It has over 100 contributors, over 1500 commits, and over 7,000 stars on GitHub. DGL is also unique in our list for offering a flexible choice of back-end. Models can have PyTorch, TensorFlow, or MXNet running under the hood, while offering a largely similar experience to the one driving an experiment. It’s one of the longer-lived libraries still under active development on our list, with a first commit going back to April 2018. DGL was used recently to build the SE(3) transformer, a powerful graph transformer with both rotational and translational equivariance that is suspected to be a building block or inspiration for AlphaFold2. This model, the successor to the already impressive AlphaFold, was the star behind DeepMind’s impressive, show-winning performance at the 2020 CASP14 protein structure prediction challenge. That event prompted some major news outlets to herald AlphaFold2 as the first AI project to solve a major scientific challenge.
DGL is built around the neural message passing paradigm described by Gilmer et al. in 2017. This provides a flexible framework and it covers most types of graph layers for building graph neural networks. As you’ll notice from reading through the code repository and documentation, DGL is an expansive project. This also means there are plenty (nearly 200) open issues, a ripe opportunity for someone looking to contribute to a graph deep learning project with a big impact. DGL is used for a number of specialized applications, to the extent where several additional libraries have been built on top of it. DGL-LifeSci is a library built specifically for deep learning graphs as applied to chem- and bio-informatics, while DGL-KE is built for working with knowledge graph embeddings. Both of those bonus libraries are developed by AWS Labs.
 
1. PyTorch Geometric
 

Source
 
The library topping our list is none other than PyTorch Geometric. PyTorch Geometric, or PyG to friends, is a mature geometric deep learning library with over 10,000 stars and 4400 commits, most of these being the output of one very prolific PhD student rusty1s. PyG sports a very long list of implemented graph neural network layers. Not only does it run deep graph networks quite quickly, but PyG is also built for other types of geometric deep learning such as point cloud and mesh-based models.
PyG has a well written tutorial introduction by example, and having been developed since 2017, it’s pretty well established and well-supported by a community of users and just over 140 contributors. Using PyG will be very familiar for anyone who has worked with PyTorch before, with the most noticeable difference being some differences in the data input. Instead of the usual forward(x) programming pattern, you’ll instead get used to using forward(batch), where batch is a data structure that contains all the information describing graph features and connections.
For new projects with a free hand in choosing a library, PyTorch Geometric is pretty tough to beat.
For example, here’s how the libraries compare to each other:


Name
License
Stars
Language, Flavor
Main Contributor(s)


GeometricFlux.jl
MIT
180
Julia Language, Flux.jl
yuehua


PyTorch GNN
MIT
206
Python, PyTorch
Microsoft


Jraph
Apache 2.0
536
Python, JAX
DeepMind


Spektral
MIT
1,700
Python, TF2/keras
danielegrattarola


Graph Nets
Apache 2.0
4,800
Python, PyTorch
DeepMind


Deep Graph Library
Apache 2.0
7,000
Python, PyTorch, TF, MxNet
Distributed MLC


PyTorch Geometric
MIT
10,600
Python, PyTorch
rusty1s



 
Choosing a Deep Learning Library
 
In many cases, your choice of a deep graph learning library will be heavily influenced by a previous choice of deep learning library made by you, your employer, or maybe your supervising professor. If you are fond of the Keras API and TensorFlow or need to retain consistent dependencies with a pre-existing code base, for example, Spektral may be the right library for you. We wouldn’t recommend starting a new project with DeepMind’s Graph Nets and TensorFlow 1, but the library does still get occasional updates and may be a reasonable choice to support legacy projects.
If you’re lucky enough to be starting from scratch, you have a couple of enticing options. If you think the deliberate productivity + execution speed prioritization of the Julia Programming is the future of machine learning and scientific programming, GeometricFlux.jl is an exciting prospect. If you are intrigued by functional programming paradigms and want to retain some of the speed advantages from just-in-time compilation (like Julia) but would prefer to stick with Python, the JAX-based Jraph is an attractive option. Finally, if you want a fast, capable library at a relatively established and mature state of development, it’s going to be hard to go wrong with PyTorch Geometric.
 
Original. Reposted with permission.
Related:

How to Build An Image Classifier in Few Lines of Code with Flash
Free From Stanford: Machine Learning with Graphs
A Graph-based Text Similarity Method with Named Entity Information in NLP"
https://www.kdnuggets.com/2020/10/imbalanced-data-machine-learning.html,Dealing with Imbalanced Data in Machine Learning,This article presents tools & techniques for handling data when it's imbalanced.,"By Derrick Mwiti, Data Scientist.
comments
As an ML engineer or data scientist, sometimes you inevitably find yourself in a situation where you have hundreds of records for one class label and thousands of records for another class label.
Upon training your model you obtain an accuracy above 90%. You then realize that the model is predicting everything as if it’s in the class with the majority of records. Excellent examples of this are fraud detection problems and churn prediction problems, where the majority of the records are in the negative class. What do you do in such a scenario? That will be the focus of this post.
 
Collect More Data
 
The most straightforward and obvious thing to do is to collect more data, especially data points on the minority class. This will obviously improve the performance of the model. However, this is not always possible. Apart from the cost one would have to incur, sometimes it's not feasible to collect more data. For example, in the case of churn prediction and fraud detection, you can’t just wait for more incidences to occur so that you can collect more data.
 
Consider Metrics Other than Accuracy
 
Accuracy is not a good way to measure the performance of a model where the class labels are imbalanced. In this case, it's prudent to consider other metrics such as precision, recall, Area Under the Curve (AUC) — just to mention a few.
Precision measures the ratio of the true positives among all the samples that were predicted as true positives and false positives. For example, out of the number of people our model predicted would churn, how many actually churned?
 

 
Recall measures the ratio of the true positives from the sum of the true positives and the false negatives. For example, the percentage of people who churned that our model predicted would churn.
 

 
The AUC is obtained from the Receiver Operating Characteristics (ROC) curve. The curve is obtained by plotting the true positive rate against the false positive rate. The false positive rate is obtained by dividing the false positives by the sum of the false positives and the true negatives.
AUC closer to one is better, since it indicates that the model is able to find the true positives.
 
Emphasize the Minority Class
 
Another way to deal with imbalanced data is to have your model focus on the minority class. This can be done by computing the class weights. The model will focus on the class with a higher weight. Eventually, the model will be able to learn equally from both classes. The weights can be computed with the help of scikit-learn.

from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight(‘balanced’, y.unique(), y)
array([ 0.51722354, 15.01501502])


You can then pass these weights when training the model. For example, in the case of logistic regression:

class_weights = {
 0:0.51722354,
 1:15.01501502
}lr = LogisticRegression(C=3.0, fit_intercept=True, warm_start = True, class_weight=class_weights)


Alternatively, you can pass the class weights as balanced and the weights will be automatically adjusted.

lr = LogisticRegression(C=3.0, fit_intercept=True, warm_start = True, class_weight=’balanced’)


Here’s the ROC curve before the weights are adjusted.

 

 
And here’s the ROC curve after the weights have been adjusted. Note the AUC moved from 0.69 to 0.87.
 

 
Try Different Algorithms
 
As you focus on the right metrics for imbalanced data, you can also try out different algorithms. Generally, tree-based algorithms perform better on imbalanced data. Furthermore, some algorithms such as LightGBM have hyperparameters that can be tuned to indicate that the data is not balanced.
 
Generate Synthetic Data
 
You can also generate synthetic data to increase the number of records in the minority class — usually known as oversampling. This is usually done on the training set after doing the train test split. In Python, this can be done using the Imblearn package. One of the strategies that can be implemented from the package is known as the Synthetic Minority Over-sampling Technique (SMOTE). The technique is based on k-nearest neighbors.
When using SMOTE:

The first parameter is a float that indicates the ratio of the number of samples in the minority class to the number of samples in the majority class, once resampling has been done.
The number of neighbors to be used to generate the synthetic samples can be specified via the k_neighbors parameter.


from imblearn.over_sampling import SMOTEsmote = SMOTE(0.8)X_resampled,y_resampled = smote.fit_resample(X.values,y.values)pd.Series(y_resampled).value_counts()0    9667
1    7733 
dtype: int64


You can then fit your resampled data to your model.

model = LogisticRegression()model.fit(X_resampled,y_resampled)predictions = model.predict(X_test)


 
Undersample the Majority Class
 
You can also experiment on reducing the number of samples in the majority class. One such strategy that can be implemented is the NearMiss method. You can also specify the ratio just like in SMOTE, as well as the number of neighbors via n_neighbors.

from imblearn.under_sampling import NearMissunderSample = NearMiss(0.3,random_state=1545)pd.Series(y_resampled).value_counts()0  1110 1  333 dtype: int64


 
Final Thoughts
 
Other techniques that can be used include using building an ensemble of weak learners to create a strong classifier. Metrics such as precision-recall curve and area under curve (PR, AUC) are also worth trying when the positive class is the most important.
As always, you should experiment with different techniques and settle on the ones that give you the best results for your specific problems. Hopefully, this piece has given some insights on how to get started.
Code available here.
 
Bio: Derrick Mwiti is a data scientist who has a great passion for sharing knowledge. He is an avid contributor to the data science community via blogs such as Heartbeat, Towards Data Science, Datacamp, Neptune AI, KDnuggets just to mention a few. His content has been viewed over a million times on the internet. Derrick is also an author and online instructor. He also trains and works with various institutions to implement data science solutions as well as to upskill their staff. Derrick’s studied Mathematics and Computer Science from the Multimedia University, he also is an alumnus of the Meltwater Entrepreneurial School of Technology. If the world of Data Science, Machine Learning, and Deep Learning interest you, you might want to check his Complete Data Science & Machine Learning Bootcamp in Python course.
Original. Reposted with permission.
Related:

How to fix an Unbalanced Dataset
The 5 Most Useful Techniques to Handle Imbalanced Datasets
Pro Tips: How to deal with Class Imbalance and Missing Labels"
https://www.kdnuggets.com/2020/09/international-alternatives-kaggle-data-science-competitions.html,International alternatives to Kaggle for Data Science / Machine Learning competitions,"While Kaggle might be the most well-known, go-to data science competition platform to test your skills at model building and performance, additional regional platforms are available around the world that offer even more opportunities to learn... and win.","comments
By Frederik Bussler, Growth at Apteo.

By author.
We’ve all heard of Kaggle, but that also means there’s more competition — recently, Kaggle reached 5 million users. Further, not all competitions are open to everyone in the world. Here’s the policy of one competition, for instance:

“Members of the Kaggle community who are not United States Citizens or legal permanent residents at the time of entry are allowed to participate in the Competition but are not eligible to win prizes. If a team has one or more members who are not prize eligible, then the entire team is not prize eligible.”

By trying out other competition platforms, you can be a “big fish in a small pond,” as there are a lot fewer competitors.
Keep in mind that AI competitions aren’t the end-all-be-all if you want to enter the industry, as you’ll need knowledge in statistics, computing, communication, and more — not just knowing how to build models.
Besides ranking in competitions, you’ll want to work on practical projects that you can share with the world. Ideally, your projects can resonate with non-technical audiences as well, such as hiring managers who often don’t understand the intricacies of the field. To do so, you can use no-code analytics tools like Apteo that let you share very simple and easy-to-understand analyses.
That being said, let’s dive in.
 
1. DrivenData
 
DrivenData is open to everyone around the world (with the exception of OFAC sanctioned countries like Cuba, Iran, Iraq, North Korea, Sudan, and Syria).

DrivenData Eligibility rules. Captured by author.
Current competitions include Alzheimer’s research, hateful meme detection, predicting flu vaccines, and more.
 
2. CROWDANALYTIX
 
CROWDANALYTIX features competitions like cement quality forecasting, with participants from around the world. Currently, competitions are slowing down, but there’s also a great blog to learn more about data science topics in the meantime.
Check out Damian Boh’s experience working on a CrowdANALYTIX competition: How I Won Top Five in a Deep Learning Competition
 
3. Signate
 

Photo by Louie Martinez on Unsplash.
Signate is basically Japan’s Kaggle and has current competitions about vehicle driving image recognition, flattening the curve, and more.
 
4. Zindi
 
Zindi is a pan-African data science competition platform with challenges including African language NLP, insurance recommendations, a mental health chatbot, and more.
GIZ AI4D Africa Language Challenge - Round 2
 
5. Alibaba Cloud Tianchi
 
Alibaba’s Tianchi platform boasts 400,000 data scientists on its platform, working on challenges like image-based 3D shape retrieval, 3D object reconstruction, and instance segmentation.
 
6. Analytics Vidhya
 
Analytics Vidhya, besides being a great data science resource, is India’s go-to data science competition platform, with current challenges including loan prediction, sales prediction, times series forecasting, recommendation engines, and more.
 
7. CodaLab
 
CodaLab is a French-based data science competition platform popular in Europe, created as a joint venture between Microsoft and Stanford University in 2013.
Today, CodaLab boasts 40,000 users and 1,000 competitions, including an on-going robotics vision challenge.
 
Conclusion
 
You can’t win Kaggle prizes as a non-American, but there are fortunately many regional alternatives available.
While we didn’t cover regions like South or Central America — which don’t have their own local platforms — you can use truly open-access sites like DrivenData no matter where you are.
Original. Reposted with permission.
 
Related:

Lessons From My First Kaggle Competition
What my first Silver Medal taught me about Text Classification and Kaggle in general?
3 Best Sites to Find Datasets for your Data Science Projects"
https://www.kdnuggets.com/2021/03/kdnuggets-survey-data-community-job-satisfaction.html,Are you satisfied in your job? Take our Data Community Job Satisfaction Survey,The latest KDnuggets survey is looking to determine the job satisfaction levels of the data community. Take a few moments to contribute your answer and help paint a picture of the current situation.,"By Matthew Mayo, KDnuggets.
Data scientist may no longer be the ""sexiest"" job around, but perhaps it, along with related data roles, are still some of the most satisfying?
The latest KDnuggets survey is looking to probe the data community's job satisfaction, and is asking:


Your job satisfaction
How many years have you been in this job?
Are you looking for a new job?
Your employment type
Your job title (choose closest, and ignore Senior/Junior/Principal, etc.)



Image source: Open Sourced Workplace
 


This poll is closed - results and analysis coming soon!
View the current results here.
Thanks to everyone who has made time to participate."
https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html,How to Create Unbiased Machine Learning Models,"In this post we discuss the concepts of bias and fairness in the Machine Learning world, and show how ML biases often reflect existing biases in society. Additionally, We discuss various methods for testing and enforcing fairness in ML models.","comments
By Philip Tannor, Co-Founder & CEO of Deepchecks.

Image by Clker-Free-Vector-Images from Pixabay
  
AI systems are becoming increasingly popular and central in many industries. They decide who might get a loan from the bank, whether an individual should be convicted, and we may even entrust them with our lives when using systems such as autonomous vehicles in the near future. Thus, there is a growing need for mechanisms to harness and control these systems so that we may ensure that they behave as desired.
One important issue that has been gaining popularity in the last few years is fairness. While usually ML models are evaluated based on metrics such as accuracy, the idea of fairness is that we must ensure that our models are unbiased with regard to attributes such as gender, race and other selected attributes.
A classic example of an episode regarding racial bias in AI systems, is the COMPAS software system, developed by Northpointe, which aims to assist US courts with assessing the likelihood of a defendant becoming a recidivist. Propublica published an article which claims that this system is biased against blacks, giving them higher risk ratings.

ML system bias against African Americans? (source)
 
In this post we will try to understand where biases in ML models originate, and explore methods for creating unbiased models.
 
Where Does Bias Come From?
 

""Humans are the weakest link""
—Bruce Schneier

 
In the field of Cybersecurity it is often said that “humans are the weakest link” (Schneier). This idea applies in our case as well. Biases are in fact introduced into ML models by humans unintentionally.
Remember, an ML model can only be as good as the data it’s trained on, and thus if the training data contains biases, we can expect our model to mimic those same biases. Some representative examples for this can be found in the field of word embeddings in NLP. Word embeddings are learned dense vector representations of words, that are meant to capture semantic information of a word, which can then be fed to ML models for different downstream tasks. Thus, embeddings of words with similar meanings are expected to be “close” to each other.

Word embeddings can capture the semantic meaning of words. (source)
 
It turns out that the embedded space can be used to extract relations between words, and to find analogies as well. A classic example for this is the well known king-man+woman=queen equation. However, if we substitute the word “doctor” for the word “king” we get “nurse” as the female equivalent of the “doctor”. This undesired result simply reflects existing gender biases in our society and history. If in most available texts doctors are generally male and nurses are generally female, that’s what our model will understand.

doctor = nlp.vocab['doctor']
man = nlp.vocab['man']
woman = nlp.vocab['woman']
result = doctor.vector - man.vector + woman.vector
print(most_similar(result))
 
Output: nurse

Code example: man is to doctor as woman is to nurse according to gensim word2vec (source)
 
Culture Specific Tendencies
 
Currently, the most used language on the internet is English. Much of the research and products in the field of Data Science and ML is done in English as well. Thus, many of the “natural” datasets that are used to create huge language models tend to match American thought and culture, and may be biased towards other nationalities and cultures.

Cultural bias: GPT-2 needs active steering in order to produce a positive paragraph with the given prompt. (source)
 
Synthetic Datasets
 
Some biases in the data may be created unintentionally in the process of the dataset’s construction. During construction and evaluation people are more likely to notice and pay attention to details they are familiar with. A well known example for an image classification mistake, is when Google Photos misclassified black people as gorillas. While a single misclassification of this sort may not have a strong impact on the overall evaluation metrics, it is a sensitive issue, and could have a large impact on the product and the way customers relate to it. 

Racist AI algorithm? Misclassification of black people as gorillas. (source)
 
In conclusion, no dataset is perfect. Whether a dataset is handcrafted or “natural”, it is likely to reflect the biases of it’s creators, and thus the resulting model will contain the same biases as well.
 
 
Creating fair ML models
 
There are multiple proposed methods for creating fair ML models, which generally fall into one of the following stages.
 
Preprocessing
 
A naive approach to creating ML models that are unbiased with respect to sensitive attributes is to simply remove these attributes from the data, so that the model cannot use them for its prediction. However, it is not always straightforward to divide attributes into clear cut categories. For example, a person’s name may be correlated with their gender or ethnicity, nevertheless we would not necessarily want to regard this attribute as sensitive. More sophisticated approaches attempt to use dimensionality reduction techniques in order to eliminate sensitive attributes.
 
 
At Training Time
 
An elegant method for creating unbiased ML models is using adversarial debiasing. In this method we simultaneously train two models. The adversary model is trained to predict the protected attributes given the predictors prediction or hidden representation. The predictor is trained to succeed on the original task while making the adversary fail, thus minimizing the bias.

Adversarial debiasing illustration: The predictor loss function consists of two terms, the predictor loss, and the adversarial loss. (source)
 
This method can achieve great results for debiasing models without having to “throw away” the input data, however, it may suffer from difficulties that arise in general when training adversarial networks.
 
 
Post Processing
 
In the post processing stage we get the model’s predictions as probabilities, but we can still choose how to act based on these outputs, for example we can move the decision threshold for different groups in order to meet our fairness requirements.
One way to ensure model fairness in the post processing stage is to look at the intersection of the area under the ROC curve for all groups. The intersection represents TPRs and FPRs that can be achieved for all classes simultaneously. Note that in order to satisfy the desired result of  equal TPRs and FPRs for all classes one might need to purposefully choose to get less good results on some of the classes.

The colored region is what’s achievable while fulfilling the separability criterion for fairness. (source)
 
Another method for debiasing a model in the post processing stage involves calibrating the predictions for each class independently. Calibration is a method for ensuring that the probability outputs of a classification model indeed reflect the matching ratio of positive labels. Formally, a classification model is calibrated if for each value of r:

When a model is properly calibrated error rates will be similar across the different values of protected attributes.
 
 
Conclusion
 
To sum it up, we have discussed the concepts of bias and fairness in the ML world, we have seen that model biases often reflect existing biases in society. There are various ways in which we could enforce and test for fairness in our models, and hopefully, using these methods will lead to more just decision making in AI assisted systems around the world.
 
Further Reading
 
Gender bias in word embeddings
Propublica article
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, & Hanna Wallach. (2018). A Reductions Approach to Fair Classification.
Brian Hu Zhang, Blake Lemoine, & Margaret Mitchell. (2018). Mitigating Unwanted Biases with Adversarial Learning.
Solon Barocas, Moritz Hardt, & Arvind Narayanan (2019). Fairness and Machine Learning. fairmlbook.org.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, & Aram Galstyan. (2019). A Survey on Bias and Fairness in Machine Learning.
 
Bio: Philip Tannor is Co-Founder & CEO of Deepchecks.
Related:

Data Protection Techniques Needed to Guarantee Privacy
What Makes AI Trustworthy?
Ethics, Fairness, and Bias in AI"
https://www.kdnuggets.com/2021/06/7-data-security-best-practices-2021.html,7 Data Security Best Practices for 2021,Here are seven data security best practices to adopt this year.,"By Devin Partida, Editor-in-Chief of ReHack.com.
comments

Photo by Pixabay from Pexels
 
Data security is a growing concern for any company as cybercrime continues to prosper. Since data scientists’ entire work revolves around potentially sensitive data, they face more pressure than most. If you suffer a data breach, you may be unable to perform your job, in addition to potential financial and reputational losses.
IBM estimates that a data breach costs $3.86 million on average. Depending on the severity of the incident, it could also cost your reputation or even your job. With that in mind, here are seven data security best practices to adopt this year.
 
1. Use Only What You Need
 
One of the best ways to secure your data is to minimize what you store. While it’s tempting to collect as much data as possible, especially when training machine learning models, this makes you more vulnerable. You can only lose what you have, so go through your databases and get rid of anything that isn’t necessary.
Part of this principle is keeping an updated record of the data you have on hand. If you get to a point where you no longer need some information, purge it from your database. Holding on to legacy data doesn’t help you and only means you have more to lose.
 
2. Mask Sensitive Data
 
The “use only what you need” principle applies to the type of data you store, too. Many data science operations don’t require user-specific information, so don’t store it if you don’t need it. If you must use sensitive data like personal identifiers, you should mask it.
One of the most common ways to mask sensitive data is to use a substitution cipher. Tokenization, which substitutes real values with dummy data, is another option and generally safer, as it places the encrypted values in a separate database. No matter which method you use, make sure you scrub your data of all sensitive info that isn’t necessary first.
 
3. Collaborate Carefully
 
Data science is often a collaborative process, and you should think about how you communicate with collaborators. While email may be convenient, it isn’t encrypted by default, so it’s unsuitable for sharing data or credentials to access databases. There are many available services made specifically for sensitive file-sharing, so these are a better option.
You should also keep trust to a minimum, no matter who you’re working with. People should only be able to access what’s critical for their job. You may even consider obfuscating information before sharing it, if possible, to mitigate the impact of any potential breaches.
 
4. Encrypt as Much as Possible
 
When you do share data, you should encrypt it. You should also encrypt your data when it’s sitting in your database. While encryption isn’t a cure-all for all your security concerns, it’s a low-cost way to add another layer of protection.
Many of the best data encryption tools today won’t slow your processes much, either. Look through your options to find something that can encrypt your data at rest and in motion in all scenarios. While this won’t necessarily stop breaches from happening, it will mitigate their cost.
 
5. Secure More Than Your Databases
 
Remember that security applies to more than just where you store your data. Your databases should be the area you pay the most attention to, but they shouldn’t be your only concern. Backups, connected applications, and analytics servers can all serve as backdoors to your data, so they need protection too.
Any program, drive, or file that touches your data should be secure. As you work on this, it’s easier when your data has fewer connections. Minimizing the things that have access to your databases makes your job easier and offers more protection.
 
6. Take Care With Third-Party Cloud Vendors
 
If you use a third-party cloud like AWS, be careful not to become complacent with security. Unfortunately, many users do, as a recent study revealed that 82% of companies give these vendors highly privileged access. Third-party clouds are not inherently risky, but you do need to take security into your own hands.
Check your permissions to make sure you grant the least privilege to your vendor and other applications. Use strong credentials, including multi-factor authentication, and rotate these regularly. If you don’t know what to do, many of these vendors provide security best practices you can reference.
 
7. Establish a Clear Governance Policy
 
Finally, you should establish a clear and specific governance policy for your whole team. Having a written document of what people should and shouldn’t do will help ensure safe user behavior. If someone makes a mistake that jeopardizes security, you can refer to the policy to see what went wrong.
Your governance policy should define everybody’s role in security. You may have a rotating schedule for who monitors and documents incoming and outgoing data. You may give everyone a static role. Whatever you do, make it specific and clear, and ensure everyone understands it.
 
Data Science Security Must Improve
 
Data science is playing an increasingly central role in business today. As this trend continues, your work becomes a more valuable target for cybercriminals. Data science teams must embrace security in light of these rising threats.
Start with these best practices, then look for other, smaller areas where you can increase security. When your data is secure, you can work with confidence and impress potential clients.
 
Bio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com.
Related:

How Automation Is Improving the Role of Data Scientists
Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions
How Data Professionals Can Add More Variation to Their Resumes"
https://www.kdnuggets.com/2021/06/data-visualization-feature-selection.html,This Data Visualization is the First Step for Effective Feature Selection,"Understanding the most important features to use is crucial for developing a model that performs well. Knowing which features to consider requires experimentation, and proper visualization of your data can help clarify your initial selections. The scatter pairplot is a great place to start.","By Benjamin Obi Tayo, Ph.D., DataScienceHub.
comments

Image by Benjamin O. Tayo.
The scatter pairplot is a visualization of pairwise relationships in a dataset and is the first step for effective feature selection. It provides a qualitative analysis of the pairwise correlation between features and is a powerful tool for feature selection and dimensionality reduction. For an introduction of the pairplot using the seaborn package, see this link: https://seaborn.pydata.org/generated/seaborn.pairplot.html
In this article, we will analyze a portfolio of stocks to examine the ones that are strongly correlated to the overall market. The portfolio contains 22 stocks (see Table 1) from different sectors such as Healthcare, Real Estate, Consumer Discretionary, Energy, Industrials, Telecommunication Services, Information Technology, Consumer Staples, and Financials.
 



Symbol
Name
Symbol
Name
Symbol
Name


AAL
American Airlines
EDIT
Editas Medicine
UAL
United Airlines


AAPL
Apple
HPP
Hudson Pacific Properties
WEN
Wendy


ABT
Abbott Laboratories
JNJ
Johnson & Johnson
WFC
Wells Fargo


BNTX
BioNTech
MRNA
Moderna
WMT
Walmart


BXP
Boston Properties
MRO
Marathon Oil Corporation
XOM
Exxon Mobile


CCL
Carnival Corporation
PFE
Pfizer
SP500
Stock Market Index


DAL
Delta Airlines
SLG
SL Green Realty




DVN
Devon Energy
TSLA
Tesla





Table 1. Portfolio of 22 stocks from diverse sectors.
Our goal is to answer the question: which stocks in the portfolio correlate strongly with the stock market? We will use the S&P 500 index as a measure of the total stock market. We will assume a threshold correlation coefficient of 70% for a stock to be considered to be strongly correlated to the S&P 500.
 
Data Collection and Processing
 
Raw data were obtained from the yahoo finance website: https://finance.yahoo.com/
The historical data for each stock has information on daily open price, high price, low price, and closing price. The CSV file was downloaded for each stock, and then the column “close” was extracted and combined to create the dataset, which can be found here: portfolio.csv
 
Generate Scatter Pairplot
 

import numpy as np
import pandas as pd
import pylab
import matplotlib.pyplot as plt
import seaborn as sns

url = 'https://raw.githubusercontent.com/bot13956/datasets/master/portfolio.csv'
data = pd.read_csv(url)
data.head()

cols = data.columns[1:24]
sns.pairplot(data[cols], height=2.0)




 
Calculate Covariance Matrix
 
The scatter pairplot is the first step, which provides a qualitative analysis of pairwise correlations between features. To quantify the degree of correlation, the covariance matrix has to be computed.

from sklearn.preprocessing import StandardScaler
stdsc = StandardScaler()
X_std = stdsc.fit_transform(data[cols].iloc[:,range(0,23)].values)

cov_mat = np.cov(X_std.T, bias= True)

import seaborn as sns
plt.figure(figsize=(13,13))
sns.set(font_scale=1.2)
hm = sns.heatmap(cov_mat,
                 cbar=True,
                 annot=True,
                 square=True,
                 fmt='.2f',
                 annot_kws={'size': 12},
                 yticklabels=cols,
                 xticklabels=cols)
plt.title('Covariance matrix showing correlation coefficients')
plt.tight_layout()
plt.show()




 
 
Compressed Output Showing Pairplots and Correlation Coefficients
 
Since we are only interested in the correlations between the 22 stocks in the portfolio with the S&P 500, Figure 1 below shows the final output from our analysis.

Figure 1. Scatter pairplots and correlation coefficients between portfolio stocks and the S&P 500.
Figure 1 shows that out of the 22 stocks, 8 have a correlation coefficient less than 70%. Interestingly, except for WEN stock, all the other stocks have a positive correlation with the S&P 500 index.
The full covariance matrix is shown in Figure 2.

Figure 2. Covariance matrix visualization.
In summary, we’ve shown how the scatter pairplot can be used as a first step for feature selection. Other advanced methods for feature selection and dimensionality reduction include the following: PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis).
 
Related:

A simple static visualization can often be the best approach
How to create stunning visualizations using python from scratch
Creating Good Meaningful Plots: Some Principles"
https://www.kdnuggets.com/2021/06/pandas-vs-sql.html,Pandas vs SQL: When Data Scientists Should Use Each Tool,"Exploring data sets and understanding its structure, content, and relationships is a routine and core process for any Data Scientist. Multiple tools exist for performing such analysis, and we take a deep dive into the benefits and different approaches of two important tools, SQL and Pandas.","comments
By Matthew Przybyla, Senior Data Scientist at Favor Delivery.

Photo by rigel on Unsplash.
Both of these tools are important to not only data scientists but also to those in similar positions like data analytics and business intelligence. With that being said, when should data scientists specifically use pandas over SQL and vice versa? In some situations, you can get away with just using SQL, and some other times, pandas is much easier to use, especially for data scientists who focus on research in a Jupyter Notebook setting. Below, I will discuss when you should use SQL and when you should use pandas. Keep in mind that both of these tools have specific use cases, but there are many times where their functionality overlap, and that is what I will be comparing below as well.
 
Pandas
 

Photo by Kalen Kemp on Unsplash.
Pandas is an open-source data analysis tool in the Python programing language. The benefit of pandas starts when you already have your main dataset, usually from a SQL query. This main difference can mean that the two tools are separate. However, you can also perform several of the same functions in each respective tool. For example, you can create new features from existing columns in pandas, perhaps easier and faster than in SQL.
It is important to note that I am not comparing what pandas does that SQL cannot do and vice versa. I will be picking the tool that can do the function more efficiently or preferable for data science work — in my opinion, from personal experience.
Here are times where using pandas is more beneficial than SQL — while also having the same functionality as SQL:

creating calculated fields from existing features

When incorporating a more complex SQL query, you often are incorporating subqueries as well in order to divide values from different columns. In pandas, you can simply divide features much easier like the following:

df[""new_column""] = df[""first_column""]/df[""second_column""]



 
The code above is showing how you can divide two separate columns and assign those values to a new column. In this case, you are performing the feature creation on the whole entire dataset or dataframe. You can use this function in both feature exploration and feature engineering in the process of data science.

grouping by

Also referring to subqueries, grouping by in SQL can become quite complex and require lines and lines of code that can be visually overwhelming. In pandas, you can simply group by one line of code. I am not referring to the ‘group by’ at the end of a simple select from table query, but one where there are multiple subqueries involved.

df.groupby(by=""first_column"").mean()



 
This result would be returning the mean of the first_column for every column in the dataframe. There are many other ways to use this grouping function, which are outlined nicely in the pandas documentation.

checking data types

In SQL, you will often have to cast types, but sometimes it can be a little clearer to see the way pandas lays out data types in a vertical format rather than scrolling through a horizontal output in SQL. You can expect some examples of data types returned to be int64, float64, datetime64[ns], and object.

df.dtypes



 
While these are all fairly simple functions of pandas and SQL, in SQL, they are particularly tricky and sometimes just much easier to implement in a pandas dataframe. Now, let’s look at what SQL is better at performing.
 
SQL
 

Photo by Caspar Camille Rubin on Unsplash.
SQL is probably the language that is used most by the most amount of different positions. For example, a data engineer could use SQL, a Tableau developer, or a product manager. With that being said, data scientists tend to use SQL frequently. It is important to note that there are several different versions of SQL, usually all having a similar function, just slightly formatted differently.
Here are times where using SQL is more beneficial than pandas — while also having the same functionality as pandas:

WHERE clause

This clause in SQL is used frequently and can also be performed in pandas. In pandas, however, it is slightly more difficult or less intuitive. For example, you have to write out redundant code, whereas, in SQL, you simply need the WHERE.

SELECT id
FROM table
   WHERE id > 100



 
In pandas, it would be something like:

df[df[""id""] > 100][""id""]



 
Yes, both are simple, but SQL is just a little more intuitive.

JOINS

Pandas has a few ways to join, which can be a little overwhelming, whereas in SQL, you can perform simple joins like the following: INNER, LEFT, RIGHT.

SELECT
   one.column_A,
   two.column_B
FROM first_table one
   INNER JOIN second_table two ON two.id = one.id



 
In this code, joining is slightly easier to read than in pandas, where you have to merge dataframes, and especially as you merge more than two dataframes, it can be quite complex in pandas. SQL can perform multiple joins, whether it be INNER, etc., all in the same query.
All of these examples, whether it be SQL or pandas, can be used in at least the exploratory data analysis portion of the data science process, as well as in feature engineering, and querying model results once they are stored in a database.
 
Summary
 
This comparison of pandas versus SQL is more of a personal preference. With that being said, you may feel the opposite of my opinion. However, I hope it still sheds light on the differences between pandas and SQL, as well as what you can perform the same in both tools, using slightly different coding techniques and a different language altogether.
To summarize, we have compared the benefits of using pandas over SQL and vice versa for a few of their shared functions:

 creating calculated fields from existing features
grouping by
checking data types
WHERE clause
JOINS

 
Original. Reposted with permission.
 
Related:

Every Complex DataFrame Manipulation, Explained & Visualized Intuitively
Data Preparation in SQL, with Cheat Sheet!
Introduction to Pandas for Data Science"
https://www.kdnuggets.com/2020/12/5-free-books-learn-statistics-data-science.html,5 Free Books to Learn Statistics for Data Science,Learn all the statistics you need for data science for free.,"comments
By Rebecca Vickery, Data Scientist


Photo by Daniel Schludi on Unsplash

 
Statistics is a fundamental skill that data scientists use every day. It is the branch of mathematics that allows us to collect, describe, interpret, visualise, and make inferences about data. Data scientists will use it for data analysis, experiment design, and statistical modelling.
Statistics is also essential for machine learning. We will use statistics to understand the data prior to training a model. When we take samples of data for training and testing our models we need to employ statistical techniques to ensure fairness. When evaluating the performance of a model we need statistics to assess the variability of the predictions and assess accuracy.

“If statistics are boring, you’ve got the wrong numbers.”, Edward Tufte

These are just some of the ways in which statistics are employed by data scientists. If you are studying data science it is therefore essential to develop a good understanding of these statistical techniques.
This is one area where books can be a particularly useful study tool as detailed explanations of statistical concepts is essential to your understanding.
Here are my top 5 free books for learning statistics for data science.
 
Practical Statistics for Data Scientists
 
by Peter Bruce and Andrew Bruce


Image: amazon.co.uk


Read for free here.
Main topics covered:

Data structures.
Descriptive statistics.
Probability.
Machine learning.

Suitable for: Complete beginners.
Statistics is a very broad field, and only part of it is relevant to data science. This book is extremely good at only covering the areas related to data science. So if you are looking for a book that will quickly give you just enough understanding to be able to practice data science then this book is definitely the one to choose.
It is filled with a lot of practical coded examples (written in R), gives very clear explanations for any statistical terms used and also links out to other resources for further reading.
This is overall an excellent book to cover off the basics and is suitable for an absolute beginner to the field.
 
Think Stats
 
by Allen B. Downey


Image: greenteapress.com

 
Read for free here.
Main topics covered:

Statistical thinking.
Distributions.
Hypothesis testing.
Correlation.

Suitable for: Beginners with basic Python.
The introduction for this book states that “this book is about turning knowledge into data” and it does a very good job of introducing statistical concepts through practical examples of data analysis.

“this book is about turning knowledge into data”

It is another book that covers only the concepts directly related to data science and also contains lots of code examples, this time written in Python. It is aimed heavily at programmers and relies on using that skill to understand the key statistical concepts introduced. This book is therefore ideally suited to those who already have at least a basic grasp of Python.
 
Bayesian Methods for Hackers
 
by Cameron Davidson-Pilon


Image: amazon.com

 
Read for free here.
Main topics covered:

Bayesian inference.
Loss functions.
Bayesian machine learning.
Priors.

Suitable for: Non-statisticians with a working knowledge of Python.
Bayesian inference is a branch of statistics that deals with understanding uncertainty. As a data scientist uncertainty is something you will need to model on a very regular basis. If you are building a machine learning model, for example, you will need to be able to understand the uncertainty around the predictions that your model is delivering.
Bayesian methods can be quite abstract and difficult to understand. This book aimed firmly at programmers (so some Python is a prerequisite), is the only material I have found that explains these concepts in a simple enough way for a non-statistician to understand. There are coded examples throughout and the Github repository, where the chapters are hosted, contains a large selection of notebooks. It is, therefore, an excellent hands-on introduction to this subject.
 
Statistics in Plain English
 
by Timothy C. Urdan


Image: amazon.co.uk

 
Read for free here.
Main topics covered:

Regression.
Distributions.
Factor analysis.
Probability.

Suitable for: Non-statisticians with any level of programming experience.
This book covers general statistical techniques rather than just those aimed at data scientists or programmers. It is however written in a very straight forward style and covers a wide range and depth of statistical concepts in a very simple to understand way.
The book was originally written for students studying a non-mathematics based course where an understanding of statistics is required, such as the social sciences. It, therefore, covers enough theory to understand the techniques but doesn’t assume an existing mathematical background. It is, therefore, an ideal book to read if you are coming into data science without a math-based degree.
 
Computer Age Statistical Inference
 
by Bradley Efron and Trevor Hastie


Image: amazon.co.uk

 
Read for free here.
Main topics covered:

Bayesian and frequentist inference.
Large scale hypothesis testing.
Machine learning.
Deep learning.

Suitable for: Someone with a basic understanding of statistics and statistical notation. No programming required.
This book covers the theory behind most of the popular machine learning algorithms used by data scientists today. It also gives a thorough introduction to both Bayesian and Frequentist statistical inference methodologies.
The second half of the book, which covers machine learning algorithms, is some of the best material I have seen on this subject. Each explanation is in-depth and uses practical examples such as the classification of spam data which makes quite complex ideas easier to digest. The book is most suited to those who have already covered the basics of statistics for data analysis and are familiar with some statistical notation.
 
The books I included in this article cover enough topics for a complete beginner to learn all the statistics needed for data science. They can all be read for free online but most also have a print version that can be purchased if you prefer to read physical books. Statistics is an essential component of the data science toolset and something which often requires in-depth reading to truly understand the concepts. Something which these books can provide.
For more data science reading lists please check out my previous articles below.
5 Free Books for Learning Python for Data Science
A completely free reading list for learning Python
 
Completely Free Machine Learning Reading List
10 free books to read if you are studying machine learning
 
Reading List for Applied AI
Six books to read if you are applying AI to your business in 2020
 
Thanks for reading!
I send out a monthly newsletter if you would like to join please sign up via this link. Looking forward to being part of your learning journey!
 
Bio: Rebecca Vickery is learning data science through self study. Data Scientist @ Holiday Extras. Co-Founder of alGo.
Original. Reposted with permission.
Related:

The Best Free Data Science eBooks: 2020 Update
Top 5 Free Machine Learning and Deep Learning eBooks Everyone should read
Mathematics for Machine Learning: The Free eBook"
https://www.kdnuggets.com/2021/04/most-demand-skills-data-scientists.html,The Most In-Demand Skills for Data Scientists in 2021,"If you are preparing to make a career as a Data Scientist or are looking for opportunities to skill-up in your current role, this analysis of in-demand skills for 2021, based on over 15,000 Data Scientist job postings, should offer you a good idea as to which programming languages and software tools are increasing and decreasing in importance.","By Terence Shin, Data Scientist | MSc Analytics & MBA student.
comments
 

Image created by Author.
I just wanted to start off by saying that this is heavily inspired by Jeff Hale’s articles that he wrote back in 2018/2019. I’m writing this simply because I wanted to get a more up-to-date analysis of what skills are in demand today, and I’m sharing this because I’m assuming that there are people out there that also want to see an updated version of the most in-demand skills for data scientists in 2021.
Take what you want from this analysis — it’s obvious that the insights gathered from web scraping job postings do not offer a perfect correlation to what data science skills are actually most demanded. However, I think this gives a good indication of what general skills you should focus more on, and likewise, stray away from.
With that said, I hope you enjoy this, and let’s dive into it!
 
Methodology
 
For this analysis, I webscraped and accumulated over 15,000 job postings from Indeed, Monster, and SimplyHired. I didn’t webscrape LinkedIn because I ran into Captcha issues trying to scrape it.
I then checked to see how many job postings included each term that I was searching. The list of terms that I was searching was as follows (if you want to see any other skills, please mention it in the comments so I can add it for next year’s analysis!):

Python, SQL, R, Java, Git, C, MATLAB, Excel, C++, JavaScript, C#, Julia, Scala, SAS
Scikit-learn, Pandas, NumPy, SciPy
Matplotlib, Looker, Tableau
TensorFlow, PyTorch, Keras
Spark, Hadoop, AWS, GCP, Hive, Azure, Google Cloud, MongoDB, BigQuery
Docker, Kubernetes, Airflow
NoSQL, MySQL, PostgreSQL
Caffe, Alteryx, Perl, Cassandra, Linux

After getting the counts from each source, I summed them up and then divided it over the total number of data scientist job postings to get a percentage. For example, Python’s value of 0.77 means that 77% of the job postings had Python in it.
Finally, I compared the results to the analysis done by Jeff Hale in 2019 to get the percentage change from 2019 to 2021.
 
Results
 
Top Skills
Below are the top 25 most in-demand data science skills in 2021, ranked from highest to lowest:

Image created by Author.
Top Programming Languages
To get a more granular look, the chart below shows the top programming languages for data scientists:

Image created by Author.
It’s no surprise that Python, SQL, and R are the top three programming languages.
Personally, I also stand by the fact that you should know either Python or R as well as SQL. I started with Python, and I’ll probably stick with Python for the rest of my life. It’s so far ahead in terms of open source contributions, and it’s straightforward to learn. SQL is arguably the most important skill to learn across any type of data-related profession, whether you’re a data scientist, data engineer, data analyst, business analyst, the list goes on.
Top Python Libraries
Similarly, the chart below shows the top Python Libraries for Data Scientists:

Image created by Author.
TensorFlow ranks first, as it is one of the most popular libraries of Python for deep learning. PyTorch is a strong alternative, hence its ranking not too far behind.
Scikit-learn is arguably the most important library in Python for machine learning. After cleaning and manipulating your data with Pandas and/or NumPy, scikit-learn is used to build machine learning models as it has tons of tools used for predictive modelling and analysis.
In my opinion, Pandas, NumPy, and SciPy are also essential for data scientists despite their representation above.
Fastest Growing and Declining Skills
The charts below show the fastest growing and declining skills from 2019 to 2021:

Image created by Author.

Image created by Author.
Here are a few takeaways from the two charts above:

There is a huge increase in skills related to the cloud, like AWS and GCP.
Similarly, there is also a large increase in skills related to deep learning, like PyTorch and TensorFlow.
SQL and Python continue to grow in importance, while R remains stagnant.
Apache products, like Hadoop, Hive, and Spark, continue to decline in importance.

 
Original. Reposted with permission.
 
Related:

7 Most Recommended Skills to Learn to be a Data Scientist
5 Supporting Skills That Can Help You Get a Data Science Job
9 Skills You Need to Become a Data Engineer
The Most in Demand Skills for Data Scientists, by Jeff Hale, Nov 2018."
https://www.kdnuggets.com/2021/02/dannet-triggers-deep-cnn-revolution.html,2011: DanNet triggers deep CNN revolution,"In 2021, we are celebrating the 10-year anniversary of DanNet, which, in 2011, was the first pure deep convolutional neural network (CNN) to win computer vision contests. Read about its history here.","comments
By Jürgen Schmidhuber (@SchmidhuberAI)

 
Abstract. In 2021, we are celebrating the 10-year anniversary of DanNet, named after my outstanding Romanian postdoc Dan Claudiu Cireșan (aka Dan Ciresan). In 2011, DanNet was the first pure deep convolutional neural network (CNN) to win computer vision contests. For a while, it enjoyed a monopoly. From 2011 to 2012 it won every contest it entered, winning four of them in a row (15 May 2011, 6 Aug 2011, 1 Mar 2012, 10 Sep 2012), driven by a very fast implementation based on graphics processing units (GPUs). Remarkably, already in 2011, DanNet achieved the first superhuman performance in a vision challenge, although compute was still 100 times more expensive than today. In July 2012, our CVPR paper on DanNet hit the computer vision community. The similar AlexNet (citing DanNet) joined the party in Dec 2012. Our even much deeper Highway Net (May 2015) and its special case ResNet (Dec 2015) further improved performance (a ResNet is a Highway Net whose gates are always open). Today, a decade after DanNet, everybody is using fast deep CNNs for computer vision.
CNNs originated over 4 decades ago [CNN1-4]. The basic CNN architecture with convolutional layers and downsampling layers is due to Kunihiko Fukushima (1979) [CNN1, CNN1+]. In 1987, NNs with convolutions were combined by Alex Waibel [CNN1a,b] with weight sharing and backpropagation, a technique from 1970 [BP1-4] [R7]. Yann LeCun's team later contributed important improvements of CNNs, especially for images, e.g., [CNN2] [CNN4] [T20] (Sec. XVIII). The popular downsampling variant called ""max-pooling"" was introduced by Juyang Weng et al. (1993) [CNN3]. In 2010, my own team at the Swiss AI Lab IDSIA showed [MLP1] that unsupervised pre-training is not necessary to train deep NNs (a reviewer called this a ""wake-up call to the machine learning community""— compare the survey blog post [MLP2]).
One year later, our team with my postdocs Dan Cireșan & Ueli Meier and my PhD student Jonathan Masci (a fellow co-founder of NNAISENSE) greatly sped up the training of deep CNNs. Our fast GPU-based [GPUNN] CNN of 1 Feb 2011 [GPUCNN1,2,6], often called ""DanNet,"" was a practical breakthrough. Published later that year at IJCAI [GPUCNN1], it was much deeper and faster than earlier GPU-accelerated CNNs of 2006 [GPUCNN]. DanNet showed that deep CNNs worked far better than the existing state-of-the-art for recognizing objects in images [GPUCNN2,2+,5,6].
On a sunny day in Silicon Valley, at IJCNN 2011, DanNet blew away the competition and achieved the first superhuman visual pattern recognition in an international contest [GPUCNN2-3,5]. Even the New York Times mentioned this. DanNet performed twice as good as human test subjects and three times better than the already impressive second place entry by LeCun's team [SER11]. Compare Sec. D & Sec. XVIII of [T20].
Since 2011, DanNet has attracted tremendous interest from industry. Its temporary monopoly on winning computer vision competitions made it the first deep CNN to win: a Chinese handwriting contest (ICDAR, May 2011), a traffic sign recognition contest (IJCNN, Aug 2011), an image segmentation contest (ISBI, May 2012), and a contest on object detection in large images (ICPR, Sept 2012). The latter was actually a medical imaging contest on cancer detection [GPUCNN8]. Our CNN image scanners were 1000 times faster than previous methods [SCAN]. The significance of these kind of improvements for the health care industry is obvious. Today IBM, Siemens, Google, and many startups are pursuing this approach.
In 2011, we also introduced our deep neural nets to Arcelor Mittal, the world's largest steel producer, and were able to greatly improve steel defect detection [ST]. To the best of my knowledge, this was the first deep learning breakthrough in heavy industry. A significant part of modern computer vision is extending our work of 2011, e.g., [DL1-4] and Sec. 19 of [MIR].
A follow up technical report on DanNet in Feb 2012 summarized some of the recent breakthroughs. In July 2012, DanNet was also presented at CVPR, the leading computer vision conference [GPUCNN3]. This helped to spread the word in the computer vision community. As of 2020, the CVPR article was the most cited DanNet paper, albeit not the first [GPUCNN1-3,6].
After DanNet had won 4 image recognition competitions, the similar GPU-accelerated ""AlexNet"" won the ImageNet [IM09] 2012 contest [GPUCNN4-5] [R6]. Unlike DanNet, AlexNet used Christoph v. d. Malsburg's rectified linear neurons (ReLUs) [CMB] (1973) and a variant of Stephen J. Hanson's stochastic delta rule (1990) called ""dropout"" [Drop1] [T20]. While both of these techniques helped, they are not really required to win vision contests [GPUCNN5] [R6]. Back then, the only really important CNN-related task was to greatly accelerate known techniques for training CNNs through GPUs. Compare Sec. XIV of [T20].
We continued to make CNNs and other neural nets even deeper and better. Until 2015, deep networks had at most a few tens of layers, e.g., 20-30 layers. But in May 2015, our Highway Net [HW1] [HW3] [HW] [R5] was the first working extremely deep feedforward neural net with hundreds of layers. The Highway Net is based on the LSTM principle [LSTM1-2] which enables much deeper learning. Its special case called ""ResNet"" [HW2] (the ImageNet 2015 winner of Dec 2015) is a Highway Net whose gates are always open (compare [HW] & Sec. 4 of [MIR]). Highway Nets perform roughly as well as ResNets on ImageNet [HW3]. Highway layers are also often used for natural language processing [HW3] (compare [MIR] [DEC] [T20]).
The original successes of DanNet required a precise understanding of the inner workings of GPUs [GPUCNN1-3]. Today, convenient software packages shield the user from such details, and compute is roughly 100 times cheaper than 10 years ago when our results set the stage for the recent decade of deep learning [DEC]. Many current commercial neural net applications are based on what started in 2011 [DL1-4] [DEC].
 
Acknowledgments
 
Thanks to several expert reviewers for useful comments. (Let me know under juergen@idsia.ch if you can spot any remaining error.) The contents of this article may be used for educational and non-commercial purposes, including articles for Wikipedia and similar sites.
 
References
 
[MLP1] D. C. Ciresan, U. Meier, L. M. Gambardella, J. Schmidhuber. Deep Big Simple Neural Nets For Handwritten Digit Recognition. Neural Computation 22(12): 3207-3220, 2010. ArXiv Preprint (1 March 2010). [Showed that plain backprop for deep standard NNs is sufficient to break benchmark records, without any unsupervised pre-training.]
[MLP2] J. Schmidhuber (Sep 2020). 10-year anniversary of supervised deep learning breakthrough (2010). No unsupervised pre-training. The rest is history
[MIR] J. Schmidhuber (2019). Deep Learning: Our Miraculous Year 1990-1991. See also arxiv:2005.05744.
[DEC] J. Schmidhuber (2020). The 2010s: Our Decade of Deep Learning / Outlook on the 2020s.
[DL1] J. Schmidhuber, 2015. Deep Learning in neural networks: An overview. Neural Networks, 61, 85-117. More.
[DL2] J. Schmidhuber, 2015. Deep Learning. Scholarpedia, 10(11):32832.
[DL4] J. Schmidhuber, 2017. Our impact on the world's most valuable public companies: 1. Apple, 2. Alphabet (Google), 3. Microsoft, 4. Facebook, 5. Amazon ....
[T20] J. Schmidhuber (2020). Critique of 2018 Turing Award for deep learning.
[CNN1] K. Fukushima: Neural network model for a mechanism of pattern recognition unaffected by shift in position—Neocognitron. Trans. IECE, vol. J62-A, no. 10, pp. 658-665, 1979. [The first deep convolutional neural network architecture, with alternating convolutional layers and downsampling layers. In Japanese. English version: [CNN1+]. More in Scholarpedia.]
[CNN1+] K. Fukushima: Neocognitron: a self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, vol. 36, no. 4, pp. 193-202 (April 1980). Link.
[CNN1a] A. Waibel. Phoneme Recognition Using Time-Delay Neural Networks. Meeting of IEICE, Tokyo, Japan, 1987. [First application of backpropagation [BP1][BP2] and weight-sharing to a convolutional architecture.]
[CNN1b] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano and K. J. Lang. Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37, no. 3, pp. 328-339, March 1989.
[CNN2] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel: Backpropagation Applied to Handwritten Zip Code Recognition, Neural Computation, 1(4):541-551, 1989. PDF.
[CNN3] Weng, J., Ahuja, N., and Huang, T. S. (1993). Learning recognition and segmentation of 3-D objects from 2-D images. Proc. 4th Intl. Conf. Computer Vision, Berlin, Germany, pp. 121-128. [A CNN whose downsampling layers use Max-Pooling (which has become very popular) instead of Fukushima's Spatial Averaging [CNN1].]
[CNN4] M. A. Ranzato, Y. LeCun: A Sparse and Locally Shift Invariant Feature Extractor Applied to Document Images. Proc. ICDAR, 2007
[IM09] J. Deng, R. Socher, L.J. Li, K. Li, L. Fei-Fei (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255). IEEE, 2009.
[GPUNN] Oh, K.-S. and Jung, K. (2004). GPU implementation of neural networks. Pattern Recognition, 37(6):1311-1314. [Speeding up traditional NNs on GPU by a factor of 20.]
[GPUCNN] K. Chellapilla, S. Puri, P. Simard. High performance convolutional neural networks for document processing. International Workshop on Frontiers in Handwriting Recognition, 2006. [Speeding up shallow CNNs on GPU by a factor of 4.]
[GPUCNN1] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, J. Schmidhuber. Flexible, High Performance Convolutional Neural Networks for Image Classification. International Joint Conference on Artificial Intelligence (IJCAI-2011, Barcelona), 2011. PDF. ArXiv preprint (1 Feb 2011). [Speeding up deep CNNs on GPU by a factor of 60. Used to win four important computer vision competitions 2011-2012 before others won any with similar approaches.]
[GPUCNN2] D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. A Committee of Neural Networks for Traffic Sign Classification. International Joint Conference on Neural Networks (IJCNN-2011, San Francisco), 2011. PDF. HTML overview. [At IJCNN 2011, DanNet achieved the first superhuman performance in a computer vision contest, with half the error rate of humans, and one third the error rate of the closest competitor. This led to massive interest from industry.]
[GPUCNN2+] D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks 32: 333-338, 2012. PDF of preprint.
[GPUCNN3] D. C. Ciresan, U. Meier, J. Schmidhuber. Multi-column Deep Neural Networks for Image Classification. Proc. IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012, p 3642-3649, July 2012. PDF. Longer TR of Feb 2012: arXiv:1202.2745v1 [cs.CV]. More.
[GPUCNN4] A. Krizhevsky, I. Sutskever, G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. NIPS 25, MIT Press, Dec 2012. PDF.
[GPUCNN5] J. Schmidhuber. History of computer vision contests won by deep CNNs on GPU. March 2017. [How IDSIA used deep and fast GPU-based CNNs to win four important computer vision competitions 2011-2012 before others won contests using similar approaches.]
[GPUCNN6] J. Schmidhuber, D. Ciresan, U. Meier, J. Masci, A. Graves. On Fast Deep Nets for AGI Vision. In Proc. Fourth Conference on Artificial General Intelligence (AGI-11), Google, Mountain View, California, 2011. PDF.
[GPUCNN7] D. C. Ciresan, A. Giusti, L. M. Gambardella, J. Schmidhuber. Mitosis Detection in Breast Cancer Histology Images using Deep Neural Networks. MICCAI 2013. PDF.
[GPUCNN8] J. Schmidhuber. First deep learner to win a contest on object detection in large images— first deep learner to win a medical imaging contest (2012). HTML. [How IDSIA used GPU-based CNNs to win the ICPR 2012 Contest on Mitosis Detection and the MICCAI 2013 Grand Challenge.]
[SER11] P. Sermanet, Y. LeCun. Traffic sign recognition with multi-scale convolutional networks. Proc. IJCNN 2011, p 2809-2813, IEEE, 2011
[SCAN] J. Masci, A. Giusti, D. Ciresan, G. Fricout, J. Schmidhuber. A Fast Learning Algorithm for Image Segmentation with Max-Pooling Convolutional Networks. ICIP 2013. Preprint arXiv:1302.1690.
[ST] J. Masci, U. Meier, D. Ciresan, G. Fricout, J. Schmidhuber Steel Defect Classification with Max-Pooling Convolutional Neural Networks. Proc. IJCNN 2012. PDF.
[HW] J. Schmidhuber (2015): Overview of Highway Networks: First working really deep feedforward neural networks with over 100 layers. (Updated 2020 for 5-year anniversary.)
[HW1] R. K. Srivastava, K. Greff, J. Schmidhuber. Highway networks. Preprints arXiv:1505.00387 (May 2015) and arXiv:1507.06228 (July 2015). Also at NIPS 2015. [The first working very deep feedforward nets with over 100 layers. Let g, t, h, denote non-linear differentiable functions. Each non-input layer of a highway net computes g(x)x + t(x)h(x), where x is the data from the previous layer. (Like LSTM with forget gates [LSTM2] for RNNs.) Resnets [HW2] are a special case of this where the gates are always open: g(x)=t(x)=const=1. Highway Nets perform roughly as well as ResNets [HW2] on ImageNet [HW3]. Highway layers are also often used for natural language processing, where the simpler residual layers do not work as well [HW3]. More.]
[HW1a] R. K. Srivastava, K. Greff, J. Schmidhuber. Highway networks. Presentation at the Deep Learning Workshop, ICML'15, July 10-11, 2015. Link.
[HW2] He, K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image recognition. Preprint arXiv:1512.03385 (Dec 2015). Residual nets are a special case of Highway Nets [HW1] where the gates are open: g(x)=1 (a typical highway net initialization) and t(x)=1. More.
[HW3] K. Greff, R. K. Srivastava, J. Schmidhuber. Highway and Residual Networks learn Unrolled Iterative Estimation. Preprint arxiv:1612.07771 (2016). Also at ICLR 2017.
[LSTM1] S. Hochreiter, J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735-1780, 1997. PDF. More.
[LSTM2] F. A. Gers, J. Schmidhuber, F. Cummins. Learning to Forget: Continual Prediction with LSTM. Neural Computation, 12(10):2451-2471, 2000. PDF. [The ""vanilla LSTM architecture"" with forget gates that everybody is using today, e.g., in Google's Tensorflow.]
[BP1] S. Linnainmaa. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's Thesis (in Finnish), Univ. Helsinki, 1970. See chapters 6-7 and FORTRAN code on pages 58-60. PDF. See also BIT 16, 146-160, 1976. Link. [The first publication of ""modern"" backpropagation, also known as the reverse mode of automatic differentiation.]
[BP2] P. J. Werbos. Applications of advances in nonlinear sensitivity analysis. In R. Drenick, F. Kozin, (eds): System Modeling and Optimization: Proc. IFIP, Springer, 1982. PDF. [First application of backpropagation [BP1] to neural networks. Extending preliminary thoughts in his 1974 thesis.]
[BP4] J. Schmidhuber. Who invented backpropagation? More [DL2].
[R5] Reddit/ML, 2019. The 1997 LSTM paper by Hochreiter & Schmidhuber has become the most cited deep learning research paper of the 20th century.
[R6] Reddit/ML, 2019. DanNet, the CUDA CNN of Dan Ciresan in J. Schmidhuber's team, won 4 image recognition challenges prior to AlexNet.
[R7] Reddit/ML, 2019. J. Schmidhuber on Seppo Linnainmaa, inventor of backpropagation in 1970.
[Drop1] Hanson, S. J. (1990). A Stochastic Version of the Delta Rule, PHYSICA D,42, 265-272. [Dropout is a special case of the stochastic delta rule—compare preprint arXiv:1808.03578, 2018.]
[CMB] C. v. d. Malsburg (1973). Self-Organization of Orientation Sensitive Cells in the Striate Cortex. Kybernetik, 14:85-100, 1973. [See Table 1 for rectified linear units or ReLUs. Possibly this was also the first work on applying an EM algorithm to neural nets.]

 
Original. Reposted with permission.
Related:

5 Papers on CNNs Every Data Scientist Should Read
Introduction to Convolutional Neural Networks
Deep Learning in Neural Networks: An Overview"
https://www.kdnuggets.com/2021/07/kafka-open-source-data-pipeline-processing-real-time-data.html,How to Use Kafka Connect to Create an Open Source Data Pipeline for Processing Real-Time Data,"This article shows you how to create a real-time data pipeline using only pure open source technologies. These include Kafka Connect, Apache Kafka, Kibana and more.","comments
By Paul Brebner, Technology Evangelist at Instaclustr
Kafka Connect is a particularly powerful open source data streaming tool that makes it pretty darn painless to pair Kafka with other data technologies. As a distributed technology, Kafka Connect offers particularly high availability and elastic scaling independent of Kafka clusters. Using source or sink connectors to send data to and from Kafka topics, Kafka Connect enables integrations with multiple non-Kafka technologies with no code needed.

Robust open source Kafka connectors are available for many popular data technologies, as is the opportunity to write your own. This article walks through a real-world, real-data use case for how to use Kafka Connect to integrate real-time streaming data from Kafka with Elasticsearch (to enable the scalable search of indexed Kafka records) and Kibana (in order to visualize those results). 

For an interesting use case that highlights the advantages of Kafka and Kafka Connect, I was inspired by the CDC’s COVID-19 data tracker. The Kafka-enabled tracker collects real-time COVID testing data from multiple locations, in multiple formats and using multiple protocols, and processes those events into easily-consumable, visualized results. The tracker also has the necessary data governance to make sure results arrive quickly and can be trusted.
I began searching for a similarly complex and compelling use case – but ideally one less fraught than the pandemic. Eventually I came upon an interesting domain, one that included publicly available streaming REST APIs and rich data in a simple JSON format: lunar tides.
 
 
Lunar tide data
 
Tides follow the lunar day, a 24-hour-50-minute period during which the planet fully rotates to the same point beneath the orbiting moon. Each lunar day has two high tides and two low tides caused by the moon’s gravitational pull:

Source 1 National Oceanic and Atmospheric Administration
 
The National Oceanic and Atmospheric Administration (NOAA) provides a REST API that makes it easy to retrieve detailed sensor data from its global tidal stations. 

For example, the following REST call specifies the station ID, data type (I chose sea level) and datum (mean sea level), and requests the single more recent result in metric units: 

https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?date=latest&station=8724580&product=water_level&datum=msl&units=metric&time_zone=gmt&application=instaclustr&format=json


This call returns a JSON result with the latitude and longitude of the station, the time, and the water level value. Note that you must remember what your call was in order to understand the data type, datum, and units of the returned results!
 

{""metadata"": {
   ""id"":""8724580"",
   ""name"":""Key West"",
   ""lat"":""24.5508”,
   ""lon"":""-81.8081""},
 ""data"":[{
   ""t"":""2020-09-24 04:18"",
   ""v"":""0.597"",
      ""s"":""0.005"", ""f"":""1,0,0,0"", ""q"":""p""}]}


 
Starting the data pipeline (with a REST source connector)
 
To begin creating the Kafka Connect streaming data pipeline, we must first prepare a Kafka cluster and a Kafka Connect cluster. 

Next, we introduce a REST connector, such as this available open source one. We’ll deploy it to an AWS S3 bucket (use these instructions if needed). Then we’ll tell the Kafka Connect cluster to use the S3 bucket, sync it to be visible within the cluster, configure the connector, and finally get it running. This “BYOC” (Bring Your Own Connector) approach ensures that you have limitless options for finding a connector that meets your specific needs.

The following example demonstrates using a “curl” command to configure a 100% open source Kafka Connect deployment to use a REST API. Note that you’ll need to change the URL, name, and password to match your own deployment:

curl https://connectorClusterIP:8083/connectors -k -u name:password -X POST -H 'Content-Type: application/json' -d '

{
    ""name"": ""source_rest_tide_1"",
    ""config"": {
      ""key.converter"":""org.apache.kafka.connect.storage.StringConverter"",
      ""value.converter"":""org.apache.kafka.connect.storage.StringConverter"",
      ""connector.class"": ""com.tm.kafka.connect.rest.RestSourceConnector"",
      ""tasks.max"": ""1"",
      ""rest.source.poll.interval.ms"": ""600000"",
      ""rest.source.method"": ""GET"",
      ""rest.source.url"": ""https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?date=latest&station=8454000&product=water_level&datum=msl&units=metric&time_zone=gmt&application=instaclustr&format=json"",
      ""rest.source.headers"": ""Content-Type:application/json,Accept:application/json"",
      ""rest.source.topic.selector"": ""com.tm.kafka.connect.rest.selector.SimpleTopicSelector"",
      ""rest.source.destination.topics"": ""tides-topic""
    }
}


The connector task created by this code polls the REST API in 10-minute intervals, writing the result to the “tides-topic” Kafka topic. By randomly choosing five total tidal sensors to collect data this way, tidal data is now filling the tides topic via five configurations and five connectors.

 
 
Ending the pipeline (with an Elasticsearch sink connector)
 
To give this tide data somewhere to go, we’ll introduce an Elasticsearch cluster and Kibana at the end of the pipeline. We’ll configure an open source Elasticsearch sink connector to send Elasticsearch the data.

The following sample configuration uses the sink name, class, Elasticsearch index, and our Kafka topic. If an index doesn’t exist already, one with default mappings will be created. 

curl https://connectorClusterIP:8083/connectors -k -u name:password -X POST -H 'Content-Type: application/json' -d '
{
  ""name"" : ""elastic-sink-tides"",
  ""config"" :
  {
    ""connector.class"" : ""com.datamountaineer.streamreactor.connect.elastic7.ElasticSinkConnector"",
    ""tasks.max"" : 3,
    ""topics"" : ""tides"",
    ""connect.elastic.hosts"" : ”ip"",
    ""connect.elastic.port"" : 9201,
    ""connect.elastic.kcql"" : ""INSERT INTO tides-index SELECT * FROM tides-topic"",
    ""connect.elastic.use.http.username"" : ”elasticName"",
    ""connect.elastic.use.http.password"" : ”elasticPassword""
  }
}'


The pipeline is now operational. However, all tide data arriving in the Tides index is a string, due to the default index mappings.

Custom mapping is required to correctly graph our time series data. We’ll create this custom mapping for the tides-index below, using the JSON “t” field for the custom date, “v” as a double, and “name” as the keyword for aggregation:

curl -u elasticName:elasticPassword ”elasticURL:9201/tides-index""  -X PUT -H 'Content-Type: application/json' -d'
{
""mappings"" : {
  ""properties"" : {
     ""data"" : {
        ""properties"" : {
             ""t"" : { ""type"" : ""date"",
                     ""format"" : ""yyyy-MM-dd HH:mm""
             },
             ""v"" : { ""type"" : ""double"" },
             ""f"" : { ""type"" : ""text"" },
             ""q"" : { ""type"" : ""text"" },
             ""s"" : { ""type"" : ""text"" }
             }
       },
       ""metadata"" : {
          ""properties"" : {
             ""id"" : { ""type"" : ""text"" },
             ""lat"" : { ""type"" : ""text"" },
             ""long"" : { ""type"" : ""text"" },
             ""name"" : { ""type"" : ”keyword"" } }}}}         }'


Elasticsearch “reindexing” (deleting the index and reindexing all data) is typically required each time you change an Elasticsearch index mapping. Data can either be replayed from an existing Kafka sink connector, as we have in this use case, or sourced using the Elasticsearch reindex operation. 
 
 
Visualizing data with Kibana
 
To visualize the tide data, we’ll first create an index pattern in Kibana, with “t” configured as the timefilter field. We’ll then create a visualization, choosing a line graph type. Lastly, we’ll configure the graph settings such that the y-axis displays the average tide level over 30 minutes and the x-axis shows that data over time. 
The result is a graph of changes in the tides for the five sample stations that the pipeline collects data from:

 
Results
 
The periodic nature of tides is plain to see in our visualization, with two high tides occurring each lunar day.

More surprisingly, the range between high and low tides is different at each global station. This is due to the influences of not just the moon, but the sun, local geography, weather, and climate change. This example Kafka Connect pipeline utilizes Kafka, Elasticsearch and Kibana to helpfully demonstrate the power of visualizations: they can often reveal what raw data cannot!
 
Bio: Paul Brebner is the Technology Evangelist at Instaclustr, which provides a managed service platform of open source technologies such as Apache Cassandra, Apache Spark, OpenSearch, Redis, and Apache Kafka.
Related:

5 Python Data Processing Tips & Code Snippets
What’s ETL?
Date Processing and Feature Engineering in Python"
https://www.kdnuggets.com/2021/02/gilbert-people-skills-analytical-thinkers.html,People Skills for Analytical Thinkers,Research shows that people skills are becoming more important with the rise of AI. A great way to boost these skills is by reading the new book: People Skills for Analytical Thinkers.,"Your analytical skills are incredibly valuable – especially in your work with data.
However, rational thinking alone isn't enough.
Have you ever:

Told people about your idea, but then no one seemed to care?
Explained your analysis, leaving your colleague confused?
Worked hard on a data product, only to discover that the business never uses it?

You’re not alone. Many data professionals have this experience. Luckily, there is a way to prevent the frustration: people skills.
Research shows that people skills are becoming more important with the rise of AI.
A great way to boost these skills is by reading the new book of Gilbert Eijkelenboom: People Skills for Analytical Thinkers.

This is not just another book on communication: it’s written in an analytical language. Through data and algorithm metaphors, you’ll deepen your knowledge of human behavior. The writing style makes it easy to read.
In the book, you’ll learn: 

To better understand your own behavior
How to say ‘no’ without offending your colleagues
How to become persuasive and tell stories with data

Do you want people to USE your data products and ACT on your insights?
Then this book, filled with academic insights, exercises, and stories, is exactly what you need.
About the author
As a former professional poker player, you can find Gilbert Eijkelenboom wherever psychology and data meet. While Gilbert’s academic background is in Behavioral Science, he has built a career in Analytics consulting. Combining both worlds, Gilbert founded the company MindSpeaking: soft skills for data science & analytics.
He loves to help data professionals make a bigger impact with their data skills. Next to training, Gilbert enjoys writing. Last year, he published the bestselling book ""People Skills for Analytical Thinkers"" and his online content reaches more than 1,000,000 online views each year."
https://www.kdnuggets.com/2020/09/implementing-deep-learning-library-scratch-python.html,Implementing a Deep Learning Library from Scratch in Python,A beginner’s guide to understanding the fundamental building blocks of deep learning platforms.,"comments
By Parmeet Bhatia, Machine Learning Practitioner and Deep Learning Enthusiast
Deep Learning has evolved from simple neural networks to quite complex architectures in a short span of time. To support this rapid expansion, many different deep learning platforms and libraries are developed along the way. One of the primary goals for these libraries is to provide easy to use interfaces for building and training deep learning models, that would allow users to focus more on the tasks at hand. To achieve this, it may require to hide core implementation units behind several abstraction layers that make it difficult to understand basic underlying principles on which deep learning libraries are based. Hence the goal of this article is to provide insights on building blocks of deep learning library. We first go through some background on Deep Learning to understand functional requirements and then walk through a simple yet complete library in python using NumPy that is capable of end-to-end training of neural network models (of very simple types). Along the way, we will learn various components of a deep learning framework. The library is just under 100 lines of code and hence should be fairly easy to follow. The complete source code can be found at https://github.com/parmeet/dll_numpy
 
Background
 
Typically a deep learning computation library (like TensorFlow and PyTorch) consists of components shown in the figure below.


Components of Deep Learning Framework

 
Operators
Also used interchangeably with layers, they are the basic building blocks of any neural network. Operators are vector-valued functions that transform the data. Some commonly used operators are layers like linear, convolution, and pooling, and activation functions like ReLU and Sigmoid.
 
Optimizers
They are the backbones of any deep learning library. They provide the necessary recipe to update model parameters using their gradients with respect to the optimization objective. Some well-known optimizers are SGD, RMSProp, and Adam.
 
Loss Functions
They are closed-form and differentiable mathematical expressions that are used as surrogates for the optimization objective of the problem at hand. For example, cross-entropy loss and Hinge loss are commonly used loss functions for the classification tasks.
 
Initializers
They provide the initial values for the model parameters at the start of training. Initialization plays an important role in training deep neural networks, as bad parameter initialization can lead to slow or no convergence. There are many ways one can initialize the network weights like small random weights drawn from the normal distribution. You may have a look at https://keras.io/initializers/ for a comprehensive list.
 
Regularizers
They provide the necessary control mechanism to avoid overfitting and promote generalization. One can regulate overfitting either through explicit or implicit measures. Explicit methods impose structural constraints on the weights, for example, minimization of their L1-Norm and L2-Norm that make the weights sparser and uniform respectively. Implicit measures are specialized operators that do the transformation of intermediate representations, either through explicit normalization, for example, BatchNorm, or by changing the network connectivity, for example, DropOut and DropConnect.

The above-mentioned components basically belong to the front-end part of the library. By front-end, I mean the components that are exposed to the user for them to efficiently design neural network architectures. On the back-end side, these libraries provide support for automatically calculating gradients of the loss function with respect to various parameters in the model. This technique is commonly referred to as Automatic Differentiation (AD).

 
Automatic Differentiation (AD)
Every deep learning library provides a flavor of AD so that a user can focus on defining the model structure (computation graph)and delegate the task of gradients computation to the AD module. Let us go through an example to see how it works. Say we want to calculate partial derivatives of the following function with respect to its input variables X₁ and X₂:

Y = sin(x₁)+X₁*X₂


The following figure, which I have borrowed from https://en.wikipedia.org/wiki/Automatic_differentiation, shows it’s computation graph and calculation of derivatives via chain-rule.


Computation graph and calculation of derivatives via chain-rule

 
What you see in the above figure is a flavor of reverse-mode automatic differentiation (AD). The well known Back-propagation algorithm is a special case of the above algorithm where the function at the top is loss function. AD exploits the fact that every composite function consists of elementary arithmetic operations and elementary functions, and hence the derivatives can be computed by recursively applying the chain-rule to these operations.
 
Implementation
 
In the previous section, we have gone through all the necessary components to come up with our first deep learning library that can do end-to-end training. To keep things simple, I will mimic the design pattern of the Caffe Library. Here we define two abstract classes: A “Function” class and an “Optimizer” class. In addition, there is a “Tensor” class which is a simple structure containing two NumPy multi-dimensional arrays, one for holding the value of parameters and another for holding their gradients. All the parameters in various layers/operators will be of type “Tensor”. Before we dig deeper, the following figure provides a high-level overview of the library.


UML diagram of Library

 
At the time of this writing, the library comes with the implementation of the linear layer, ReLU activation, and SoftMaxLoss Layer along with the SGD optimizer. Hence the library can be used to train a classification model comprising of fully connected layers and ReLU non-linearity. Lets now go through some details of the two abstract classes we have.
The “Function” abstract class provides an interface for operators and is defined as follows:

Abstract Function class
 
All the operators are implemented by inheriting the “Function” abstract class. Each operator must provide an implementation of forward(…) and backward(…) methods and optionally implement getParams function to provide access to its parameters (if any). The forward(…) method receives the input and returns its transformation by the operator. It will also do any house-keeping necessary to compute the gradients. The backward(…) method receives partial derivatives of the loss function with respect to the operator’s output and implements the partial derivatives of loss with respect to the operator’s input and parameters (if there are any). Note that backward(…) function essentially provides the capability for our library to perform automatic differentiation.
To make things concrete let’s look at the implementation of the Linear function as shown in the following code snippet:

Implementation of Linear function
 
The forward(…) function implements the transformation of the form Y = X*W+b and returns it. It also stores the input X as this is needed to compute the gradients of W in the backward function. The backward(…) function receives partial derivatives dY of loss with respect to the output Y and implements the partial derivatives with respect to input X and parameters W and b. Furthermore, it returns the partial derivatives with respect to the input X, that will be passed on to the previous layer.
The abstract “Optimizer” class provides an interface for optimizers and is defined as follows:

Abstract Optimizer class
 
All the optimizers are implemented by inheriting the “Optimizer” base class. The concrete optimization class must provide the implementation for the step() function. This method updates the model parameters using their partial derivatives with respect to the loss we are optimizing. The reference to various model parameters is provided in the __init__(…) function. Note that the common functionality of resetting gradients is implemented in the base class itself.
To make things concrete, let’s look at the implementation of stochastic gradient descent (SGD) with momentum and weight decay.

 
Getting to the real stuff
 
To this end, we have all the ingredients to train a (deep) neural network model using our library. To do so, we would need the following:

Model: This is our computation graph
Data and Target: This is our training data
Loss Function: Surrogate for our optimization objective
Optimizer: To update model parameters

The following pseudo-code depicts a typical training cycle:

model #computation graph
data,target #training data
loss_fn #optimization objective
optim #optimizer to update model parameters to minimize lossRepeat:#until convergence or for predefined number of epochs
   optim.zeroGrad() #set all gradients  to zero
   output = model.forward(data) #get output from  model
   loss   = loss_fn(output,target) #calculate loss
   grad   = loss.backward() #calculate gradient of loss w.r.t output
   model.backward(grad) #calculate gradients for all the parameters
   optim.step() #update model parameters


Though not a necessary ingredient for a deep learning library, it may be a good idea to encapsulate the above functionality in a class so that we don’t have to repeat ourselves every time we need to train a new model (this is in line with the philosophy of higher-level abstraction frameworks like Keras). To achieve this, let’s define a class “Model” as shown in the following code snippet:

This class serves the following functionalities:

Computation Graph: Through add(…) function, one can define a sequential model. Internally, the class will simply store all the operators in a list named computation_graph.
Parameter Initialization: The class will automatically initialize model parameters with small random values drawn from uniform distribution at the start of training.
Model Training: Through fit(…) function, the class provides a common interface to train the models. This function requires the training data, optimizer, and the loss function.
Model Inference: Through predict(…) function, the class provides a common interface for making predictions using the trained model.

Since this class does not serve as a fundamental building block for deep learning, I implemented it in a separate module called utilities.py. Note that the fit(…) function makes use of DataGenerator Class whose implementation is also provided in the utilities.py module. This class is just a wrapper around our training data and generate mini-batches for each training iteration.
 
Training our first model
 
Let’s now go through the final piece of code that trains a neural network model using the proposed library. Inspired by the blog-post of Andrej Karapathy, I am going to train a hidden layer neural network model on spiral data. The code for generating the data and it’s visualization is available in the utilities.py file.


Spiral data with three classes

 
A three-class spiral data is shown in the above figure. The data is non-linearly separable. So we hope that our one hidden layer neural network can learn the non-linear decision boundary. Bringing it all together, the following code snippet will train our model.

End to end code for training a neural network model
 
The following figure shows the same spiral data together with the decision boundaries of the trained model.


Spiral data with the corresponding decision boundaries of the trained model

 
Concluding remarks
 
With the ever-increasing complexity of deep learning models, the libraries tend to grow at exponential rates both in terms of functionalities and their underlying implementation. That said, the very core functionalities can still be implemented in a relatively small number of lines of code. Although the library can be used to train end-to-end neural network models (of very simple types), it is still restricted in many ways that make deep learning frameworks usable in various domains including (but not limited to) vision, speech, and text. With that said, I think this is also an opportunity to fork the base implementation and add missing functionalities to get your hands-on experience. Some of the things you can try to implement are:

Operators: Convolution Pooling etc.
Optimizers: Adam RMSProp etc.
Regularizers: BatchNorm DropOut etc.

I hope this article gives you a glimpse of what happens under the hood when you use any deep learning library to train your models. Thank you for your attention and I look forward to your comments or any questions in the comment section.
 
Bio: Parmeet Bhatia is a Machine learning practitioner and deep learning enthusiast. He is an experienced Machine Learning Engineer and R&D professional with a demonstrated history of developing and productization of ML and data-driven products. He is highly passionate about building end-to-end intelligent systems at scale.
Original. Reposted with permission.
Related:

Autograd: The Best Machine Learning Library You’re Not Using?
10 Things You Didn’t Know About Scikit-Learn
Deep Learning for Signal Processing: What You Need to Know"
https://www.kdnuggets.com/2020/11/top-stories-2020-oct.html,Top October Stories: Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science; fastcore: An Underrated Python Library,Also: Goodhart's Law for Data Science and what happens when a measure becomes a target? How to become a Data Scientist: a step-by-step guide; 10 Best Machine Learning Courses in 2020.,"October was a record month for KDnuggets, with an all-time high number of visitors, who had an abundance of great content to choose.  Here are the most popular KDnuggets posts in October.

Most Viewed - Platinum Badge (>24,000 UPV)

Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science, by Benjamin Obi Tayo (*)
fastcore: An Underrated Python Library, by Hamel Husain (*)
Goodhart's Law for Data Science and what happens when a measure becomes a target?, by Jamil Mirabito (*)


Most Viewed - Gold Badge (>12,000 UPV)

How to become a Data Scientist: a step-by-step guide, by Great Learning
 PerceptiLabs - A GUI and Visual API for TensorFlow, by PerceptiLabs (*)
 A step-by-step guide for creating an authentic data science portfolio project, by Felix Vemmer
 How to Explain Key Machine Learning Algorithms at an Interview, by Terence Shin


Most Viewed - Silver Badge (> 6,000 UPV)

 Text Mining with R: The Free eBook, by Matthew Mayo (*)
 The unspoken difference between junior and senior data scientists, by Misra Turp (*)
 Roadmap to Natural Language Processing (NLP), by Pier Paolo Ippolito
 How to ace the data science coding challenge, by Benjamin Obi Tayo (*)
 Ain't No Such a Thing as a Citizen Data Scientist, by Venkat Raman (*)
 10 Best Machine Learning Courses in 2020, by Ahmad Bin Shafiq
 Good-bye Big Data. Hello, Massive Data!, by Sqream (*)
 Free From MIT: Intro to Computational Thinking and Data Science, by Matthew Mayo




Most Viewed - Platinum Badge (>1,000 shares)

Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science, by Benjamin Obi Tayo


Most Shared - Gold Badge (>500 shares)

 10 Best Machine Learning Courses in 2020, by Ahmad Bin Shafiq
How to become a Data Scientist: a step-by-step guide, by Great Learning
 How to Explain Key Machine Learning Algorithms at an Interview, by Terence Shin


Most Shared - Silver Badge (>250 shares)

 Free From MIT: Intro to Computational Thinking and Data Science, by Matthew Mayo
 How LinkedIn Uses Machine Learning in its Recruiter Recommendation Systems, by Jesus Rodriguez (*)
 Software Engineering Tips and Best Practices for Data Science, by Ahmed Besbes (*)
 An Introduction to AI, updated, by Imtiaz Adam (*)
 fastcore: An Underrated Python Library, by Hamel Husain
 Free Introductory Machine Learning Course From Amazon, by Matthew Mayo (*)
 A step-by-step guide for creating an authentic data science portfolio project, by Felix Vemmer
 Roadmap to Natural Language Processing (NLP), by Pier Paolo Ippolito
 Annotated Machine Learning Research Papers, by Matthew Mayo (*)


(*) indicates that badge added or upgraded based on these monthly results.

Most Shareable (Viral) Blogs
Among the top blogs, here are the blogs with the highest ratio of shares/unique views, which suggests that people who read it really liked it. 

 An Introduction to AI, updated, by Imtiaz Adam
 How LinkedIn Uses Machine Learning in its Recruiter Recommendation Systems, by Jesus Rodriguez
 5 Best Practices for Putting Machine Learning Models Into Production, by Sigmoid
 Annotated Machine Learning Research Papers, by Matthew Mayo
 Free Introductory Machine Learning Course From Amazon, by Matthew Mayo"
https://www.kdnuggets.com/2021/07/sas-building-tech-skills.html,Building Tech Skills in 2021,"With all the workforce changes last year, it is not surprising that employees lack the skills to meet new demands. To be ready for today’s challenges, companies need sound methods to assess what skills their employees have, the ability to identify the gaps, and a plan to upskill them for success. You can read the survey results here, along with predicted learning and development trends, and insights for upskilling, cross-skilling and reskilling your workforce.","Sponsored Post.

With new technologies come new ways of doing business – and new skill sets. Should you hire new talent or train existing teams? SAS worked with HR Dive and Coursera on surveys that explore how managers are approaching upskilling, reskilling and cross-skilling – and what employees say they need – to build tech skills in 2021. 
With all the workforce changes last year, it is not surprising that employees lack the skills to meet new demands. Many short-term development plans are becoming irrelevant. In fact, almost nine out of 10 managers surveyed said their employee development plans needed to change to reflect the skills gaps that organizations face today. Here’s what they found:

88% of managers said they believed their employee development plans needed to change in 2021.
50% of managers said employees needed more upskilling, reskilling and cross-skilling, and 41% said they themselves needed those same opportunities.
32% of managers said that when considering the types of skills employees should focus on increasing, employees needed more technical skills and soft skills, and 20% of managers said that they themselves could use more technical skills.

Despite these challenges, managers and employees agree on many things when it comes to building new skills, including what to learn and how to learn it. In fact, both survey groups ranked technical skills, including data science, AI, machine learning, programming and advanced analytics, as most important.
To be ready for today’s challenges, companies need sound methods to assess what skills their employees have, the ability to identify the gaps, and a plan to upskill them for success. They also need to offer ongoing learning and skills development opportunities to recruit new, top-performing employees. Do you feel ready? You can read the survey results here, along with predicted learning and development trends, and insights for upskilling, cross-skilling and reskilling your workforce."
https://www.kdnuggets.com/2021/05/ai-books-read-2021.html,AI Books you should read in 2021,"As of late, every year seems to be a ""break-out"" year for AI. So, it's time for you to get ready for the future in the age of automation. This collection of books will help you prepare for the many opportunities to come, many of which may not have yet been imagined.","comments
By Przemek Chojecki, CEO Contentyze.
2020 was the year of remote work and growing automation. We learned to work from home, and many companies went completely digital. Artificial Intelligence got a huge boost last year — more data to learn on, more processes to automate and optimize. It seems 2021 and beyond will hold even more opportunities for the applications of AI. Let’s prepare for this bright new future with a list of books.

Best Artificial Intelligence books to read in 2021. Prepare for the future of automation.
 
Sociological AI books
 
The most pressing issue is how AI will influence us as a human race and what future it will bring us. Elon Musk often warns about the policies — we have to make our policies right —to make sure that we will have a sustainable development of Artificial Intelligence rather than bad experiments bringing catastrophic consequences and deepening inequalities.

Homo Deus: A Brief History of Tomorrow is now a classic book about how technology, Artificial Intelligence, in particular, impacts societies. The author presents a historical perspective and discusses ramifications for our present and future. A must-read!

The Singularity is Near popularized the term ‘Singularity’, a moment when artificial intelligence algorithms will be as intelligent as humans on average across all disciplines (think Alexa on steroids).
Ray Kurzweil, the author, is a well-known futurist who lives by his own principles, trying to live to the singularity moment around 2040. Enter the world of the Singularity and what it will change in how we operate as humans.
 
Philosophical AI books
 
Looking into the future, we can see the Singularity, a moment when Artificial Intelligence capabilities will surpass those of humans on average across all domains. This will be a moment of profound change, and though it might never come (or it’ll be here by 2040), it’s worth thinking from this future perspective about what we are doing right now, especially who we are as humans and where we’re going.

Superintelligence is the classic reference for thinking about Artificial Intelligence and Artificial General Intelligence. Nick Bostrom analyses different levels of AI, arriving at the concept of AGI, an AI at the level and beyond general human intelligence, able to cope with any creative task. A dense book, but worth it!

Life 3.0 is another take on Artificial General Intelligence and what it might mean for humans if AI were much more intelligent than us. It has really great examples and starts with a fascinating story of how AGI might enter the world and allow one company to dominate. A vivid narrative!
 
Business-oriented AI books
 
Artificial Intelligence wouldn’t be that popular if it hadn’t found so many applications across so many sectors in the business world, now more than ever. Using AI in the form of machine learning is ubiquitous. Here’s a very practical list of business books focused on Artificial Intelligence.

AI Superpowers is a book on how China jumped on the AI wagon and started innovating on a massive scale. Kai-Fu Lee demonstrates the differences between Silicon Valley and the Chinese start-up ecosystem. Great lessons for anyone interested in entrepreneurship as well as what it takes to build AI start-ups.

Artificial Intelligence Business is a concise guide to Artificial Intelligence for business people and commercially oriented data scientists. It goes through all the most popular applications across branches like finance, logistics, entertainment, and more. You’ll find many examples of companies using AI to their benefit, as well as methods for implementing AI in your organization.

Applied Artificial Intelligence is designed with business leaders in mind. It gives you a framework to think about Machine Learning, Deep Learning, AGI, ANI, and shows multiple use-cases in today's business world. Interesting for people interested in policy-making.
 
Original. Reposted with permission.
 
Related:

Data Science Books You Should Start Reading in 2021
10 Best Machine Learning Textbooks that All Data Scientists Should Read
Artificial Intelligence Books to Read in 2020"
https://www.kdnuggets.com/2020/11/best-data-science-certification-never-heard.html,The Best Data Science Certification You’ve Never Heard Of,"The CDMP is the best data strategy certification you’ve never heard of. (And honestly, when you consider the fact that you’re probably working a job that didn’t exist ten years ago, it’s not surprising that this certification isn’t widespread just yet.)","By Nicole Janeway Bills, Data Scientist at Atlas Research.
comments


Swimming upstream to address data quality issues. Photo by alleksana on Pexels.

 
**Update 10/12: I’m now recognized as a CDMP Associate after passing the Fundamentals Exam. Questions for me? Drop them in the comments or join the study group.**
**Update 8/15: it’s recently come to my attention that the certification exams are open book, which is extremely exciting because it means less time memorizing and more time working with data in a real world setting. Also, I am starting a study group on Facebook — join for help with your exam prep.**
Eight years ago, data science was proclaimed “the sexiest job of the 21st century.” Yet plodding through hours of data munging still feels decidedly unsexy. If anything, the storied rise of the data science career has illustrated just how poorly most organizations are doing when it comes to managing their data.
Enter the Certified Data Management Professional (CDMP) from Data Management Association International (DAMA). The CDMP is the best data strategy certification you’ve never heard of. (And honestly, when you consider the fact that you’re probably working a job that didn’t exist ten years ago, it’s not surprising that this certification isn’t widespread just yet.)
Data strategy is a crucial discipline that spans end-to-end management of the data lifecycle as well as associated aspects of data governance and key considerations of data ethics.
This article outlines the hows and whys of getting the CDMP, which lays the groundwork for effective thought leadership on data strategy. It also includes a survey — you can offer your thoughts on the most important aspects of data management for data science and check out the consensus of the community.
In this guide:

About the CDMP Exam
How to prepare for CDMP
What’s tested on the CDMP
Survey —most important aspect of data management
Why data scientists should get CDMP certified

Disclaimer: this post is not sponsored by DAMA International — views reflected are mine alone. I’m including an affiliate link to the DMBOK on Amazon, the reference guide that is required for the exam, given that it’s an open book test. Buying the exam through this link helps support my writing on Data Science and Data Strategy — thanks in advance.
 
About the CDMP Exam
 
Training for the CDMP confers expertise across 14 areas related to data strategy (which I’ll cover in more detail in a later section). The test is open book, but the 100 questions on the exam must be completed within 90 minutes — not a lot of time to be looking things up. Therefore, it’s important to be extremely familiar with the reference material.
When you schedule the exam ($300), DAMA provides 40 practice questions that are pretty reflective of the difficulty of the actual exam. As a further resource, check out this article about the process of studying for a certification.
It’s possible to sit for the exam online while monitored via webcam ($11 proctoring fee). The format of the exam is multiple choice — choose the single correct option out of five. You can mark questions and come back to them. At the conclusion of test taking, you get immediate feedback on your score.
Anything over 60% is considered passing. This is just fine if you’re interested in getting your CDMP Associate certification and moving along. If you’re interested in the advanced tiers of CDMP certification, you’ll have to pass with a 70% (CDMP Practitioner) or 80% (CDMP Master). To get certified at the highest level, CDMP Fellow, you’ll need to attain the Master Certification and also demonstrate industry experience and contribution to the field. Each of these advanced certifications also require passing two Specialist exams.
This brings me to my final point, which is about why — purely from a career advancement standpoint — you should chose to put yourself through the studying and exam taking process for CDMP: certification from DAMA is associated with high-end positions in leadership, management, and data architecture. (Think of CDMP as getting credentialed into a semi-secret society of data ninjas.) Increasingly, enterprise roles and federal contracts related to data management are requesting CDMP certification. Read more.


via CDMP

 
Pros:

Provides well-rounded knowledge base on topics related to data strategy
Open book test means less time spent on route memorization
Four tiers for different levels of data management professionals
60% score requirement to pass lowest level of certification
Associated with elite roles
Provides 3 year membership to DAMA International
$311 exam fee is cheaper than other data-related certifications from Microsoft and The Open Group

Cons:

DAMA is not backed by a major tech company (e.g. Amazon, Google, Microsoft) that is actively pushing marketing efforts and driving brand recognition for CDMP certification — this means that CDMP is likely to be recognized as valuable mainly among individuals who are already familiar with data management
$311 exam fee is relatively expensive compared to AWS Cloud Practitioner cert ($100) or GPC certs ($200)

Alternatives:

Microsoft Certified Solutions Associate (MCSA) — modularized certifications focusing on various Microsoft products ($330+)
Microsoft Certified Solutions Expert (MCSE) — builds on the MCSA with integrated certifications on topics such as Core Infrastructure, Data Management & Analytics, and Productivity ($495+)
The Open Group Architecture Framework (TOGAF) —various levels of certification on high-level framework for software development and enterprise architecture methodology ($550+)
Scaled Agile Framework (SAFe) — role-based certifications for software engineering teams ($995)

 
How to prepare for CDMP
 
Given that CDMP is an open book test, to study for the exam, all that’s needed is the DAMA Body of Knowledge book (DMBOK $55). It’s around 600 pages, but if you mainly focus your study time on Chapter 1 (Data Management), diagrams & schemas, roles & responsibilities, and definitions, then this should get you 80% of the way toward a passing score.
In terms of how to use DMBOK, one test taker recommended 4–6 hours per weekend for 8–10 weeks. Another approach could be reading a couple pages each morning and evening. However you tackle it, make sure you’re incorporating spaced repetition into your studying methodology.
In addition to being your study guide for the exam, the DMBOK is of course useful as reference book, and you can drop it on your colleague’s desk if they need to learn data strategy or if they’ve nodded off during a webinar.
 
What’s tested on the CDMP
 
The CDMP covers 14 topics —I’ve listed them in order of the prevalence with which they occur on the exam and provided a brief definition for each.
Data Governance ( 11%) — practices and processes to ensure formal management of data assets. Read more.
Data Quality ( 11%) — assuring data is fit for consumption based on its accuracy, completeness, consistency, integrity, reasonability, timeliness, uniqueness/deduplication, validity, and accessibility. Read more.
Data Modelling and Design ( 11%) — translation of business needs into technical specifications. Read more.
Metadata Management (11%) — information about data collected. Read more.
Master and Reference Data Management (10%) — reference data is information used to categorize other data found in a database, or information that is solely for relating data in a database to information beyond the boundaries of the organization. Master reference data refers to information that is shared across a number of systems within the organization. Read more.
Data Warehousing and Business Intelligence (10%) — a data warehouse stores information from operational systems (as well as other data resources, potentially) in a way that is optimized to support decision-making processes. Business intelligence refers to the use of technology to gather and analyze data, then translate it into useful information. Read more.
Document and Content Management (6%) — technologies, methods, and tools used to organize and store an organization’s documents. Read more.
Data Integration and Interoperability ( 6%) — use of technical and business processes to merge data from different sources, with the goal of readily and efficiently providing access to valuable information. Read more.
Data Architecture (6%) — specifications to describe existing state, define data requirements, guide data integration, and control data assets, according to the organization’s data strategy. Read more.
Data Security ( 6%) — implementation of policies and procedures to ensure people and things take the right actions with data and information assets, even in the presence of malicious inputs. Read more.
Data Storage and Operations ( 6%) — characterization of hardware or software that holds, deletes, backs up, organizes, and secures an organization’s information. Read more.
Data Management Process ( 2%) — end-to-end management of data, including collection, control, protection, delivery, and enhancement. Read more.
Big Data ( 2%) — extremely large datasets, often composed of various structured, unstructured, and semi-structured data types. Read more.
Data Ethics ( 2%) — code of conduct encompassing data handling, algorithms, and other practices to ensure that data is used appropriately in a moral context. Read more.
 
Why data scientists should get CDMP certified
 
Still not convinced why data strategy is important? Let’s take a look from the perspective of a data scientist aiming to increase their knowledge and earning potential.


Photo by Franki Chamaki on Unsplash. The signage is a trademark of Hivery, a company that leverages AI for the retail industry.

 
It’s been said that a data scientist sits at the nexus of statistics, computer science, and domain knowledge. Why would you want to add one more thing to your plate?

Successwise, you’re better off being good at two complementary skills than being excellent at one

Scott Adams, author and creator of the Dilbert comics, offers the idea that “every skill you acquire doubles your odds of success.” He acknowledges this may be somewhat of an oversimplification — “obviously some skills are more valuable than others, and the twelfth skill you acquire might have less value than each of the first eleven” — but the point is that sometimes it’s better to go wide than to go deep.
Setting aside the relative magnitude of the benefit (because I seriously doubt it’s 2x per skill… thank you, law of diminishing marginal returns), it seems unquestionable that broadening your skillset can lead to more significant gains relative to toiling away at learning one specific skills. In a nutshell, this is why I think it’s important for a data scientist to learn data strategy.
Generally speaking, having diversity in your skillset allows you to:

Problem solve more effectively by drawing on cross-disciplinary learnings
Communicate better with your teammates from other specialties
Get your foot in the door in terms of gaining access to new projects

Understanding data strategy transforms you from being a data consumer into an empowered data advocate at your organization. It’s worth putting up with all the tongue twister acronyms (DMBOK — really? Couldn’t they have just called it The Data Management Book?) in order to deepen your appreciation for the end-to-end knowledge generating process.
 
Other articles to diversify your skills
 
Using Java to Fix Your Data Science Problems
Get speedy access to quality data by understanding the basics of this widely-used programming language.
 
Comprehensive Guide to the Data Warehouse
Learn about the role of the data warehouse as the master store of analysis-ready datasets.
 
How to Ace the AWS Cloud Practitioner Certification with Minimal Effort
Forecast: cloudy with a 100% chance of passing on your first try.
 
If you enjoyed reading this article, follow me on Medium, LinkedIn, and Twitter for more ideas to advance your data science skills. Join the study group for the CDMP Exam. Buy the DMBOK.
 
Bio: Nicole Janeway Bills is Data Scientist with experience in commercial and federal consulting. She helps organizations leverage their top asset: a simple and robust Data Strategy. Sign up for more of her writing.
Original. Reposted with permission.
Related:

10 Underrated Python Skills
6 Lessons Learned in 6 Months as a Data Scientist
5 Must-Read Data Science Papers (and How to Use Them)"
https://www.kdnuggets.com/2020/09/potential-predictive-analytics-labor-industries.html,The Potential of Predictive Analytics in Labor Industries,Predictive analytics isn't just for white-collar work. Check out these five examples that show its potential in blue-collar jobs and industries as well.,"By Devin Partida, Editor-in-Chief of ReHack.com.
comments


Photo by Laurel and Michael Evans on Unsplash

 
People often think of predictive analytics tools as primarily resourceful for white-collar work. However, these five examples show that it offers plenty of potential in blue-collar jobs, too.
 
1. Auto Mechanics Get Diagnostic Help
 
Most professionals who address problems with cars only see those vehicles once someone notices symptoms. By that time, the issue may be so extensive that it costs the vehicle owner hundreds or thousands of dollars to fix, plus increases the manual labor required by the mechanic.
There's been a more recent move towards getting diagnostic information throughout the whole lifecycle of a vehicle. That approach lets technicians see problems earlier, often before they cause symptoms. With the help of data processing in the cloud and wireless networks, mechanics can get real-time status updates for individual vehicles or entire fleets and become proactive about keeping them at peak performance.
Knowing about a problem before a vehicle arrives in a shop also aids in better planning. For example, if a technician feels nearly certain about the cause of an issue, they could order a part and have it ready to install before ever seeing the vehicle.
 
2. Facility Maintenance Crews View Valuable Details
 
Predictive maintenance also assists the teams of people who oversee building maintenance. Facility managers often connect sensors on equipment used for building climate control. They can then see notifications about faulty components or other problems before breakdowns occur. People can collect more than 350,000 data points annually by analyzing 10 metrics every 15 minutes.
Moreover, this use of data science can show users the likely effects of specific tweaks before they make them. For example, if a building maintenance manager wants to focus on energy savings, a dashboard could reveal what changes would give the biggest payoffs. That knowledge removes costly and potentially frustrating guesswork.
Many customers know the importance of getting regular maintenance on their heating and air conditioning equipment. However, problems can still occur between appointments. Applying advanced predictions to the equation reduces the chances of service disruptions in commercial buildings.
 
3. Predictive Analytics Keep Truck Drivers Safer
 
Along with the earlier discussion about relying on analytics to spot automotive malfunctions before they happen, people who manage or operate commercial fleets have another compelling reason to invest in predictive analytics. They can increase driver safety by predicting which employees are most likely to have accidents. The tools can also highlight the drivers with above-average safe driving practices.
Many of today's solutions that utilize data science examine factors both within and outside of a driver's control. For example, a tool may measure a person's driving habits, as well as environmental elements such as traffic and weather. This approach to risk management allows supervisors to intervene and provide coaching before mishaps occur.
People interested in using analytics this way have dozens of possible metrics to track. However, GPS tracking is especially advantageous in this use case. It allows showing the time spent in particular locations. If a driver takes a break or needs to stop and pick up a delivery, managers can see how long those pauses take. Similarly, they can monitor if drivers spend too much time in traffic and need to reroute.
 
4. Data Science Brings Increased Visibility to Construction
 
Construction projects can be difficult to complete on time and under budget. However, predictive analytics could give an unprecedented level of transparency to such endeavors at all stages. Applying data science to the preconstruction phase helps managers understand where costs could get out of control or which aspects might take longer than expected to complete.
Analyzing data at later stages also helps project managers take corrective action to keep things on track before problems happen. This capability makes it easy to keep clients pleased and in the loop, too.
Making the most of analytics in construction requires taking the information out of silos. People also need to ensure that the imported data is in the right format for the tool they want to use. Taking care of those foundational necessities will help them have better overall outcomes.
 
5. Warehouse Management Improves With Predictive Analytics
 
People are also increasingly interested in using data-based predictions to enhance warehouse management. For example, a dashboard could calculate the possible number of hours saved if a company improves a process or reduces the distance a worker must travel when transporting goods around the facility.
An advanced tool could also determine how a facility's labor needs will change as seasonal demands fluctuate. Knowing those details helps company leaders know when to hire more team members to avoid overstretching the current workforce.
High-tech analytics platforms play a crucial role in injury reduction, too. If a tool examines historical accident data, it could help decision-makers choose what to change so that people stay healthy and able to give their full contributions during a day at work.
 
Intriguing Reasons to Use Predictive Analytics
 
These five use cases illustrate why people should not immediately assume that there is no place for data science in blue-collar work. Analyzing data in the right ways can improve effectiveness, productivity and safety, among other desirable aims.
 
Bio: Devin Partida is a big data and technology writer, as well as the Editor-in-Chief of ReHack.com.
Related:

Top 8 Data Science Use Cases in Construction
Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions
How Data Science Is Keeping People Safe During COVID-19"
https://www.kdnuggets.com/2021/04/gradient-boosted-trees-conceptual-explanation.html,Gradient Boosted Decision Trees – A Conceptual Explanation,"Gradient boosted decision trees involves implementing several models and aggregating their results. These boosted models have become popular thanks to their performance in machine learning competitions on Kaggle. In this article, we’ll see what gradient boosted decision trees are all about.","By Derrick Mwiti, Data Scientist.
comments
Gradient boosted decision trees have proven to outperform other models. It’s because boosting involves implementing several models and aggregating their results.
Gradient boosted models have recently become popular thanks to their performance in machine learning competitions on Kaggle.
In this article, we’ll see what gradient boosted decision trees are all about.
 
Gradient boosting
 
In gradient boosting, an ensemble of weak learners is used to improve the performance of a machine learning model. The weak learners are usually decision trees. Combined, their output results in better models.
In case of regression, the final result is generated from the average of all weak learners. With classification, the final result can be computed as the class with the majority of votes from weak learners.
In gradient boosting, weak learners work sequentially. Each model tries to improve on the error from the previous model. This is different from the bagging technique, where several models are fitted on subsets of the data in a parallel manner. These subsets are usually drawn randomly with replacement. A great example of bagging is in Random Forests®.
The boosting process looks like this:

Build an initial model with the data,
Run predictions on the whole data set,
Calculate the error using the predictions and the actual values,
Assign more weight to the incorrect predictions,
Create another model that attempts to fix errors from the last model,
Run predictions on the entire dataset with the new model,
Create several models with each model aiming at correcting the errors generated by the previous one,
Obtain the final model by weighting the mean of all the models.

 
Boosting algorithms in machine learning
 
Let’s take a look at boosting algorithms in machine learning.
 
AdaBoost
 
AdaBoost fits a sequence of weak learners to the data. It then assigns more weight to incorrect predictions, and less weight to correct ones. This way the algorithm focuses more on observations that are harder to predict. The final result is obtained from the majority vote in classification, or the average in regression.
You can implement this algorithm using Scikit-learn. The `n_estimators` argument can be passed to it to indicate the number of weak learners needed. You can control the contribution of each weak learner using the `learning_rate` argument.
The algorithm uses decision trees as the base estimators by default. The base estimators and the parameters of the decision trees can be tuned to improve the performance of the model. By default, decision trees in AdaBoost have a single split.
Classification using AdaBoost
You can use the `AdaBoostClassifier` from Scikit-learn to implement the AdaBoost model for classification problems. As you can see below, the parameters of the base estimator can be tuned to your preference. The classifier also accepts the number of estimators you want. This is the number of decision trees you need for the model.

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
base_estimator=DecisionTreeClassifier(max_depth=1,criterion='gini', splitter='best', min_samples_split=2)
model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=100)
model.fit(X_train, y_train)


Regression using AdaBoost
Applying AdaBoost to regression problems is similar to the classification process, with just a few cosmetic changes. First, you have to import the `AdaBoostRegressor`. Then, for the base estimator, you can use the `DecisionTreeRegressor`. Just like the previous one, you can tune the parameters of the decision tree regressor.

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
base_estimator = DecisionTreeRegressor(max_depth=1, splitter='best', min_samples_split=2)
model = AdaBoostRegressor(base_estimator=base_estimator,n_estimators=100)
model.fit(X_train, y_train)


 
Scikit-learn gradient boosting estimator
 
Gradient boosting is different from AdaBoost, because the loss function optimization is done via gradient descent. Like AdaBoost, it also uses decision trees as weak learners. It also sequentially fits the trees. When adding subsequent trees, loss is minimized using gradient descent.
In the Scikit-learn implementation, you can specify the number of trees. This is a parameter that should be looked at keenly, because specifying too many trees can lead to overfitting. On the other hand, specifying a very small number of trees can lead to underfitting.
The algorithm lets you specify the learning rate. This dictates how fast the model will learn. A low learning rate will often require more trees in the model. This means more training time.
Let’s now take a look at the implementation of gradient boosted trees in Scikit-learn.
Classification with the Scikit-learn gradient boosting estimator
This is implemented using the `GradientBoostingClassifier`. Some of the parameters expected by this algorithm include:

`loss` defining the loss function to be optimized
`learning_rate` that determines the contribution of each tree
`n_estimatorst` dictates the number of decision trees
`max_depth` is the maximum depth of each estimator


from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1)
gbc.fit(X_train,y_train)


After fitting the classifier, you can obtain the importance of the features using the `feauture_importances_` attribute. This is usually referred to as the Gini importance.

gbc.feature_importances_



The higher the value, the more important the feature is. The values in the obtained array will sum to 1.
Note: Impurity-based importances are not always accurate, especially when there are too many features. In that case, you should consider using permutation-based importances.
Regression with the Scikit-learn gradient boosting estimator
The Scikit-learn gradient boosting estimator can be implemented for regression using `GradientBoostingRegressor`. It takes parameters that are similar to the classification one:

loss,
number of estimators,
maximum depth of the trees,
learning rate…

…just to mention a few.

from sklearn.ensemble import GradientBoostingRegressor
params = {'n_estimators': 500,
          'max_depth': 4,
          'min_samples_split': 5,
          'learning_rate': 0.01,
          'loss': 'ls'}
gbc = GradientBoostingRegressor(**params)
gbc.fit(X_train,y_train)


Like the classification model, you can also obtain the feature importances for the regression algorithm.

gbc.feature_importances_


 
XGBoost
 
XGBoost is a gradient boosting library supported for Java, Python, Java and C++, R, and Julia. It also uses an ensemble of weak decision trees.
It’s a linear model that does tree learning through parallel computations. The algorithm also ships with features for performing cross-validation, and showing the feature’s importance. The main features of this model are:

accepts sparse input for tree booster and linear booster,
supports custom evaluation and objective functions,
`Dmatrix`, its optimized data structure improves its performance.

Let’s take a look at how you can apply XGBoost in Python. The parameters accepted by the algorithm include:

`objective` to define the type of task, say regression or classification;
`colsample_bytree` the subsample ratio of columns when constructing each tree. Subsampling happens once in every iteration. This number is usually a value between 0 and 1;
`learning_rate` that determines how fast or slow the model will learn;
`max_depth` indicates the maximum depth for each tree. The more the trees, the greater model complexity, and the higher chances of overfitting;
`alpha` is the L1 regularization on weights;
`n_estimators` is the number of decision trees to fit.

Classification with XGBoost
After importing the algorithm, you define the parameters that you would like to use. Since this is a classification problem, the `binary: logistic` objective function is used. The next step is to use the `XGBClassifier` and unpack the defined parameters. You can tune these parameters until you obtain the ones that are optimal for your problem.

import xgboost as xgb
params = {""objective"":""binary:logistic"",'colsample_bytree': 0.3,'learning_rate': 0.1,
                'max_depth': 5, 'alpha': 10}
classification = xgb.XGBClassifier(**params)
classification.fit(X_train, y_train)


Regression with XGBoost
In regression, the `XGBRegressor` is used instead. The objective function, in this case, will be the `reg:squarederror`.

import xgboost as xgb
params = {""objective"":""reg:squarederror"",'colsample_bytree': 0.3,'learning_rate': 0.1,
                'max_depth': 5, 'alpha': 10}
regressor = xgb.XGBRegressor(**params)
regressor.fit(X_train, y_train)


The XGBoost models also allow you to obtain the feature importances via the `feature_importances_` attribute.

regressor.feature_importances_



You can easily visualize them using Matplotlib. This is done using the `plot_importance` function from XGBoost.

import matplotlib.pyplot as plt
xgb.plot_importance(regressor)
plt.rcParams['figure.figsize'] = [5, 5]
plt.show()



The `save_model` function can be used for saving your model. You can then send this model to your model registry.

regressor.save_model(""model.pkl"")


Check Neptune docs about integration with XGBoost and with matplotlib.
 
LightGBM
 
LightGBM is different from other gradient boosting frameworks because it uses a leaf-wise tree growth algorithm. Leaf-wise tree growth algorithms are known to converge faster than depth-wise growth algorithms. However, they’re more prone to overfitting.
Source

 
The algorithm is histogram-based, so it places continuous values into discrete bins. This leads to faster training and efficient memory utilization.
Other notable features from this algorithm include:

support for GPU training,
native support for categorical features,
ability to handle large-scale data,
handles missing values by default.

Let’s take a look at some of the main parameters of this algorithm:

`max_depth` the maximum depth of each tree;
`objective` which defaults to regression;
`learning_rate` the boosting learning rate;
`n_estimators` the number of decision trees to fit;
`device_type` whether you’re working on a CPU or GPU.

Classification with LightGBM
Training a binary classification model can be done by setting `binary` as the objective. If it’s a multi-classification problem, the `multiclass` objective is used.
The dataset is also converted to LightGBM’s `Dataset` format. Training the model is then done using the `train` function. You can also pass the validation datasets using the `valid_sets` parameter.

import lightgbm as lgb
lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)
params = {'boosting_type': 'gbdt',
              'objective': 'binary',
              'num_leaves': 40,
              'learning_rate': 0.1,
              'feature_fraction': 0.9
              }
gbm = lgb.train(params,
    lgb_train,
    num_boost_round=200,
    valid_sets=[lgb_train, lgb_eval],
    valid_names=['train','valid'],
   )



Regression with LightGBM
For regression with LightGBM, you just need to change the objective to `regression`. The boosting type is Gradient Boosting Decision Tree by default.
If you like, you can change this to the random forest algorithm, `dart` — Dropouts meet Multiple Additive Regression Trees, or  `goss` — Gradient-based One-Side Sampling.

import lightgbm as lgb
lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)
params = {'boosting_type': 'gbdt',
              'objective': 'regression',
              'num_leaves': 40,
              'learning_rate': 0.1,
              'feature_fraction': 0.9
              }
gbm = lgb.train(params,
    lgb_train,
    num_boost_round=200,
    valid_sets=[lgb_train, lgb_eval],
    valid_names=['train','valid'],
   )


You can also use LightGBM to plot the model’s feature importance.

lgb.plot_importance(gbm)



LightGBM also has a built-in function for saving the model. That function is `save_model`.

gbm.save_model('mode.pkl')


 
CatBoost
 
CatBoost is a depth-wise gradient boosting library developed by Yandex. The algorithm grows a balanced tree using oblivious decision trees.
It uses the same features to make the right and left split at each level of the tree.
For example in the image below, you can see that `297,value>0.5` is used through that level.

Other notable features of CatBoost include:

native support for categorical features,
supports training on multiple GPUs,
results in good performance with the default parameters,
fast prediction via CatBoost’s model applier,
handles missing values natively,
support for regression and classification problems.

Let’s now mention a couple of training parameters from CatBoost:

`loss_function` the loss to be used for classification or regression;
`eval_metric` the model’s evaluation metric;
`n_estimators` the maximum number of decision trees;
`learning_rate` determines how fast or slow the model will learn;
`depth` the maximum depth for each tree;
`ignored_features` determines the features that should be ignored during training;
`nan_mode` the method that will be used to deal with missing values;
`cat_features` an array of categorical columns;
`text_features` for declaring text-based columns.

Classification with CatBoost
For classification problems,`CatBoostClassifier` is used. Setting `plot=True` during the training process will visualize the model.

from catboost import CatBoostClassifier
model = CatBoostClassifier()
model.fit(X_train,y_train,verbose=False, plot=True)



Regression with CatBoost
In the case of regression, the `CatBoostRegressor` is used.

from catboost import CatBoostRegressor
model = CatBoostRegressor()
model.fit(X_train,y_train,verbose=False, plot=True)


You can also use the `feature_importances_` to obtain the ranking of the features by their importance.

model.feature_importances_



The algorithm also provides support for performing cross-validation. This is done using the `cv` function while passing the required parameters.
Passing `plot=”True”` will visualize the cross-validation process. The `cv` function expects the dataset to be in CatBoost’s `Pool` format.

from catboost import Pool, cv
params = {""iterations"": 100,
          ""depth"": 2,
          ""loss_function"": ""RMSE"",
          ""verbose"": False}
cv_dataset = Pool(data=X_train,
                  label=y_train)
scores = cv(cv_dataset,
            params,
            fold_count=2, 
            plot=True)


You can also use CatBoost to perform a grid search. This is done using the `grid_search` function. After searching, CatBoost trains on the best parameters.
You should not have fitted the model before this process. Passing the `plot=True` parameter will visualize the grid search process.

grid = {'learning_rate': [0.03, 0.1],
        'depth': [4, 6, 10],
        'l2_leaf_reg': [1, 3, 5, 7, 9]}

grid_search_result = model.grid_search(grid, X=X_train, y=y_train, plot=True)


CatBoost also enables you to visualize a single tree in the model. This is done using the `plot_tree` function and passing the index of the tree you would like to visualize.

model.plot_tree(tree_idx=0)



 
Advantages of gradient boosting trees
 
There are several reasons as to why you would consider using gradient boosting tree algorithms:

generally more accurate compare to other modes,
train faster especially on larger datasets,
most of them provide support handling categorical features,
some of them handle missing values natively.

 
Disadvantages of gradient boosting trees
 
Let’s now address some of the challenges faced when using gradient boosted trees:

prone to overfitting: this can be solved by applying L1 and L2 regularization penalties. You can try a low learning rate as well;
models can be computationally expensive and take a long time to train, especially on CPUs;
hard to interpret the final models.

 
Final thoughts
 
In this article, we explored how to implement gradient boosting decision trees in your machine learning problems. We also walked through various boosting-based algorithms that you can start using right away.
Specifically, we’ve covered:

what is gradient boosting,
how gradient boosting works,
various types of gradient boosting algorithms,
how to use gradient boosting algorithms for regression and classification problems,
the advantages of gradient boosting trees,
disadvantages of gradient boosting trees,

…and so much more.
You’re all set to start boosting your machine learning models.
 
Resources
 

Gradient boosting in TensorFlow
Histogram-based gradient boosting 
Classification notebook 
Regression notebook 

 
Bio: Derrick Mwiti is a data scientist who has a great passion for sharing knowledge. He is an avid contributor to the data science community via blogs such as Heartbeat, Towards Data Science, Datacamp, Neptune AI, KDnuggets just to mention a few. His content has been viewed over a million times on the internet. Derrick is also an author and online instructor. He also trains and works with various institutions to implement data science solutions as well as to upskill their staff. You might want to check his Complete Data Science & Machine Learning Bootcamp in Python course.
Original. Reposted with permission.
Related:

LightGBM: A Highly-Efficient Gradient Boosting Decision Tree
The Best Machine Learning Frameworks & Extensions for Scikit-learn
Fast Gradient Boosting with CatBoost"
https://www.kdnuggets.com/2020/08/data-science-skills-superpower.html,These Data Science Skills will be your Superpower,"Learning data science means learning the hard skills of statistics, programming, and machine learning. To complete your training, a broader set of soft skills will round out your capabilities as an effective and successful professional Data Scientist.","By Benjamin Obi Tayo, Ph.D., DataScienceHub.
comments
Most academic training programs in data science are focused mostly on teaching hard skills. Time and time again, industry data, market trends, and insights from top business leaders highlight soft skills as a key component to success in the workplace. This article will discuss the essential hard and soft skills for success in data science practice.
 
Hard Skills
 
1. Mathematics and Statistics Skills
Math skills are essential in data science and machine learning. For more about the basic math skills needed for data science and machine learning, please see this article: How Much Math do I need in Data Science?
2. Essential Programming Skills
Programming skills are essential in data science. Since Python and R are considered the 2 most popular programming languages in data science, essential knowledge in both languages is crucial. For more information on essential programming skills needed for data science, please see this article: How Much Programming do I need in Data Science?
3. Data Wrangling and Preprocessing Skills
Data is key for any analysis in data science, be it inferential analysis, predictive analysis, or prescriptive analysis. The predictive power of a model depends on the quality of the data that was used in building the model. Data comes in different forms such as text, table, image, voice, or video. Most often, data that is used for analysis has to be mined, processed, and transformed to render it to a form suitable for further analysis.
i) Data Wrangling: The process of data wrangling is a critical step for any data scientist. Very rarely is data easily accessible in a data science project for analysis. It’s more likely for the data to be in a file, a database, or extracted from documents such as web pages, tweets, or PDFs. Knowing how to wrangle and clean data will enable you to derive critical insights from your data that would otherwise be hidden.
ii) Data Preprocessing: Knowledge about data preprocessing is very important and include topics such as:
a) Dealing with missing data
b) Data imputation
c) Handling categorical data
d) Encoding class labels for classification problems
e) Techniques of feature transformation and dimensionality reduction such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).
4. Data Visualization Skills
Understand the essential components of good data visualization (see figure below). Be able to use several data visualization packages, such as matplotlib, seaborn, and ggplot2.

Typical workflow for a data visualization project. Image by Benjamin O. Tayo.
5. Basic Machine Learning Skills
Machine Learning is a very important branch of data science. It is important to understand the machine learning framework: Problem Framing, Data Analysis, Model Building, Testing & Evaluation, and Model Application.

Typical workflow for a machine learning project. Image by Benjamin O. Tayo.
Find out more about the machine learning framework from here: Machine Learning Process Tutorial.
6. Skills from Real World Capstone Data Science Projects
Skills acquired from course work alone will not make you a data scientist. A qualified data scientist must be able to demonstrate evidence of successful completion of a real-world data science project that includes every stage in data science and machine learning process such as problem framing, data acquisition and analysis, model building, model testing, model evaluation, and deploying the model. Real-world data science projects could be found in the following:
a) Kaggle Projects
b) Internships
c) From Interviews
 
Soft Skills
 
1. Communication Skills
Data scientists need to be able to communicate their ideas with other members of the team or with business administrators in their organizations. Good communication skills would play a key role here to be able to convey and present very technical information to people with little or no understanding of technical concepts in data science. Good communication skills will help foster an atmosphere of unity and togetherness with other team members such as data analysts, data engineers, field engineers, etc.
2. Be a Lifelong Learner
Data science is a field that is ever-evolving, so be prepared to embrace and learn new technologies. One way to keep in touch with developments in the field is to network with other data scientists. Some platforms that promote networking are LinkedIn, GitHub, and Medium (Towards Data Science and Towards AI publications). These platforms are very useful for up-to-date information about recent developments in the field.
3. Team Player Skills
As a data scientist, you will be working in a team of data analysts, engineers, administrators, so you need good communication skills. You need to be a good listener too, especially during early project development phases where you need to rely on engineers or other personnel to be able to design and frame a good data science project. Being a good team player would help you to thrive in a business environment and maintain good relationships with other members of your team as well as administrators or directors of your organization.
4. Business Acumen Skills
A very important skill set that is essential for practical applications is business acumen. Business acumen is the ability to draw out meaningful conclusions from a model that can lead to important and cost-saving data-driven decision making. Acquiring business acumen skills is therefore essential for practical data scientists.
5. Ethical Skills in Data Science
Understand the implication of your project. Be truthful to yourself. Avoid manipulating data or using a method that will intentionally produce bias in results. Be ethical in all phases from data collection to analysis, to model building, analysis, testing, and application. Avoid fabricating results for the purpose of misleading or manipulating your audience. Be ethical in the way you interpret the findings from your data science project.
 
Summary and Conclusion
 
In summary, we’ve discussed several essential skills needed for practicing data scientists. While academic training programs do a good job to teach hard skills, soft skills are essential for success in the real world.
Original. Reposted with permission.
Related:

Top 5 must-have Data Science skills for 2020
The Most In Demand Tech Skills for Data Scientists
Which Data Science Skills are core and which are hot/emerging ones?"
https://www.kdnuggets.com/2020/08/microsoft-dowhy-framework-causal-inference.html,Microsoft’s DoWhy is a Cool Framework for Causal Inference,"Inspired by Judea Pearl’s do-calculus for causal inference, the open source framework provides a programmatic interface for popular causal inference methods.","By Jesus Rodriguez, Intotheblock.
comments


Source: https://www.microsoft.com/en-us/research/blog/dowhy-a-library-for-causal-inference/

 

I recently started a new newsletter focus on AI education. TheSequence is a no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:


 
The human mind has a remarkable ability to associate causes with a specific event. From the outcome of an election to an object dropping on the floor, we are constantly associating chains of events that cause a specific effect. Neuropsychology refers to this cognitive ability as causal reasoning. Computer science and economics study a specific form of causal reasoning known as causal inference which focuses on exploring relationships between two observed variables. Over the years, machine learning has produced many methods for causal inference but they remain mostly difficult to use in mainstream applications. Recently, Microsoft Research open sourced DoWhy, a framework for causal thinking and analysis.
The challenge with causal inference is not that is a new discipline, quite the opposite, but that the current methods represent a very small and simplistic version of causal reasoning. Most models that try to connect causes such as linear regression rely on empirical analysis that makes some assumption about the data. Pure causal inference relies on counterfactual analysis which is a closer representation to how humans make decisions. Imagine a scenario in which you are traveling with your families for vacations to an unknown destination. Before and after the vacation you are wrestling with a few counterfactual questions:

Answering these questions is the focus of causal inference. Unlike supervised learning, causal inference depends on estimation of unobserved quantities. This if often known as the “fundamental problem” of causal inference which implies that a model never has a purely objective evaluation through a held-out test set. In our vacation example, you can either observe the effects on going on vacation or not going on vacations but never both. This challenge forces causal inference to make critical assumptions about the data generation process. Traditional machine learning frameworks for causal inference try to take shortcuts around the “fundamental problem” resulting on a very frustrating experience for data scientists and developers.
 
Introducing DoWhy
 

Microsoft’s DoWhy is a Python-based library for causal inference and analysis that attempts to streamline the adoption of causal reasoning in machine learning applications. Inspired by Judea Pearl’s do-calculus for causal inference, DoWhy combines several causal inference methods under a simple programming model that removes many of the complexities of traditional approaches. Compared to its predecessors, DoWhy makes three key contributions to the implementation of causal inference models.


Provides a principled way of modeling a given problem as a causal graph so that all assumptions explicit.
Provides a unified interface for many popular causal inference methods, combining the two major frameworks of graphical models and potential outcomes.
Automatically tests for the validity of assumptions if possible and assesses robustness of the estimate to violations.

Conceptually, DoWhy was created following two guiding principles: asking causal assumptions explicit and testing robustness of the estimates to violations of those assumptions. In other words, DoWhy separates the identification of a causal effect from the estimation of its relevance which enables the inference of very sophisticated causal relationships.
To accomplish its goal, DoWhy models any causal inference problem in a workflow with four fundamental steps: model, identify, estimate and refute.


Model: DoWhy models each problem using a graph of causal relationships. The current version of DoWhy supports two formats for graph input: gml (preferred) and dot. The graph might include prior knowledge of the causal relationships in the variables but DoWhy does not make any immediate assumptions.
Identify: Using the input graph, DoWhy finds all possible ways of identifying a desired causal effect based on the graphical model. It uses graph-based criteria and do-calculus to find potential ways find expressions that can identify the causal effect
Estimate: DoWhy estimates the causal effect using statistical methods such as matching or instrumental variables. The current version of DoWhy supports estimation methods based such as propensity-based-stratification or propensity-score-matching that focus on estimating the treatment assignment as well as regression techniques that focus on estimating the response surface.
Verify: Finally, DoWhy uses different robustness methods to verify the validity of the causal effect.

 
Using DoWhy
 
Developers can start using DoWhy by installing the Python module using the following command:

python setup.py install


Like any other machine learning program, the first step of a DoWhy application is to load the dataset. In this example, imagine that we are trying to infer the correlation between different medical treatments and outcomes represented by the following dataset.

Treatment    Outcome        w0
0   2.964978   5.858518 -3.173399
1   3.696709   7.945649 -1.936995
2   2.125228   4.076005 -3.975566
3   6.635687  13.471594  0.772480
4   9.600072  19.577649  3.922406


DoWhy relies on pandas dataframes to capture the input data:

rvar = 1 if np.random.uniform() >0.5 else 0 
data_dict = dowhy.datasets.xy_dataset(10000, effect=rvar, sd_error=0.2) 
df = data_dict['df']
print(df[[""Treatment"", ""Outcome"", ""w0""]].head())


At this point, we simply need about four steps to infer causal relationships between the variables. The four steps correspond to the four operations of DoWhy: model, estimate, infer and refute. We can start by modeling the problem as a causal graph:

model= CausalModel(
        data=df,
        treatment=data_dict[""treatment_name""],
        outcome=data_dict[""outcome_name""],
        common_causes=data_dict[""common_causes_names""],
        instruments=data_dict[""instrument_names""])
model.view_model(layout=""dot"")
from IPython.display import Image, display
display(Image(filename=""causal_model.png""))




Source: https://microsoft.github.io/dowhy/

 
The next step is to identify the causal relationships in the graph:

identified_estimand = model.identify_effect()


Now we can estimate the causal effect and determine if the estimation is correct. This example uses linear regression for simplicity:

estimate = model.estimate_effect(identified_estimand,
        method_name=""backdoor.linear_regression"")
# Plot Slope of line between treamtent and outcome =causal effect
dowhy.plotter.plot_causal_effect(estimate, df[data_dict[""treatment_name""]], df[data_dict[""outcome_name""]])




Source: https://microsoft.github.io/dowhy/

 
Finally, we can use different technique to refute the causal estimator:

res_random=model.refute_estimate(identified_estimand, estimate, method_name=""random_common_cause"")


DoWhy is a very simple and useful framework to implement causal inference models. The current version can be used as a standalone library or integrated into popular deep learning frameworks such as TensorFlow or PyTorch. The combination of multiple causal inference methods under a single framework and the four-step simple programming model makes DoWhy incredibly simple to use for data scientist tackling causal inference problems.
 
Original. Reposted with permission.
Related:

Facebook Uses Bayesian Optimization to Conduct Better Experiments in Machine Learning Models
Learning by Forgetting: Deep Neural Networks and the Jennifer Aniston Neuron
Netflix’s Polynote is a New Open Source Framework to Build Better Data Science Notebooks"
https://www.kdnuggets.com/2021/03/informs-virtual-career-fair.html,Hiring or Looking to Get Hired in Data Science/Analytics? The INFORMS Virtual Career Fair is for You,"Hiring or looking to get hired in Data Science / Analytics? The INFORMS Virtual Career Fair, April 13, is for you. Register today!","Sponsored Post.

 
Hiring or Looking to Get Hired in Data Science/Analytics? The INFORMS Virtual Career Fair is for You
 
The 2020 INFORMS Virtual Career Fair saw hundreds of highly qualified job seekers in data science, analytics, AI/ML, and more, match with leading employers across both industry and academia. This year, INFORMS is looking to repeat – and build upon – the success of last year’s event with our next Virtual Career Fair, hosted alongside the Virtual 2021 INFORMS Business Analytics Conference. 
 
Employers: Expect Top Talent and Diverse Skill Sets
 
The Virtual Career Fair provides employers with the opportunity to meet with and collect resumes from interested job seekers ranging from seasoned professionals in academia or industry to freshly-minted PhD and master’s students just entering the workforce. 
Participation in the INFORMS Virtual Career Fair will raise the visibility of your organization with both Business Analytics Conference participants and registered job seekers. All Career Fair events are widely advertised – not only to job seekers already enrolled in INFORMS Career Center – but also to thousands of professionals attending the meeting.
Top benefits of participation include:

The ability to arrange public and private LIVE chats from your own virtual booth
Unrestricted access to candidate profile details and basic contact info (before, during, and after the event)
Unlimited job postings

 
Jobseekers: Meet with Organizations Who Value Your Expertise and Skills
 
Sign up for FREE and create your profile today. Registrants can browse available job openings and meet with individual recruiters through public and private chats. Once you register and upload a resume to your profile, organizations may send messages to your account regarding available openings – even before the Virtual Career Fair opens. Register today so employers can review your qualifications and begin reaching out.
Tips for success: 

PREPARE FOR THE FAIR: Review participating organizations’ profiles as they are added to the website and formulate questions before joining chat sessions.
Review job listings to be informed about positions that interest you.
Be aware: Not all employers post positions in the Virtual Career Fair system and may refer you to their websites instead.

 
Still Charting Your Course? Let INFORMS Be Your Career Guide
 
Want to learn more about career paths in our industry? We examined job titles, years of experience, and analytic skills to create a framework that defines technical job skills, requirements, and career pathways under the “analytics” umbrella. 
Visit the Analytics Career Pathways page and download the report to check out what we found!"
https://www.kdnuggets.com/2021/04/essential-math-data-science-linear-transformation-matrices.html,Essential Math for Data Science: Linear Transformation with Matrices,"You’ll start seeing matrices, not only as operations on numbers, but also as a way to transform vector spaces. This conception will give you the foundations needed to understand more complex linear algebra concepts like matrix decomposition.","comments
By Hadrien Jean, Machine Learning Scientist

 
As you can see in Essential Math for Data Science, being able to manipulate vectors and matrices is critical to create machine learning and deep learning pipelines, for instance for reshaping your raw data before using it with machine learning libraries.
The goal of this chapter is to get you to the next level of understanding of vectors and matrices. You’ll start seeing matrices, not only as operations on numbers, but also as a way to transform vector spaces. This conception will give you the foundations needed to understand more complex linear algebra concepts like matrix decomposition. You’ll build up on what you learned about vector addition and scalar multiplication to understand linear combinations of vectors.
 
Linear Transformations
 
Intuition
 
A linear transformation (or simply transformation, sometimes called linear map) is a mapping between two vector spaces: it takes a vector as input and transforms it into a new output vector. A function is said to be linear if the properties of additivity and scalar multiplication are preserved, that is, the same result is obtained if these operations are done before or after the transformation. Linear functions are synonymously called linear transformations.

Linear transformations notation 
You can encounter the following notation to describe a linear transformation: T(v). This refers to the vector v transformed by T. A transformation T is associated with a specific matrix. Since additivity and scalar multiplication must be preserved in linear transformation, you can write:

and
 



 
Linear Transformations as Vectors and Matrices
 
In linear algebra, the information concerning a linear transformation can be represented as a matrix. Moreover, every linear transformation can be expressed as a matrix.
When you do the linear transformation associated with a matrix, we say that you apply the matrix to the vector. More concretely, it means that you calculate the matrix-vector product of the matrix and the vector. In this case, the matrix can sometimes be called a transformation matrix. For instance, you can apply a matrix A to a vector v with their product Av.

Applying matrices Keep in mind that, to apply a matrix to a vector, you left multiply the vector by the matrix: the matrix is on the left to the vector.
When you multiply multiple matrices, the corresponding linear transformations are combined in the order from right to left.
For instance, let's say that a matrix A does a 45-degree clockwise rotation and a matrix B does a stretching, the product BA means that you first do the rotation and then the stretching.
This shows that the matrix product is:

Not commutative (AB ≠ BA): the stretching then the rotation is a different transformation than the rotation then the stretching.
Associative (A(BC)) = ((AB)C): the same transformations associated with the matrices A, B and C are done in the same order.

A matrix-vector product can thus be considered as a way to transform a vector. You can see in Essential Math for Data Science that the shape of A and v must match for the product to be possible.


 
Geometric Interpretation
 
A good way to understand the relationship between matrices and linear transformations is to actually visualize these transformations. To do that, you’ll use a grid of points in a two-dimensional space, each point corresponding to a vector (it is easier to visualize points instead of arrows pointing from the origin).
Let’s start by creating the grid using the function meshgrid() from Numpy:

x = np.arange(-10, 10, 1)
y = np.arange(-10, 10, 1)

xx, yy = np.meshgrid(x, y)


The meshgrid() function allows you to create all combinations of points from the arrays x and y. Let’s plot the scatter plot corresponding to xx and yy.

plt.scatter(xx, yy, s=20, c=xx+yy)
# [...] Add axis, x and y with the same scale



Figure 1: Each point corresponds to the combination of x and y values.
 
You can see the grid in Figure 1. The color corresponds to the addition of xx and yy values. This will make transformations easier to visualize.
 
The Linear Transformation associated with a Matrix
 
As a first example, let’s visualize the transformation associated with the following two-dimensional square matrix.

Consider that each point of the grid is a vector defined by two coordinates (x and y).
Let’s create the transformation matrix T:

T = np.array([
    [-1, 0],
    [0, -1]
])


First, you need to structure the points of the grid to be able to apply the matrix to each of them. For now, you have two 20 by 20 matrices (xx and yy) corresponding to  points, each having a x value (matrix xx) and a y value (yy). Let’s create a 2 by 400 matrix with xx flatten as the first column and yy as the second column.

xy =  np.vstack([xx.flatten(), yy.flatten()])
xy.shape



(2, 400)


You have now 400 points, each with two coordinates. Let’s apply the transformation matrix T to the first two-dimensional point (xy[:, 0]), for instance:

T @ xy[:, 0]



array([10, 10])


You can similarly apply TT to each point by calculating its product with the matrix containing all points:

trans = T @ xy
trans.shape



(2, 400)



You can see that the shape is still (2,400). Each transformed vector (that is, each point of the grid) is one of the column of this new matrix. Now, let’s reshape this array to have two arrays with a similar shape to xx and yy.

xx_transformed = trans[0].reshape(xx.shape)
yy_transformed = trans[1].reshape(yy.shape)



Let’s plot the grid before and after the transformation:

f, axes = plt.subplots(1, 2, figsize=(6, 3))
axes[0].scatter(xx, yy, s=10, c=xx+yy)
axes[1].scatter(xx_transformed, yy_transformed, s=10, c=xx+yy)
# [...] Add axis, x and y witht the same scale



Figure 2: The grid of points before (left) and after (right) its transformation by the matrix TT.
 
Figure 2 shows that the matrix T rotated the points of the grid.
 
Shapes of the Input and Output Vectors
 
In the previous example, the output vectors have the same number of dimensions than the input vectors (two dimensions).
You might notice that the shape of the transformation matrix must match the shape of the vectors you want to transform.

Figure 3: Shape of the transformation of the grid points by TT.
 
Figure 3 illustrates the shapes of this example. The first matrix with a shape (2, 2) is the transformation matrix T and the second matrix with a shape (2, 400) corresponds to the 400 vectors stacked. As illustrated in blue, the number of rows of the T corresponds to the number of dimensions of the output vectors. As illustrated in red, the transformation matrix must have the same number of columns as the number of dimensions of the matrix you want to transform.
More generally, the size of the transformation matrix tells you the input and output dimensions. An m by n transformation matrix transforms n-dimensional vectors to mm-dimensional vectors.
 
Stretching and Rotation
 
Let’s now visualize the transformation associated with the following matrix:

Let’s proceed as in the previous example:

T = np.array([
    [1.3, -2.4],
    [0.1, 2]
])
trans = T @ xy

xx_transformed = trans[0].reshape(xx.shape)
yy_transformed = trans[1].reshape(yy.shape)

f, axes = plt.subplots(1, 2, figsize=(6, 3))
axes[0].scatter(xx, yy, s=10, c=xx+yy)
axes[1].scatter(xx_transformed, yy_transformed, s=10, c=xx+yy)
# [...] Add axis, x and y witht the same scale



Figure 4: The grid of points before (left) and after (right) the transformation by the new matrix T.
 
Figure 4 shows that the transformation is different from the previous rotation. This time, there is a rotation, but also a stretching of the space.

Are these transformations linear? You might wonder why these transformations are called ""linear"". You saw that a linear transformation implies that the properties of additivity and scalar multiplication are preserved.
Geometrically, there is linearity if the vectors lying on the same line in the input space are also on the same line in the output space, and if the origin remains at the same location.


 
Special Cases
 
Inverse Matrices
 
Transforming the space with a matrix can be reversed if the matrix is invertible. In this case, the inverse T−1 of the matrix T is associated with a transformation that takes back the space to the initial state after T has been applied.
Let’s take again the example of the transformation associated with the following matrix:

You’ll plot the initial grid of point, the grid after being transformed by T, and the grid after successive application of T and T−1 (remember that matrices must be left-multiplied):

T = np.array([
    [1.3, -2.4],
    [0.1, 2]
])
trans = T @ xy

T_inv = np.linalg.inv(T)

un_trans = T_inv @ T @ xy

f, axes = plt.subplots(1, 3, figsize=(9, 3))
axes[0].scatter(xx, yy, s=10, c=xx+yy)
axes[1].scatter(trans[0].reshape(xx.shape), trans[1].reshape(yy.shape), s=10, c=xx+yy)
axes[2].scatter(un_trans[0].reshape(xx.shape), un_trans[1].reshape(yy.shape), s=10, c=xx+yy)

# [...] Add axis, x and y witht the same scale




Figure 5: Inverse of a transformation: the initial space (left) is transformed with the matrix T (middle) and transformed back using T−1 (right).
 
As you can see in Figure 5, the inverse T−1 of the matrix T is associated with a transformation that reverses the one associated with T.
Mathematically, the transformation of a vector v by T is defined as:
Tv
To transform it back, you multiply by the inverse of T:
T−1Tv

Order of the matrix products Note that the order of the products is from right to left. The vector on the right of the product is first transformed by T and then the result is transformed by T−1.


As you can see in Essential Math for Data Science, , so you have:

meaning that you get back the initial vector v.
 
Non Invertible Matrices
 
The linear transformation associated with a singular matrix (that is a non invertible matrix) can’t be reversed. It can occur when there is a loss of information with the transformation. Take the following matrix:

Let’s see how it transforms the space:

T = np.array([
    [3, 6],
    [2, 4],
])
trans = T @ xy

f, axes = plt.subplots(1, 2, figsize=(6, 3))
axes[0].scatter(xx, yy, s=10, c=xx+yy)
axes[1].scatter(trans[0].reshape(xx.shape), trans[1].reshape(yy.shape), s=10, c=xx+yy)
# [...] Add axis, x and y witht the same scale




Figure 6: The initial space (left) is transformed into a line (right) with the matrix T. Multiple input vectors land on the same location in the output space.
 
You can see in Figure 6 that the transformed vectors are on a line. There are points that land on the same place after the transformation. Thus, it is not possible to go back. In this case, the matrix T is not invertible: it is singular.
 
Bio: Hadrien Jean is a machine learning scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog (hadrienj.github.io).
Original. Reposted with permission.
Related:

Essential Math for Data Science: Probability Density and Probability Mass Functions
Essential Math for Data Science: Integrals And Area Under The Curve
Essential Math for Data Science: The Poisson Distribution"
https://www.kdnuggets.com/2021/07/accept-null-hypothesis-wrong-intuitive-explanation.html,Why Saying “We Accept the Null Hypothesis” is Wrong: An Intuitive Explanation,"“The opposite of ‘Rejecting the Null’ is ‘Accepting’ isn’t it?”. Well, it is not so simple as it is construed. We need to rise above antonyms and understand one crucial concept.","comments
By Venkat Raman, Co-Founder at Aryma Labs


Source: Unsplash

 
We often come across YouTube videos, posts, blogs and private courses wherein they say “We accept the Null Hypothesis” instead of saying “We fail to reject the Null hypothesis”.
If you correct them, they would say what's the big difference? “The opposite of ‘Rejecting the Null’ is ‘Accepting’ isn’t it?”.
Well, it is not so simple as it is construed. We need to rise above antonyms and understand one crucial concept. That crucial concept is ‘Popperian falsification’.
This concept or philosophy also holds key to why we use the language “Fail to reject the Null”.
Basically, the Popperian falsification implies that ‘Science is never settled’. It keeps changing or evolving. Theories held sacrosanct today could be refuted tomorrow.


The Popperian falsification implies that ‘Science is never settled’. It keeps changing or evolving. Theories held sacrosanct today could be refuted tomorrow.


So under this principle, scientists never proclaim “X theory is true”. Instead what they try to do is, they try to prove that “the theory X is wrong”. This is called the principle of falsification.
Now having tried your best and you still could not prove the theory X is wrong, what would you say? You would say “I failed to prove theory X is wrong”. Ah.. now can you see the parallels between “I failed to prove theory X is wrong” and “We fail to reject the Null ”.
Now lets come to why you can’t say “we accept the Null hypothesis”.
We could not prove theory X is wrong. But does that really mean theory X is correct? No, somebody more smarter than in the future could prove theory x is wrong. There always exists that possibility. Remember above that we said “science is never settled”.
A more classic example is that of the ‘Black Swan’. “Suppose a theory proposes that all swans are white. The obvious way to prove the theory is to check that every swan really is white — but there’s a problem. No matter how many white swans you find, you can never be sure there isn’t a black swan lurking somewhere. So, you can never prove the theory is true. In contrast, finding one solitary black swan guarantees that the theory is false.”
Note: The post is merely to drive home the point how the language “we fail to reject” came about. It is not a post favoring inductive reasoning over deductive reasoning or vice versa. Neither it is an effort to prove or disprove Karl Popper’s falsification principle.
Reference (black swan example): https://www.newscientist.com/people/karl-popper/#ixzz70d4aPeIj
Your comments and opinions are welcome.
You can reach out to me on: Linkedin and Twitter
 
Bio: Venkat Raman is Co-Founder at Aryma Labs.
Original. Reposted with permission.
Related:

Abstraction and Data Science: Not a great combination
5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist
Managing Your Reusable Python Code as a Data Scientist"
https://www.kdnuggets.com/2020/08/accelerated-computer-vision-free-course-amazon.html,Accelerated Computer Vision: A Free Course From Amazon,"Amazon's Machine Learning University is making its online courses available to the public, and this time we look at its Accelerated Computer Vision offering.","By Matthew Mayo, KDnuggets.
comments

 
Amazon's Machine Learning University is making its online courses, previously only available to Amazon employees, freely-available to the public.
One of the first such courses made publicly available is Accelerated Computer Vision, a course which describes itself as follows:

This repository contains slides, notebooks, and datasets for the Machine Learning University (MLU) Computer Vision class. Our mission is to make Machine Learning accessible to everyone. We have courses available across many topics of machine learning and believe knowledge of ML can be a key enabler for success. This class is designed to help you get started with Computer Vision, learn about widely used Machine Learning techniques and apply them to real-world problems.

This concise course, taught by Rachel Hu, an applied scientist at AWS Deep Engine Science, is made up of a playlist of video lectures, along with a GitHub repository of course notebooks, slides, and data. The short videos and accompanying courseware material is collected into into 3 overarching lessons, organized as follows:

 
Lesson notebooks for the course focus on acquiring the skills for practical CV implementation, and have the following focuses, providing insight into what you can expect to learn along the way:

Neural Networks
Convolutional Neural Networks
Final Project Overview
AlexNet
AutoGluon for Computer Vision
ResNet
YOLO

The course also includes the implementation of a final project in practical computer vision, the Jupyter notebook for which begins as follows, providing additional hints of expected learning outcomes:

 
This appears to be a great introductory course for computer vision beginners. Amazon has done the community a service by opening their Machine Learning University courses to the public, and between the Accelerated Computer Vision course and the other currently available offerings, it appears that those wishing to learn machine learning from a world class machine learning organization now have another option.
 
Related:

Accelerated Natural Language Processing: A Free Course
Going Beyond Superficial: Data Science MOOCs with Substance
Awesome Machine Learning and AI Courses"
https://www.kdnuggets.com/2020/10/data-protection-techniques-guarantee-privacy.html,Data Protection Techniques Needed to Guarantee Privacy,This article takes a look at the concepts of data privacy and personal data. It presents several privacy protection techniques and explains how they contribute to preserving the privacy of individuals.,"comments
By Elise Devaux, Statice
The economics, legal, and corporate implications of data privacy are now too strong to be ignored. In the last decades, different privacy-enhancing techniques were proposed to respond to ever-increasing requirements of technical and societal nature. But how do you decide which approach to use? 
 
The scope of data privacy
 
Two years after the General Data Protection Regulation (GDPR) introduction, personal data is at the epicenter of today’s privacy requirements. Over 60 jurisdictions modern privacy and data protection laws to regulate personal data processing. 
Under the GDPR, any information relating to an identified or identifiable natural person is personal data. This inclusive definition encompasses both direct and indirect identifiers. Direct identifiers are information that explicitly identifies a person, such as a name, a social security number, or biometric data. Indirect, or quasi, identifiers refer to information that can be combined with additional data to identify a person. They are, for example, medical records, dates and places of birth, or financial information. 

 
Traditionally, a risk hierarchy existed between these two types of attributes. Direct identifiers were perceived as more “sensitive” than quasi-identifiers. In many data releases, only the former attributes were subject to some privacy protection mechanism, while the latter were released in clear. Such releases were often followed by prompt re-identification of the supposedly ‘protected’ subjects. It soon became apparent that quasi-identifiers could be just as ‘sensitive’ as direct identifiers. With the GDPR, this notion has finally made it into law: both types of attributes are put on the same level, identifiers and quasi-identifiers attributes are personal data and present an equally important privacy breach risk.
Nowadays protection laws strictly regulate personal data processing. This makes a strong case for implementing privacy protection techniques. Indeed, failure to comply exposes companies to severe penalties. Besides, implementing proper privacy protections might lead to customer trust increase. In a world plagued by data breaches and privacy violations, people are increasingly concerned about what happens to their data. And finally, data breaches targeting personal data are costing companies money. Personal data remains the most expensive item to lose in a breach.
 
Most often, it is personal data that we seek to protect. However, any sensitive data can require privacy protection. For example, any business information, such as financial information or trade secrets, can need to remain private. Protecting business or classified information is usually a matter of holding on to corporate value.
These elements could incite companies to decrease the amount of personal data they collect and process. However, information is at the core of product and service development for many companies. By deleting personal data, companies would pass on many of the benefits and values they could derive from it. For all these reasons, companies must find privacy protection mechanisms to remain data-driven while safeguarding individuals' privacy.  
 
Anonymization as means of privacy protection
 
A way to guarantee the privacy of personal and sensitive data is to anonymize it. Anonymization refers to the process of irreversibly transforming data to prevent the re-identification of individuals. Meaning that if a company releases an anonymized dataset, it’s theoretically impossible to re-identify a person from it, either directly or indirectly. Anonymization represents the highest form of privacy protection. However, perfect anonymity of data is rarely achieved, as it would render the data almost useless.
For some time, the middle ground has been to use lighter privacy protection mechanisms, mechanisms such as data masking or pseudonymization. These processes aim at protecting data by removing or altering its direct, sometimes indirect, identifiers. It's quite frequent to see the term ""anonymization"" in references to these methods. However, the two have clear legal and technical implications. 

 
On the technical side, these techniques produce data with different levels of privacy protection. Masking or pseudonymization techniques can be reversed. They complicate the identification of individuals but don't remove the possibility of re-identifying someone. They're a weaker privacy protection mechanism than anonymization, which minimizes to greater extent re-identification risks.
On the legal side, modern data privacy laws, such as the GDPR and the California Consumer Privacy Act (CCPA), differentiate between pseudonymization and anonymization processes. The latest is considered private enough that anonymized data isn’t subject to personal data protection laws anymore. Pseudonymized data, on the other hand, still represents a risk for individual privacy and must be handled as personal data. 
 
Privacy protection techniques
 
Different privacy protection techniques offer different protection levels. It is essential to know and understand how they work. Depending on the nature of the application and the data, you can choose one technique or another.
As mentioned above, pseudonymization, or data masking, is commonly used to protect data privacy. It consists of altering data, most of the time, direct identifiers, to protect individuals' privacy in the datasets. There are several techniques to produce pseudonymized data:

Encryption: hiding sensitive data using a cipher protected by an encryption key.
Shuffling: scrambling data within a column to disassociate its original other attributes.
Suppression: nulling or removing from the dataset the sensitive columns.
Redaction: masking out parts of the entirety of a column’s values.


 
Data generalization is another approach to protect privacy. The underlying idea is to prevent a person’s identification by reducing the details available in the data. There are, as well, various approaches to generalizing data. For example, it's possible to generate an aggregated value from the sensitive data or use a value range in place of a numerical value. Among data generalization techniques, we can find popular privacy models such as k-anonymity, l-diversity, t-closeness expressly designed to mitigate data disclosure risks. 

 
Synthetic data offers an alternative approach to privacy protection. Instead of altering or masking the original data, one generates completely new artificial data. Machine learning models help create this new dataset that mimics the statistical properties of the original data. 

 
 
The shortcomings of traditional data protection techniques
 
Data masking and generalization techniques are popular protection mechanisms. However, they present some drawbacks that it’s essential to know. 
Pseudonymization presents limitations when it comes to data privacy. In multiple instances, researchers proved that this technique could lead to re-identification and disclosure of individual' identities. Most of the time, when you mask or remove data identifiers, the quasi-identifiers are still present. They can link individuals across secondary data sources, ultimately disclosing people's identity and breaching privacy. 

 
On the other hand, data generalization methods are known to deteriorate the value of the data. For example, when values are highly aggregated, the resulting data loses its statistical granularity. This granularity could have brought value to analyses. Researchers showed that some generalization methods, such as k-anonymity, had privacy loopholes. 
Synthetic data allows finding a balance between the data privacy and data utility shortcomings of other methods. However, it can also present limitations and risks depending on the technology and the process used to generate the synthetic data. For example, some techniques can fail at properly mimicking data that holds unusual data points. Synthetic datasets can also suffer from privacy breaches when proper protections aren’t ensured. 
There are more and more opportunities to address the privacy shortcomings of traditional techniques. Technologies are showing increasing signs of maturity. The number of PrivacyTech companies is rising quickly, and the industry is quickly attracting large fundings. Public bodies are starting to promote privacy-preserving technology. For instance, the UK government acknowledged in its National Data Strategy that synthetic data and other privacy-enhancing technologies represent an opportunity for innovation. Leading industry analysts such as Gartner also recognizes the rise of these technologies. 
 
Bio: Elise Devaux (@elise_deux) is a tech enthusiast digital marketing manager, working at Statice, a startup specialized in synthetic data as a privacy-preserving solution.
Related:

How “Anonymous” is Anonymized Data?
10 Use Cases for Privacy-Preserving Synthetic Data
10 Steps for Tackling Data Privacy and Security Laws in 2020"
https://www.kdnuggets.com/2021/04/8-most-common-data-scientists.html,The 8 Most Common Data Scientists,"Admit it all you wanna-be, newbie, and old-old-school Data Scientists on the planet, whether you like it or not, you've probably behaved like one of these types. Or two. Or all eight.","comments
By JABDE, Journal of Astrological Big Data Ecology.

For those new to dealing with engineers and data scientists, it’s very tough to understand what flavor statistician you may be dealing with, how to manage them, and how much to trust them. Statistics can be closer to an art than a science. When it comes to the different types of statisticians, there are almost as varied as sexual identities. Many statisticians and analysts can have multiple identities, but they all generally follow the Dunning-Kruger Effect. Experience and confidence are not one-to-one relationships.
Here is a guide to the top 8 most common types of data scientists.
 
1. The Unashamed Frequentist
 
This type of statistician worships the p-value and is often the most confident of their answer. You can spot these analysts by a lack of predictive behavior and overuse of null hypothesis testing. This flavor of analyst will report what happened and not much else. While the unashamed frequentist is easy to understand and works the fastest, they are very boring and prone to accidental and sometimes intentional p-hacking.
 
2. The Data Bro
 
Ready for the new hotness of the most popular statistical library? The data bro typically does not have a heavy math background, but they know how to code by google. The easiest way to spot a data bro is by their flashy presentations, fancy visualizations, and their blind use of open-source statistics libraries. The data bro is fantastic at presentations and, under the right circumstances, can quickly turn around a beautifully intuitive data product. Unfortunately, the data bro is the most susceptible to using faulty online libraries developed by non-statisticians or misapplying statistical tests because the graph just looked too cool.
 
3. The Novice
 
When a statistician is new to the game, you can tell by their obsession with doing things by the book and an unwillingness to take guesses. The novice will spend a week researching the best performance metric to determine if their analysis is working. With no experience or intuition, their days are spent trying multiple new methods on one data set and writing a thesis until all possible options have been exhausted. They won’t be novices for long but will never be sure of themselves until they have more experience.
 
4. The Reacher
 
Ever want to design a neural network for a simple regression problem? The Reacher has. The Reacher will use the most complicated method possible for analysis for the sake of trying something cool. The Reacher is a high risk, medium to low reward analyst who will take way too long to create a simple data product by providing analysis that nobody asked for that will only be appreciated and understood by a niche audience.
 
5. The one-trick pony
 
Some tools are so great that with a little bit of creativity, you can solve just about any problem you may encounter. However, just because anything can be solved with a nonlinear algorithm doesn’t mean that it should. The one-trick pony has learned one of these adaptive methods very well. While having a complete understanding of the method, the one-trick pony may forget to check assumptions and misapply their favorite method. A screwdriver may not be meant to hammer a nail, but you can with enough force.
 
6. The Philosopher
 
If you ever ask a statistician what one of their metrics or results means only to receive a sermon on the different ways of interpreting results, you’re dealing with a philosopher. For the philosopher, everything is a figment of their imagination, and nothing is real. You can debate the meaning of one word with the philosopher for an hour only to discover that they don’t believe in words or that the word meant something else 200 years ago, and that’s the definition they use. None of their models are correct, but some of them are ‘useful’ as long as you understand the technical risk. The philosopher is similar to the novice in that they will never feel like they have confidently answered the question.
 
7. The Fake Bayesian
 
If you ever hear an analyst mention that the difference between Frequentists and Bayesians is that Bayesians win in Vegas, but you’ve never heard them talk about priors, you’re talking to a fake Bayesian. Nate Silver is their god. These analysts will religiously read FiveThirtyEight and assume that their methods are inherently better if they just use Bayes Theorem without realizing that nearly every statistical method employs Bayes Theorem. The fake Bayesian is a sign of a small amount of experience but is more prone to overconfidence.
 
8. The True Bayesian
 
I don’t know any of these types, so I’m going to have to get back to you. I assume they’re pretty cool people.
 
Original. Reposted with permission.
 
Related:

Cartoon: Data Scientist vs Data Engineer
Cartoon: Machine Learning takes a vacation
Cartoon: What Else Can AI Guess From Your Face?"
https://www.kdnuggets.com/2021/06/data-storytelling.html,"Data storytelling: brains are built for visuals, but hearts turn on stories","Today, we need much more than just numbers about our organization to understand, gain insights, and take relevant actions. While visualizations of the data are important, making an emotional connection with the stories behind the data is key. If you want to sell a story, send a missile to the heart.","comments
By Hrvoje Smolic, Founder at Graphite Note, Qualia Data Sciences and Qualia d.o.o..
 
Yes, the charts are important...
 
For every executive who is about to pull up the first slide of a presentation in a boardroom filled with eager associates, there is usually that moment just before speaking when nerves fray, last-minute doubts fill the mind, and fear of failure can weaken the resolve. Did I choose the right image to start with? Is the text too small to read? Is the right tone conveyed throughout the presentation? The usual recommendations of how best to present data most effectively crowd into the mind — don’t clutter, choose relevant KPIs, keep it simple, choose layouts carefully, less is more, and be cautious with colors.

Source: Image by Nong Vang, unsplash.com.
 
...but the key is emotion.
 
These are all helpful ideas, but none of them can hold a candle to the most important concept of all, emotion.
You see, whereas brains are built for visuals, hearts turn on stories.

Visualization might be important, but emotion is key. If you want to sell a story, send a missile to the heart.

Don’t get me wrong, I believe in the power of graphs and charts. Stacked columns, line charts, waterfall, and scatter plots all have their place in data presentation. However, at its core, data visualization is a tool that transforms data into actionable insight by using it to tell a story.

Charts can be the start, not the end of the communication.

The psychologist Jerome Bruner claims that we are 22 times more likely to remember a fact when it has been wrapped in a story. Data-driven storytelling is a powerful force as it takes stats and metrics and puts them into context through a narrative that everyone inside or outside of an organization can grasp.
Without question, human beings are, first and foremost, visual creatures. Our brains are built for visual information, as the following data shows:

90% of the information processed by the brain is visual.
It takes only 13 milliseconds for the human brain to process an image.
The human brain processes images 60,000 times faster than text.
80% of people remember what they see, compared to ten percent what they hear and 20 percent of what they read.
In response to a recent survey, 95% of B2B buyers said that they wanted shorter and highly visual content.
Publishers that feature visual content grow traffic 12 times faster than those who don’t.

However, as anyone who has sat in a darkened movie theater and felt a warm tear roll down her cheek knows, it is the emotion that makes life’s moments unforgettable.

Source: Image by Robina Weermeijer, unsplash.com.
 
The use of emotion can make data presentation unforgettable
 
As Carl Bucher once stated, “They may forget what you said, but they will never forget how you made them feel.”
“Out of clutter, find simplicity,” said Albert Einstein. This is a great motto for presenters. Simple messages resonate deeply if extraneous items are being stripped away.

Emotions cut through the clutter like nothing else.

Before putting any design elements in place, think about the end goal. What are the most important elements that need to be showcased? Who is the audience? What is the emotional punch you want to be conveyed on the final slide? To build successful dashboards, a designer needs to put himself in the audience’s shoes.
Designers should always try to provide maximum information. Without the right context, numbers that might seem extremely obvious to one person might be perplexing to others.
All the axes should be named, and titles should be added to all charts. Comparison values should also be included. The rule of thumb here is to use the most common comparisons, for example, comparison against a set target, against a preceding period, or against a projected value.
Comparisons also add emotion as they provide benchmarks of understanding.
It is easy to recognize the emotional power of a line chart that reveals sales are falling off a cliff—or taking off into the ether.
The artist Kenneth Noland once said, “For me, context is the key — from that comes the understanding of everything.”

Without providing context, it’s impossible to know whether numbers are good or bad, typical or atypical.

Without comparison values, numbers on a dashboard are meaningless for viewers. And more importantly, users won’t know if any action is warranted.

Source: Image by Author.
 
Design: Intelligence Made Visible
 
Dashboard design best practices concern more than just good metrics and well-thought-out charts. The second step of dashboard design is the placement of charts on a dashboard. If your dashboard is well organized visually, information can easily be found.
Poor layout forces users to think more as they try to grasp a point the presenter is trying to make. Nobody likes to look for data in a jungle of charts and a maze of tangled numbers.
The general rule is that the key information should be displayed first — on the top of the screen, upper left-hand corner. Most cultures read from left to right, then from top to bottom, which means viewers intuitively look at the upper-left part of a page first and read down from there.
“Color is a power which directly influences the soul,” states the famous Russian artist Wassily Kandinsky. Who can argue with one of the greatest painters and art theorists of the 20th century? Color is used by advertisers everywhere to convey an emotional message that resonates deeply within any audience. Without a shadow of a doubt, the use of color is one of the most important best practices in dashboard design.
 
Graphite Note Proposition
 
We propose data analytics to “go vertical,” so there will be much more natural, easier to explain insights and conclusions by design.

Source: Graphite Note Notebook.
You have a title text block, graph block, predictive analytics block, explanation text block, several graph blocks, etc. A notebook-style data analysis.
This will allow data analysts to explain the whole process beautifully, tell a data story, and engage the audience (your team). And that will, in turn, supercharge decision-making because decisions are made on an emotional level, where the story hits.
 
Data Storytelling
 
Of course, you don’t need to go all Joseph Campbell Hero of a Thousand Faces with your presentations. Charting a mythical story filled with a mixture of heroes, mentors, tricksters, shapeshifters, guardians, and shadows isn’t the goal, but recognizing how myth can turn each story point — or chart— is. Understanding the power of the story adds a subtextual level to any data presentation that, although not recognized by an audience, will pull on their emotions.
Dale Carnegie once said, “There are always three speeches for every one you actually gave. The one you practiced, the one you gave, and the one you wish you gave.” Utilizing proper data presentation techniques and wrapping emotion into the power of storytelling will help ensure that the speech you give is not separated into three but combined into one.
Above all else, it will be the speech you had wished you given, and you won’t suffer pangs of regret or wishful thinking afterward.
Emotion is a tricky element to include, and it must always, always, always be united with the truth. However, if you can wrap your data presentation in a compelling story, you’ll not only touch your audience’s heart but also have them eating out of the palm of your hand.
Original. Reposted with permission.
 
Related:

Telling a Great Data Story: A Visualization Decision Tree
Why data analysts should choose stories over statistics
How to frame the right questions to be answered using data"
https://www.kdnuggets.com/2021/01/graph-theory-why-care.html,"What is Graph Theory, and Why Should You Care?",Go from graph theory to path optimization.,"comments
By Vegard Flovik


A graph visualization issued from the Opte Project, a tentative cartography of the Internet

 
Graph theory might sound like an intimidating and abstract topic to you, so why should you even spend your time reading an article about it? However, although it might not sound very applicable, there are actually an abundance of useful and important applications of graph theory! In this article, I will try to explain briefly what some of these applications are. In doing so, I will do my best to convince you that having at least some basic knowledge of this topic can be useful in solving some interesting problems you might come across.
In this article, I will through a concrete example show how a route planning/optimization task can be formulated and solved using graph theory. More specifically, I will consider a large warehouse consisting of 1000s of different items in various locations/pickup points. The challenge here is, given a list of items, which path should you follow through the warehouse to pickup all items, but at the same time minimize the total distance traveled? For those of you familiar with these kind of problems, this has quite some resemblance to the famous traveling salesman problem. (A well known problem in combinatorial optimization, important in theoretical computer science and operations research).
As you might have realized, the goal of this article is not to give a comprehensive introduction to graph theory (which would be quite a tremendous task). Through a real-world example, I will rather try to convince you that knowing at least some basics of graph theory can prove to be very useful!
I will start with a brief historical introduction to the field of graph theory, and highlight the importance and the wide range of useful applications in many vastly different fields. Following this more general introduction, I will then shift focus to the warehouse optimization example discussed above.
 
The history of Graph Theory
 
The basic idea of graphs were first introduced in the 18th century by the Swiss mathematician Leonhard Euler, one of the most eminent mathematicians of the 18th century (and of all time, really). His work on the famous “Seven Bridges of Königsberg problem”, are commonly quoted as origin of graph theory.
The city of Königsberg in Prussia (now Kaliningrad, Russia) was set on both sides of the Pregel River, and included two large islands — Kneiphof and Lomse — which were connected to each other, or to the two mainland portions of the city, by seven bridges (as illustrated in the below figure to the left). The problem was to devise a walk through the city that would cross each of those bridges once and only once.
Euler, recognizing that the relevant constraints were the four bodies of land & the seven bridges, drew out the first known visual representation of a modern graph. A modern graph, as seen in bottom-right image, is represented by a set of points, known as vertices or nodes, connected by a set of lines known as edges.


Credit: Wikipedia

 
This abstraction from a concrete problem concerning a city and bridges etc. to a graph makes the problem tractable mathematically, as this abstract representation includes only the information important for solving the problem. Euler actually proved that this specific problem has no solution. However, the difficulty he faced was the development of a suitable technique of analysis, and of subsequent tests that established this assertion with mathematical rigor. From there, the branch of math known as graph theory lay dormant for decades. In modern times, however, it’s applications are finally exploding.
 
Introduction to Graph Theory
 
As mentioned previously, I do not aim to give a comprehensive introduction to graph theory. The following section still contains some of the basics when it comes to different kind of graphs etc., which is of relevance to the example we will discuss later on path optimization.
Graph Theory is ultimately the study of relationships. Given a set of nodes & connections, which can abstract anything from city layouts to computer data, graph theory provides a helpful tool to quantify & simplify the many moving parts of dynamic systems. Studying graphs through a framework provides answers to many arrangement, networking, optimization, matching and operational problems.
Graphs can be used to model many types of relations and processes in physical, biological, social and information systems, and has a wide range of useful applications such as e.g.

Finding communities in networks, such as social media (friend/connection recommendations), or in the recent days for possible spread of COVID19 in the community through contacts.
Ranking/ordering hyperlinks in search engines.
GPS/Google maps to find the shortest path home.
Study of molecules and atoms in chemistry.
DNA sequencing
Computer network security
….. and many more….



A simple example of a graph with 6 nodes

 


A slightly more complex social media network. Credit: Martin Grandjean Wikimedia

 
As mentioned, there are several types of graphs that describe different kind of problems (and the constraints within them). A nice walk-through of various types of graphs can also be found in a previous article by Kelvin Jose, and the below section represents a subset of that article.
 
Types of Graphs
 
There are different types of graph representations available and we have to make sure that we understand the kind of graph we are working with when programmatically solving a problem which includes graphs.

Undirected Graphs

As the name shows, there won’t be any specified directions between nodes. So an edge from node A to B would be identical to the edge from B to A.

In the above graph, each node could represent different cities and the edges show the bidirectional roads.

Directed Graphs (DiGraphs)

Unlike undirected graphs, directed graphs have orientation or direction among different nodes. That means if you have an edge from node A to B, you can move only from A to B.


Credit: WikiMedia

 
Like the previous example, if we consider nodes as cities, we have a direction from city 1 to 2. That means, you can drive from city 1 to 2 but not back to city 1, because there is no direction back to city 1 from 2. But if we closely examine the graph, we can see cities with bi-direction. For example cities 3 and 4 have directions to both sides.

Weighted Graphs

Many graphs can have edges containing a weight associated to represent a real world implication such as cost, distance, quantity etc …


Credit: Estefania Cassingena Navone via freecodecamp.org

 
Weighted graphs could be either directed or undirected graph. The one we have in this example is an undirected weighted graph. The cost (or distance) from the green to the orange node (and vice versa) is 3. Like our previous example, if you want to travel between two cities, say city green and orange, we would have to drive 3 miles. These metrics are self-defined and could be changed according to the situations. For a more elaborated example, consider you have to travel to city pink from green. If you look at the city graph, we can’t find any direct roads or edges between the two cities. So what we can do is to travel via another city. The most promising routes would be starting from green to pink via orange and blue. If the weights are costs between cities, we would have to spend 11$ to travel via blue to reach pink but if we take the other route via orange, we would only have to pay 10$ for the trip.
There may be several weights associated with each edge, including distance, travel time, or monetary cost. Such weighted graphs are commonly used to program GPS’s, and travel-planning search engines that compare flight times and costs.
 
Graph Theory → Route optimization
 
Having (hopefully) convinced you that graph theory is worth knowing something about, it is now time to focus on our example case of route planning when picking items in our warehouse.
 
Challenge:
 
The challenge here is that given a “picking list” as input, we should find the shortest route that passes all the pickup points, but also complies to the restrictions with regard to where it is possible/allowed to drive. The assumptions and constraints here are that crossing between corridors in the warehouse is only allowed at marked “turning points”. Also, the direction of travel must follow the specified legal driving direction for each corridor.
 
Solution:
 
This problem can be formulated as an optimization problem in graph theory. All pickup points in the warehouse form a “node” in the graph, where the edges represent permitted lanes/corridors and distances between the nodes. To introduce the problem more formally, let us start from a simplified example.
The graph below represents 2 corridors with 5 shelves/pickup-points per corridor. All shelves are here represented as a node in the graph, with an address ranging from 1–10. The arrows indicate the permitted driving direction, where the double arrows indicate that you can drive either way. Simple enough, right?

Being able to represent the permitted driving routes in the form of a graph, means that we can use mathematical techniques known from graph theory to find the optimal “driving route” between the nodes (i.e., the stock shelves in our warehouse).
The example graph above can be described mathematically through an «adjacency matrix». The adjacency matrix to the right in the below figure is thus a representation of our «warehouse graph», which indicates all permitted driving routes between the various nodes.


Example 1: You are allowed to travel from node 2 → 3, but not from 3 → 2. This is indicated by the “1” in the adjacency matrix to the right.
Example 2: You are allowed to go from both node 8 → 3, and from 3 → 8, again indicated by the “1”`s in the adjacency matrix (which in this case is symmetric when it comes to travel direction).

 
Back to our warehouse problem:
 
A real warehouse is of course bigger and more complex than the above example. However, the main principles of how to represent the problem through a graph remains the same. To make the real problem slightly simpler (and more visually suitable for this article), I have reduced the total number of shelves/pickup-points (approximately every 50th shelf included, marked with black squares in the below figure). All pickup points are given an address (“node number”) from 1–74. The other relevant constraints mentioned earlier, such as permitted driving directions in each of the corridors, as well as the allowed “turning points” and shortcuts between the corridors are also indicated in the figure..


Graph representation of our simplified warehouse

 
The next step is then to represent this graph in the form of a adjacency matrix. Since we are here interested in finding both the optimal route and total distance, we must also include the driving distance between the various nodes in the matrix.


Adjacency matrix for the “warehouse graph”

 
This matrix indicates all constraints with regard to both the permitted direction of travel, which “shortcuts” are permitted, any other restrictions as well as the driving distance between the nodes (illustrated through the color). As an example, the “shortcut” between nodes 21 and 41 shown in the graph representation can clearly be identified also in the adjacency matrix. The “white areas” of the matrix represents the paths that are not allowed, indicated through an “infinite” distance between those nodes.
 
From graph representation to path optimization
 
Just having an abstracted representation of our warehouse in the form of a graph, does of course not solve our actual problem. The idea is rather that through this graph representation, we can now use the mathematical framework and algorithms from graph theory to solve it!
Since graph optimization is a well-known field in mathematics, there are several methods and algorithms that can solve this type of problem. In this example case, I have based the solution on the “Floyd-Warshall algorithm”, which is a well known algorithm for finding shortest paths in a weighted graph. A single execution of the algorithm will find the lengths (summed weights) of shortest paths between all pairs of nodes. Although it does not return details of the paths themselves, it is possible to reconstruct the paths with simple modifications to the algorithm.
If you give this algorithm as input a “picking order list” where you go through a list of items you want to pick, you should then be able to obtain the optimal route which minimize the total driving distance to collect all items on the list.
Example: Let us start by visualizing the results for a (short) picking list as follows: Start from node «0», pick up items at location/node 15, 45, 58 and 73 (where these locations are illustrated in the figure below). The algorithm finds the shortest allowable route between these points through calculating the “distance matrix”, D, which can then be used to determine the total driving distance between all locations/nodes in the picking list.

Step 1: D[0][15] → 90 m
Step 2: D[15][45] →52 m
Step 3: D[45][58] → 34 m
Step 4: D[58][73] → 92 m

Total distance = 268m


Optimized driving route from picking list

 
Have tested several “picking lists” as input and verifying the proposed driving routes and calculated distance, the algorithm has been able to find the optimal route in all cases. The algorithm respects all the imposed constraints, such as the permitted direction of travel, and uses all permitted “shortcuts” to minimize the total distance.
 
From path optimization to useful insights
 
As shown through the above example, we have developed an optimization algorithm that calculates the optimal driving route via all points on a picking order list (for a simplified version of the warehouse). By providing a list of picking orders as input, one can thus relatively easily calculate statistics on typical mileage per. picking order. These statistics can then also be filtered on various information such as item type, customer, date, etc. In the following section, I have thus picked a few examples on how one can extract interesting statistics from such a path optimization tool.
In doing this, I first generated 10.000 picking order lists where the number of items per list ranges from 1–30 items, located at random pickup points in the warehouse (address 3–74 in the figure above). By performing the path optimization procedure over all these picking list, we can then extract some interesting statistics.
Example 1: Calculate mileage as a function of the number of units per. picking order list. Here, you would naturally assume that the total mileage increases the more items you have to pick. But, at some level, this will start to flatten out. This is due to the fact that one eventually has to stop by all the corridors in the warehouse to pick up goods, which then prevents us from making use of clever “shortcuts” to minimize the total driving distance. This tendency can be illustrated in the figure below to the left, which illustrates that for more than approximately 15–20 units per picking order, adding extra items does not make the total mileage much longer (as you have to drive through all corridors of the warehouse anyway). Note that the figures show a “density plot” of the distribution of typical mileage per. picking orders list.
Another interesting statistic, which shows the same trend, is the distribution of driving distance per picked item in the figure to the right. Here, we see that for picking lists with few items, the typical mileage per. item is relatively high (with a large variance, depending on how “lucky” we are with some items being located in the same corridor etc.). For picking lists with several items though, the mileage per. item is gradually decreasing. This type of statistic can thus be interesting to investigate closer, in order to optimize how many items each picking order list should contain in order to minimize the mileage per picked item.


Estimating driving distance per list/item vs. number of items per list.

 
Example 2: Here I have used real-world data that also contains additional information in the form of a customer ID (here shown for only two customers). We can then take a closer look at the distribution in mileage per. picking order list for the two customers. For example, do you typically have to drive longer distances to pick the goods of one customer versus another? And, should you charge that customer extra for this additional cost?
The below figure to the left shows the distribution in mileage for «Customer 1» and «Customer 2» respectively. One of the things we can interpret from this is that for customer 2, most picking order lists have a noticeably shorter driving distance compared to customer 1. This can also be shown by calculating the average mileage per. picking order list for the two customers (figure to the right).

This type of information can e.g. be used to implement pricing models where the product price to the customer is also based on mileage per order. For customers where the order involves more driving (and thus also more time and higher cost) you can consider invoicing extra compared to orders that involve short driving distances.
 
Summary:
 
In the end, I hope I have convinced you that graph theory is not just some abstract mathematical concept, but that it actually has many useful and interesting applications! Hopefully, the examples above will be useful for some of you in solving similar problems later, or at least satisfy some of your curiosity when it comes to graph theory and some of its applications.
The cases discussed in the article covers just a few examples that illustrate some of the possibilities that exist. If you have previous experience and ideas on the topic, it would be interesting to hear your thoughts in the comments below!
Did you find the article interesting? If so, you might also like some of my other articles on topics such as AI, Machine Learning, physics, etc., which you can find in the links below and on my medium author profile: https://medium.com/@vflovik

Deep learning based reverse image search for industrial applications
Deep Transfer Learning for Image Classification
Building an AI that can read your mind
The hidden risk of AI and Big Data
How to use machine learning for anomaly detection and condition monitoring
How (not) to use Machine Learning for time series forecasting: Avoiding the pitfalls
How to use machine learning for production optimization: Using data to improve performance
How do you teach physics to AI systems?
Can we build artificial brain networks using nanoscale magnets?
Artificial Intelligence in Supply Chain Management: Utilizing data to drive operational performance

I also discuss various topics related to AI/machine learning in the workshop presentation below: “ From hype to real-world applications”. I hope you found these resources interesting and useful!
 
Bio: Vegard Flovik is working on machine learning and advanced analytics at Kongsberg Digital.
Original. Reposted with permission.
Related:

Graph Representation Learning: The Free eBook
Building a Deep Learning Based Reverse Image Search
How (not) to use Machine Learning for time series forecasting: The sequel"
https://www.kdnuggets.com/2021/09/openai-codex-challenges.html,Behind OpenAI Codex: 5 Fascinating Challenges About Building Codex You Didn’t Know About,Some ML engineering and modeling challenges encountering during the construction of Codex.,"By Jesus Rodriguez, Intotheblock.
comments


Source: https://bdtechtalks.com/2021/07/15/openai-codex-ai-programming/

 
A couple of weeks ago, OpenAI astonished the artificial intelligence(AI) world with the release of Codex, a massive model that can translate natural language into code. Codex can effectively generate end to end from basic language instructions. If you don’t believe me, you should watch this video which can be considered one of the best AI demos of all time 😉


Video Credit: OpenAI

 
A lot has been written about Codex’s capabilities since its initial launch.
However, I have been more intrigued by the small requirements that become incredibly relevant to build a model of this magnitude. Deep diving into Codex, there are a few interesting things I found that thought would be good to highlight:
 
1. Codex is proficient in about a dozen languages but it was trained for Python
 
I found this incredibly insightful. The original goal of OpenAI was to make Codex proficient in Python but it turns out that the model picked up other languages during the pretraining process. This speaks to the unique capabilities of language pretrained models.
 
2. Testing Codex’s was more than tricky
 
The AI community has been amazed by the research behind Codex but I think the engineering side has been as impressive. One aspect that I was particularly intrigued about was the testing part. How in the world do you test live code without taking massive risks. It turns out that the OpenAI team put a ton of work building very sophisticated sandboxes to test the outputs from Codex in isolation.
 
3. Matching semantics to code is far from trivial
 
Training a model in all the source code in the world sounds cool but its far from trivial. After all, not all code is created equal. Code in Github can be poorly documented while notebooks can have rich semantic information. Similarly, code snippets in Stack Overflow have richer levels of semantic information. Mapping code sections to language semantics was one of the challenges of building Codex.
 
4. Codex still struggles with task decomposition
 
If you think how programmers work, we tend to decompose a problem into smaller tasks and produce code for those. It turns out that Codex is great at the latter but still struggles in problem decomposition tasks. This shouldn’t be surprising if we think that problem decomposition requires very complex cognitive skills.
 
5. Supervised Fine-Tuning was a huge part of building Codex
 
Code in the internet appears in all sorts of levels of completeness, documentation, syntactic richness etc. Training a model in such a diverse code sets can produce unreliable results. In that sense OpenAI had to undergo a massive supervised fine-tuning effort.
 
These are some of the aspects about Codex that are not super well-known but that have been major contributors to the success of the first version of the model. Codex success was both due to advanced ML research as a massive ML engineering and infrastructure efforts.
 
Bio: Jesus Rodriguez is currently a CTO at Intotheblock. He is a technology expert, executive investor and startup advisor. Jesus founded Tellago, an award winning software development firm focused helping companies become great software organizations by leveraging new enterprise software trends.
Original. Reposted with permission.
Related:

GitHub Copilot Open Source Alternatives
Jurassic-1 Language Models and AI21 Studio
How to Train a BERT Model From Scratch"
https://www.kdnuggets.com/2021/05/dataops-5-things-need-know.html,DataOps: 5 things that you need to know,DataOps (Data Operations) has assumed a critical role in the age of big data to drive definitive impact on business outcomes. This process-oriented and agile methodology synergizes the components of DevOps and the capabilities of data engineers and data scientists to support data-focused workloads in enterprises. Here is a detailed look at DataOps.,"comments
By Sigmoid
 
1. What is DataOps?
 
In simple terms, DataOps can be defined as a methodology that offers speed and agility to data pipelines, thereby enhancing the quality of data and delivery practices. DataOps enables greater collaboration within organizations and drives data initiatives at scale. With the help of automation, DataOps improves the availability, accessibility, and integration of data. It brings people, processes, and technology together to deliver reliable and high-quality data to all stakeholders. Rooted in the agile methodology, DataOps aims at offering optimal consumer experience by continuous delivery of analytic insights.
 
2. How is DataOps different from DevOps?
 
DataOps is often considered DevOps applied to data analytics. However, DataOps is more than just that. It also combines key capabilities of data engineers and data scientists to offer a robust, process-driven structure to data-focused enterprises. DevOps combines software development and IT operations to ensure continuous delivery in the systems development lifecycle. Whereas, DataOps also brings niche capabilities of key contributors in the data chain – data developers, data analysts, data scientists, and data engineers – to ensure greater collaboration in the development of data flows.  Also, while comparing DataOps and DevOps, it is worth noting that DevOps focuses on transforming the delivery capability of software development teams whereas DataOps emphasizes the transformation of analytics models and intelligence systems with the help of data engineers.
 
3. Why is DataOps integral to data engineering?
 
Data engineers play an important role in ensuring that data is properly managed across the entire analytics trail. Additionally, they are entrusted with the responsibility of optimal use and safety of data.  DataOps help facilitate the key functional areas of data engineers by enabling them with end-to-end orchestration of tools, data, codes, and organizational data environment. It can boost the collaboration and communication within the teams to adapt with evolving customer needs. In simple terms, DataOps strengthens the hands of data engineers by offering greater collaboration between various data stakeholders and helping them achieve reliability, scalability, and agility.
 
4. What role do DataOps engineers play in enabling advanced enterprise analytics?
 
Now, the role of a DataOps engineer is slightly different from that of a data engineer. The DataOps engineer meticulously defines and manages the environment in which the data is developed. The role also includes offering guidance and design support to data engineers around workflows. As far as advanced enterprise analytics is concerned, DataOps engineers play a significant role in automating data development and integration. With their in-depth knowledge of software development and agile methodologies, DataOps engineers contribute to enterprise analytics by tracking document sources through metadata cataloging as well as building metric platforms to standardize calculations. Some of the key roles of a DataOps engineer are:

Test automation
Creation of code repositories
Framework orchestration
Collaboration and workflow management
Lineage and impact analysis
Data preparation and integration


 
5. What are the popular technology platforms commonly used by DataOps teams?
 
It is fair to say that DataOps is still an evolving discipline and data-focused organizations are learning more about it every day. However, with technological innovations, a number of platforms have already made a mark and are growing their impact across the industry. Here are some of the popular platforms used by DataOps teams.

Kubernetes 
Kubernetes is an open-source orchestration platform that allows companies to combine multiple docker containers into a single unit. This makes the development process much faster and simple. Kubernetes can help teams manage data scheduling on nodes within a single cluster, simplify workloads and categorize containers into logical units for easy discovery and management.
ELK (Elasticsearch, Logstash, Kibana)
The ELK Stack of Elastic is a widely preferred log management platform which comprises three distinct open-source software packages – Elasticsearch, Logstash and Kibana. While Elasticsearch is a NoSQL database that runs on Lucene search engine, Logstash is a log pipeline solution that accepts data inputs from multiple sources and performs data transformations. Kibana on the other hand, is essentially a visualization layer which operates on top of elastic search.
Docker
The Docker platform is often regarded as the simplest and straight forward tool which can help companies scale high-end applications using containers and securely run them on the cloud. Security is one of the key aspects that differentiates the platform as it provides a secure environment for testing and execution.
Git 
Git is a leading version-control application that allows companies to effectively manage and store version updates in data files. It can control and define a particular data analytics pipeline such as source code, algorithms, HTML, parameter files, configuration files, containers and logs. Since these data artefacts are simply source code, Git makes them easily discoverable and manageable.
Jenkins 
Jenkins is an open-source server-based application which helps companies seamlessly orchestrate a range of activities to achieve continuous automated integration. The platform supports the end-to-end development lifecycle of an application from development to deployment while speeding up the process through automated testing.
Datadog
Datadog is an open-source cloud monitoring platform that facilitates full visibility across an application stack by allowing companies to monitor metrics, traces and logs through a unified dashboard. The platform comes with around 400 built-in integrations and a predefined dashboard that simplifies the process.

 
Rounding Up
 
With time the complexity and scale of enterprise AI and ML applications will only increase, giving rise to the need of convergence in data management practices. In order to meet customer demands and deploy applications faster, organizations need to reconcile data cataloging and accessibility functions while ensuring integrity. And, this is where DataOps practices can help companies create a difference.
 
Bio: Jagan is a Dev Ops evangelist who leads the Dev Ops practice at Sigmoid. He has instrumental in maintaining and supporting highly critical data systems for clients across CPG, Retail, AdTech, BFSI, QSR and Hi-Tech verticals.
Original. Reposted with permission.
Related:

Model Experiments, Tracking and Registration using MLflow on Databricks
Data Scientist, Data Engineer & Other Data Careers, Explained
Why You Should Consider Being a Data Engineer Instead of a Data Scientist"
https://www.kdnuggets.com/2021/01/cloud-data-warehouse-future-data-storage.html,Cloud Data Warehouse is The Future of Data Storage,"Today, cloud data storage accounts for 45% of all enterprise data and by Q2 2021, that number could grow to 53%. Now is the time to embrace cloud than now.","comments
By Nitin Kumar, Sigmoid

 
Big data storage with cloud – doubling down on the opportunities
 
The last decade has seen the rapid growth of cloud adoption rates across industries. Today, cloud data storage accounts for 45% of all enterprise data and by Q2 2021, that number could grow to 53%.
The writing’s on the wall – given the direction in which the industry is moving, there’s no better time to embrace cloud than now. And, one thing that is going to play a critical role in enterprise cloud transition strategy is cloud storage or a cloud data warehouse.

 
The advantages of a cloud data warehouse
 
The ability to integrate data from multiple channels allows organizations to effectively leverage Business Intelligence (BI) tools and derive meaningful insights. It’s important to remember that BI tools are highly limited in their data preparation capabilities. Cloud data warehouses can be a game changer here as they help BI tools leverage the wealth of reliable and correctly structured data to generate actionable business insights.
The present-day cloud data warehouse (CDW) forms the core of the data analytics architecture. Having a data warehouse on the cloud enables businesses to store very large amounts of data sustainably and gain several advantages by leveraging advanced data analytics. These include:

Cost reduction: Providers take care of hardware, upgrades, maintenance, and outage management, which makes cloud less expensive than on-premise infrastructure.
Data security: A single point of access simplifies the cloud data security conundrum, making it arguably safer than on-premise data storage. It also allows the integration of additional safety measures such as VPNs and cloud encryption.
Reliability: Leading cloud warehouse providers like Amazon, Microsoft, and Google report 99.99% uptime. Cloud promises to make anywhere and anytime access to services, tools and data a reality. This level of service and reliability enhances customer experiences and provides a robust platform for business continuity.
Scalability and enhanced accessibility:  One of the defining features of cloud infrastructure is its vertical and horizontal scalability. It is ready to meet the data requirements as per organizational demands removing the data volume restrictions effectively.


Moreover, efficient data integration and data governance facilitates seamless data management, which in turn enhances data accessibility. Businesses that work with massive data for quick decision making, have achieved benefits including faster access to data and reduced infrastructure cost. Considering these features, it is safe to say that cloud data warehouses will define how organizations access and leverage Business Intelligence (BI) going forward.
But even as we venture into a cloud-essential future, enterprises have their work cut out for them. They must fortify their cloud strategies and conduct a thorough cloud readiness assessment. One of the first steps to achieve that is facilitating the seamless migration of data to a cloud data warehouse.
 
Data migration: how to go about it?
 
The benefits of cloud data warehouses make them an indispensable driver of digital transformation. But how do enterprises go about migrating business data to a cloud infrastructure from a legacy environment? There is no one right way to develop an effective migration strategy, but there are a few essential steps that enterprises need to take:

Determining the type of data storage: It is important to understand that the enterprise data transformation journey is a gradual process. There are two parts to this preliminary step:

Creating data lake or data warehouse: One of the most fundamental aspects of choosing a cloud storage destination is to understand the differences between a data lake and a data warehouse. Even though the terms are often used interchangeably, there are vast differences in terms of structure and purpose. A data lake is a vast pool of unstructured data, the purpose of which is not yet determined. A data warehouse on the other hand, is a repository for structured and filtered data stored with a definite purpose.
The data stored within data lakes are highly accessible, whereas it is more complicated and cost intensive to change or update the data within data warehouses. Moreover, since the data within data lakes is unprocessed, it requires specialized tools and data scientists to make good use of it. Organizations need to account for these differences and choose what suits their purpose. In many cases, organizations need to choose both.

Choosing the right cloud provider: When it comes to choosing the right cloud warehouse provider, the options are plenty. And each provider has their pros and cons. Businesses need to identify the cloud warehouse that uniquely matches their specific requirements. This is an important aspect of selecting and operationalizing cloud migration services. Organizations can try migrating a small data set to multiple cloud warehouses to determine which options work best for them, both in terms of performance and cost. Doing so can give them a better perspective in terms of congruity.
Copying all existing data: This step depends on the sheer volume of the data they have. But it is also important to consider the data schema and format before the final transfer. The schema must be transferred before loading and used while setting up the replication process.
Enabling continuous replication: The next step is setting up data synchronization. This can be done either manually or by using data pipeline services to manage schema and data replication. This is a critical step since the rest of the components can only be transferred once the synchronization is secure.
Building the analytics infrastructure and data application migration: Business Intelligence and analytics infrastructure setup should follow once the migration pipeline is created. This is a relatively low risk task. However, legacy applications may pose a unique challenge. In some cases, ensuring a smooth transition and an optimum performance involves replacing ODBC drivers, rewriting queries, or even changing the data model.
Migrating transformations: The final step is to recreate the transformations to produce final data models in the new cloud ecosystem. It is recommended that enterprises follow the ELT (extract, load, transform) model of transfer as an alternative to ETL (extract, transform, load) to accelerate data processing in a cloud environment.


 
Accelerating business growth with cloud warehouses
 
Cloud data warehouse adoptions are growing at a CAGR of nearly 15%. To keep up, business leaders must recalibrate their cloud strategy and leverage the benefits of an ever-expanding cloud ecosystem through cloud warehouses. This will allow them to harness the power of business intelligence and embrace emerging technology and trends like edge computing and AI/ML. This, in turn, will boost their capability and preparedness to enter new markets, and promises a significant business advantages.
The recent economic turmoil of the pandemic emphasizes the need for alternatives to legacy systems. It has become imperative for businesses to embrace cloud consulting and move to cloud warehouses. This will help them in steadying the ship in the short-term while targeting sustained growth and expansion in the long-term.
 
Bio: Nitin Kumar is Engineering Manager at Sigmoid and has a decade of experience working with Big Data technologies. He is passionate about solving business problems across Banking, CPG, Retail, and QSR domains through his expertise in open source and cloud technologies.
Original. Reposted with permission.
Related:

Meet whale! The stupidly simple data discovery tool
Feature Store vs Data Warehouse
4 Myths of Big Data and 4 Ways to Improve with Deep Data"
https://www.kdnuggets.com/2021/07/top-blogs-rewards-jun.html,KDnuggets Top Blogs Rewards for June 2021,These top blogs were winners of KDnuggets Top Blog Rewards Program for June: 5 Tasks To Automate With Python; Data Scientists Will be Extinct in 10 Years; How to Generate Automated PDF Documents with Python; How I Doubled My Income with Data Science and Machine Learning; Pandas vs SQL: When Data Scientists Should Use Each Tool; Top 10 Data Science Projects for Beginners.,"By Gregory Piatetsky, KDnuggets.
KDnuggets Top Blog Rewards Program now has the winners for the month of June - congratulations to all winners below! Here are the top 6 blogs published in June 2021 whose authors will share the $2,000 (USD) reward amount: 

 5 Tasks To Automate With Python, by Dylan Roy
 Data Scientists Will be Extinct in 10 Years, by Michael Mew
 How to Generate Automated PDF Documents with Python, by Mohammad Khorasani
 How I Doubled My Income with Data Science and Machine Learning, by Terence Shin
 Pandas vs SQL: When Data Scientists Should Use Each Tool, by Matthew Przybyla
 Top 10 Data Science Projects for Beginners, by Natassha Selvaraj

But don't give up on Data Science yet because of the popularity of the opinion blog that Data Scientists will be extinct in 10 years.
In the latest KDnuggets Poll, a majority of respondents were optimistic about Data Scientist prospects - see Relax! Data Scientists will not go extinct in 10 years, but the role will change.
We started the rewards program to encourage more high-quality and especially original (unpublished) contributions to KDnuggets.
For each month, beginning in May 2021, we will determine 6 most viewed blogs published on KDnuggets that month (excluding blogs by KDnuggets Editors Matthew Mayo and Gregory Piatetsky), and will distribute $2,000 (USD) among the authors of these top blogs. 
While we appreciate and publish the reposts, the goal of this program is to get more great original (not published previously) articles, which will be rewarded at the rate of 3X the reposts. 
Note: An original submission first published on KDnuggets may be reposted elsewhere two weeks after KDnuggets publication, provided there is a link to KDnuggets blog and text ""Originally published on KDnuggets"".
Here are the details on KDnuggets Top Blogs Reward Program .
To submit a blog to KDnuggets, please follow the submission guidelines."
https://www.kdnuggets.com/2020/08/data-science-machine-learning-capability-python.html,Setting Up Your Data Science & Machine Learning Capability in Python,"With the rich and dynamic ecosystem of Python continuing to be a leading programming language for data science and machine learning, establishing and maintaining a cost-effective development environment is crucial to your business impact. So, do you rent or buy? This overview considers the hidden and obvious factors involved in selecting and implementing your Python platform.","comments
By M. Sebastian Metti, Founder and CEO of Saturn Cloud.
 
Why Python?
 
Python is the clear winning programming language in data science & machine learning (DSML). With its rich and dynamic open-source software ecosystem, Python stands unmatched in how adaptable, reliable, and functional it is. If you disagree with this premise, then please take a quick detour here.

Python has over 8 million users (SlashData) (Image Credit: HackerNoon).
 
The Purpose of Your Data Science & Machine Learning Capability
 
Your goal as a lead of a DSML team is to deliver the best return on investment to the business. The business invests in the DSML capability with a budget for staff and resources, while your job is to deliver the maximum business impact you can.
Your business impact can be measured in many ways. The most high-level objectives are cost optimization, risk optimization, and revenue growth. You may focus on a variety of specific metrics within each objective, such as customer acquisition cost optimization, churn prediction, fraud detection, patient health outcomes, or personalized product recommendations.

Anything that diverts goal-setting, budget, and execution from this purpose drives down the ROI your team can deliver. Where the attention goes, the energy flows, to quote a self-improvement guru.
 
Renting vs. Owning
 
This a re-framing of the classic Buy vs. Build discussion in the context of many DSML platforms offering “pay as you go” pricing now, much like Amazon Web Services. I feel it’s necessary to rephrase the discussion because, unlike “Buying” where you pay a fixed cost, whether or not you use it, “Renting” implies that you only pay for when you use it. This is much more convenient for the end-user.
As you begin to set up your DSML platform in Python, you can own the internal architecture, or you can rent it from a vendor. I’ll use Saturn Cloud as the primary vendor, but depending on your needs, you might want to check out Domino Data Lab (fixed annual license fee model) or Databricks (DS&ML platform for Scala and Spark, not Python).
The Hidden Cost of Owning
Owning a DSML capability carries inherent “scope creep” issues that are not in plain view from the outset. It is all too easy to expect owning the capability as simply integrating your favorite open source tools: Jupyter, Dask or PySpark, Prefect or Airflow, Kubernetes, NVIDIA RAPIDS, Bokeh, Plotly, Streamlit, etc.
Here is a shortlist of “scope creep” dealbreakers we hear from our customers who have previously tried to own a DSML capability:

Setting up and managing cloud hosting and support for AWS, Azure, GCP, or on-premise
Ensuring enterprise-grade security of code and data; even more burdensome if you are in a highly regulated industry
Configuration: executing work on the proper infrastructure which exposes the appropriate resources and libraries for the task at hand
Monitoring, e.g., ensuring minimal downtime
User management: managing employee access to systems and information
Access control: controlling what users can do and see within an application
Managing existing OSS package versioning and integrating new OSS packages
Support for end-users; managing consultations with OSS experts

Each of these bullets has a list of further burdens that may not be attractive. In fact, some of it is so painful that our Saturn Cloud co-founder and CTO, Hugo Shi, wrote an article on Kubernetes just to vent.
The Obvious Cost of Owning
Here are the cost components of ownership that you need to consider as you build your DSML capability.
Example 1: Owning Results in Higher Total Cost
Your team is tasked with developing a customer churn model. If you could predict churn, sales could take proactive measures to retain more accounts. Your company generates $100M in annual sales, and there’s an opportunity to reduce churn from 10% to 5%, or by $5M annually. To keep it simple, we’ll assume you’re a SaaS company with 100% gross margins.
Figure 1: Renting = Automated DevOps.

Assumes FTE cost of $150K.
Given the cost savings in automating DevOps, the renting scenario generates higher ROI due to less total spend.
Example 2: Owning Carries High Opportunity Cost
Now let’s assume in both scenarios your team is 9 FTEs, but in the renting scenario, all 9 are dedicated to Data Science & ML. A team of 9 FTEs can produce 50% more output than a team of 6 FTEs, so with the spare capacity, you take on a second project around customer personalization. Let’s assume this project could result in 5% higher software sales in year 1.
Figure 2: Renting = Force Multiplier.

Assumes FTE cost of $150K.
Notice that in the renting scenario, you’re actually spending more money, but with the same team size, you can generate higher ROI. By shifting labor spend to Data Science & ML from DevOps, your team is more efficient and can tackle more positive ROI projects in the same period. The owning scenario carries an inherent opportunity cost, which is not inherent in the renting scenario.
In both scenarios, the ROI of renting outperforms that of owning a DSML capability. It is also worth noting that cloud computing pricing has dropped significantly over the past decade, whereas labor costs for data science, machine learning, and DevOps have increased significantly.
 
A Cautionary Tale
 
Not every organization needs to rent DSML architecture. But, it is much easier and less risky to rent first before you own.
“Rent before you own.”
I have spoken with hundreds of DSML leaders in the past couple of years. A good portion of them lead their teams into owning DSML architecture without renting, and without assessing the obvious and hidden costs of owning. All too often, they turn back halfway, realizing renting is cheaper, easier, more flexible, and allows them to stay focused. Furthermore, many developers on the teams expected they would be only part of building the architecture upfront, but later had to serve in full-time support roles, spending much less time on interesting scientific projects they joined the company for!
 
It’s Somebody Else’s Problem Now
 
...is what you’ll be saying when you rent the architecture. Yes, all the integration of open-source tools, open-source version management, building state-of-the-art security around data and code, building enterprise administration architecture, cloud hosting, support services, open-source expert consultations — say it with me — somebody else’s problem!
Not only is that offloaded, but you get some pretty great benefits from a dedicated team working on it.

Greater Performance: Saturn’s tooling offers up to 100x faster runtime than Apache Spark, Pandas, and other data processing tools
Instant Delivery: You subscribe, you have it immediately in your virtual private cloud
Expert Support: Leading committers of Python OSS available to support you
Smooth Experience: Immediate integration and updating of open source tools
Native Integrations: Amazon Web Services, Snowflake, and other cloud services
Seamless Teamwork Tools: Interactive and Collaborative DSML Capabilities
Automation: Data Pipelines and Workflow Orchestration with Prefect
Beautiful: Intuitive, State-of-the-art User Interface
Flexibility: Pay As You Go and Cancel Whenever

 
Concluding: Your Pythonic DSML Capability
 
Ownership Model: Team and budget are divided in using DSML capability to create value and supporting DSML capability.

Source: Saturn Cloud.
Rent Model: Entire team and budget are streamlined towards using rented DSML capability to create value.

Source: Saturn Cloud.
The purpose of your DSML capability is to maximize its ROI. You want as much of your budget going towards that target: whether the endpoint is faster stock market trading decision-making, recommending new marketing investment, running more drug discovery models, and so on.
My advice is:

Choose Python for its unmatched open source ecosystem
Choose to rent before you buy

Good luck, and if you are curious about Saturn Cloud, please check us out here.
Related:

Alternative Cloud Hosted Data Science Environments
Understanding Cloud Data Services
Data Science Tools Popularity, animated"
https://www.kdnuggets.com/2021/07/deep-learning-gpu-accelerate-data-science-data-analytics.html,Not Only for Deep Learning: How GPUs Accelerate Data Science & Data Analytics,Modern AI/ML systems’ success has been critically dependent on their ability to process massive amounts of raw data in a parallel fashion using task-optimized hardware. Can we leverage the power of GPU and distributed computing for regular data processing jobs too?,"By Kevin Vu, Exxact Corp.
comments

 
How GPUs Accelerate Data Science & Data Analytics
 
Artificial intelligence (AI) is set to transform global productivity, working patterns, and lifestyles and create enormous wealth. Research firm Gartner expects the global AI economy to increase from about $1.2 trillion last year to about $3.9 Trillion by 2022, while McKinsey sees it delivering global economic activity of around $13 trillion by 2030. In many ways, at its core, this transformation is fueled by powerful Machine Learning (ML) tools and techniques.
It is now well established that the modern AI/ML systems’ success has been critically dependent on their ability to process massive amounts of raw data in a parallel fashion using task-optimized hardware. Therefore, use of specialized hardware like Graphics Processing Units (GPUs) played a significant role in this early success. Since then, a lot of emphasis has been given on building highly optimized software tools and customized mathematical processing engines (both hardware and software) to leverage the power and architecture of GPUs and parallel computing.
While the use of GPUs and distributed computing is widely discussed in the academic and business circles for core AI/ML tasks (e.g. running a 100-layer deep neural network for image classification or billion-parameter BERT speech synthesis model), they find less coverage when it comes to their utility for regular data science and data engineering tasks. These data-related tasks are the essential precursor to any ML workload in an AI pipeline and they often constitute a majority percentage of the time and intellectual effort spent by a data scientist or even a ML engineer.
In fact, recently, the famous AI pioneer Andrew Ng talked about moving from a model-centric to a data-centric approach for AI tools development. This means spending much more time with the raw data and preprocessing it before an actual AI workload executes on your pipeline.
You can watch Andrew’s interview here: https://www.youtube.com/watch?v=06-AZXmwHjo

This brings us to an important question...
 
Can we leverage the power of GPU and distributed computing for regular data processing jobs too?
 
The answer is not trivial, and needs some special consideration and knowledge sharing. In this article, we will try to show some of the tools and platforms that can be used for this purpose.

Image source
 
RAPIDS: Leverage GPU for Data Science
 
The RAPIDS suite of open source software libraries and APIs gives you the ability to execute end-to-end data science and analytics pipelines entirely on GPUs. NVIDIA incubated this project and built tools to take advantage of CUDA primitives for low-level compute optimization. It specifically focuses on exposing GPU parallelism and high-bandwidth memory speed features through the friendly Python language popular with all the data scientists and analytics professionals.
Common data preparation and wrangling tasks are highly valued in the RAPIDS ecosystem as they take up a significant amount of time in a typical data science pipeline. A familiar dataframe-like API has been developed with a lot of optimization and robustness built-in. It has also been customized to integrate with a variety of ML algorithms for end-to-end pipeline accelerations with incurring serialization costs.
RAPIDS also includes a significant amount of internal support for multi-node, multi-GPU deployment and distributed processing. It integrates with other libraries which make out-of-memory (i.e. dataset size larger than individual computer RAM) data processing easy and accessible for individual data scientists.
Here are the most prominent libraries that are included in the RAPIDS ecosystem.
 
CuPy
 
A CUDA-powered array library that looks and feels like Numpy, the foundation of all numerical computing and ML with Python. It uses CUDA-related libraries including cuBLAS, cuDNN, cuRand, cuSolver, cuSPARSE, cuFFT and NCCL to make full use of the GPU architecture with the goal of providing GPU-accelerated computing with Python.
CuPy’s interface is highly similar to that of NumPy and can be used as a simple drop-in replacement for most use cases. Here is the module-level detailed list of API compatibility between CuPy and NumPy.
View the CuPy Comparison Table.
The speedup over NumPy can be mind-boggling depending on the data type and use case. Here is a speedup comparison between CuPy and NumPy for two different array sizes and for various common numerical operations - FFT, slicing, sum and standard deviation, matrix multiplication, SVD - that are widely used by almost all ML algorithms.

CuPy speeds compared to NumPy, Image source
 
CuDF
 
Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data. It provides a pandas-like API that will be familiar to almost all data engineers & data scientists, so they can use it to easily accelerate their workflows using powerful GPUs without going into the details of CUDA programming.
Currently, cuDF is supported only on Linux, and with Python versions 3.7 and later. Other requirements are,

CUDA 11.0+
NVIDIA driver 450.80.02+
Pascal architecture or better (Compute Capability >=6.0)

View more about this powerful library in the API docs for CuDF.
Finally, data scientists and analysts (i.e. those who do not necessarily use deep learning in any of their daily tasks) can rejoice and use powerful AI-workstations like the following to enhance their productivity.

Data science workstation from Exxact Corporation, Image source
 
CuML
 
cuML enables data scientists, analysts, and researchers to run traditional/ classical ML algorithmic tasks with (mostly) tabular datasets on GPUs without going into the details of CUDA programming. In most cases, cuML's Python API matches that of the popular Python library Scikit-learn to make the transition to GPU hardware fast and painless.
View the GitHub repo for CuML documentation to learn more.
CuML also integrates with Dask, wherever it can, to offer multi-GPU and multi-node-GPU support for an ever-increasing set of algorithms that takes advantage of such distributed processing.
 
CuGraph
 
CuGraph is a collection of GPU accelerated graph algorithms that process data found in GPU DataFrames. The vision of cuGraph is to make graph analysis ubiquitous to the point that users just think in terms of analysis and not technologies or frameworks.
Data scientists familiar with Python will quickly pick up how cuGraph integrates with the Pandas-like API of cuDF. Likewise, users familiar with NetworkX will quickly recognize the NetworkX-like API provided in cuGraph, with the goal to allow existing code to be ported with minimal effort into RAPIDS.
Currently, it supports all kinds of graph analytics algorithms,

Centrality
Community
Link analysis
Link prediction
Traversal

Many scientific and business analytics tasks involve use of extensive graph algorithms on large datasets. Libraries like cuGraph lend the assurance of higher productivity to those engineers when they invest in GPU-powered workstations.

Empower social graph analytics using GPU-accelerated computing, Image source
 
The Overall Pipeline for GPU Data Science
 
RAPIDS envisions a whole pipeline for GPU-powered data science task flow as follows. Note that deep learning, which has traditionally been the primary focus of GPU-based computing, is only a sub-component of this system.

The GPU Data Science Pipeline, Image source
 
Dask: Distributed Analytics With Python
 
As we observed, modern data processing pipelines can often benefit from distributed processing of large data chunks. This is slightly different from the parallelism offered by the thousands of cores in a single GPU. This is more about how to split up a mundane data processing (which may occur much before the dataset is ready for ML algorithms) into chunks and process in using multiple compute nodes.
These computing nodes can be GPU cores or they can even be simple logical/ virtual cores of CPU.
By design, most widely popular data science libraries like Pandas, Numpy, and Scikit-learn cannot take advantage of truly distributed processing easily. Dask tries to solve this problem by bringing the features of intelligent task scheduling and big data chunk handling into regular Python code. Naturally, it is composed of two parts:

Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.
“Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of the aforementioned dynamic task schedulers.

Here is an illustrative diagram of a typical Dask task-flow.

Official Dask Flow Documentation, Image source
 
Easy to Convert Existing Codebase
 
Familiarity is at the core of the Dask design, so that a typical data scientist can just pick up his/her existing Pandas/Numpy based codebase and convert it to Dask code following a minimal learning curve. Here are some of the canonical examples from their official documentation.

Dask Documentation comparing Pandas and NumPy, Image source
 
Dask-ML for Scalability Challenges
 
There are different kinds of scalability challenges for ML engineers. The following figure illustrates them. The machine learning library Dask-ML offers something for each of these scenarios. Therefore, one can focus on either model-centric exploration or data-centric development based on the unique business or research requirements.
Most importantly, focus on familiarity again plays a role here and the DASK-ML API is designed to mimic that of the widely popular Scikit-learn API.

Dask Documentation for XGBRegressor, Image source
 

 
Dask Benefits from Multi-Core CPU Systems
 
It is to be noted that the primary attractiveness of Dask comes from its role as a high-level, efficient task scheduler that can work with any Python code or data structure. Consequently, it is not dependent on a GPU to boost existing data science workloads with distributed processing.
Even multi-core CPU systems can take full advantage of Dask if the code is written to focus on that. Major changes in the code are not required.
You can distribute your convex optimization routine or hyperparameter search among many cores of your laptop using Dask. Or, you can just process different parts of a simple DataFrame based on some filtering criteria using the full multi-core parallelism. This opens up the possibility of boosting the productivity of all the data scientists and analysts who do not need to buy expensive graphics cards for their machine but can just invest in a workstation with 16 or 24 CPU cores.
 
Summary of Distributed Data Science Powered by GPUs
 
In this article, we discussed some exciting new developments in the Python data science ecosystem which enables common data scientists, analysts, science researchers, academics, to use GPU-powered hardware systems for a much wider variety of data related tasks than just what is related to image classification and natural language processing. This will surely broaden the appeal of such hardware systems to these large sections of users and democratize the data science user base even more.
We also touched upon the possibilities of distributed analytics with the Dask library which can leverage multi-core CPU workstations.
Hopefully, this kind of convergence of powerful hardware and modern software stack will open up endless possibilities for highly efficient data science workflows.
 
Original. Reposted with permission.
Related:

How to Use NVIDIA GPU Accelerated Libraries
Good-bye Big Data. Hello, Massive Data!
Abstraction and Data Science: Not a great combination"
https://www.kdnuggets.com/2021/01/unsupervised-learning-predictive-maintenance-auto-encoders.html,Unsupervised Learning for Predictive Maintenance using Auto-Encoders,"This article outlines a machine learning approach to detect and diagnose anomalies in the context of machine maintenance, along with a number of introductory concepts, including: Introduction to machine maintenance; What is predictive maintenance?; ​​​​Approaches for machine diagnosis; Machine diagnosis using machine learning","comments
By Ankush Kundaliya & Aditya Aggarwal, Abzooba
This article outlines a machine learning approach to detect and diagnose anomalies in the context of machine maintenance. However, before we dive into the approach, we will gather a brief understanding of machine maintenance. This article is arranged as below:

Introduction to machine maintenance
What is predictive maintenance? ​​​​​​​
Approaches for machine diagnosis
Machine diagnosis using machine learning

 
1) Introduction to Machine Maintenance
 
For any industrial machinery equipment, owners want to increase operational flexibility and reduce operating costs. To achieve this objective, system engineers mainly focuses on 3 attributes of the machinery.

Reliability (R): It is defined as the probability of a machine or machine component operating as expected without failure for a given period of time. The commonly used metric for it is ""Mean time between failure"" i.e. ""Total operating time"" / ""Total failures"" 
Maintainability (M): It is defined as the probability that a machine or machine component can be repaired within a specified period of time. The commonly used metric for it is ""Mean time to repair"" i.e. ""Total downtime"" / ""Total outages""
Availability (A): It is defined as the probability that a machine or machine component is functional at a given point in time. Availability depends on reliability and maintainability, defined as ""Total operating time""/(""Total operating time"" + ""Total downtime"")

Maintenance strategy significantly improve the reliability and availability of assets and, as a result, decreases the number of unpredicted breakdowns. With advancements in technology, maintenance strategies have also evolved over time as summarized in Table 1.
​​​​​​​






Breakdown Maintenance or Run to Failure


Preventive Maintenance or Scheduled Maintenance


Predictive Maintenance or Condition-based Maintenance




Definition


Maintenance actions are taken only after a breakdown happened.


Planned maintenance actions after specific time intervals.


Maintenance actions are taken according to the actual condition of the operating equipment assessed through condition monitoring procedures.




Principle


Fail and fix 
Reactive
Unscheduled


Time-based
Preventive
Scheduled at regular intervals


Predictive
Preventive
Condition-based Just-in-time




Pros


Cost-effective for small, non-critical equipment


A proactive strategy that helps to minimize downtime, prevent costly repairs caused by secondary damages.


Reduces maintenance costs, downtime, secondary damage, and avoids unnecessary parts replacement




Cons


Costly downtime, extensive secondary damages


The cost of maintenance is very high. Unplanned breakdowns can still occur.


None




​​​​​​​Table 1: Maintenance strategies (evolved with time in the order from left to right)
In this article, we will be talking about a machine learning approach that aligns with the predictive maintenance strategy. Hence, let's understand ""what is predictive maintenance?"" before getting into the actual approach.​​​​​​​
 
2) What is Predictive Maintenance?
 
Predictive maintenance is determined based on the actual condition of the machine and its components also known as condition-based maintenance (CBM). CBM suggests maintenance action only when there is evidence of abnormal behaviours from a component.
CBM heavily relies upon diagnostic (what is current condition?) and prognostic (what will be the condition in future?) information from the machine and its components. Both serve as different objective as shown in table 2.





Diagnostic (What is condition currently?)
Prognostic (What will be the condition in future?)


Definition
Diagnostics is the process of determining the current health status and the equipment deterioration using information delivered by the condition-monitoring system.
Prognostics is the ability to forecast the machine deterioration using information gathered from the machine and its components (like vibrations, change in temperature, change in pressure, current consumption, etc).


Objective

i. fault detection – fault is about to happen;
ii. fault isolation - locates the faulty component
iii. fault identification - determine the root cause of the fault


i. forecasting the impending failures and
ii. estimating the remaining useful life




Table 2: Success of CBM relies on diagnostic and prognostic ability both
In this article, we will be addressing the diagnosis process of the CBM approach which includes anomaly detection, isolation, and identification to assist root cause analysis and plan maintenance.
 
3) Approaches for machine diagnosis
 
The first goal of the diagnostics is to identify the malfunctioning components. When observations from an operating machine differ from the expected behaviour then the real need for diagnostics arises. There are many approaches to do diagnostics and a few commonly used ones are listed in Table 3.





#


Diagnostic approach


Description


Limitations / Disadvantages



1
Fault Trees[1]

Fault tree analysis is a top-down approach that was originally developed in Bell laboratories in the year 1962.
It uses predefined logics to identify the component level failures that lead to occur system-level failure.

1) Need a lot of domain expertise   
2) Expensive to build and maintain


2
Rule based[2]

As the name suggests, knowledge for diagnosis is captured in the form of IF-THEN rules. Rule-based systems are built with the help of expert diagnosticians to capture associations between the symptoms of an abnormal system and the underlying failures/faults.

​​​​​​​1) Need a lot of domain expertise
2) Expensive to build and maintain


3
Model based

A machine learning based model learns how the system components are connected and how they normally behave. Model is then tasked to identify those machine components which, when assumed to function abnormally, will account for the difference between the observed and expected behaviour.

1) Need high computing resources
2) Requires a good amount of historical data



Table 3: Commonly used approaches to do machine diagnostics
In this article, we will discuss modelling based approach with a case study.
 
4) Machine diagnosis using machine learning
 
We will be explaining the ML approach using a case study on ""Condition-based predictive maintenance of Gas Turbines in a Power Plant"". The solution is using the ideas discussed in the paper [3] from AAAI Conference on Artificial Intelligence, Jul 2019.
This solution is designed to address the most commonly faced challenges as listed below - 

In most real-world scenarios, it is very difficult to get a sufficient amount of anomaly events data points in historical data. This makes supervised learning techniques infeasible to detect or classify anomalies from normal behaviour.
In multivariate time series data, it not only requires to capture the temporal dependency in each time series but also needs to encode the inter-correlations between different pairs of time series.
In real-world applications, it is common to have noise which may not eventually lead to a true system failure. Therefore, an anomaly detection system should provide operators with an anomaly scores indicating the severity of incident.

 
4.1) Modelling methodology
 
We attempt to model an accurate short-term estimate of gas turbine engine performance and integrity conditions which can be invaluable for maintenance strategy and planning. We build our model based on the operational data of a gas turbine engine collected from different sensors deployed for monitoring the engine's status.
We have the historical data from n sensors, monitoring the engine's status for a period T, 
i.e., 𝑋 = (𝑥1, … , 𝑥𝑛 ) 𝑇 ∈ ℝ𝑛∗𝑇 ,
During this period T, we assume that there were no anomalies or fault events and engine was operated in normal operating condition. Given this data, we train a model to learn different statuses of an engine during its normal operations and detect difference in engine’s status during abnormal operations. We aim to detect anomaly events during operations and diagnose the severity and root cause of the anomaly.​​​​​​​
The modelling methodology is unsupervised learning using auto-encoders that learns how to represent original data into a compressed encoded representation and then learns how to reconstruct the original input data from the encoded representation. More details about model is given in next section 4.1.1.
 
4.1.1) Multi-Scale Convolutional Recurrent Encoder-Decoder (MSCRED)
 
MSCRED is an unsupervised learning technique that learns the normal operating conditions of the equipment from operational data by learning the signature matrices representing the different states of operation of the machine in normal conditions. We train our model only on the normal signature matrices and assume that the signature matrices of machines in abnormal operations differ from the normal operations.​​​​​​​
What is a Signature Matrix?
A signature matrix is a way of representing the data wherein the multivariate time series data is transformed into correlation matrices to characterize the system status. The inter-correlations between different pairs of time series in a multivariate time series segment capture the shape similarities and value scale correlation between pairs of time series. 


Fig 1. Signature Matrices

 
Characterizing System Status with Signature Matrices:

We first fix the window-sizes for different resolutions and the step-size by which to slide these windows. For example, suppose we have w1, w2, w3 windows such that w3>w2>w1 and step-size t, we right-align these windows on the time series and divide it into segments while sliding by step-size.
Next for a segment, for each window-size, we calculate the correlations between different pairs of multivariate time series to get n * n matrices Mt, where n is the number of sensors/time-series. For e.g. if we have 30 sensors then it will give us a matrix of 30*30 for a window.
We stack these correlation matrices from segments of different resolutions (window-size) together to form the signature matrices. For e.g. for 3 different window-size, the signature matrix is of dimension 30*30*3.​​​​​​​​​

MSCRED modelling framework:​​​​​​​ Here are steps to create an unsupervised model using MSCRED modelling framework.
Step 1: Construct multi-resolution signature matrices to characterize multiple levels of the system operational statuses across different time steps (segments) as discussed in the section above. Multi-resolution signatures are used to reduce the operational noise in data and help us indicate the severity of abnormal incidents.


Fig 2. MSCRED Model Framework: (a) Signature matrices encoding via CNN. (b) Temporal patterns modelling by attention based convLSTM. (c) Signature matrices decoding via deconvolution neural networks. (d) Loss function.

 
Step 2: Use a convolutional encoder to capture and encode the inter-sensor correlation patterns from the signature matrices (as shown in part (a) of Fig 2). 
Step 3: Use an attention-based Convolutional Long-Short Term Memory (ConvLSTM) network to capture the temporal patterns (as shown in part (b) of Fig 2).
Step 4: Use a convolutional decoder to reconstruct the signature matrices from the feature maps which encode the inter-sensor correlations and temporal information (as shown in part (c) of Fig 2).  
Step 5: The residual error between reconstructed signature matrices and original signature matrices is then utilized to detect and diagnose anomalies (as shown in part (d) of Fig 2).​​​​​​​​
 
4.1.2) Anomaly Detection and Root Cause Identification
 
Steps to detect anomaly and identify the root cause is as below.

We utilize the residual error matrix to detect the anomaly and identify the root cause(s).
Anomaly score is calculated for each window by adding up the absolute value of residuals in the residual matrix.
Anomaly score greater than a defined threshold is marked as an anomaly. Analysing the corresponding residual matrix to find the rows and columns with the higher error in the residual matrix give us the root cause or affected components. 
The signature matrices of operational data includes channels (s = 3 windows) that capture system status at different scales. Anomaly severity is given by computing the anomaly scores from residual error matrices of three channels, i.e., small, medium and large with size w – 10, 30, and 60, respectively as shown in fig 3.



Fig 3. Anomaly Diagnosis Results

 
Analysis of anomalous residual matrices helps operator in root cause analysis and identify affected components. This method captures the temporal patterns in the time series as well as the inter-sensor correlation patterns. For machine diagnosis, this method is claimed to outperform other state-of-the-art models.
References

Ramana PV, Fault Tree Analysis, https://sixsigmastudyguide.com/fault-tree-analysis/
Xiao-Wen  Deng, Qing-Shui  Gao, Chu  Zhang, Di  Hu, Tao  Yang, ""Rule - based Fault Diagnosis Expert System for Wind Turbine"", ITM Web Conf. 11 07005 (2017), DOI: 10.1051/itmconf/20171107005
Chuxu Zhang and Dongjin Song and Yuncong Chen and Xinyang Feng and Cristian Lumezanu and Wei Cheng and Jingchao Ni and Bo Zong and Haifeng Chen and Nitesh V. Chawla, A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data,  AAAI 2019: 1409-1416

 
Ankush Kundaliya is a Data Scientist at Abzooba. With more than 5 years of extensive experience in the field of data science, Ankush has expertise in building data-driven solutions to complex business problems using advance deep learning and machine learning algorithms. Having worked across multiple domains of industries including IT Service Management, Human Resources, Manufacturing, Life Sciences, and Financial Services, he has diverse knowledge and business acumen.
Aditya Aggarwal serves as Data Science – Practice Lead at Abzooba Inc. With more than 12+ years experience in driving business goals through data driven solutions, Aditya specializes in predictive analytics, machine learning, business intelligence & business strategy across range of industries.
Related:

Top Obstacles to Overcome when Implementing Predictive Maintenance
Moving Predictive Maintenance from Theory to Practice
The Potential of Predictive Analytics in Labor Industries"
https://www.kdnuggets.com/2021/09/datacated-expo-oct-5.html,"DATAcated Expo, Oct 5, Live-streamed,Explore new AI / Data Science Tech","The DATAcated Expo, hosted by DATAcated founder Kate Strachnyi, is coming up on October 5, 2021 from 11am - 6pm ET. Live-streamed on LinkedIn, the free event provides the community with an opportunity to explore and discover innovative technologies in data science & analytics.","Sponsored Post.

 
The DATAcated Expo is coming up on October 5, 2021 from 11am - 6pm ET. This innovative, virtual conference is live-streamed on LinkedIn, and hosted by Kate Strachnyi, founder of DATAcated. The event is free to attend and provides the community with an opportunity to explore and discover innovative technologies in data science & analytics.
SPEAKERS: Expert speakers will provide real-life case studies and demos of technologies across a variety of data science & analytics processes. You’ll hear from Scott Taylor – The Data Whisperer, Greg Coquillo – Technology Manager at Amazon, Aishwarya Srinivasan - AI & Innovation Leader- Business Development at IBM, Susan Walsh – The Classification Guru, George Firican – founder of LightsOnData, Tina Huang - Data Scientist at Goldman Sachs, Bruno Aziza - Head of Data & Analytics, Google Cloud, and many more!
TOPICS: The presentations are centered around the data analytics process and will take participants on a journey from identifying the business use case to collecting, cleaning, exploring, visualizing, and ultimately communicating insights.  We’ll also cover ethical use of data, as well as other special topics.
NETWORKING: This live event will provide several opportunities for networking and building relationships in the data community. 
GIVEAWAYS: The DATAcated Expo is partnered with amazing organizations that will provide fun giveaways throughout the event. 
REGISTRATION: Make sure you sign up to this FREE event and add it to your calendar here: https://datacated.com/expo/ - registration for the event also enables you to receive the recordings from all sessions. 
We hope to see you there!"
https://www.kdnuggets.com/2021/06/ai-with-feature-store.html,Beyond Brainless AI with a Feature Store,"AI-powered products that are limited to the data available within its application are like jellyfish: its autonomic system makes it functional, but it lacks a brain. However, you can evolve your models with data enriched ""brains"" through the help of a feature store.","comments
By Jim Dowling, CEO of Logical Clocks, Associate Professor at KTH Royal Institute of Technology.
TLDR; Machine learning models are only as good as the data (features) they are trained on. In enterprises, data scientists can often train very effective models in the lab - when given a free hand on which data to use. However, many of those data sources are not available in production environments due to disconnected systems and data silos. An AI-powered product that is limited to the data available within its application silo cannot recall historical data about its users or relevant contextual data from external sources. It is like a jellyfish - its autonomic system makes it functional and useful, but it lacks a brain. You can, however, evolve your models from brain-free AI to Total Recall AI with the help of a Feature Store, a centralized platform that can provide models with low latency access to data spanning the whole enterprise.

 
Autonomic AI
 
Jellyfish are undoubtedly complex creatures with sophisticated behaviour - they move, mate, and munch. They eat and discard waste from the same opening. Yet, they have no brain - their autonomic system suffices for their needs. The biggest breakthroughs in AI in recent years have been enabled by deep learning, which requires large volumes of data and specialized compute hardware (e.g., GPUs). However, just like a jellyfish, recent successes in image processing and NLP with deep learning required no brain - no working memory, history, or context.
Much of deep learning today is Jellyfish AI. We have made incredible progress in identifying objects in images and translating natural language. Yet, such deep learning models typically only require the immediate input - the image or the text - to make their predictions. The input signal is information-rich. These image and NLP models seldom require a 'brain' to augment the input with context or memories. Google translate doesn’t need to know the historical enmity between the Scots and the Irish in whether it's spelled Whisky or Whiskey. Jellyfish AI is impressive - the input data is information-rich, and models can learn fantastically advanced behaviour from labeled examples. All “knowledge” needed to make predictions is embedded in the model. The model does not need to have working memory (e.g., it doesn’t need to know the user has clicked 10 times on your website during the last minute).
Now compare using AI for image classification or NLP to building a web application that will use AI to interact with a user browsing a website. The immediate input data your application receives from your web browser are clicks on a mouse or a keyboard. The input signal is information-light - it is difficult to train a useful model using only user clicks. However, large Internet companies collect reams of information about users from many different sources and transform that user data into features (information-rich signals that are ready to be used for either training models or making predictions with models). Models can then combine the click features with historical features about users and contextual features to build information-rich inputs to models. For example, you could augment the user’s action with everything you know about a user’s history and context to increase the user's engagement with the product. The feature store for machine learning (ML) stores and serves these features to models. We believe that AI-powered products that can easily access historical and contextual features will lead the next wave of AI in the enterprise, and those products will need a feature store for ML.
 
Data Scientist and ML Engineer Disconnect
 
A frequent source of tension in enterprises is between “naive” data scientists and “street-wise” ML engineers. Motivated by good software engineering practices, many ML engineers believe that ML models should be self-contained, and tension can arise with data scientists who want to include features in their models that are “obviously not available in the production system.”
However, data scientists are tasked with building the best models they can to add to the bottom line - engage more users, increase revenue, reduce costs. They know they can train better models with more data and more diverse sources of data. For example, a data scientist trying to predict if a financial transaction is suspected of money laundering or not might discover that a powerful feature is the graph of financial transfers related to this individual in the previous day/week/month. They can reduce false alerts of money launder by a factor of 100*, reducing the costs of investigating the false alerts, saving the business millions of dollars per year. The data scientist hands the model over the wall to the ML engineer, who dismisses the idea of including the graph-based features in the production environment, and tension arises when communicating what is possible and what is not possible in production. The data scientist is crestfallen - but need not be.
The Feature Store is now the de facto enterprise platform for storing historical and contextual features for AI-powered products. The Feature Store is, in effect, the brain for AI-powered products, the three-eyed Raven that enables the model to access the history and state of the whole enterprise, not just the local state in the application.
Feature Stores enable applications or model serving infrastructure to take information-light inputs (such as a cookie identifying a user or a shopping cart session) and enrich it with features harvested from anywhere in the enterprise or beyond to build feature vectors capable of making better predictions. And as we know from Deep Learning, model accuracy improves predictably with more features and data, so there will be an increasing trend towards adding more and more features to models to improve their accuracy. Andrew Ng has recently been advocating this approach that he calls data-centric development instead of the more traditional model-centric development. Another noticeable trend in large enterprises is building faster and more scalable Feature stores that can supply those features within the time budget available to the AI-powered product. But AI is going to revolutionize enterprise software products, so how do we make sure our AI-enabled products are not just Jellyfish AI?
 
Enabling AI-enabled Products with a Feature Store
 

Anti-Pattern: Re-implementing the “feature engineering” code for the serving layer is non-DRY. This introduces the risk of ‘skew’ between the features used to train models and the features served to operational models.
How do we avoid limiting AI-enabled products to only using the input features collected by the application itself? Models will benefit from access to all data that the enterprise has collected about the user, product, or its context. A potential source of friction here, however, is the dominant architectural preference for microservices and data stove-pipes. Models themselves are being deployed as microservices in model-serving infrastructures, like KFServing, TensorFlow Serving, or Nvidia Triton. How can we give these models access to more features?

Anti-Pattern: Microservice-based Online Feature Store. Microservices can be used to compute features in real-time from raw input data. When features can be pre-computed, microservices is an anti-pattern. This architecture adds latency, needs to be made highly available, handles hotspots, and microservices consume resources even when they are not needed. Serverless functions might be acceptable in the case where seconds of warmup latency is tolerable. But the microservices should still be reused to compute the training data - otherwise, there is a risk of training/serving skew.
Without a Feature Store, applications could contact microservices or databases to compute or retrieve the historical and contextual features (data), respectively. Computing the features in the application itself is an anti-pattern as it duplicates the feature engineering code - that code should already exist to generate the training data for the model. Re-implementing feature engineering logic in applications also introduces the risk of skew between the features computed in the application and the features computed for training. If serving and training environments use the same programming language, they could avoid non-DRY code by reusing a versioned library that computes the features. However, even if feature engineering logic is written in Python in both training and serving, it may use PySpark for training and Python for serving or different versions of Python. Versioned libraries can help but are not a general solution to the feature skew problem.
The Feature Store solves the training/serving skew problem by computing the features once in a feature pipeline. The feature pipeline is then reused to (1) create training data and (2) save those pre-engineered features to the Feature Store. The serving infrastructure can then retrieve those features when needed to make predictions. For example, when an application wants to make a prediction about a user, it would supply the user’s ID, shopping cart ID, session ID, or location to retrieve pre-engineered features from the Feature Store. The features are retrieved as a feature vector, and the feature vector is sent to the model that makes the prediction. The Feature Store service for retrieving feature vectors is commonly known as the Online Feature Store. The logic for retrieving features from the Online Feature Store can also be implemented in model serving infrastructure, not just in applications. The advantage of looking up features in serving infrastructure is that it keeps the application logic cleaner, and the application just sends IDs and real-time features to the model serving infrastructure, that in turn, builds the feature vector, sends it to the model for prediction, and returns the result to the application. Low latency and high throughput are important properties for the online feature store - the faster you can retrieve features and the more features you can include in a given time budget, the more accurate models you should be able to deploy in production. To quote DoorDash:
‍“Latency on feature stores is a part of model serving, and model serving latencies tend to be in the low milliseconds range. Thus, read latency has to be proportionately lower.” ‍

AI-enabled products use models to make predictions, and those models need an Online Feature Store to provide them with historical and contextual data (features) to make better predictions.
 
Data-Centric AI with the Online Feature Store
 
So, to summarize, if you want to give your ML models a brain, connect them up to a feature store. For enterprises building personalized services, the feature store can enrich their models with a 360-degree enterprise-wide view of the customer - not just a product-specific view of the customer. The feature store enables more accurate predictions through more data being available to make those predictions, and this ultimately enables products with better user experience, increased engagement, and the product intelligence now expected by users.
* This is based on a true story.
Original. Reposted with permission.
 
Bio: Jim Dowling (@jim_dowling) is the CEO at Logical Clocks and an Associate Professor at KTH Royal Institute of Technology. He is lead architect of the open-source Hopsworks platform, the world's first enterprise Feature Store along with an advanced end-to-end ML platform.
Related:

Feature Store as a Foundation for Machine Learning
Feature Store vs Data Warehouse
Feature stores – how to avoid feeling that every day is Groundhog Day"
https://www.kdnuggets.com/2021/01/best-python-ide-code-editors.html,Best Python IDEs and Code Editors You Should Know,"Developing machine learning algorithms requires implementing countless libraries and integrating many supporting tools and software packages. All this magic must be written by you in yet another tool -- the IDE -- that is fundamental to all your code work and can drive your productivity. These top Python IDEs and code editors are among the best tools available for you to consider, and are reviewed with their noteworthy features.","comments
By Claire D. Costa, Content Writer and Strategist at Digitalogy LLC.

Photo by luis gomes from Pexels.
Python is an experiment in how much freedom programmers need. Too much freedom and nobody can read another’s code; too little and expressiveness is endangered. - Guido van Rossum
Since its creation, Python has rapidly evolved into a multi-faceted programming language, becoming the choice of several diverse projects ranging from web applications to being deployed into Artificial Intelligence, Machine Learning, Deep Learning, and more.
Python comes with numerous features such as its simplicity, enormous collection of packages and libraries, with relatively faster execution of programs, to list a few.
GitHub’s second-most popular language and the most popular language for machine learning.
For a programmer, a Code Editor or an IDE is the first point of contact with any programming language, making its selection one of the most crucial steps in the journey ahead. Throughout this article, we’ll discuss some of the top Python IDEs and Code Editors, along with the reasons why you should and shouldn’t pick them for your next project.
According to StackOverflow, Python is the fastest-growing major programming language: Stack Overflow Developer Survey 2019
 
What is an Integrated Development Environment (IDE)?
 
An IDE stands for Integrated Development Environment and includes not just the standard code editor for managing the code but also provides a comprehensive set of tools for its debugging, execution, and testing, which is an absolute must for software development. Some IDEs also come with built-in compilers and interpreters. Listed below are some of the standard features common IDEs offer within a single dedicated environment:

Syntax highlighting
Build automation
Version control
Visual programming
Code formatting and completion
Code refactoring
Support for integration with external tools

 
IDE vs. Code Editor
 
A Code Editor or an IDE is the most fundamental piece of software for any programmer, and it is what they start and end their day. To achieve its maximum potential, the best starting point is a Code Editor or an IDE that essentially lets you work with Python, but that’s not all. A host of programming languages can work entirely without an IDE, while some are IDE-dependent.
Code Editor — A Code Editor is a core piece of software that programmers use for application development. Think of it as a simple text editor but with additional programming-specific advanced features such as:

Syntax highlighting
Code formatting
Split file viewing and editing
Instant project switching
Multiple selections
Cross-platform support
Light-weight

IDE — On the other hand, an IDE comes with a suite of tools that help in not just developing the application but also in its testing, debugging, refactoring, and automating builds. Needless to say, in most cases, an IDE can offer all features of a Code Editor, but a Code Editor cannot replace an IDE.
 
Best Python IDEs and Code Editors in 2020
 
Choosing the right tools for a job is critical. Similarly, when starting a new project, as a programmer, you have a lot of options when it comes to selecting the perfect Code Editor or IDE. There are loads of IDEs and Code Editors out there for Python, and in this section, we’ll discuss some of the best ones available with their benefits and weaknesses.

PyCharm


Image Source — PyCharm.

Category: IDE
First Release Date: 2010
Platform Compatibility: Windows, macOS, Linux
Who It’s For: Intermediate to advanced Python users
Supporting Languages: Python, Javascript, CoffeeScript, etc.
Price: Freemium (free limited feature community version, paid full-featured professional version)
Download: PyCharm Download Link
Popular Companies using Pycharm Python IDE - Twitter, HP, Thoughtworks, GROUPON, and Telephonic.

Developed by JetBrains, PyCharm is a cross-platform IDE that offers a variety of features such as version control, graphical debugger, integrated unit tester, and pairs well for web development and Data Science tasks. With PyCharm’s API, developers can create their custom plugins for adding new features to the IDE. Other features include:

Code completion
Live updates to code changes
Python refactoring
Support for full-stack web development
Support for scientific tools such as matplotlib, numpy, and scipy
Support for Git, Mercurial and more
Comes with paid and community editions

Advantages —

Can boost productivity and code quality
Highly active community for support

Disadvantages —

Can be slow to load
Requires changing default settings for existing projects for best compatibility
The initial installation might be difficult

Screenshot for Reference

Image Source — PyCharm.

Spyder


Image Source — Spyder.

Category: IDE
First Release Year: 2009
Platform Compatibility: Windows, macOS, Linux
Who It’s For: Python data scientists
Price: Free
Download: Spyder Download Link

Spyder comes with support for packages like NumPy, SciPy, Matplotlib, and Pandas. Targeted towards scientists, engineers, and data analysts, Spyder offers advanced data exploration, analysis, and visualization tools. Features of this cross-platform IDE include:

Code completion
Syntax highlighting
Code benchmarking via Profiler
Multi-project handling
Find in Files feature
History log
Internal console for introspection
Third-party plugins support

Advantages —

Includes support for numerous scientific tools
Comes with an amazing community support
Interactive console
Lightweight

Disadvantages —

Comes with execution dependencies
Can be a bit challenging at first for newcomers

Screenshot for Reference

Image Source — Spyder.|
spyder-ide/spyder

Eclipse + Pydev



Category: IDE
First Release Year: 2001 — for Eclipse, 2003 — for Pydev
Platform Compatibility: Windows, macOS, Linux
Who It’s For: Intermediate to advanced Python users
Supporting Languages: Python, (Eclipse supports Java and many other programming languages)
Price: Free
Download: PyDev Download Link
Popular Companies using PyDev and Eclipse Python IDE — Hike, Edify, Accenture, Wongnai, and Webedia.

Eclipse is one of the top IDEs available, supporting a broad range of programming languages for application development, including Python. Primarily created for developing Java applications, support for other programming languages is introduced via plugins. The plugin used for Python development is Pydev and offers additional benefits over Eclipse IDE, such as:

Django, Pylint, and unit test integration
Interactive console
Remote debugger
Go to definition
Type hinting
Auto code completion with auto import

Advantages —

Easy to use
Programmer friendly features
Free

Disadvantages —

Complex user interface makes it challenging to work with
If you’re a beginner, then using Eclipse will be difficult

Screenshot for Reference

Image Source — Pydev.

IDLE


Image Source — Python.

Category: IDE
First Release Year: 1998
Platform Compatibility: Windows, macOS, Linux
Who It’s For: Beginning Python users
Price: Free
Download: IDLE Download Link
Popular Companies using IDLE Python IDE — Google, Wikipedia, CERN, Yahoo, and NASA.

Short for Integrated Development and Learning Environment, IDLE has been bundled with Python as its default IDE for more than 15 years. IDLE is a cross-platform IDE and offers a basic set of features to keep it unburdened. The features offered include:

Shell window with colorized code, input, output, and error messages
Support for a multi-window text editor
Code auto-completion
Code formatting
Search within files
Debugger with breakpoints
Supports smart indentation

Advantages —

Perfect for beginners and educational institutions

Disadvantages —

Lacks features offered by more advanced IDEs, such as project management capabilities

IDLE - Python 3.8.3 documentation

Wing


Image Source — Wing.

Category: IDE
First Release Year: September 7, 2000
Platform: Windows, Linux, and Mac
Who It’s For: Intermediate to advanced Python users
Price: $179 per user for a year of commercial use, $245 per user for a permanent commercial use license
Download: Wing Download Link
Popular Companies using Wing Python IDE — Facebook, Google, Intel, Apple, and NASA

The feature-rich IDE for Python, Wing, was developed to make development faster with the introduction of intelligent features such as smart editor and simple code navigation. Wing comes in 101, Personal, and Pro variants with Pro being the most feature-rich and the only paid one. Other notable features by Wing include:

Code completion, error detection, and quality analysis
Smart refactoring capabilities
Interactive debugger
Unit tester integration
Customizable interface
Support for remote development
Support for frameworks such as Django, Flask, and more

Advantages —

Works well with version control systems such as Git
Strong debugging capabilities

Disadvantages —

Lacks a compelling user interface

 

Cloud9 IDE


Image Source — AmazonCloud9.

Category: IDE
First Release Year: 2010
Platform: Linux/MacOS/Windows
Popular Companies using Cloud9 Python IDE — Linkedin, Salesforce, Mailchimp, Mozilla, Edify, and Soundcloud.

Part of Amazon’s Web Services, Cloud9 IDE gives you access to a cloud-based IDE, requiring just a browser. All the code is executed on Amazon’s infrastructure, translating to a seamless and lightweight development experience. Features include:

Requires minimal project configuration
Powerful code editor
Code highlight, formatting, and completion capabilities
Built-in terminal
Strong debugger
Real-time pair programming capabilities
Instantaneous project setup, covering most programming languages and libraries
Unobstructed access to several AWS services via terminal

Advantages —

Enables painless development of serverless applications
Remarkably robust and globally accessible infrastructure

Disadvantages —

Depends entirely on internet access

 

Sublime Text 3


Image Source — Sublime.

Category: Code Editor
First Release Year: 2008
Platform Compatibility: Windows, macOS, Linux
Who It’s For: Beginner, Professional
Supporting Languages: Python and C#
Price: Freemium
Download: Sublime text 3 Download Link
Popular Companies using Sublime Text Python IDE — Starbucks, Myntra, Trivago, Stack, and Zapier.

Sublime Text is one of the most commonly used cross-platform Code Editors and supports several programming languages, including Python. Sublime offers various features such as plenty of themes for visual customization, a clean and distraction-free user interface, and supports package manager for extending the core functionality via plugins. Other features include:

Up-to-date plugins via Package Manager
File auto-save
Macros
Syntax highlight and code auto-completion
Simultaneous code editing
Go to anything, definition, and symbol

Advantages —

Uncluttered user interface
Split editing
Fast and high-performance editor

Disadvantages —

Annoying popup to buy sublime license
Confusingly large number of shortcuts
Complicated package manager



Visual Studio Code


Image Source — Visual Studio Code.

Category: IDE
First Release Year: 2015
Platform Compatibility: Windows, macOS, Linux
Who It’s For: Professional
Supporting Languages: All the major programming languages (Python, C++, C#, CSS, Dockerfile, Go, HTML, Java, JavaScript, JSON, Less, Markdown, PHP, PowerShell, Python, SCSS, T-SQL, TypeScript.)
Price: Free
Download: Visual Studio Code Download Link
Popular Companies using Visual Source Code (Python IDE) — The Delta Group, TwentyEight, Inc., Focus Ponte Global, Creative Mettle, and National Audubon Society, Inc.

Developed by Microsoft, Visual Studio Code is an acclaimed cross-platform code editor that is highly customizable and allows development in several programming languages, including Python. It offers a wide variety of features to programmers, such as smart debugging, customizability, plugin support for extending core features. Key highlights include:

Built-in support for Git and version control
Code refactoring
Integrated terminal
IntelliSense for smarter code highlight and completion
Intuitive code debugging capabilities
Seamless deployment to Azure

Advantages —

Regularly updated with active community support
Free

Disadvantages —

Vast collection of plugins can make finding the right one challenging
Lackluster handling of large files
Longer launch time

Screenshot for Reference

Image Source — Visual Studio Code.
Python in Visual Studio Code

Atom


Image Source — Atom.

Category: Code Editor
First Release Year: 2014
Platform Compatibility: Windows, macOS, Linux
Who It’s For: Beginner, Professional
Supporting Languages: Python, HTML, Java and 34 other languages.
Price: Free
Download: Atom Download Link
Popular Companies using Atom (Python IDE) — Accenture, Hubspot, Figma, Lyft, and Typeform.

Developed by Github, the top dog in source-code hosting and software version controlling, Atom is a lightweight and cross-platform Code Editor for Python and many other programming languages. Atom provides a lot of features in the form of packages, that enhances its core features. It’s built on HTML, JavaScript, CSS, and Node.js, with the underlying framework being Electron. Features offered include:

Support for third-party packages via built-in Package Manager
Supports developer collaboration
Over 8000 feature and user experience-extending packages
Support for multi-pane file access
Smart code completion
Customizability options

Advantages —

Lightweight code editor
Community-driven development and support

Disadvantages —

Recent updates have increased RAM usage
Some tweaking required in settings before use



Jupyter


Image source — Jupyter.

Category: IDE
First Release Year: February 2015
Browser Compatibility: Chrome, Firefox, Safari
Price: Free
Download: Jupyter Download Link
Popular Companies of Using Jupyter Python IDE — Google, Bloomberg, Microsoft, IBM, and Soundcloud.

Also known as Project Jupyter, it is an open-source and cross-platform IDE that many data scientists and analysts prefer over other tools. Perfect for working on technologies such as AI, ML, DL, along with several programming languages, Python included. Jupyter Notebooks offer seamless creation and sharing of code, text, and equations for various purposes, including analysis, visualization, and development. Features offered include:

Code formatting and highlight
Easy sharing via email, Dropbox
Produces interactive output
Plays well with Big Data
Can be run from local and cloud machines

Advantages —

Requires minimal setup
Perfect for quick data analysis

Disadvantages —

Inexperienced users may find Jupyter complicated

Screenshot for Reference

Image source — Jupyter.
 
How to Choose Best Python IDEs and Code Editors for Yourself
 
Picking the right IDE or Code Editor can mean the difference in saving time with quicker development or losing it due to reckless decisions. We have mentioned a lot of IDEs and Code Editors in the previous section with some of its noteworthy features. If you’re confused about which one you should pick for your next Python project, then we recommend you give it a quick read. After all, what would a programmer be without a proper set of IDEs and Code Editors?
Note: To eliminate problems of different kinds, I want to alert you to the fact this article represents just my personal opinion I want to share, and you possess every right to disagree with it.
Original. Reposted with permission.
 
Bio: Claire D. Costa is a Content Crafter and Marketer at Digitalogy, a tech sourcing and custom matchmaking marketplace that connects people with pre-screened and top-notch developers and designers based on their specific needs across the globe.
Related:

Here are the Most Popular Python IDEs/Editors
New Poll: What Python IDE / Editor you used the most in 2020?
Netflix’s Polynote is a New Open Source Framework to Build Better Data Science Notebooks"
https://www.kdnuggets.com/2020/10/feature-ranking-recursive-feature-elimination-scikit-learn.html,Feature Ranking with Recursive Feature Elimination in Scikit-Learn,This article covers using scikit-learn to obtain the optimal number of features for your machine learning project.,"By Derrick Mwiti, Data Scientist.
comments


Photo by Element5 Digital on Unsplash

 
Feature selection is an important task for any machine learning application. This is especially crucial when the data in question has many features. The optimal number of features also leads to improved model accuracy. Obtaining the most important features and the number of optimal features can be obtained via feature importance or feature ranking. In this piece, we’ll explore feature ranking.
 
Recursive Feature Elimination
 
The first item needed for recursive feature elimination is an estimator; for example, a linear model or a decision tree model.
These models have coefficients for linear models and feature importances in decision tree models. In selecting the optimal number of features, the estimator is trained and the features are selected via the coefficients, or via the feature importances. The least important features are removed. This process is repeated recursively until the optimal number of features is obtained.
 
Application in Sklearn
 
Scikit-learn makes it possible to implement recursive feature elimination via the sklearn.feature_selection.RFE class. The class takes the following parameters:

estimator — a machine learning estimator that can provide features importances via the coef_ or feature_importances_ attributes.
n_features_to_select — the number of features to select. Selects half if it's not specified.
step — an integer that indicates the number of features to be removed at each iteration, or a number between 0 and 1 to indicate the percentage of features to remove at each iteration.

Once fitted, the following attributes can be obtained:

ranking_ — the ranking of the features.
n_features_ — the number of features that have been selected.
support_ — an array that indicates whether or not a feature was selected.

 
Application
 
As noted earlier, we’ll need to work with an estimator that offers a feature_importance_s attribute or a coeff_ attribute. Let’s work through a quick example. The dataset has 13 features—we’ll work on getting the optimal number of features.

import pandas as pddf = pd.read_csv(‘heart.csv’)df.head()



Let’s obtain the X and y features.

X = df.drop([‘target’],axis=1)
y = df[‘target’]


We’ll split it into a testing and training set to prepare for modeling:

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)


Let’s get a couple of imports out of the way:

Pipeline — since we’ll perform some cross-validation. It’s best practice in order to avoid data leakage.
RepeatedStratifiedKFold — for repeated stratified cross-validation.
cross_val_score — for evaluating the score on cross-validation.
GradientBoostingClassifier — the estimator we’ll use.
numpy — so that we can compute the mean of the scores.


from sklearn.pipeline import Pipeline
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import RFE
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier


The first step is to create an instance of the RFE class while specifying the estimator and the number of features you’d like to select. In this case, we’re selecting 6:

rfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=6)


Next, we create an instance of the model we’d like to use:

model = GradientBoostingClassifier()


We’ll use a Pipeline to transform the data. In the Pipeline we specify rfe for the feature selection step and the model that’ll be used in the next step.
We then specify a RepeatedStratifiedKFold with 10 splits and 5 repeats. The stratified K fold ensures that the number of samples from each class is well balanced in each fold. RepeatedStratifiedKFold repeats the stratified K fold the specified number of times, with a different randomization in each repetition.

pipe = Pipeline([(‘Feature Selection’, rfe), (‘Model’, model)])
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=36851234)
n_scores = cross_val_score(pipe, X_train, y_train, scoring=’accuracy’, cv=cv, n_jobs=-1)
np.mean(n_scores)


The next step is to fit this pipeline to the dataset.

pipe.fit(X_train, y_train)


With that in place, we can check the support and the ranking. The support indicates whether or not a feature was chosen.

rfe.support_
array([ True, False,  True, False,  True, False, False,  True, False,True, False,  True,  True])


We can put that into a dataframe and check the result.

pd.DataFrame(rfe.support_,index=X.columns,columns=[‘Rank’])



We can also check the relative rankings.

rf_df = pd.DataFrame(rfe.ranking_,index=X.columns,columns=[‘Rank’]).sort_values(by=’Rank’,ascending=True)rf_df.head()



 
Automatic Feature Selection
 
Instead of manually configuring the number of features, it would be very nice if we could automatically select them. This can be achieved via recursive feature elimination and cross-validation. This is done via the sklearn.feature_selection.RFECV class. The class takes the following parameters:

estimator — similar to the RFE class.
min_features_to_select — the minimum number of features to be selected.
cv— the cross-validation splitting strategy.

The attributes returned are:

n_features_ — the optimal number of features selected via cross-validation.
support_ — the array containing information on the selection of a feature.
ranking_ — the ranking of the features.
grid_scores_ — the scores obtained from cross-validation.

The first step is to import the class and create its instance.

from sklearn.feature_selection import RFECVrfecv = RFECV(estimator=GradientBoostingClassifier())


The next step is to specify the pipeline and the cv. In this pipeline we use the just created rfecv.

pipeline = Pipeline([(‘Feature Selection’, rfecv), (‘Model’, model)])
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=36851234)
n_scores = cross_val_score(pipeline, X_train, y_train, scoring=’accuracy’, cv=cv, n_jobs=-1)
np.mean(n_scores)


Let’s fit the pipeline and then obtain the optimal number of features.

pipeline.fit(X_train,y_train)


The optimal number of features can be obtained via the n_features_ attribute.

print(“Optimal number of features : %d” % rfecv.n_features_)Optimal number of features : 7


The rankings and support can be obtained just like last time.

rfecv.support_rfecv_df = pd.DataFrame(rfecv.ranking_,index=X.columns,columns=[‘Rank’]).sort_values(by=’Rank’,ascending=True)
rfecv_df.head()


With the grid_scores_ we can plot a graph showing the cross-validated scores.

import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.xlabel(“Number of features selected”)
plt.ylabel(“Cross validation score (nb of correct classifications)”)
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()




Numbers of features against the accuracy plot

 
Final Thoughts
 
The process for applying this in a regression problem is the same. Just ensure to use regression metrics instead of accuracy. I hope this piece has given you some insight on selecting the optimal number of features for your machine learning problems.
mwitiderrick/Feature-Ranking-with-Recursive-Feature-Elimination
Feature Ranking with Recursive Feature Elimination - mwitiderrick/Feature-Ranking-with-Recursive-Feature-Elimination
 
 
Bio: Derrick Mwiti is a data analyst, a writer, and a mentor. He is driven by delivering great results in every task, and is a mentor at Lapid Leaders Africa.
Original. Reposted with permission.
Related:

How I Consistently Improve My Machine Learning Models From 80% to Over 90% Accuracy
LightGBM: A Highly-Efficient Gradient Boosting Decision Tree
Fast Gradient Boosting with CatBoost"
https://www.kdnuggets.com/2020/10/5-must-read-data-science-papers.html,5 Must-Read Data Science Papers (and How to Use Them),"Keeping ahead of the latest developments in a field is key to advancing your skills and your career. Five foundational ideas from recent data science papers are highlighted here with tips on how to leverage these advancements in your work, and keep you on top of the machine learning game.","By Nicole Janeway Bills, Data Scientist at Atlas Research.
comments

Photo by Rabie Madaci on Unsplash.
Data science might be a young field, but that doesn’t mean you won’t face expectations about having an awareness of certain topics. This article covers several of the most important recent developments and influential thought pieces.
Topics covered in these papers range from the orchestration of the DS workflow to breakthroughs in faster neural networks to a rethinking of our fundamental approach to problem solving with statistics. For each paper, I offer ideas for how you can apply these ideas to your own work
 
#1 — Hidden Technical Debt in Machine Learning Systems
 
The team at Google Research provides clear instructions on antipatterns to avoid when setting up your data science workflow. This paper borrows the metaphor of technical debt from software engineering and applies it to data science.

via DataBricks.
As the next paper explores in greater detail, building a machine learning product is a highly specialized subset of software engineering, so it makes sense that many lessons drawn from this discipline will apply to data science as well.
How to use: follow the experts’ practical tips to streamline development and production.
 
#2 — Software 2.0
 
This classic post from Andrej Karpathy articulated the paradigm that machine learning models are software applications with code based on data.
If data science is software, what exactly are we building towards? Ben Bengafort explored this question in an influential blog post called “The Age of the Data Product.”

The data product represents the operationalization phase of an ML project. Photo by Noémi Macavei-Katócz on Unsplash.
How to use: read more about how the data product fits into the model selection process.
 
#3 — BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
 
In this paper, the team at Google Research put forward the natural language processing (NLP) model that represented a step-function increase in our capabilities in for text analysis.
Though there’s some controversy over exactly why BERT works so well, this is a great reminder that the machine learning field may have uncovered successful approaches without fully understanding how they work. As with nature, artificial neural networks are steeped in mystery.

 In this delightful clip, the Director of Data Science at Nordstrom explains how artificial neural nets draw inspiration from nature.
How to use:

The BERT paper is imminently readable and contains some suggested default hyperparameter settings as a valuable starting point (see Appendix A.3).
Whether or not you’re new to NLP, check out Jay Alammar’s “A Visual Guide to Using BERT for the First Time” for a charming illustration of BERT’s capabilities.
Also, check out ktrain, a package that sits atop Keras (which in turn sits atop TensorFlow) that allows you to effortlessly implement BERT in your work. Arun Maiya developed this powerful library to enable speed to insight for NLP, image recognition, and graph-based approaches.

 
#4 — The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks
 
While NLP models are getting larger (see GPT-3’s 175 billion parameters), there’s been an orthogonal effort to find smaller, faster, more efficient neural networks. These networks promise quicker runtimes, lower training costs, and less demand for compute resources.
In this groundbreaking paper, machine learning wiz kids Jonathan Frankle and Michael Carbin outline a pruning approach to uncover sparse sub-networks that can attain comparable performance to the original, significantly larger neural network.

via Nolan Day’s “Breaking down the Lottery Ticket Hypothesis.”
The Lottery Ticket refers to the connections with initial weights that make them particularly effective. The finding offers many advantages in storage, runtime, and computational performance - and won a best paper award at ICLR 2019. Further research has built on this technique, proving its applicability and applying it to an originally sparse network.
How to use:

Consider pruning your neural nets before putting them into production. Pruning network weights can reduce the number of parameters by 90%+ while still achieving the same level of performance as the original network.
Also, check out this episode of the Data Exchange podcast where Ben Lorica talks to Neural Magic, a startup that’s looking to capitalize on techniques such as pruning and quantization with a slick UI that makes achieving sparsity easier.

Read more:

Check out this interesting sidebar from one of the “The Lottery Ticket” authors about flaws in how the machine learning community evaluates good ideas.

 
#5 — Releasing the death-grip of null hypothesis statistical testing (p < .05)
 

Classical hypothesis testing leads to over-certainty and produces the false idea that causes have been identified via statistical methods. (Read more)

Hypothesis testing predates the use of computers. Given the challenges associated with this approach (such as the fact that even statisticians find it nearly impossible to explain p-value), it may be time to consider alternatives such as somewhat precise outcome testing (SPOT).

“Significant” via xkcd.
How to use:

Check out this blog post, “The Death of the Statistical Tests of Hypotheses,” where a frustrated statistician outlines some of the challenges associated with the classical approach and explains an alternative utilizing confidence intervals.

Sign up to get notified when “Resources to Supercharge your Data Science in the Last Months of 2020” comes out
Original. Reposted with permission.
 
Bio: Nicole Janeway Bills is a machine learning engineer with experience in commercial and federal consulting. Proficient in Python, SQL, and Tableau, Nicole has business experience in natural language processing (NLP), cloud computing, statistical testing, pricing analysis, and ETL processes, and aims to use this background to connect data with business outcomes and continue to develop technical skillsets.
Related:

AI Papers to Read in 2020
Must-read NLP and Deep Learning articles for Data Scientists
13 must-read papers from AI experts"
https://www.kdnuggets.com/2020/10/advice-aspiring-data-scientists.html,Advice for Aspiring Data Scientists,Are you a student of some type asking how to get into Data Science? You've come to the right place. Read on for both common and less basic advice on entering the field and excelling in the profession.,"comments
By Tyler Richards, Data Scientist @ Facebook

 
Around once a month, I get emailed by a student of some type asking how to get into Data Science, I've answered it enough that I decided to write it out here so I can link people to it. So if you’re one of those students, welcome!
I'll segment this into basic advice, which can be found quite easily if you just google 'how to get into data science' and advice that is less common, but advice that I've found very useful over the years. I'll start with the latter, and move on to basic advice. Obviously take this with a grain of salt as all advice comes with a bit of survivorship bias.
 
Less Basic Advice:
 
1. Find a solid community
If you’re at a university, half the point of being there is to find smart, ambitious, and motivated people like yourself to learn and grow with. For my alma mater, that community was the Data Science and Informatics club. Communities/networks help you get started, keep you motivated, and are key for scoring internships and full time offers in the long term.
2. Apply Data Science to Things you Enjoy
Getting good at anything is difficult (duh), and applying data science to a field or area you care about helps you stay motivated and stand out. A couple of my examples of this are: using UF's (alma mater) student government elections to learn about machine learning approaches, or tracking my friends' Elo scores by recording our games of ping pong. These projects taught me essential skills without explicitly feeling like work.
Getting useful practice that is representative of the job you want to perform in the future is crucial because out of this practice you can only get one of two things:
a. The realization that you don't actually like this type of data science in which case you should stop reading immediately
b. Valuable experience that you can easily write about (blog) or talk about (to people who want to pay you money)
This brings me to my next point.
3. Minimize the ‘Clicks to Proof of Competence’
Recruiters will spend 15 seconds on your resume, potential teams will spend 1-5 minutes (at most) on your resume + website/Github (on average, visitors to my portfolio site spend 2 minutes and 16 seconds before moving on). Both groups often use proxies for competence like GPA, school quality, or experience in data from a tech firm (I call these: proof of status). As a result, you should very closely think about the time needed to signal to the reader that you can do whatever job they’re looking to hire for. A rough metric to consider for this is Clicks to Proof of Competence.
If the recruiter has to click on the right repository in your Github and then click through files until they find the Jupyter notebook with unreadable code (without comments nonetheless), you’ve already lost. If the recruiter sees Machine Learning on your resume, but it takes 5 clicks to see any ML product or code that you've made, you've already lost. Anyone can lie on a resume; make a point to direct the reader’s attention quickly, and you’ll be in a significantly better spot.
The way i've thought about optimizing for this metric is pretty clear on my website. It roughly takes 10 seconds to skim the text (I would bet that most people don't read it all the way through), and then immediately people can choose a Data Science project to view, which are ordered by how well they show the work I can do. For starting off in DS, I would highly recommend making a website (even a bootstrap template website is fine) and hosting it on Github pages or heroku with your own domain.
4. Learn Through Research or Entry Level Jobs
After you do those three things, see if you can convince someone to pay you to learn data science. There is a great election data science group at UF that I loved (Dr McDonald and Dr Smith run it currently), but if you go to any research group and interview with them they might pay you for your work. Eventually, with experience like that, then you can apply for internships and get paid super well. The key here is to not start out looking for the incredibly fancy DS internships, but locally at companies or research groups that have Data Science tasks but not enough money to hire a full time Data Scientist. Data Science learning compounds quickly, so start now! Given all of that, let’s move on to the more basic advice.
 
Extremely Basic Advice:
 
Data Science is mostly programming + statistics applied to whatever field you're in, so a background in those two areas is crucial.
1. Statistics
Get a good background in stats as quickly as possible (take classes, learn on your own online). Textbooks will take you far, curiosity will take you farther.
Books/resources:

Naked Statistics (basic, paid)
ISLR (Introduction to Statistical Learning in R) (textbook, free)
Statistics and Probability: Khan Academy (basic, free)

2. Programming
Learn either Python or R and get really good at it. Do something new every day, spend at least 5-10 hours per week on it as soon as possible. Learn SQL after this. You cannot skip around this.
Books/resources:

R for Data Science (free)
Machine Learning Python Cookbook (paid)
Data Science From Scratch (paid)
The SQL Tutorial for Data Analysis (free)
Intro to Comp Sci and Programming in Python (MIT Course, free)

3. Business Experience
At P&G, my data science work was applied to retail. At Facebook, to integrity problems. At Protect Democracy, to, uh, Democracy. Learning about applications of data science into some business context is hard and takes practice, and often involves a solid understanding of metrics, product analytics and incentive structures. This fits in very well with #2 from the less basic advice.
 
Fin
 
Learning data science is hard but I’ve found it to be incredibly rewarding. My final offer to you, in exchange for reading to the bottom of this long-ish piece, is to say that once you finish applying data science to a problem you’re passionate about and posting it somewhere online, DM it to me on Twitter and I promise to read it and retweet it. Good luck!
 
Bio: Tyler Richards is a Data Scientist at Facebook.
Original. Reposted with permission.
Related:

How To Decide What Data Skills To Learn
The unspoken difference between junior and senior data scientists
6 Lessons Learned in 6 Months as a Data Scientist"
https://www.kdnuggets.com/2021/06/train-joint-entities-relation-extraction-classifier-bert-spacy.html,How to Train a Joint Entities and Relation Extraction Classifier using BERT Transformer with spaCy 3,A step-by-step guide on how to train a relation extraction classifier using Transformer and spaCy3.,"comments
By Walid Amamou, Founder of UBIAI


Photo by JJ Ying on Unsplash

 
Introduction
 
One of the most useful applications of NLP technology is information extraction from unstructured texts — contracts, financial documents, healthcare records, etc. — that enables automatic data query to derive new insights. Traditionally, named entity recognition has been widely used to identify entities inside a text and store the data for advanced querying and filtering. However, if we want to semantically understand the unstructured text, NER alone is not enough since we don’t know how the entities are related to each other. Performing joint NER and relation extraction will open up a whole new way of information retrieval through knowledge graphs where you can navigate across different nodes to discover hidden relationships. Therefore, performing these tasks jointly will be beneficial.
Building on my previous article where we fine-tuned a BERT model for NER using spaCy3, we will now add relation extraction to the pipeline using the new Thinc library from spaCy. We train the relation extraction model following the steps outlined in spaCy’s documentation. We will compare the performance of the relation classifier using transformers and tok2vec algorithms. Finally, we will test the model on a job description found online.
 
Relation Classification:
 
At its core, the relation extraction model is a classifier that predicts a relation r for a given pair of entity {e1, e2}. In case of transformers, this classifier is added on top of the output hidden states. For more information about relation extraction, please read this excellent article outlining the theory of fine tuning transformer model for relation classification.
The pre-trained model that we are going to fine-tune is the roberta-base model but you can use any pre-trained model available in huggingface library by simply inputting the name in the config file (see below).
In this tutorial we are going to extract the relationship between the two entities {Experience, Skills} as Experience_in and between {Diploma, Diploma_major} as Degree_in. The goal is to extract the years of experience required in a specific skills and the diploma major associated to the required diploma. You can of course, train your own relation classifier for your own use case such as finding the cause/effect of symptoms in health records or company acquisitions in financial documents. The possibilities are limitless…
In this tutorial, we will only cover the entity relation extraction part. For fine-tuning BERT NER using spaCy 3, please refer to my previous article.
 
Data Annotation:
 
As in my previous article, we use UBIAI text annotation tool to perform the joint entity and relation annotation because of its versatile interface that allows us to switch between entity and relation annotation easily (see below):


UBIAI’s joint entity and relation annotation interface

 
For this tutorial, I have only annotated around 100 documents containing entities and relations. For production, we will certainly need more annotated data.
 
Data Preparation:
 
Before we train the model, we need to convert our annotated data to a binary spacy file. We first split the annotation generated from UBIAI into training/dev/test and save them separately. We modify the code that is provided in spaCy’s tutorial repo to create the binary file for our own annotation (conversion code).
We repeat this step for the training, dev and test dataset to generate three binary spacy files (files available in github).
 
Relation Extraction Model Training:
 
For training, we will provide the entities from our golden corpus and train the classifier on these entities.

Open a new Google Colab project and make sure to select GPU as hardware accelerator in the notebook settings. Make sure GPU is enabled by running: !nvidia-smi
Install spacy-nightly:


!pip install -U spacy-nightly --pre



Install the wheel package and clone spacy’s relation extraction repo:


!pip install -U pip setuptools wheel
!python -m spacy project clone tutorials/rel_component



Install transformer pipeline and spacy transformers library:


!python -m spacy download en_core_web_trf
!pip install -U spacy transformers



Change directory to rel_component folder: cd rel_component
Create a folder with the name “data” inside rel_component and upload the training, dev and test binary files into it:



Training folder

 

Open project.yml file and update the training, dev and test path:


train_file: ""data/relations_training.spacy""dev_file: ""data/relations_dev.spacy""test_file: ""data/relations_test.spacy""



You can change the pre-trained transformer model (if you want to use a different language, for example), by going to the configs/rel_trf.cfg and entering the name of the model:


[components.transformer.model]@architectures = ""spacy-transformers.TransformerModel.v1""name = ""roberta-base"" # Transformer model from huggingfacetokenizer_config = {""use_fast"": true}



Before we start the training, we will decrease the max_length in configs/rel_trf.cfg from the default 100 token to 20 to increase the efficiency of our model. The max_length corresponds to the maximum distance between two entities above which they will not be considered for relation classification. As a result, two entities from the same document will be classified, as long as they are within a maximum distance (in number of tokens) of each other.


[components.relation_extractor.model.create_instance_tensor.get_instances]@misc = ""rel_instance_generator.v1""max_length = 20



We are finally ready to train and evaluate the relation extraction model; just run the commands below:


!spacy project run train_gpu # command to train train transformers
!spacy project run evaluate # command to evaluate on test dataset


You should start seeing the P, R and F score start getting updated:


Model training in progress

 
After the model is done training, the evaluation on the test data set will immediately start and display the predicted versus golden labels. The model will be saved in a folder named “training” along with the scores of our model.
To train the non-transformer model tok2vec, run the following command instead:

!spacy project run train_cpu # command to train train tok2vec
!spacy project run evaluate


We can compare the performance of the two models:

# Transformer model
""performance"":{""rel_micro_p"":0.8476190476,""rel_micro_r"":0.9468085106,""rel_micro_f"":0.8944723618,}
# Tok2vec model
  ""performance"":{""rel_micro_p"":0.8604651163,""rel_micro_r"":0.7872340426,""rel_micro_f"":0.8222222222,}


The transformer based model’s precision and recall scores are significantly better than tok2vec and demonstrate the usefulness of transformers when dealing with low amount of annotated data.
 
Joint Entity and Relation Extraction Pipeline:
 
Assuming that we have already trained a transformer NER model as in my previous post, we will extract entities from a job description found online (that was not part of the training nor the dev set) and feed them to the relation extraction model to classify the relationship.

Install spacy transformers and transformer pipeline
Load the NER model and extract entities:


import spacynlp = spacy.load(""NER Model Repo/model-best"")Text=['''2+ years of non-internship professional software development experience
Programming experience with at least one modern language such as Java, C++, or C# including object-oriented design.1+ years of experience contributing to the architecture and design (architecture, design patterns, reliability and scaling) of new and current systems.Bachelor / MS Degree in Computer Science. Preferably a PhD in data science.8+ years of professional experience in software development. 2+ years of experience in project management.Experience in mentoring junior software engineers to improve their skills, and make them more effective, product software engineers.Experience in data structures, algorithm design, complexity analysis, object-oriented design.3+ years experience in at least one modern programming language such as Java, Scala, Python, C++, C#Experience in professional software engineering practices & best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operationsExperience in communicating with users, other technical teams, and management to collect requirements, describe software product features, and technical designs.Experience with building complex software systems that have been successfully delivered to customersProven ability to take a project from scoping requirements through actual launch of the project, with experience in the subsequent operation of the system in production''']for doc in nlp.pipe(text, disable=[""tagger""]):   print(f""spans: {[(e.start, e.text, e.label_) for e in doc.ents]}"")



We print the extracted entities:


spans: [(0, '2+ years', 'EXPERIENCE'), (7, 'professional software development', 'SKILLS'), (12, 'Programming', 'SKILLS'), (22, 'Java', 'SKILLS'), (24, 'C++', 'SKILLS'), (27, 'C#', 'SKILLS'), (30, 'object-oriented design', 'SKILLS'), (36, '1+ years', 'EXPERIENCE'), (41, 'contributing to the', 'SKILLS'), (46, 'design', 'SKILLS'), (48, 'architecture', 'SKILLS'), (50, 'design patterns', 'SKILLS'), (55, 'scaling', 'SKILLS'), (60, 'current systems', 'SKILLS'), (64, 'Bachelor', 'DIPLOMA'), (68, 'Computer Science', 'DIPLOMA_MAJOR'), (75, '8+ years', 'EXPERIENCE'), (82, 'software development', 'SKILLS'), (88, 'mentoring junior software engineers', 'SKILLS'), (103, 'product software engineers', 'SKILLS'), (110, 'data structures', 'SKILLS'), (113, 'algorithm design', 'SKILLS'), (116, 'complexity analysis', 'SKILLS'), (119, 'object-oriented design', 'SKILLS'), (135, 'Java', 'SKILLS'), (137, 'Scala', 'SKILLS'), (139, 'Python', 'SKILLS'), (141, 'C++', 'SKILLS'), (143, 'C#', 'SKILLS'), (148, 'professional software engineering', 'SKILLS'), (151, 'practices', 'SKILLS'), (153, 'best practices', 'SKILLS'), (158, 'software development', 'SKILLS'), (164, 'coding', 'SKILLS'), (167, 'code reviews', 'SKILLS'), (170, 'source control management', 'SKILLS'), (174, 'build processes', 'SKILLS'), (177, 'testing', 'SKILLS'), (180, 'operations', 'SKILLS'), (184, 'communicating', 'SKILLS'), (193, 'management', 'SKILLS'), (199, 'software product', 'SKILLS'), (204, 'technical designs', 'SKILLS'), (210, 'building complex software systems', 'SKILLS'), (229, 'scoping requirements', 'SKILLS')]


We have successfully extracted all the skills, number of years of experience, diploma and diploma major from the text! Next we load the relation extraction model and classify the relationship between the entities.
Note: Make sure to copy rel_pipe and rel_model from the scripts folder into your main folder:


Scripts folder

 

import randomimport typerfrom pathlib import Pathimport spacyfrom spacy.tokens import DocBin, Docfrom spacy.training.example import Examplefrom rel_pipe import make_relation_extractor, score_relationsfrom rel_model import create_relation_model, create_classification_layer, create_instances, create_tensors# We load the relation extraction (REL) modelnlp2 = spacy.load(""training/model-best"")# We take the entities generated from the NER pipeline and input them to the REL pipelinefor name, proc in nlp2.pipeline:
          doc = proc(doc)# Here, we split the paragraph into sentences and apply the relation extraction for each pair of entities found in each sentence.for value, rel_dict in doc._.rel.items():
        for sent in doc.sents:
          for e in sent.ents:
            for b in sent.ents:
              if e.start == value[0] and b.start == value[1]:
                if rel_dict['EXPERIENCE_IN'] >=0.9 :
                  print(f"" entities: {e.text, b.text} --> predicted relation: {rel_dict}"")


Here we display all the entities having a relationship Experience_in with confidence score higher than 90%:

""entities"":(""2+ years"", ""professional software development"""") --> predicted relation"":
{""DEGREE_IN"":1.2778723e-07,""EXPERIENCE_IN"":0.9694631}""entities"":""(""""1+ years"", ""contributing to the"""") -->
predicted relation"":
{""DEGREE_IN"":1.4581254e-07,""EXPERIENCE_IN"":0.9205434}""entities"":""(""""1+ years"",""design"""") --> 
predicted relation"":
{""DEGREE_IN"":1.8895419e-07,""EXPERIENCE_IN"":0.94121873}""entities"":""(""""1+ years"",""architecture"""") --> 
predicted relation"":
{""DEGREE_IN"":1.9635708e-07,""EXPERIENCE_IN"":0.9399484}""entities"":""(""""1+ years"",""design patterns"""") --> 
predicted relation"":
{""DEGREE_IN"":1.9823732e-07,""EXPERIENCE_IN"":0.9423302}""entities"":""(""""1+ years"", ""scaling"""") --> 
predicted relation"":
{""DEGREE_IN"":1.892173e-07,""EXPERIENCE_IN"":0.96628445}entities: ('2+ years', 'project management') --> 
predicted relation:
{'DEGREE_IN': 5.175297e-07, 'EXPERIENCE_IN': 0.9911635}""entities"":""(""""8+ years"",""software development"""") -->
predicted relation"":
{""DEGREE_IN"":4.914319e-08,""EXPERIENCE_IN"":0.994812}""entities"":""(""""3+ years"",""Java"""") -->
predicted relation"":
{""DEGREE_IN"":9.288566e-08,""EXPERIENCE_IN"":0.99975795}""entities"":""(""""3+ years"",""Scala"""") --> 
predicted relation"":
{""DEGREE_IN"":2.8477e-07,""EXPERIENCE_IN"":0.99982494}""entities"":""(""""3+ years"",""Python"""") -->
predicted relation"":
{""DEGREE_IN"":3.3149718e-07,""EXPERIENCE_IN"":0.9998517}""entities"":""(""""3+ years"",""C++"""") -->
predicted relation"":
{""DEGREE_IN"":2.2569053e-07,""EXPERIENCE_IN"":0.99986637}


Remarkably, we were able to extract almost all the years of experience along with their respective skills correctly with with no false positives or negatives!
Let’s look at the entities having relationship Degree_in:

entities: ('Bachelor / MS', 'Computer Science') -->
predicted relation: 
{'DEGREE_IN': 0.9943974, 'EXPERIENCE_IN':1.8361954e-09} entities: ('PhD', 'data science') --> predicted relation: {'DEGREE_IN': 0.98883855, 'EXPERIENCE_IN': 5.2092592e-09}


Again, we successfully extracted all the relationships between diploma and diploma major!
This again demonstrates how easy it is to fine tune transformer models to your own domain specific case with low amount of annotated data, whether it is for NER or relation extraction.
With only a hundred of annotated documents, we were able to train a relation classifier with good performance. Furthermore, we can use this initial model to auto-annotate hundreds more of unlabeled data with minimal correction. This can significantly speed up the annotation process and improve model performance.
 
Conclusion:
 
Transformers have truly transformed the domain of NLP and I am particularly excited about their application in information extraction. I would like to give a shoutout to explosion AI(spaCy developers) and huggingface for providing open source solutions that facilitates the adoption of transformers.
If you need data annotation for your project, don’t hesitate to try out UBIAI annotation tool. We provide numerous programmable labeling solutions (such as ML auto-annotation, regular expressions, dictionaries, etc…) to minimize hand annotation.
Lastly, checkout this article to learn how to leverage the NER and relation extraction models to build knowledge graphs and extract new insights.
If you have any comment, please leave a note below or email at admin@ubiai.tools!
 
Bio: Walid Amamou is the Founder of UBIAI, an annotation tool for NLP applications, and holds a PhD in Physics.
Original. Reposted with permission.
Related:

How to Fine-Tune BERT Transformer with spaCy 3
Building a Knowledge Graph for Job Search Using BERT
Fine-Tuning Transformer Model for Invoice Recognition"
https://www.kdnuggets.com/2021/01/greatlearning-data-science-analytics-career-trends-2021.html,Data Science and Analytics Career Trends for 2021,Let's check out what are the new data science and analytics career trends for 2021 that may also shape the career options in the future.,"Sponsored Post.

 
Data Science is a hot topic these days! So it stands to reason that data science and analytics related careers are becoming more and more popular (So much so that Data Scientist was even called the hottest job of the 21st century!) There are many new trends in these fields as it is a growing technology and changing every year. These range from the addition of data science to various new industries like marketing, finance, etc. to new innovations such as Decision Intelligence, Data as a Service, etc. 
Many of these innovations and trends impact careers related to data science and analytics. This is especially true as data science is a multidisciplinary field that also interacts with various other technologies like Artificial Intelligence, Machine Learning, Deep Learning, the Internet of Things, etc. So let's check out what are the new data science and analytics career trends for 2021 that may also shape the career options in the future.
 
Common Data Science and Analytics Career Trends
 
There are many emerging trends that can shape your data science and analytics career in 2021 and in the future. Some of these are terms that you might have never heard before but may become common technologies in the future. After all, who had heard of Data Science 10 years ago?! There are also some of these trends that may die out in the future but that is not known right now.  So let's check out these popular career trends that may become popular in 2021 here:

Rise of Decision Intelligence
There is no doubt that data science and its related fields like artificial intelligence are becoming more and more popular over time. In fact, any big companies you can think of, whether it's Google, Facebook, Apple (Even Netflix!) all use these technologies heavily. But in the future, many more companies will adopt decision intelligence. This means using the data and advanced methods like machine learning to obtain insights and make decisions based on those insights. It is estimated that more than 33% of organizations will use decision intelligence, whether they are tech companies or not.
The popularity of Data Stories
Data dashboards are extremely popular within companies these days. They allow the decision-makers to understand the data with the help of different graphs and charts. But data stories are becoming even more popular than data dashboards and they may play a critical role in the future. These data stories are crafted as stories that take the viewers on a journey of the data and easily explain all the conclusions that are available from the data analysis. Unlike dashboards, the users don't need any detailed technical knowledge to understand these stories. 
Increase of Cloud Services
More and more organizations are adopting data science and artificial intelligence these days but this technology is expensive and the data is not suitable for traditional storage models. Hence the increase in cloud Services! Data as a Service (DaaS) is becoming more and more popular these days because companies can use cloud storage to obtain actionable insights from their data even if they don't have data infrastructure in house. This is making data science more democratic and providing opportunities to small companies who would never be able to afford the cost of data analytics otherwise. 
Branching of Data Analytics
When you think of data, what comes to mind? Mainly data is associated with rows and rows of numbers in the form of tables. But this is not the only type of data that companies possess. In fact, data also include audio, video, texts, customer feedback, etc. that is much more difficult to analyze. But soon enough, data analytics will branch from merely textual data to all the other forms of data. This is much more difficult to do but will also provide companies with a three-dimensional view of their data which they can analyze to obtain maximum profit. This is already used in Sentiment Analysis, which is a branch of artificial intelligence used to analyze the general sentiment from the data.

 
Different Industries that are Using Data Science and Analytics
 
Now, these are some general trends in Data Science and Analytics that will be observed in 2021 and the coming years. However, there are many ways in which Data Science is changing the shape of different industries like marketing, finance, etc. Given enough time, Data Science might be a part of all the industries in the world, not just tech! So let's understand the role of this technology in different industries.

Marketing and Retail
Data Science already plays a huge role in marketing and retail. The most basic thing that almost every company uses is marketing and retail dashboards that visualize the data to see the hidden patterns and trends. Data Science and Machine Learning are also very useful for customer analysis wherein customer data is collected to understand the demographic of customers, the products that are popular with different types of customers, their likes and dislikes, and how to market a particular product to a section of customers. 
Web and Social Media Analytics
The internet and social media are a treasure trove of data! Google alone processes around 20 petabytes of data every day (That's approximately 1 followed by 15 zeros) Companies can use the web and social media analytics to obtain data about their customers and feedback on their performance which can be used to improve their bottom line. Sentiment analysis is a great example of this where companies can obtain their customer reviews from the internet or social media and to understanding the sentiment of the customers towards the company. 
Supply Chain and Logistics
Supply Chain and Logistics may sound boring but it is a critical aspect for companies. Can you imagine Amazon working if their system for transporting products from point A to point B crashed? No! Therefore, Data Science and Analytics is an extremely important part of the Supply Chain and Logistics that companies can use for Inventory Management, Procurement Analysis, Inventory Classification, etc. For example, data analytics algorithms can be used to understand the correlation between demand and supply for companies and create methods to increase sales by always ensuring in-demand items are available.
Finance and Risk Analytics
FinTech is a technology trend that is becoming more and more popular with time. It involved finance companies using cutting edge technology like Data Science and Artificial Intelligence to improve in areas like risk analytics, fraud detection, algorithmic trading, etc. Many big banks and finance companies use Data Science to analyze their large store of data to optimize their risk scoring models and decrease their risks. This data can include financial transactions, lending schemes, interest rates, customer interactions, customer trustworthiness, etc.

 
How to Leverage these Data Science and Analytics trends in 2021?
 
As you have seen, there are various new Data Science and Analytics trends emerging in 2021. You can leverage them to learn more about Data Science and improve your career using the data science courses offered by GreatLearning in collaboration with The University of Texas. These programs will teach you right from the basics of Data Science such as Python, Business Statistica, and Data Visualization to various techniques of Machine Learning such as Supervised and Unsupervised algorithms. You will also get direct domain exposure by doing projects relating to Data Science in different industries like Marketing and Retail, Web and Social Media Analytics, Supply Chain and Logistics, and Finance and Risk Analytics. Some of these projects include Facebook Comments Prediction, Retail Sales Prediction, Insurance Data Visualization, etc."
https://www.kdnuggets.com/2021/04/ab-testing-7-common-questions-answers-data-science-interviews-2.html,"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2","In this second article in this series, we’ll continue to take an interview-driven approach by linking some of the most commonly asked interview questions to different components of A/B testing, including selecting ideas for testing, designing A/B tests, evaluating test results, and making ship or no ship decisions.","By Emma Ding, Data Scientist & Software Engineer at Airbnb.
comments
 
Note: This is the second part of this article. You can read the first part here.
 
Analyzing Tests Results


Photo by Scott Graham on Unsplash

 
Novelty and Primacy Effects
 
When there’s a change in the product, people react to it differently. Some are used to the way a product works and are reluctant to change. This is called the primacy effect or change aversion. Others may welcome changes, and a new feature attracts them to use the product more. This is called the novelty effect. However, both effects will not last long as people’s behavior will stabilize after a certain amount of time. If an A/B test has a larger or smaller initial effect, it’s probably due to novel or primacy effects. This is a common problem in practice, and many interview questions are about this topic. A sample interview question is:


We ran an A/B test on a new feature and the test won, so we launched the change to all users. However, after launching the feature for a week, we found that the treatment effect quickly declined. What is happening?


The answer is the novelty effect. Over time, as the novelty wears off, repeat usage will be decreased so we observe a declining treatment effect.
Now you understand both novelty and primacy effects, how do we address the potential issues? This is a typical follow-up question during interviews.
One way to deal with such effects is to completely rule out the possibility of those effects. We could run tests only on first-time users because the novelty effect and primacy effect obviously doesn’t affect such users. If we already have a test running and we want to analyze if there’s a novelty or primacy effect, we could 1) compare new users’ results in the control group to those in the treatment group to evaluate novelty effect 2) compare first-time users’ results with existing users’ results in the treatment group to get an actual estimate of the impact of the novelty or primacy effect.
 
Multiple testing problem
 
In the simplest form of an A/B test, there are two variants: Control (A) and treatment (B). Sometimes, we run a test with multiple variants to see which one is the best amongst all the features. It can happen when we want to test multiple colors of a button or test different home pages. Then we’ll have more than one treatment group. In this case, we should not simply use the same significance level of 0.05 to decide whether the test is significant because we are dealing with more than 2 variants, and the probability of false discoveries increases. For example, if we have 3 treatment groups to compare with the control group, what is the chance of observing at least 1 false positive (assume our significance level is 0.05)?
We could get the probability that there is no false positives (assuming the groups are independent),
 
Pr(FP = 0) = 0.95 * 0.95 * 0.95 = 0.857
 
then obtain the probability that there’s at least 1 false positive
 
Pr(FP >= 1) = 1 — Pr(FP = 0) = 0.143
 
With only 3 treatment groups (4 variants), the probability of a false positive (or Type I error) is over 14%. This is called the “multiple testing” problem. A sample interview question is


We are running a test with 10 variants, trying different versions of our landing page. One treatment wins and the p-value is less than .05. Would you make the change?


The answer is no because of the multiple testing problem. There are several ways to approach it. One commonly used method is Bonferroni correction. It divides the significance level 0.05 by the number of tests. For the interview question, since we are measuring 10 tests, then the significance level for the test should be 0.05 divided by 10 which is 0.005. Basically, we only claim a test if significant if it shows a p-value of less than 0.005. The drawback of Bonferroni correction is that it tends to be too conservative.
Another method is to control the false discovery rate (FDR):
 
FDR = E[# of false positive / # of rejections]
 
It measures out of all of the rejections of the null hypothesis, that is, all the metrics that you declare to have a statistically significant difference. How many of them had a real difference as opposed to how many were false positives. This only makes sense if you have a huge number of metrics, say hundreds. Suppose we have 200 metrics and cap FDR at 0.05. This means we’re okay with seeing false positives 5 of the time. We will observe at least 10 false positive in those 200 metrics every time.
 
Making Decisions


Photo by You X Ventures on Unsplash

 
Ideally, we see practically significant treatment results, and we could consider launching the feature to all users. But sometimes, we see contradicting results, such as one metric goes up while another one goes down, so we need to make a win-lost tradeoff. A sample interview question is:


After running a test, you see the desired metric, such as the click-through rate is going up while the number of impressions is decreasing. How would you make a decision?


In reality, it can be very involved to make product launch decisions because various factors are taken into consideration, such as the complexity of implementation, project management effort, customer support cost, maintenance cost, opportunity cost, etc.
During interviews, we could provide a simplified version of the solution, focusing on the current objective of the experiment. Is it to maximize engagement, retention, revenue, or something else? Also, we want to quantify the negative impact, i.e. the negative shift in a non-goal metric, to help us make the decision. For instance, if revenue is the goal, we could choose it over maximizing engagement assuming the negative impact is acceptable.
 
Resources
 
Lastly, I’d like to recommend two resources for you to learn more about A/B testing.

Udacity’s free A/B testing course covers all the fundamentals of A/B testing.
Trustworthy online controlled experiments — A practical guide to A/B testing by Ron Kohavi, Diane Tang, and Ya Xu. It has in-depth knowledge on how to run A/B tests in industry, the potential pitfalls, and solutions. It contains a lot of useful stuff, so I actually plan to write a post to summarize the content of the book. Stay tuned if you are interested!

 
Bio: Emma Ding is a Data Scientist & Software Engineer at Airbnb.
Original. Reposted with permission.
Related:

A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1
5 Things to Know About A/B Testing
How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals"
https://www.kdnuggets.com/2021/01/comprehensive-guide-normal-distribution.html,Comprehensive Guide to the Normal Distribution,Drop in for some tips on how this fundamental statistics concept can improve your data science.,"By Nicole Janeway Bills, Data Scientist at Atlas Research.
comments


Photo by Cameron Casey from Pexels

 
The distribution of data refers to the way the data is spread out. In this article, we’ll discuss the essential concepts related to the normal distribution:

Ways to measure normality
Methods to transform a dataset to fit the normal distribution
Use of the normal distribution to represent naturally occurring phenomena and offer statistical insights

 
Overview
 
Data distribution is of great importance in statistics because we are pretty much always sampling from a population where the full distribution is unknown. The distribution of our sample may put limitations on the statistical techniques available to us.


Normal distribution, where f(x) = probability density function, σ = standard deviation, and μ = mean

 
The normal distribution is a frequently observed continuous probability distribution. When a dataset conforms to the normal distribution, it is possible to utilize many handy techniques to explore the data:

Knowledge of the percentage of data within each standard deviation
Linear least squares regression
Inference based on the sample mean (e.g., t-test)

In some cases, it’s beneficial to transform a skewed dataset so that it conforms to the normal distribution, thereby unlocking the use of this set of statistical techniques. This is more likely to be relevant when your data is almost normally distributed except for some distortion. More on this in a moment.
Normal distributions have the following features:

Symmetric bell shape
Mean and median are equal (at the center of the distribution)
≈68% of the data falls within 1 standard deviation of the mean
≈95% of the data falls within 2 standard deviations of the mean
≈99.7% of the data falls within 3 standard deviations of the mean



M.W. Toews via Wikipedia

 
Here are some terms you should be familiar with relevant to a general overview of the normal distribution:

Normal Distribution: a symmetric probability distribution that is frequently used to represent real-valued random variables; sometimes called the bell curve or Gaussian distribution
Standard Deviation: measure of the amount of variation or dispersion of a set of values; calculated as the square root of variance
Variance: the distance of each data point from the mean

 
How to use the Normal Distribution
 
If your dataset does not conform to the normal distribution, here are some suggestions:

Collect more data: a small sample size or lack of data quality could be distorting your otherwise normally distributed dataset. As is often the case in Data Science, the solution could be to collect more data.
Reduce sources of variance: reduction of outliers could result in normally distributed data.
Apply a power transform: for skewed data, you might choose to apply the Box-Cox method, which refers to taking the square root and the log of the observation.

In the sections that follow, we’ll explore some measures of normality and how you would use them in a Data Science project.
 
Skewness
 
Skewness is a measure of asymmetry relative to the mean. Here’s a graph of a left skewed distribution.


Rodolfo Hermans via Wikipedia

 
💡 I’ve always found this to be a bit counterintuitive, so it’s worth paying close attention here. This graph has negative skewness. This means that the tail of the distribution is longer on the left. The counterintuitive bit (to me at least) is that most of the data points are clustered to the right. Do not be tempted to confuse with right or positive skewness, which would be represented by this graph’s mirror image.
 
How to use Skewness
 
Understanding skewness is important because it is a key factor in model performance. To measure skewness, use skew from the scipy.stats module.


via SciPy

 
The skewness measure can clue us in to potential deviation in model performance across the feature values. A positively skewed feature, like the second array above, will enable better performance on lower values, given that we’re providing more data in that range (opposed to higher value outliers).
 
Kurtosis
 
From Greek kurtos, meaning curved, kurtosis is a measure of the tailedness of the distribution. Kurtosis is typically measured relative to 0, the kurtosis value of the normal distribution using Fisher’s definition. A positive kurtosis value indicates “fatter” tails (i.e., a slimmer bell curve with more outliers).


The Laplace Distribution has kurtosis > 0. via John D. Cook Consulting.

 
How to use Kurtosis
 
Understanding kurtosis provides a lens to the presence of outliers in a dataset. To measure kurtosis, use kurtosis from the scipy.stats module.


via SciPy

 
A negative kurtosis value indicates data that is more tightly grouped around the mean with fewer outliers.
 
A caveat about the Normal Distribution
 
You may have heard that many naturally occurring datasets conform to the normal distribution. This claim has been made for everything from IQ to human heights.
While it’s true that the normal distribution is drawn from observations of nature and does occur frequently, we risk oversimplification by applying this assumption too liberally.

The normal model often doesn’t fit well in the extremes. It often underestimates the probability of rare events. The Black Swan by Nassim Nicholas Taleb gives numerous examples of rare events that were not as rare as a normal distribution would predict.

Why heights are not normally distributed
In my previous post, I speculated on why heights are normally distributed, that is, why their statistical distribution...
 
 
Summary
 
In this brief article on the normal distribution, we covered some fundamental concepts, how it is measured, and how it is used. Be careful not to overapply the normal distribution or you risk discounting the likelihood of outliers. Hope this article provided some insight on this commonly observed and highly useful statistical concept.
 
More Articles You Might Enjoy
 
How to Use Clustering to Create a Neighborhood Explorer Tool
A step-by-step walkthrough of using sklearn’s clustering algorithm to create an interactive dashboard for your city.
 
Data Science for the New Normal — Lessons from a $1.4B Startup
Post-COVID, machine learning is increasingly crucial for business success.
 
10 Python Skills They Don’t Teach in Bootcamp
Ascend to new heights in Data Science and Machine Learning with this list of coding tips.
 
 
Bio: Nicole Janeway Bills is Data Scientist with experience in commercial and federal consulting. She helps organizations leverage their top asset: a simple and robust Data Strategy. Sign up for more of her writing.
Original. Reposted with permission.
Related:

Overview of data distributions
Essential Math for Data Science: The Poisson Distribution
Looking Normal(ly Distributed)"
https://www.kdnuggets.com/2021/02/northwestern-ms-data-science.html,Online MS in Data Science from Northwestern,"Advance your data science career with Northwestern. Build the essential technical, analytical, and leadership skills needed for careers in today's data-driven world in Northwestern's Master of Science in Data Science program. Apply now.","Advance your data science career with Northwestern
      

	The integration of data science and business strategy has created a demand for 
	professionals who can make sense of big data. Build the essential technical, 
	analytical, and leadership skills needed for careers in today's data-driven world in 
	Northwestern's Master of Science in Data Science program. 
      

	You'll learn from an accomplished faculty of leading industry experts. You can 
	choose from a wide range of specializations and electives to suit your goals as you 
	earn your Northwestern University master's degree entirely online.
      













	      Summer Application Deadline
	    

	      April 15
	    






	      Fall Application Deadline
	    

	      July 15
	    

















 Sports reporting is dense withstatistics. Northwestern has helpedme present difficult concepts so thatviewers aren't put off by the math. Inbroadcasting, the need tocommunicate data is critical.”  
	    

	      Edward Egros 
	    

	      SPORTS REPORTER AND ANCHOR FORKDFW DALLAS 
	    









	Related Programs
      

	  Master of Science in Information Systems


	  Master of Science in Health Analytics










		Northwestern UniversitySchool of Professional Studies


	      339 East Chicago Avenue
	      Chicago, Illinois 60611


312-503-2579




	      CONNECT WITH US"
https://www.kdnuggets.com/2021/06/managing-reusable-python-code-data-scientist.html,Managing Your Reusable Python Code as a Data Scientist,"Here are a few approaches that I have settled on for managing my own reusable Python code as a data scientist, presented from most to least general code use, and aimed at beginners.","By Matthew Mayo, KDnuggets.
comments

Photo by Chris Ried on Unsplash
 
There are lots of different approaches to managing your own code, which will differ depending on your requirements, personality, technical know-how, role, and numerous other factors. While a highly-experienced developer may have an incredibly regimented method of organizing their code across multiple languages, projects, and use cases, a data analyst that rarely writes their own code may be much more ad hoc and lackadaisical out of lack of necessity. There really is no right or wrong, it's simply a matter of what works — and is appropriate — for you.
To be specific, what I'm referring to by ""managing code"" is how you organize, store, and recall different pieces of code you, yourself, have written and found useful as long-term additions to your programming toolbox. Programming is all about automating, and so if, as someone who writes code, you find that you are performing similar tasks repetitively, it's only makes sense that you somehow automated the recalling of the code associated with that task. 
This is why you are already using third-party libraries. No need to re-implement a support vector machine code base from scratch every time you want to use it; instead, you make use of a library — perhaps Scikit-learn — and take advantage of the collective work of numerous folks perfecting some code over time.
Extending this idea to the personal programming sphere only makes sense. You may already be doing this (I hope you are), but if not, here are a few approaches that I have settled on for managing my own reusable Python code as a data scientist, presented from most to least general code use.
 
Full-blown libraries
 
This is the most general approach there is, and what could be argued is the most ""professional""; however, this alone does not make it the right choice all the time.
If you find that you are using the same functionality in numerous use cases, and doing so regularly, this is the way to go. This also makes sense if the functionality you want to reuse is easily parameterizable; that is, the task can be handled over and over again by writing and calling a generalized function with variables you can define each time you call.
For example, I often find that I want to find the nth occurrence of some substring in a string, and there is no Python standard library function for this. Thus, I have a simple piece of code that accepts a string, and substring, and the nth occurrence I am looking for as input, and returns the position in the string which this nth occurrence begins (lifted long ago from here).

def find_nth(haystack, needle, n):
    start = haystack.find(needle)
    while start >= 0 and n > 1:
        start = haystack.find(needle, start+len(needle))
        n -= 1
    return start


Since I deal with a lot of text processing, I have collected this with numerous other text processing functions I regularly use and created a library that resides on my computer as any other Python library would, and am able to import this library as any other. The steps for creating the library are somewhat lengthy, though straightforward, and so I will not cover them here, but this article is one of very many that does so well.
So now that I have a textproc library, I can import and use my find_nth function easily, and as often as I like, without having to copy and paste the function into each and every program I write that I use it in. 

from textproc import find_nth

segment = line[:find_nth(line, ',', 4)].strip()


Also, if I want to extend the library to add more functions, or change the existing find_nth code, I can do so in one spot and just re-import.
 
Project-specific shared scripts
 
Perhaps you don't need a full-blown library, as the code you want to reuse doesn't seem to have a use beyond the project you are currently working on, but you do need to reuse it within a specific project. In this case, you can place the functions together in one script, and simply import that script by name. It's the poor woman's library, but it is often just what is needed.
In my graduate work I had to write a lot of code related to unsupervised learning, specifically k-means clustering. I wrote what became functions for  initializing centroids, computing distances between data points and centroids, recalculating centroids, etc., and doing numerous of these tasks using different algorithms. I soon found that keeping a separate script with copies of some of these algorithm functions was not optimal, and so moved them out into their own scripts to be imported. It worked nearly the same way as a library, but the process was path-specific, and was meant for this project only.
Soon I had scripts for different centroid initialization functions and distance computation functions, and for data-loading and processing functions as well. As this code all became more and more parameterized and generally useful, the code eventually made its way into a legitimate library.
This seems to be how things usually progress, at least in my experience: You write a function in your script that you need to use now, and you use it. The project expands, or you move on to a similar project, and you realize that same function would be handy to have now. So that function gets dropped down to a script of its own, and you import it to use. If this usefulness continues beyond the near term, and you find that function having more general and longer term use, that function now gets added to an existing library, or is the basis for a new one. 
However, another specific useful aspect of importing simple scripts is when using Jupyter notebooks. Given the ad hoc, exploratory, and experimental nature of much of what goes on in Jupyter notebooks, I'm not a fan of importing notebooks into other notebooks as modules. If I find that more than one notebook is making regular use of some code excerpt, that code goes gets dropped down into a script stored in the same folder which then gets imported into the notebook(s). This approach makes much more sense to me, and provides more stability by knowing that one notebook another notebook relies on is not being edited in a harmful manner.
 
Task-specific templates
 
I find that I often perform some of the same tasks over and over again which do not lend well to being parameterized, or are tasks which could be parameterized but with more effort than it is worth. In such cases, I employ code templating, or boiler-plating. This is much more the copying and pasting of code that I wanted to avoid in all cases at the outset of this article, but sometimes it's the right choice.
For example, I often need to ""listify,"" for lack of a better word, the contents of a Pandas DataFrame, and while writing a function that could determine the number of columns, could accept as input the columns to use, etc., often the output also needs to be tweaked, all of which points to  writing a function being far too time consuming.
In this case, I just write up a script template that can easily be changed, and keep it handy in a folder of similar templates. Here's an excerpt of listify_df, which goes from CSV file to Pandas DataFrame, to the desired HTML output.

import pandas as pd

# Read CSV file into dataframe
csv_file = 'data.csv'
df = pd.read_csv(csv_file)

# Iterate over df, creating numbered list entries
i = 1
for index, row in df.iterrows():
	entry = '<b>' + str(i) + \
			'. <a href=""' + \
			row['url'] + \
			'"">' + \
			row['title'] + \
			'</a> + \
			'\n\n<blockquote>\n' + \
			row['description'] + \
			'\n</blockquote>\n'
	i += 1
	print(entry)


In this case, clear filenames and folder organization are helpful for managing these often useful snippets.
 
Short one-liners and blocks
 
Lastly, there are a lot of repetitive snippets you probably type regularly. So why do you do that?
You should be making use of a text expansion tool to insert short ""phrases"" when needed. I use AutoKey to manage such short phrases, which are associated with trigger keywords and then inserted when those keywords are typed.
For example, do you import a lot of the same libraries for all of your projects of a particular type? I do. For instance, you could set up all of the imports you would need for working on a particular task by typing, say, #nlpimport which, once typed, is recognized as a trigger keyword and is replaced with the following:

import sys, requests

import numpy as np
import pandas as pd

import texthero
import scattertext as st

import spacy
from spacy.lang.en.stop_words import STOP_WORDS

from datasets import load_metric, list_metrics
from transformers import pipeline
from fastapi import FastAPI


It should be noted that some IDEs have these capabilities. I, myself, generally use glorified text editors to code, and so AutoKey is necessary (and incredibly useful) in my case. If you have an IDE which takes care of this, great. The point is, you shouldn't need to be typing these over and over all the time.
 
This has been an overview of approaching the management of your reusable Python code as a data scientist. I hope that you have found it useful.
 
Related:

Machine Learning Pipeline Optimization with TPOT
Data Scientists, You Need to Know How to Code
Data Scientist, Data Engineer & Other Data Careers, Explained"
https://www.kdnuggets.com/2021/06/generate-automated-pdf-documents-python.html,How to Generate Automated PDF Documents with Python,Discover how to leverage automation to create dazzling PDF documents effortlessly.,"comments
By Mohammad Khorasani, Data Scientist/Engineer Hybrid


Photo by Austin Distel on Unsplash

 
When was the last time you grappled with a PDF document? You probably don’t have to look too far back to find the answer to that question. We deal with a multitude of documents on a daily basis in our lives and an overwhelmingly large number of those are indeed PDF documents. It is fair to claim that a lot of these documents are tediously repetitive and agonizingly painful to formulate. It is about time we consider leveraging the power of automation with Python to mechanize the tedious so that we may reallocate our precious time to more pressing tasks in our lives.
Mind you, there is absolutely no need to be tech savvy and what we are going to do here should be trivial enough that our inner unsavvy laymen can tackle in short order. After reading this tutorial you will learn how to automatically generate PDF documents with your own data, charts and images all bundled together with a dazzling look and structure.
Specifically, in this tutorial we will automate the following actions:

Creating PDF documents
Inserting images
Inserting text and numbers
Visualizing data

 
Creating PDF Documents
 
For this tutorial, we will be using FPDF which is one of the most versatile and intuitive packages used to generate PDF’s in Python. Before we proceed any further, fire up Anaconda prompt or any other Python IDE of your choice and install FPDF:

pip install FPDF


Then import the stack of libraries that we’ll be using to render our document:

import numpy as np
import pandas as pd
from fpdf import FPDF
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter


Subsequently proceed with creating the first page of your PDF document and set the font with its size and color:

pdf = FPDF(orientation = 'P', unit = 'mm', format = 'A4')
pdf.add_page()
pdf.set_font('helvetica', 'bold', 10)
pdf.set_text_color(255, 255, 255)


You can however change the font whenever you like if you need to have various typefaces.
 
Inserting Images
 
The next logical step would be to give our document a background image that sets the structure for the rest of our page. For this tutorial I used Microsoft PowerPoint to render the formatting for my background image. I simply used text boxes and other visuals to create the desired format and once I was done I grouped everything together by selecting all the elements and hitting Ctrl-G. Finally I saved the grouped elements as a PNG image by right clicking on them and selecting ‘save as picture’.


Background image. Image by author.

 
As you can see above, the background image sets the structure for our page and includes space for charts, figures, text and numbers that will be generated later on. The specific PowerPoint file used to generate this image can be downloaded here.
Subsequently insert the background image into your PDF document and configure its position with the following:

pdf.image('C:/Users/.../image.png', x = 0, y = 0, w = 210, h = 297)


Please note that you can insert as many images as you like by extending the method shown above.
 
Inserting Text and Numbers
 
Adding text and numbers can be done in two ways. We can either specify the exact location we want to place the text:

pdf.text(x, y, txt)


Or alternatively, we can create a cell and then place the text within it. This method would be more suitable for aligning or centering variable or dynamic text:

pdf.set_xy(x, y)
pdf.cell(w, h, txt, border, align, fill) 


Please note that in the methods above:

‘x’ and ‘y’ refer to the specified location on our page
‘w’ and ‘h’ refer to the dimensions of our cell
‘txt’ is the string or number that is to be displayed
‘border’ indicates if a line must be drawn around the cell (0: no, 1: yes or L: left, T: top, R: right, B: bottom)
‘align’ indicates the alignment of the text (L: left, C: center, R: right)
‘fill’ indicates whether the cell background should be filled or not (True, False).

 
Visualizing Data
 
In this part we are going to create a bar chart that will display a timeseries dataset of our credit, debit and balance values versus time. For this we will use Matplotlib to render our figures as such:

In the snippet above, credit, debit and balance are 2-dimensional lists with values for date and transaction amount respectively. Once the chart is generated and saved, it can then be inserted into our PDF document using the method shown in the previous sections.
Similarly, we can generate donut charts with the following snippet of code:

And once you are all done, you can wrap it up by generating the automated PDF document as such:

pdf.output('Automated PDF Report.pdf')


 
Conclusion
 
And there you have it, your very own automatically generated PDF report! Now you’ve learnt how to create PDF documents, insert text and images into them and you’ve also learnt how to generate and embed charts and figures. But you are by no means limited to just that, in fact you can extend these techniques to include other visuals with multiple page documents too. The sky is truly the limit.


Image by author.

 
If you want to learn more about Python and data visualization, then feel free to check out the following (affiliate linked) courses: Python for Everybody Specialization and Data Visualization with Python. In addition, feel free to explore more of my tutorials here.
 
Bio: Mohammad Khorasani is a hybrid of a data scientist and an engineer. Logistician. Candid. Realpolitik. Unlearning dogma one belief at a time. Read more of Mohammad's writings.
Original. Reposted with permission.
Related:

Data Scientists, You Need to Know How to Code
5 Tasks To Automate With Python
How to Make Python Code Run Incredibly Fast"
https://www.kdnuggets.com/2021/08/bemyapp-florida-hacks-ibm.html,Florida Hacks with IBM,Join the Florida Hacks with IBM virtual hackathon and create a project to tackle sustainability challenges. IBM will provide mentorship and data sets to help bring your ideas to life.,"Sponsored Post.

As the world evolves, there are significant observable effects of climate change on the environment. Glaciers are melting and shrinking, sea levels are rising, the oceans are warming, and droughts are becoming more frequent and severe.
The University of Florida, IBM, and the Florida Tech Council are looking for people who want to be part of the solution.
At this hackathon, we are asking you to combat climate change and create solutions that make a difference while utilizing IBM technologies, AI/ML, and/or responsible AI. Tackle sustainability issues and compete for a share of the $100k prize pool, including a $30k grand prize.
See https://floridahackswithibm.bemyapp.com/
In 6 weeks, September 13 through October 22, you and your team will build an application in one of 6 challenge areas that can change the world by using IBM AI and machine learning technologies and with support by leaders in technology and climate from the University of Florida, IBM, and other partner organizations.
As a hackathon participant, you will be provided with $200 in cloud credits to access to all of IBM’s services on the cloud.
This hackathon is free to join and open to US residents only, participants in all 50 states are encouraged to join. All experience levels are encouraged, no past experience with conservation issues is needed.
Register today to join and access exclusive data sets, IBM mentors, technical webinars, Cloud Credits, and more. https://floridahackswithibm.bemyapp.com/"
https://www.kdnuggets.com/2020/07/immuta-scale-sensitive-data-science.html,Scale sensitive data science and analytics with confidence,Listen to this on-demand webinar and hear how WorldQuant Predictive derives insights from building models on sensitive data while maximizing value and minimizing risk.,"Sponsored Post.

 
Deriving insights from and building models on sensitive data is challenging as it must be protected in a way that maximizes value, but minimizes risk. Those who achieve this balance between data utility and privacy will have a huge competitive advantage.
Listen to this on-demand webinar to hear how data science company WorldQuant Predictive achieves this and scales quantitative research in a complex, sensitive data environment."
https://www.kdnuggets.com/2021/04/consider-being-data-engineer-instead-data-scientist.html,Why You Should Consider Being a Data Engineer Instead of a Data Scientist,A new king of the jungle has emerged.,"By Terence Shin, Data Scientist | MSc Analytics & MBA student.
comments


Photo by Ryan Harvey on Unsplash

 
I just want to say that whether you choose data science or data engineering should ultimately depend on your interests and where your passion lies. However, if you’re sitting on the fence, unsure of which to choose because they are of equal interest, then keep reading!
Data science has been a hot topic for a while, but a new king of the jungle has arrived — data engineers. In this article, I’m going to share with you several reasons why you might want to consider pursuing data engineering over data science.
Note that this IS an opinionated article and take what you want from this. That being said, I hope you enjoy!
 
1. Data engineering is fundamentally more important than data science.
 
We’ve all heard the saying “garbage in, garbage out”, but only now are companies starting to truly understand the meaning of this. Machine learning and deep learning can be powerful but only in very special circumstances. Aside from the fact that there needs to be a substantial amount of data and a practical use for ML and DL, companies need to satisfy the data hierarchy of needs from the bottom up.


Image created by Author

 
The same way that we have physical needs (i.e. food and water) before social needs (i.e. the need for relationships), companies need to satisfy several requirements which generally fall under the data engineering umbrella. Notice how data science, specifically machine learning and deep learning, are the very last things that matter.
Simply put, there can be no data science without data engineering. Data engineering is the foundation for a successful data-driven company.
 
2. The demand for data engineers is growing… by a lot.
 
Like I previously said, companies are realizing the need for data engineers. Hence, there is a growing demand for data engineers at the moment and there’s proof.
According to Interview Query’s Data Science Interview report, the number of data science interviews only grew by 10% from 2019 to 2020, while the number of data engineering interviews grew by 40% in the same period of time!
As well, Mihail Eric conducted an analysis on Y-Combinator job postings and found that there were roughly 70% more data engineering roles for hire than data scientist roles.
You might be wondering, “sure the growth is much higher, but what about in terms of absolute numbers?”
I took the liberty of webscraping all Data Scientist and all Data Engineer job postings from Indeed, Monster, and SimplyHired, and I found that the number of job listings is about the same for both!
Overall there were 16577 data scientist job listings and 16262 data engineer job listings.


Image created by Author

 
3. Data engineering skills are extremely useful as a data scientist.
 
In more established companies, the work is typically segregated so that data scientists can focus on data science work and data engineers can focus on data engineering work.
But this is generally not the case for most companies. I would say that the majority of companies actually require their data scientists to know some amount of data engineering skills.


A lot of data scientists end up requiring data engineering skills.


It’s also incredibly beneficial to know data engineering skills as a data scientist and I’ll give an example: If you’re a business analyst that doesn’t know SQL, you’ll have to ask a data analyst to query information every time you want to gather insights, which creates a bottleneck in your workflow. Similarly, if you’re a data scientist without the fundamental knowledge of a data engineer, there will certainly be times when you’ll have to rely on someone else to fix an ETL pipeline or clean data as opposed to doing it on your own.
 
4. Data science is easier to learn than data engineering.
 
In my opinion, it’s much easier to learn data science as a data engineer than learn data engineering skills as a data scientist. Why? Well there’s simply more resources available for data science, and there are a number of tools and libraries that have been built to make data science easier.
And so, if you’re starting out your career, I personally think it’s more worthwhile investing your time learning data engineering than data science because you have more time to invest. When you’re working a full time job and a couple of years into your career, you might find that you don’t have the capacity or energy to invest as much time in learning. So from that perspective, I think it’s better to learn the harder realm first.
 
5. It encompasses an untapped market of opportunities.
 
I’m not just talking about job opportunities, but opportunities to innovate and make data engineering easier with new tools and methodologies.
When data science was initially hyped up, people found several barriers to learning data science, like data modeling and model deployment. Later, companies like PyCaret and Gradio emerged to solve these problems.
Currently, we are in that initial stage with data engineering, and I foresee a number of opportunities to make data engineering easier.
 
Thanks for Reading!
 
While this is an opinionated article, I hope that this sheds a bit of light as to why you may want to be a data engineer. I want to reiterate that whether you choose data science or data engineering should ultimately depend on your interests and where your passion lies. As always, I wish you the best of luck in your endeavors!
Not sure what to read next? I’ve picked another article for you:
4 Reasons Why You Shouldn’t Be a Data Scientist
Why a data science job might not be the right fit for you
 
and another one!
Want to Be a Data Scientist? Don’t Start With Machine Learning.
The biggest misconception aspiring data scientists have
 
Terence Shin

If you enjoyed this, follow me on Medium for more
Interested in collaborating? Let’s connect on LinkedIn
Sign up for my email list here!

 
Original. Reposted with permission.
Related:

Want to Be a Data Scientist? Don’t Start With Machine Learning
7 Most Recommended Skills to Learn to be a Data Scientist
The Most In-Demand Skills for Data Scientists in 2021"
https://www.kdnuggets.com/2021/05/okera-airside-security-data-governance.html,"AIRSIDE LIVE Is Where Big Data, Data Security and Data Governance Converge","Free virtual summit on June 3rd offers sessions from data industry leaders and practitioners on challenges and solutions in an ever-changing, data-driven landscape.","Sponsored Post.

For over a year, those of us who love to travel have missed that special feeling of going “airside” – when we pass security and have no more obstacles between us and our destination. Now you can go AIRSIDE with your data by discovering how to eliminate the obstacles to data security, privacy and governance in an age of ever-evolving regulations.
 
Virtual Summit Summary
 
AIRSIDE LIVE 2021 kicks off on June 3rd, 2021 and explores industry challenges and solutions related to big data, data management, security, privacy and governance. Presented by Okera, this inaugural virtual summit features thought leaders and practitioners from CNN, AWS, Microsoft, Gartner, AES, Credit Suisse, Inspire Brands, 4A’s, BigID, First San Francisco Partners, vArmour, The Bloor Group, and more. Free and open to everyone, AIRSIDE LIVE takes place from 7:30 am - 4:00 pm PT. 
 
The Opening Keynote
 
Mike Rogers, National Security Veteran, host of CNN’s Declassified, and former chairman of the U.S. House Permanent Select Committee on Intelligence, will deliver the opening keynote, Breached Data Is Fueling a Cyber War. 
 
The Scope
 
AIRSIDE LIVE will explore how organizations can align the overlapping and often competing priorities of:

Data Management - DataOps, data lifecycle management, and managing cloud, multi-cloud and hybrid environments.
Data Security - Data discovery, identity access control to data, and gaining full visibility into application and data access.
Data Privacy and Data Governance - Ever-evolving compliance regulations, data governance, building data privacy as a feature and more. 

 
The Itinerary
 
Talks, industry-focused panels, technical sessions and real-world case studies will cover the following topics and more:

Building a framework for a successful digital transformation
Creating a culture of data and ethics
The battle between data agility, data security and data privacy 
The evolution of advertising and privacy 
Enforcing zero-trust with identity access to data 
Future-proofing regulatory gray space
Embarking on the journey to trusted data quality 

 
The Crew (so far)

Ajita Abraham, Nitin Agrawal, Ed Amoroso, Sonali Bhavsar, Charles Blauner, Krish Das, David Fairman, John Fowlkes, Sarah Gadd, Arun Ganesan, Nick Halsey, Katie Hyman, Eric Kavanagh, Judy Ko, Nong Li, Dmytro Lugovyi, Sanjeev Mohan, Kelle O’Neal, Sean Otto, Alison Pepper, Stacey Rolland, Nimrod Vax, Marc Woolward, and others.

AIRSIDE LIVE will also offer multiple networking events, including giveaways where attendees have the opportunity to win a two-night stay at the TWA Airport Hotel in New York, gift cards, and swag.
Ready to help build a future where all data is protected, governed, and used responsibly? Go AIRSIDE LIVE with us for free on June 3rd! Register here."
https://www.kdnuggets.com/2021/07/5-mistakes-data-science-career.html,5 Mistakes I Wish I Had Avoided in My Data Science Career,"Everyone makes mistakes, which can be a good thing when they lead to learning and improvements over time. But, we can also try to first learn from others to expedite our personal growth. To get started, consider these lessons learned the hard way, so you don’t have to.","comments
By Tessa Xie, Senior Data Scientist at Cruise.

Photo by bruce mars on Unsplash.
When I first made the transition from finance to data science, I felt like I was on the top of the world — I got a job in my dream field, my career track is set, I will just keep my head down and work hard, what could go wrong? Well, there were a couple of things… For the following year as a data scientist, there were several mistakes that I’m glad I caught myself making early in my career. This way, I had time to reflect and course-correct before it was too late. After a while, I realized that these mistakes are quite common. In fact, I have observed a lot of DS around me still making these mistakes, unaware that they can hurt their data career in the long run.
If my 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist were what I learned from the best, the lessons in this article are those that I learned the hard way, and I hope I can help you avoid making the same mistakes.
 
Mistake 1: Seeing yourself as a foot soldier instead of a thought partner
 
Growing up, we have always been evaluated based on how well we can follow the rules and orders, especially in school. You will be the top student if you follow the textbook and practice exams and just put in the hard work. A lot of people seem to carry this “foot soldier” mindset into their working environment. In my opinion, this is the exact mindset that’s hindering a lot of data scientists from maximizing their impact and standing out from their peers. I have observed a lot of DS, especially junior ones, think they have nothing to contribute to the decision-making process and would rather retreat to the background and passively implement decisions made for them. This kicks off a vicious cycle — the less you contribute to those discussions, the less likely stakeholders will involve you in future meetings, and the less opportunity you will get to contribute in the future.
Let me give you a concrete example of the difference between a foot soldier and a thought partner in the case of model development. In the data collection and feature brainstorming meetings, the old me used to passively take notes on stakeholders’ suggestions so I can implement them “perfectly” later on. When someone proposed a feature that I knew we didn’t have data for, I would not say anything based on the assumption that they are more senior and they must know something that I overlooked. But guess what, they didn’t. I would later face the situation that 50% of the features we brainstormed would require additional data collection that would put our project deadline at risk. As a result, I often found myself in the undesirable position of the bad-news-bearing messenger in the end. Striving to be a thought partner nowadays, I involve myself early in the conversation and leverage my unique position as the person that’s closest to the data. This way, I can manage the expectations of stakeholders early on and make suggestions to help the team move forward.
How to avoid this:

Make sure you don’t hold back in meetings in which you can contribute something from the data perspective: are stakeholders’ definitions of metrics sufficient for what they want to measure? Is data available for measuring the set of metrics? If not, can we find proxies for the data we DO have?
Imposter syndrome is real, especially among junior DS. Make sure you are aware of this, and whenever you are questioning whether you should say something that “others might have already thought of” or ask a “stupid clarifying question,” YOU SHOULD.
Maintain a level of curiosity about what other people are working on. There are a lot of occasions where I found I could add value by noticing gaps other people may have overlooked due to their lack of understanding of the company’s data.

 
Mistake 2: Pigeonhole yourself into a specific area of data science
 
Do I want to be a data engineer or a data scientist? Do I want to work with marketing & sales data or do the geospatial analysis? You may have noticed that I have been using the term DS so far in this article as a general term for a lot of data-related career paths (e.g., data engineer, data scientist, data analyst, etc.). That’s because the lines are so blurred between these titles in the data world these days, especially in smaller companies. I have observed a lot of data scientists see themselves as ONLY data scientists building models and don’t pay attention to any business aspects or data engineers who only focus on data pipelining and don’t want to know anything about the modeling that’s going on in the company.
The best data talents are the ones who can wear multiple hats or are at least able to understand the processes of other data roles. This comes in especially handy if you want to work in an early stage or growth stage startup, where functions might not be as specialized yet, and you are expected to be flexible and cover a variety of data-related responsibilities. Even if you are in a clearly defined job profile, as you get more experience over time, you might discover that you are interested in transitioning into a different type of data role. This pivot will be much easier if you did not pigeonhole yourself and your skillset into the narrow focus of one specific role.
How to avoid this:

Again, be curious about the projects other data roles are working on. Schedule periodic meetings with colleagues to talk to each other about interesting projects or have different data teams share their work/projects with each other periodically.
If you can’t get exposure to other data roles at work, try to keep up/practice the data skills you don’t use during your free time. For example, if you are a data analyst and haven’t touched modeling in a while, consider practicing the skills through outside projects like a Kaggle competition.

 
Mistake 3: Not keeping up with the development in the field
 
Complacency Kills
Every soldier knows this, and every DS should, too. Being complacent about your data skills and not putting in the time to learn new ones is a common mistake. Doing this in the data field is more dangerous than in some other areas because data science is a field that’s relatively new and is still experiencing drastic changes and developments. There are constantly new algorithms, new tools, and even new programming languages being introduced.
If you don’t want to be that one data scientist who still only knows how to use STATA in 2021 (he exists, I worked with him), then you need to keep up with the developments in the field.
﻿
Don’t let this be you (GIF by GIPHY).
How to avoid this:

Sign up for online classes to learn about new concepts and algorithms or to brush up on the ones you already know but haven’t used in a while on the job. The ability to learn is a muscle everyone should keep practicing, and being a life-long learner is probably the best gift you can give to yourself.
Sign up for a DS newsletter or follow a DS blogger/publication on Medium and develop a habit of following the DS “news.”

 
Mistake 4: Overflexing your analytical muscle
 
If all you have is a hammer, everything looks like a nail. Don’t be that DS who tries to use ML on everything. When I first entered the world of data science, I was so excited about all the fancy models I learned in school and couldn’t wait to try all of them on real-world problems. But the real world is different from academic research, and the 80/20 rule is always at play.
In my previous article about “5 Lessons McKinsey Taught Me,” I wrote about how business impact and interpretability sometimes are more important than the extra several percentage points of your model’s accuracy. Sometimes maybe an assumptions-driven Excel model makes more sense than a multi-layered neural net. In those cases, don’t over-flex your analytical muscle and make your approach overkill. Instead, flex your business muscle and be the DS who also has business acumen.
How to avoid this:

Have a full range of analytical skills/tools in your armory, from simple Excel to advanced ML modeling skills, so you can always assess which tool is the best to use in the situation and not bring a gun to a knife fight.
Understand the business needs before delving into the analysis. Sometimes stakeholders would request an ML model because it’s a popular concept, and they have unrealistic expectations about what ML models can do. It’s your job as a DS to manage the expectations and help them find better and simpler ways to achieve their goals. Remember? Be a thought partner, not a foot soldier.

 
Mistake 5: Think building a data culture is someone else’s job
 
In my article “6 Essential Steps to Building a Great Data Culture,” I wrote about how the lives of data scientists can be horrible and unproductive if the company doesn’t have a great data culture. In fact, I have heard a lot of DS complaining about unproductive ad hoc data requests that should be easily handled by stakeholders in a self-sufficient fashion (for example, changing an aggregation from monthly to daily in Looker, which literally consists of two clicks). Don’t think changing that culture is someone else’s job. If you want to see changes, make them. After all, who is better positioned to build the data culture and educate stakeholders about data than data scientists themselves? Helping to build up the data culture in the company will make your life a lot easier down the road as well as your stakeholders.
How to avoid this:

Make it your responsibility to conduct training for the non-analytical stakeholders and develop self-serve resources.
Make sure you start practicing what you are preaching, start linking queries to slides, link data sources of truth to documents, and start documenting your code and databases. You can’t build up a data culture overnight, so it definitely takes patience.

I do want to point out that it’s OKAY to make mistakes in your career. The most important thing is to learn from those mistakes and to avoid them in the future. Or even better, write them down to help others avoid making the same mistakes.
 
Original. Reposted with permission.
 
Bio: Tessa Xie is an experienced Advanced Analytics Consultant skilled in data science, SQL, R, Python, Consumer Research and Economic Research with a strong engineering background following a Master's degree focused in Financial Engineering from MIT.
Related:

How a Single Mistake Wasted 3 Years of My Data Science Journey
Data Scientists think data is their #1 problem. Here’s why they’re wrong.
Learning from 3 big Data Science career mistakes"
https://www.kdnuggets.com/2021/03/bayesian-hyperparameter-optimization-tune-sklearn-pycaret.html,Bayesian Hyperparameter Optimization with tune-sklearn in PyCaret,"PyCaret, a low code Python ML library, offers several ways to tune the hyper-parameters of a created model. In this post, I'd like to show how Ray Tune is integrated with PyCaret, and how easy it is to leverage its algorithms and distributed computing to achieve results superior to default random search method.","comments
By Antoni Baum, Core Contributor to PyCaret and Contributor to Ray Tune



Here’s a situation every PyCaret user is familiar with: after selecting a promising model or two from compare_models(), it’s time to tune its hyperparameters to squeeze out all of the model’s potential with tune_model().

from pycaret.datasets import get_data
from pycaret.classification import *

data = get_data(""juice"")

exp = setup(
    data,
    target = ""Purchase"",
)
best_model = compare_models()
tuned_best_model = tune_model(best_model)


(If you would like to learn more about PyCaret — an open-source, low-code machine learning library in Python, this guide is a good place to start.)
By default, tune_model() uses the tried and tested RandomizedSearchCV from scikit-learn. However, not everyone knows about the various advanced options tune_model()provides.
In this post, I will show you how easy it is to use other state-of-the-art algorithms with PyCaret thanks to tune-sklearn, a drop-in replacement for scikit-learn’s model selection module with cutting edge hyperparameter tuning techniques. I’ll also report results from a series of benchmarks, showing how tune-sklearn is able to easily improve classification model performance.
 
Random search vs Bayesian optimization
 
Hyperparameter optimization algorithms can vary greatly in efficiency.
Random search has been a machine learning staple and for a good reason: it’s easy to implement, understand and gives good results in reasonable time. However, as the name implies, it is completely random — a lot of time can be spent on evaluating bad configurations. Considering that the amount of iterations is limited, it’d make sense for the optimization algorithm to focus on configurations that it considers promising by taking into account already evaluated configurations.
That is, in essence, the idea of Bayesian optimization (BO). BO algorithms keep track of all evaluations and use the data to construct a “surrogate probability model”, which can be evaluated a lot faster than a ML model. The more configurations have been evaluated, the more informed the algorithm becomes, and the closer the surrogate model becomes to the actual objective function. That way the algorithm can make an informed choice about which configurations to evaluate next, instead of merely sampling random ones. If you would like to learn more about Bayesian optimization, check out this excellent article by Will Koehrsen.
Fortunately, PyCaret has built-in wrappers for several optimization libraries, and in this article, we’ll be focusing on tune-sklearn.
 
tune-sklearn in PyCaret
 
tune-sklearn is a drop-in replacement for scikit-learn’s model selection module. tune-sklearn provides a scikit-learn based unified API that gives you access to various popular state of the art optimization algorithms and libraries, including Optuna and scikit-optimize. This unified API allows you to toggle between many different hyperparameter optimization libraries with just a single parameter.
tune-sklearn is powered by Ray Tune, a Python library for experiment execution and hyperparameter tuning at any scale. This means that you can scale out your tuning across multiple machines without changing your code.
To make things even simpler, as of version 2.2.0, tune-sklearn has been integrated into PyCaret. You can simply do pip install ""pycaret[full]"" and all of the optional dependencies will be taken care of.


How it all works together



!pip install ""pycaret[full]""

from pycaret.datasets import get_data
from pycaret.classification import *

data = get_data(""juice"")

exp = setup(
    data,
    target = ""Purchase"",
)
best_model = compare_models()
tuned_best_model_hyperopt = tune_model(
    best_model,
    search_library=""tune-sklearn"",
    search_algorithm=""hyperopt"",
    n_iter=20
)
tuned_best_model_optuna = tune_model(
    best_model,
    search_library=""tune-sklearn"",
    search_algorithm=""optuna"",
    n_iter=20
)


Just by adding two arguments to tune_model()you can switch from random search to tune-sklearn powered Bayesian optimization through Hyperopt or Optuna. Remember that PyCaret has built-in search spaces for all of the included models, but you can always pass your own, if you wish.
But how well do they compare to random search?
 
A simple experiment
 
In order to see how Bayesian optimization stacks up against random search, I have conducted a very simple experiment. Using the Kaggle House Prices dataset, I have created two popular regression models using PyCaret — Random Forest and Elastic Net. Then, I tuned both of them using scikit-learn’s Random Search and tune-sklearn’s Hyperopt and Optuna Searchers (20 iterations for all, minimizing RMSLE). The process was repeated three times with different seeds and the results averaged. Below is an abridged version of the code — you can find the full code here.

from pycaret.datasets import get_data
from pycaret.regression import *

data = get_data(""house"")
exp = setup(
    data,
    target = ""SalePrice"",
    test_data=data, # so that the entire dataset is used for cross validation - do not normally do this!
    session_id=42,
    fold=5
)
rf = create_model(""rf"")
en = create_model(""en"")

tune_model(rf, search_library = ""scikit-learn"", optimize=""RMSLE"", n_iter=20)
tune_model(rf, search_library = ""tune-sklearn"", search_algorithm=""hyperopt"", n_iter=20)
tune_model(rf, search_library = ""tune-sklearn"", search_algorithm=""optuna"", optimize=""RMSLE"", n_iter=20)

tune_model(en, search_library = ""scikit-learn"", optimize=""RMSLE"", n_iter=20)
tune_model(en, search_library = ""tune-sklearn"", search_algorithm=""hyperopt"", n_iter=20)
tune_model(en, search_library = ""tune-sklearn"", search_algorithm=""optuna"", optimize=""RMSLE"", n_iter=20)


Isn’t it great how easy PyCaret makes things? Anyway, here are the RMSLE scores I have obtained on my machine:


Experiment’s RMSLE scores


And to put it into perspective, here’s the percentage improvement over random search:


Percentage improvement over random search


All of that using the same number of iterations in comparable time. Remember that given the stochastic nature of the process, your mileage may vary. If your improvement is not noticeable, try increasing the number of iterations (n_iter)from the default 10. 20–30 is usually a sensible choice.
What’s great about Ray is that you can effortlessly scale beyond a single machine to a cluster of tens, hundreds or more nodes. While PyCaret doesn’t support full Ray integration yet, it is possible to initialize a Ray cluster before tuning — and tune-sklearn will automatically use it.

exp = setup(
    data,
    target = ""SalePrice"",
    session_id=42,
    fold=5
)

rf = create_model(""rf"")

tune_model(rf, search_library = ""tune-sklearn"", search_algorithm=""optuna"", optimize=""RMSLE"", n_iter=20) # Will run on Ray cluster!


Provided all of the necessary configuration is in place ( RAY_ADDRESSenvironment variable), nothing more is needed in order to leverage the power of Ray’s distributed computing for hyperparameter tuning. Because hyperparameter optimization is usually the most performance-intensive part of creating an ML model, distributed tuning with Ray can save you a lot of time.
 
Conclusion
 
In order to speed up hyperparameter optimization in PyCaret, all you need to do is install the required libraries and change two arguments in tune_model() — and thanks to built-in tune-sklearn support, you can easily leverage Ray’s distributed computing to scale up beyond your local machine.
Make sure to check out the documentation for PyCaret, Ray Tune and tune-sklearn as well as the GitHub repositories of PyCaret and tune-sklearn. Finally, if you have any questions or want to connect with the community, join PyCaret’s Slack and Ray’s Discourse.
Thanks to Richard Liaw and Moez Ali for proofreading and advice.
 
Bio: Antoni Baum is a Computer Science and Econometrics MSc student, as well as a core contributor to PyCaret and contributor to Ray Tune.
Original. Reposted with permission.
Related:

Algorithms for Advanced Hyper-Parameter Optimization/Tuning
5 Tools for Effortless Data Science
5 Things You Are Doing Wrong in PyCaret"
https://www.kdnuggets.com/2021/04/coursera-career-growing-field-google-data-analytics-certificate.html,Start a Career in a Growing Field with Google’s Data Analytics Professional Certificate,"Google's recently launched Data Analytics Professional Certificate on Coursera is great for anyone, regardless of background or experience. The program is completely online, self-paced, and costs $39 per month. Interested in preparing for a new career in a high-growth field?","Sponsored Post.

Google recently launched a Data Analytics Professional Certificate on Coursera. This program is designed to prepare learners for an entry-level role in under six months and offers a stepping stone to well-paying careers. This Certificate doesn’t have any prerequisites and is great for anyone, regardless of background or experience. The program is completely online, self-paced, and costs $39 per month. 
This Professional Certificate also includes resources to help learners enhance their resumes and prepare for interviews. Upon completion, learners can share their information with 100+ partners committed to sourcing talent from Google certificate programs, such as Deloitte, Anthem, Verizon, Snap Inc, and other top brands. Google is also launching apprenticeships in the certificate field of data analytics and offering 200,000 scholarships for their new programs to learners across the U.S., Europe, Middle East, and Africa. 
The Google Data Analytics Professional Certificate is a seven-course certificate exploring analytical skills, concepts, and tools used in many introductory data analytics roles – including SQL, Tableau, RStudio, and Kaggle. With nearly 15,000 open entry-level data analytics roles in the U.S. and an annual median entry-level salary of over $63,000, data analytics is a growing field filled with opportunities.
Interested in preparing for a new career in a high-growth field? Check out the Google Data Analytics Professional Certificate."
https://www.kdnuggets.com/2021/02/hugging-face-transformer-basics.html,Hugging Face Transformers Package – What Is It and How To Use It,"The rapid development of Transformers have brought a new wave of powerful tools to natural language processing. These models are large and very expensive to train, so pre-trained versions are shared and leveraged by researchers and practitioners. Hugging Face offers a wide variety of pre-trained transformers as open-source libraries, and you can incorporate these with only one line of code.","By Nagesh Singh Chauhan, Data Science Enthusiast.
comments

credit
 
Transformers
 
The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. The Transformer was proposed in the paper Attention Is All You Need. It is recommended reading for anyone interested in NLP.
NLP-focused startup Hugging Face recently released a major update to their popular “PyTorch Transformers” library, which establishes compatibility between PyTorch and TensorFlow 2.0, enabling users to easily move from one framework to another during the life of a model for training and evaluation purposes.
The Transformers package contains over 30 pre-trained models and 100 languages, along with eight major architectures for natural language understanding (NLU) and natural language generation (NLG):

BERT (from Google);
GPT (from OpenAI);
GPT-2 (from OpenAI);
Transformer-XL (from Google/CMU);
XLNet (from Google/CMU);
XLM (from Facebook);
RoBERTa (from Facebook);
DistilBERT (from Hugging Face).

The Transformers library no longer requires PyTorch to load models, is capable of training SOTA models in only three lines of code, and can pre-process a dataset with less than 10 lines of code. Sharing trained models also lowers computation costs and carbon emissions.
I am assuming that you are aware of Transformers and its attention mechanism. The prime aim of this article is to show how to use Hugging Face’s transformer library with TF 2.0,
Installation (You don't explicitly need PyTorch)

!pip install transformers 


 
Getting started on a task with a pipeline
 
The easiest way to use a pre-trained model on a given task is to use pipeline(). 🤗 Transformers provides the following tasks out of the box:

Sentiment analysis: is a text positive or negative?
Text generation (in English): provide a prompt, and the model will generate what follows.
Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)
Question answering: provide the model with some context and a question, extract the answer from the context.
Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.
Summarization: generate a summary of a long text.
Language Translation: translate a text into another language.
Feature extraction: return a tensor representation of the text.

Pipelines encapsulate the overall process of every NLP process:

Tokenization: Split the initial input into multiple sub-entities with … properties (i.e., tokens).
Inference: Maps every token into a more meaningful representation.
Decoding: Use the above representation to generate and/or extract the final output for the underlying task.

The overall API is exposed to the end-user through the pipeline() method with the following structurs.
 
GPT-2
 
GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text.
Since the goal of GPT-2 is to make predictions, only the decoder mechanism is used. So GPT-2 is just transformer decoders stacked above each other.
GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where the model is comfortable with large input and can generate lengthy output.
Text generation

from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
generator(""Hello, I like to play cricket,"", max_length=60, num_return_sequences=7)


 
Output:

[{'generated_text': ""Hello, I like to play cricket, but I'd rather play football! So I've decided to create a new game, The Super Bombers, on the Xbox One.\n\nAnd as a reward, you will hear the official announcement for this game!\n\nHere is what you can expect""},
 {'generated_text': 'Hello, I like to play cricket, but sometimes it\'s like being a bad sportsman,"" he says. ""Sometimes I try and make cricket harder but sometimes I am just very happy and I always try to enjoy my cricket.""\n\nWhile at Middlesex, Hautek was inspired by the'},
 {'generated_text': 'Hello, I like to play cricket, but I can\'t really understand what ""good"" and ""bad"" is. Do you have a definition of ""good"" and ""bad""?\n\nYes, I think so. I mean, people who are well trained probably don\'t have that problem with'},
 {'generated_text': 'Hello, I like to play cricket, I play the game of cricket."" The next day, he joined the family tour with his friends. It might have been a brief break for them both that he was so involved. A few days later they met at his cricket training centre, at which the pair'},
 {'generated_text': ""Hello, I like to play cricket, so I wanted to play English cricket... so I called up a friend of mine and, I remember, it wasn't really English, but it actually has lots of good stuff about it.\n\n\nDUPY: It's very interesting, especially as you""},
 {'generated_text': 'Hello, I like to play cricket, but I don\'t really like playing cricket in a stadium full of tourists; there\'s not really any point in playing. We played that game almost three years ago for cricket.\n\n""My favourite time about being here was last year in England. It was'},
 {'generated_text': 'Hello, I like to play cricket, too. The kids of the city always play a good match, I mean, the cricket team is always very young.""'}]


 

generator(""The Indian man worked as a"", max_length=10, num_return_sequences=5)


 
Output:

[{'generated_text': 'The Indian man worked as a waiter in Delhi.'},
 {'generated_text': 'The Indian man worked as a security guard for the'},
 {'generated_text': 'The Indian man worked as a waiter for around ten'},
 {'generated_text': 'The Indian man worked as a waiter on a Sunday'},
 {'generated_text': 'The Indian man worked as a barista in the'}


 
Sentiment analysis

# Allocate a pipeline for sentiment-analysis
classifier = pipeline('sentiment-analysis')
classifier('The secret of getting ahead is getting started.')


 
Output:

[{'label': 'POSITIVE', 'score': 0.9970657229423523}]


 
Question Answering

# Allocate a pipeline for question-answering
question_answerer = pipeline('question-answering')
question_answerer({
    'question': 'What is Newton's third law of motion?',
    'context': 'Newton's third law of motion states that, ""For every action there is equal and opposite reaction""'})


 
Output:

{'score': 0.6062518954277039,
 'start': 42,
 'end': 96,
 'answer': '""For every action there is equal and opposite reaction""'}


 

nlp = pipeline(""question-answering"")

context = r""""""
Microsoft was founded by Bill Gates and Paul Allen in 1975.
The property of being prime (or not) is called primality.
A simple but slow method of verifying the primality of a given number n is known as trial division.
It consists of testing whether n is a multiple of any integer between 2 and itself.
Algorithms much more efficient than trial division have been devised to test the primality of large numbers.
These include the Miller-Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical.
Particularly fast methods are available for numbers of special forms, such as Mersenne numbers.
As of January 2016, the largest known prime number has 22,338,618 decimal digits.
""""""

#Question 1
result = nlp(question=""What is a simple method to verify primality?"", context=context)

print(f""Answer 1: '{result['answer']}'"")

#Question 2
result = nlp(question=""When did Bill gates founded Microsoft?"", context=context)

print(f""Answer 2: '{result['answer']}'"")


 
Output:

Answer 1: 'trial division.'
Answer 2: '1975.'


 
BERT
 
BERT (Bidirectional Encoder Representations from Transformers) makes use of a Transformer, which learns contextual relations between words in a text. In its vanilla form, Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is used. So BERT is just transformer encoders stacked above each other.
Text prediction

unmasker = pipeline('fill-mask', model='bert-base-cased')
unmasker(""Hello, My name is [MASK]."")


 
Output:

[{'sequence': '[CLS] Hello, My name is David. [SEP]',
  'score': 0.007879073731601238,
  'token': 1681,
  'token_str': 'David'},
 {'sequence': '[CLS] Hello, My name is Kate. [SEP]',
  'score': 0.007307342253625393,
  'token': 5036,
  'token_str': 'Kate'},
 {'sequence': '[CLS] Hello, My name is Sam. [SEP]',
  'score': 0.007054011803120375,
  'token': 2687,
  'token_str': 'Sam'},
 {'sequence': '[CLS] Hello, My name is James. [SEP]',
  'score': 0.006197025533765554,
  'token': 1600,
  'token_str': 'James'},
 {'sequence': '[CLS] Hello, My name is Charlie. [SEP]',
  'score': 0.006146721541881561,
  'token': 4117,
  'token_str': 'Charlie'}]


 
Text Summarization

#Summarization is currently supported by Bart and T5.
summarizer = pipeline(""summarization"")

ARTICLE = """"""The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.
First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,
Apollo was later dedicated to President John F. Kennedy's national goal of ""landing a man on the Moon and returning him safely to the Earth"" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress.
Project Mercury was followed by the two-man Project Gemini (1962-66).
The first manned flight of Apollo was in 1968.
Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966.
Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.
Apollo used Saturn family rockets as launch vehicles.
Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.
""""""

summary=summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)[0]

print(summary['summary_text'])


 
Output:

The first manned flight of Apollo ran from 1961 to 1972 . The Apollo program was followed by the two-man ProjectGemini . It was the third mission to land on the Moon .


 
English to German translation

# English to German
translator_ger = pipeline(""translation_en_to_de"")
print(""German: "",translator_ger(""Joe Biden became the 46th president of U.S.A."", max_length=40)[0]['translation_text'])

# English to French
translator_fr = pipeline('translation_en_to_fr')
print(""French: "",translator_fr(""Joe Biden became the 46th president of U.S.A"",  max_length=40)[0]['translation_text'])


 
Output:

German:  Joe Biden wurde der 46. Präsident der USA.
French:  Joe Biden est devenu le 46e président des États-Unis


 
Conversation

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained(""microsoft/DialoGPT-medium"")
model = AutoModelForCausalLM.from_pretrained(""microsoft/DialoGPT-medium"")

# Let's chat for 5 lines
for step in range(5):
   # encode the new user input, add the eos_token and return a tensor in Pytorch
   new_user_input_ids = tokenizer.encode(input("">> User:"") + tokenizer.eos_token, return_tensors='pt')

   # append the new user input tokens to the chat history
   bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids

   # generated a response while limiting the total chat history to 1000 tokens,
   chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)

   # pretty print last output tokens from bot
   print(""DialoGPT: {}"".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))


 
Output:

>> User:Hi
DialoGPT: Hi! :D
>> User:How are you doing?
DialoGPT: I'm doing well! How are you?
>> User:I'm not that good.
DialoGPT: I'm sorry.
>> User:Thank you
DialoGPT: No problem. I'm glad you're doing well.
>> User:bye
DialoGPT: Bye! :D


 
Named Entity Recognition

from transformers import pipeline, set_seed
nlp_token_class = pipeline('ner')
nlp_token_class('Ronaldo was born in 1985, he plays for Juventus and Portugal. ')


 
Output:

[{'word': 'Ronald',
  'score': 0.9978647828102112,
  'entity': 'I-PER',
  'index': 1},
 {'word': '##o', 'score': 0.99903804063797, 'entity': 'I-PER', 'index': 2},
 {'word': 'Juventus',
  'score': 0.9977495670318604,
  'entity': 'I-ORG',
  'index': 11},
 {'word': 'Portugal',
  'score': 0.9991246461868286,
  'entity': 'I-LOC',
  'index': 13}]


 
Features Extraction

import numpy as np
nlp_features = pipeline('feature-extraction')
output = nlp_features('output = nlp_features('Deep learning is a branch of Machine learning'))
np.array(output).shape # (Samples, Tokens, Vector Size)


 
Output:

(1, 10, 768)


 
Zero-shot Learning
 
Zero-Shot learning aims to solve a task without receiving any example of that task at the training phase. The task of recognizing an object from a given image where there weren’t any example images of that object during the training phase can be considered as an example of a Zero-Shot Learning task.

classifier_zsl = pipeline(""zero-shot-classification"")

sequence_to_classify = ""Bill gates founded a company called Microsoft in the year 1975""
candidate_labels = [""Europe"", ""Sports"",'Leadership','business', ""politics"",""startup""]
classifier_zsl(sequence_to_classify, candidate_labels)


 
Output:

{'sequence': 'Bill gates founded a company called Microsoft in the year 1975',
 'labels': ['business',
  'startup',
  'Leadership',
  'Europe',
  'Sports',
  'politics'],
 'scores': [0.6144810318946838,
  0.1874515861272812,
  0.18227894604206085,
  0.006684561725705862,
  0.0063185556791722775,
  0.0027852619532495737]}


 
Using transformers in Widgets
 

import ipywidgets as widgets
nlp_qaA = pipeline('question-answering')

context = widgets.Textarea(
   value='Einstein is famous for the general theory of relativity',
   placeholder='Enter something',
   description='Context:',
   disabled=False
)

query = widgets.Text(
   value='Why is Einstein famous for ?',
   placeholder='Enter something',
   description='Question:',
   disabled=False
)

def forward(_):
   if len(context.value) > 0 and len(query.value) > 0:
       output = nlp_qaA(question=query.value, context=context.value)           
       print(output)

query.on_submit(forward)
display(context, query)



 
Want a lighter model?
 
“Distillation comes into the picture”

credit
One of the main concerns while using Transformer based models is the computational power they require. All over this article, we are using the BERT model as it can be run on common machines, but that’s not the case for all of the models.
For example, Google released a few months ago T5 an Encoder/Decoder architecture based on Transformer and available in transformers with no more than 11 billion parameters. Microsoft also recently entered the game with Turing-NLG using 17 billion parameters. This kind of model requires tens of gigabytes to store the weights and a tremendous compute infrastructure to run such models, which makes it impracticable for the common man!
With the goal of making Transformer-based NLP accessible to everyone, Hugging Face developed models that take advantage of a training process called Distillation, which allows us to drastically reduce the resources needed to run such models with almost zero drops in performance.
 
Classifying text with DistilBERT and Tensorflow
 
You can find Classifying text with DistilBERT and Tensorflow in my Kaggle notebook.
You can also find Hugging Face python notebooks on using transformers for solving various use cases.
 
Related:

Getting Started with 5 Essential Natural Language Processing Libraries
Understanding Transformers, the Data Science Way
A Deep Dive Into the Transformer Architecture – The Development of Transformer Models"
https://www.kdnuggets.com/2021/03/overview-mlops.html,Overview of MLOps,"Building a machine learning model is great, but to provide real business value, it must be made useful and maintained to remain useful over time. Machine Learning Operations (MLOps), overviewed here, is a rapidly growing space that encompasses everything required to deploy a machine learning model into production, and is a crucial aspect to delivering this sought after value.","comments
By Steve Shwartz, AI Author, Investor, and Serial Entrepreneur.

Photo: iStockPhoto / NanoStockk
Considerable data science expertise is usually required to create a dataset and build a model for a particular application.  But building a good model is usually not enough.  In fact, it is not nearly enough.  As illustrated below, developing and testing a model is just the first step.

Machine Learning Model Lifecycle.
Machine Learning Operations (MLOps) is everything else required to make that model useful, including capabilities for an automated development and deployment pipeline, monitoring, lifecycle management, and governance, as illustrated above.  Let’s look at each of these.
 
Automation Pipeline
 
Creating a production ML system requires multiple steps:  First, the data must undergo a series of transformations.  Then, the model is trained.  Usually, this requires experimentation with different network architectures and hyperparameters.  Often, it is necessary to go back to the data and try different features.  Next, the model must be validated with unit tests and integration tests.  It needs to pass tests for data and model bias and explainability.  Finally, it is deployed into a public cloud, an on-premise environment, or a hybrid environment.  Additionally, some steps in the process might require an approval workflow.
If each of these steps is performed manually, the development process tends to be slow and brittle.  Fortunately, many MLOps tools exist to automate these steps from data transformation to deployment end-to-end.  When retraining is necessary, it is an automated, reliable, and reproducible process.
 
Monitoring
 
ML models tend to work well when first deployed and then work less well over time.  As Forrester analyst, Dr. Kjell Carlsson said:  “AI models are like six-year-olds during quarantine: They need constant attention . . . otherwise, something will break.”
It is critical for deployments to include various types of monitoring so that ML teams can be alerted when this starts to happen.  Performance can degrade due to infrastructure issues such as inadequate CPU or memory.  Performance can also degrade when the real-world data that constitute the independent variables that are input to the model start to take on different characteristics than the training data, a phenomenon known as data drift.
Similarly, the model may become less applicable because real-world conditions change, a phenomenon known as concept drift.  For example, many predictive models of customer and supplier behavior were sent into a tailspin by COVID-19.
Some companies also monitor alternative models (e.g., different network architectures or different hyperparameters) to see if any of these “challenger” models starts performing better than the production model.
Often, it makes sense to put guardrails around decisions made by the model.  These guardrails are simple rules that either trigger an alert, prevent the decision, or put the decision into a workflow for human approval.
 
Lifecycle Management
 
When model performance starts to degrade due to data or model drift, model retraining and possibly model re-architecture are required.  However, the data science team shouldn’t have to start from scratch.  In developing the original model, and perhaps in prior re-architectures, they probably tested many architectures, hyperparameters, and features.  It’s critical that all these prior experiments (and results) are recorded so that the data science team doesn’t have to go back to square one.  It’s also critical for communication and collaboration between data science team members.
 
Governance
 
Machine learning models are being used for many applications that impact people like bank loan decision, medical diagnosis, and hiring/firing decisions.  The use of ML models in decision-making has been criticized for two reasons:  First, these models are subject to bias, especially if the training data results in models that discriminate based on race, color, ethnicity, national origin, religion, gender, sexual orientation, or other protected classes.  Second, these models are often black boxes that don’t explain their decision-making.
As a result, organizations that used ML-based decision-making are under pressure to ensure their models don’t discriminate and are capable of explaining their decisions.  Many MLOps vendors are incorporating tools based on academic research (e.g., SHAP and Grad-CAM) that help explain the model decisions and are using a variety of techniques to ensure that the data and models are not biased.  Additionally, they are incorporating bias and explainability tests in their monitoring protocols because models can become biased or lose explanatory capability over time.
Organizations also need to build trust and are starting to ensure that on-going performance, lack of bias, and explainability are auditable.  This requires model catalogs that not only document all the data, parameter, and architecture decisions but also log each decision and provide traceability so that it can be determined what data, model, and parameters were used for each decision, when the model was retrained or otherwise modified, and who made each change.  It is also important for auditors to be able to repeat historical transactions and to test the boundaries of model decision-making with what-if scenarios.
Security and data privacy are also key concerns for organizations using ML.  Care must be taken to ensure the personal information is protected and role-based data access capabilities are essential, especially for regulated industries.
Governments around the world are also moving quickly to regulate ML-based decision-making that affects people.  The European Union has led the way with its GDPR and CRD IV regulations.  In the US, several regulatory agencies, including the US Federal Reserve Bank and the FDA, have created regulations around ML-based decision-making for financial and medical decisions.  A more comprehensive law, the recently proposed Data Accountability and Transparency Act of 2020, is slated for Congressional consideration in 2021.  Regulations will likely evolve to the point where CEO’s need to sign off on the explainability of and the lack of bias in their ML models.
 
The MLOps Landscape
 
As we continue in 2021, the market for MLOps is exploding.  According to analyst firm Cognilytica, it is expected to be a $4 billion market by 2025.
There are big players and small players in the MLOps space.  Major ML platform vendors like Amazon, Google, Microsoft, IBM, Cloudera, Domino, DataRobot, and H2O are incorporating MLOps capabilities into their platforms.  According to Crunchbase, there are 35 private companies in the MLOps space who have raised between $1.8M and $1B in financing and who have between 3 and 2800 employees on LinkedIn:




Financing ($millions)
Number of Employees

Description


Cloudera
1000
2803
Cloudera delivers an Enterprise Data Cloud for any data, anywhere, from the Edge to AI.


Databricks
897
1757
Databricks is a software platform that helps its customers unify their analytics across business, data science, and data engineering.


DataRobot
750
1105
DataRobot brings AI technology and ROI enablement services to global enterprises.


Dataiku
246
556
Dataiku operates as an enterprise artificial intelligence and machine-learning platform.


Alteryx
163
1623
Alteryx accelerates digital transformation by unifying analytics, data science and automated processes.


H2O
151
257
H2O.ai is the open source leader in AI and automatic machine learning with a mission to democratize AI for everyone.


Domino
124
232
Domino is the world's leading Enterprise Data Science Platform, powering data science at over 20% of the Fortune 100.


Iguazio
72
83
The Iguazio Data Science Platform enables you to develop, deploy and manage AI applications at scale and in real-time


Explorium.ai
50
96
Explorium offers a data science platform powered by augmented data discovery and feature engineering


Algorithmia
38
63
Algorithmia is a machine learning model deployment and management solution that automates the MLOps for an organization


Paperspace
23
37
Paperspace powers next-generation applications built on GPUs.


Pachyderm
21
32
Pachyderm is an enterprise-grade data science platform that makes explainable, repeatable, and scalable AI/ML a reality.


Weights and Biases
20
58
Tools for experiment tracking, improved model performance, and results collaboration


OctoML
19
37
OctoML is changing how developers optimize and deploy machine learning models for their AI needs.


Arthur AI
18
28
Arthur AI is a platform that monitors the productivity of machine learning models.


Truera
17
26
Truera provides a Model Intelligence platform for enterprises to analyze machine learning.


Snorkel AI
15
39
Snorkel AI is focused on making AI practical through Snorkel Flow: the data-first platform for enterprise AI


Seldon.io
14
48
Machine Learning Deployment Platform


Fiddler Labs
13
46
Fiddler enables users to create AI solutions that are transparent, explainable, and understandable.


run.ai
13
26
Run:AI develops an automated distributed training technology that virtualizes and accelerates deep learning.


ClearML (Allegro)
11
29
ML / DL Experiment Manager and ML-Ops Open-Source Solution End-to-End Product Life-cycle Management Enterprise Solution


Verta
10
15
Verta builds software infrastructure to help enterprise data science and machine learning (ML) teams develop and deploy ML models.


cnvrg.io
8
38
cnvrg.io is a full stack data science platform that helps teams manage models, and build auto-adaptive machine learning pipelines


Datatron
8
19
Datatron provides a single model governance (management) platform for all of your ML, AI, and Data Science models in production


Comet
7
19
Comet.ml is a machine learning platform designed to help AI practitioners and teams build reliable machine learning models.


ModelOp
6
39
Govern, Monitor and Manage all models across the enterprise


WhyLabs
4
15
WhyLabs is the AI observability and monitoring company.


Arize AI
4
14
Arize AI offers a platform that explains and troubleshoots production AI.


DarwinAI
4
31
DarwinAI’s Generative Synthesis 'AI building AI' technology enables optimized and explainable deep learning.


Mona
4
11
Mona is a SaaS monitoring platform for Data and AI driven systems


Valohai
2
13
Your Managed Machine Learning Platform that lets data scientists build, deploy and track machine learning models.


Modzy
0
31
The secure ModelOps platform to discover, deploy, manage, and govern machine learning at scale—getting to value faster.


Algomox
0
17
Catalyze Your AI Transformation


Monitaur
0
8
Monitaur is a software company that provides auditability, transparency, and governance for companies using machine learning software.


Hydrosphere.io
0
3
Hydrosphere.io is a platform for AI/ML operations automation



 
Many of these companies focus on just one segment of MLOps, such as automation pipeline, monitoring, lifecycle management, or governance.  Some argue that using multiple, best-of-breed MLOps products are better for data science projects than monolithic platforms.  And some companies are building MLOps products for specific verticals.  For example, Monitaur positions itself as a best-of-breed governance solution that can work with any platform.  Monitaur is also building industry-specific MLOps governance capabilities for regulated industries, starting with insurance.  (Full disclosure:  I am an investor in Monitaur).
There are also a number of open-source MLOps projects, including:

MLFlow manages the ML lifecycle, including experimentation, reproducibility, and deployment, and includes a model registry
DVC manages version control for ML projects to make them shareable and reproducible
Polyaxon has capabilities for experimentation, lifecycle automation, collaboration, and deployment, and includes a model registry
Metaflow is a former Netflix project for managing the automation pipeline and deployment
Kubeflow has capabilities for workflow automation and deployment in Kubernetes containers

2021 promises to be an interesting year for MLOps.  We’ll likely see rapid growth, tremendous competition, and most likely, some consolidation.
 
Bio: Steve Shwartz (@sshwartz) started his AI career as a postdoc at Yale University many years ago, is a successful serial entrepreneur and investor, and is the author of “Evil Robots, Killer Computers, and Other Myths: The Truth About AI and the Future of Humanity”.
Related:

A Machine Learning Model Monitoring Checklist: 7 Things to Track
How to Use MLOps for an Effective AI Strategy
MLOps: Model Monitoring 101"
https://www.kdnuggets.com/2020/12/immuta-future-cloud-now.html,The Future of Cloud is Now,"Our recent survey of over 130 top data engineers, data architects, and executives uncovered details and trends of the current state of data engineering and DataOps.Read our survey report to learn more about these trends as well as our predictions for future obstacles and our recommendations for avoiding them.","Sponsored Post.

With the advent of ‘big data’, data changed from a byproduct of systems to a source of innovation and competitive advantage. Now, following the universal digital transformation and the current rise of AI and ML and cloud data platforms, data is considered a product in and of itself and its use, value, and significance continues to evolve.
If our understanding of data is changing, the role of the data team and DataOps is being modernized and modified faster. A thorough understanding of modern DataOps is necessary to fully grasp the current value and role of data.
Our recent survey of over 130 top data engineers, data architects, and executives uncovered details and trends of the current state of data engineering and DataOps. We identified popular trends like adoption of cloud data platforms and gained insight into winning and emerging platforms, data engineering challenges, and how organizations are handling sensitive data. 
Read our survey report to learn more about these trends as well as our predictions for future obstacles and our recommendations for avoiding them."
https://www.kdnuggets.com/2020/11/computer-vision-scale-dask-pytorch.html,Computer Vision at Scale With Dask And PyTorch,A tutorial on conducting image classification inference using the Resnet50 deep learning model at scale with using GPU clusters on Saturn Cloud. The results were: 40x faster computer vision that made a 3+ hour PyTorch model run in just 5 minutes.,"comments
By Stephanie Kirmer, Senior Data Scientist at Saturn Cloud
Applying deep learning strategies to computer vision problems has opened up a world of possibilities for data scientists. However, to use these techniques at scale to create business value, substantial computing resources need to be available – and this is just the kind of challenge Saturn Cloud is built to solve!
In this tutorial, you’ll see the steps to conducting image classification inference using the popular Resnet50 deep learning model at scale using NVIDIA GPU clusters on Saturn Cloud. Using the resources Saturn Cloud makes available, we can run the task 40x faster than a non-parallelized approach!


We’ll be classifying dog images today!

 
What you’ll learn here:
 

How to set up and manage a GPU cluster on Saturn Cloud for deep learning inference tasks
How to run inference tasks with Pytorch on the GPU cluster
How to use batch processing to accelerate your inference tasks with Pytorch on the GPU cluster

Setup
 
To begin, we need to ensure that our image dataset is available and that our GPU cluster is running.
In our case, we have stored the data on S3 and use the s3fs library to work with it, as you’ll see below. 
If you would like to use this same dataset, it is the Stanford Dogs dataset, available here: http://vision.stanford.edu/aditya86/ImageNetDogs/
To set up our Saturn GPU cluster, the process is very straightforward.

import dask_saturn
from dask_saturn import SaturnCluster

cluster = SaturnCluster(n_workers=4, scheduler_size='g4dnxlarge', worker_size='g4dn8xlarge')
client = Client(cluster)
client

 

[2020-10-15 18:52:56] INFO – dask-saturn | Cluster is ready


We are not explicitly stating it, but we are using 32 threads each on our cluster nodes, making 128 total threads.
Tip: Individual users may find that you want to adjust the number of threads, reducing it down if your files are very large – too many threads running large tasks simultaneously might require more memory than your workers have available at one time.
This step may take a moment to complete because all the AWS instances that we are requesting need to be spun up. Calling client at the end, there will monitor the spin-up process and let you know when things are ready to rock!
 
GPU Capability
 
At this point, we can confirm that our cluster has GPU capabilities, and make sure we have set everything up correctly.
First, check that the Jupyter instance has GPU capability.

torch.cuda.is_available() 



True


Awesome- now let’s also check each of our four workers.

client.run(lambda: torch.cuda.is_available())



{‘tcp://10.0.24.217:45281’: True,
‘tcp://10.0.28.232:36099’: True,
‘tcp://10.0.3.136:40143’: True,
‘tcp://10.0.3.239:40585’: True}


Here then we’ll set the “device” to always be cuda, so we can use those GPUs.

device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")


Note: If you need some help establishing how to run a single image classification, we have an expanded code notebook available at our github that can give you those instructions as well as the rest of this content.

 
Inference
 
Now, we’re ready to start doing some classification! We’re going to use some custom-written functions to do this efficiently and make sure our jobs can take full advantage of the parallelization of the GPU cluster.
 
Preprocessing
 
Single Image Processing
 

@dask.delayed
def preprocess(path, fs=__builtins__):
    '''Ingest images directly from S3, apply transformations,
    and extract the ground truth and image identifier. Accepts
    a filepath. '''
    
    transform = transforms.Compose([
        transforms.Resize(256), 
        transforms.CenterCrop(250), 
        transforms.ToTensor()])

    with fs.open(path, 'rb') as f:
        img = Image.open(f).convert(""RGB"")
        nvis = transform(img)

    truth = re.search('dogs/Images/n[0-9]+-([^/]+)/n[0-9]+_[0-9]+.jpg', path).group(1)
    name = re.search('dogs/Images/n[0-9]+-[a-zA-Z-_]+/(n[0-9]+_[0-9]+).jpg', path).group(1)
    
    return [name, nvis, truth]


This function allows us to process one image, but of course, we have a lot of images to work with here! We’re going to use some list comprehension strategies to create our batches and get them ready for our inference.
First, we break the list of images we have from our S3 file path into chunks that will define the batches.

3fpath = 's3://dask-datasets/dogs/Images/*/*.jpg'

batch_breaks = [list(batch) for batch in toolz.partition_all(60, s3.glob(s3fpath))]


Then we’ll process each file into nested lists. Then we’ll reformat this list setup slightly and we’re ready to go!

image_batches = [[preprocess(x, fs=s3) for x in y] for y in batch_breaks]


Notice that we have used the Dask delayed decorator on all of this- we don’t want it to actually run yet, but to wait until we are doing work in parallel on the GPU cluster!
 
Format Batches
 
This little step just makes sure that the batches of images are organized in the way that the model will expect them.

@dask.delayed
def reformat(batch):
    flat_list = [item for item in batch]
    tensors = [x[1] for x in flat_list]
    names = [x[0] for x in flat_list]
    labels = [x[2] for x in flat_list]
    return [names, tensors, labels]
    
image_batches = [reformat(result) for result in image_batches]


 
Run the Model
 
Now we are ready to do the inference task! This is going to have a few steps, all of which are contained in functions described below, but we’ll talk through them so everything is clear.
Our unit of work at this point is batches of 60 images at a time, which we created in the section above. They are all neatly arranged in lists so that we can work with them effectively.
One thing we need to do with the lists is to “stack” the tensors. We could do this earlier in our process, but because we are using the Dask delayed decorator on the preprocessing, our functions actually do not know that they are receiving tensors until later in the process. Therefore, we’re delaying the “stacking” as well by putting it inside this function that comes after the preprocessing.

@dask.delayed
def run_batch_to_s3(iteritem):
    ''' Accepts iterable result of preprocessing, 
    generates inferences and evaluates. '''
    
    with s3.open('s3://dask-datasets/dogs/imagenet1000_clsidx_to_labels.txt') as f:
        classes = [line.strip() for line in f.readlines()]
  
    names, images, truelabels = iteritem
    
    images = torch.stack(images)
... 


So now we have our tensors stacked so that batches can be passed to the model. We are going to retrieve our model using pretty simple syntax:

...
    resnet = models.resnet50(pretrained=True)
    resnet = resnet.to(device)
    resnet.eval()
...


Conveniently, we load the library torchvision which contains several useful pretrained models and datasets. That’s where we are grabbing Resnet50 from. Calling the method .to(device) allows us to pass the model object to our workers, giving them the ability to run the inference without having to reach back to the client.
Now we are ready to run inference! It is inside the same function, styled this way:

...
    images = images.to(device)
    pred_batch = resnet(images)
...


We pass our image stack (just the batch we are working on) to the workers and then run the inference, returning predictions for that batch.
 
Result Evaluation
 
The predictions and truth we have so far, however, are not really human-readable or comparable, so we’ll use the functions that follow to fix them up and get us interpretable results.

def evaluate_pred_batch(batch, gtruth, classes):
    ''' Accepts batch of images, returns human readable predictions. '''
    _, indices = torch.sort(batch, descending=True)
    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100
    
    preds = []
    labslist = []
    for i in range(len(batch)):
        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]
        preds.append(pred)

        labs = gtruth[i]
        labslist.append(labs)
        
    return(preds, labslist)


This takes our results from the model, and a few other elements, to return nice readable predictions and the probabilities the model assigned.

preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)


From here, we’re nearly done! We want to pass our results back to S3 in a tidy, human-readable way, so the rest of the function handles that. It will iterate over each image because these functionalities are not batch handling. is_match is one of our custom functions, which you can check out below.

...
    for j in range(0, len(images)):
        predicted = preds[j]
        groundtruth = labslist[j]
        name = names[j]
        match = is_match(groundtruth, predicted)

        outcome = {'name': name, 'ground_truth': groundtruth, 'prediction': predicted, 'evaluation': match}

        # Write each result to S3 directly
        with s3.open(f""s3://dask-datasets/dogs/preds/{name}.pkl"", ""wb"") as f:
            pickle.dump(outcome, f)
...


 
Put It All Together
 
Now, we aren’t going to patch together all these functions by hand, instead, we have assembled them in one single delayed function that will do the work for us. Importantly, we can then map this across all our batches of images across the cluster!

def evaluate_pred_batch(batch, gtruth, classes):
    ''' Accepts batch of images, returns human readable predictions. '''
    _, indices = torch.sort(batch, descending=True)
    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100
    
    preds = []
    labslist = []
    for i in range(len(batch)):
        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]
        preds.append(pred)

        labs = gtruth[i]
        labslist.append(labs)
        
    return(preds, labslist)

def is_match(la, ev):
    ''' Evaluate human readable prediction against ground truth. 
    (Used in both methods)'''
    if re.search(la.replace('_', ' '), str(ev).replace('_', ' ')):
        match = True
    else:
        match = False
    return(match)    


@dask.delayed
def run_batch_to_s3(iteritem):
    ''' Accepts iterable result of preprocessing, 
    generates inferences and evaluates. '''
    
    with s3.open('s3://dask-datasets/dogs/imagenet1000_clsidx_to_labels.txt') as f:
        classes = [line.strip() for line in f.readlines()]
  
    names, images, truelabels = iteritem
    
    images = torch.stack(images)
    
    with torch.no_grad():
        # Set up model
        resnet = models.resnet50(pretrained=True)
        resnet = resnet.to(device)
        resnet.eval()

        # run model on batch
        images = images.to(device)
        pred_batch = resnet(images)
        
        #Evaluate batch
        preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)

        #Organize prediction results
        for j in range(0, len(images)):
            predicted = preds[j]
            groundtruth = labslist[j]
            name = names[j]
            match = is_match(groundtruth, predicted)
            
            outcome = {'name': name, 'ground_truth': groundtruth, 'prediction': predicted, 'evaluation': match}
            
            # Write each result to S3 directly
            with s3.open(f""s3://dask-datasets/dogs/preds/{name}.pkl"", ""wb"") as f:
                pickle.dump(outcome, f)
        
        return(names)


 
On the Cluster
 
We have really done all the hard work already and can let our functions take it from here. We’ll be using the .map method to distribute our tasks efficiently.

futures = client.map(run_batch_to_s3, image_batches) 
futures_gathered = client.gather(futures)
futures_computed = client.compute(futures_gathered, sync=False)


With map we ensure all our batches will get the function applied to them. With gather, we can collect all the results simultaneously rather than one by one. With compute(sync=False) we return all the futures, ready to be calculated when we want them. This may seem arduous, but these steps are required to allow us to iterate over the future.
Now we actually run the tasks, and we also have a simple error handling system just in case any of our files are messed up or anything goes haywire.

import logging

results = []
errors = []
for fut in futures:
    try:
        result = fut.result()
    except Exception as e:
        errors.append(e)
        logging.error(e)
    else:
        results.extend(result)


 
Evaluate
 
We want to make sure we have high-quality results coming out of this model, of course! First, we can peek at a single result.

with s3.open('s3://dask-datasets/dogs/preds/n02086240_1082.pkl', 'rb') as data:
    old_list = pickle.load(data)
    old_list



{‘name’: ‘n02086240_1082’,
‘ground_truth’: ‘Shih-Tzu’,
‘prediction’: [(b”203: ‘West Highland white terrier’,”, 3.0289587812148966e-05)],
‘evaluation’: False}


While we have a wrong prediction here, we have the sort of results we expect! To do a more thorough review, we would download all the results files, then just check to see how many have evaluation: True.
Number of dog photos examined: 20580
Number of dogs classified correctly: 13806
The percent of dogs classified correctly: 67.085%
Not perfect, but good looking results overall!
 
Comparing Performance
 
So, we have managed to classify over 20,000 images in about 5 minutes. That sounds good, but what is the alternative?




Technique
Runtime




No Cluster with Batching
3 hours, 21 minutes, 13 sec


GPU Cluster with Batching
5 minutes, 15 sec



 
Adding a GPU cluster makes a HUGE difference! If you’d like to see this work for yourself, sign up for your free trial of Saturn Cloud today!
 
Bio: Stephanie Kirmer is a Senior Data Scientist at Saturn Cloud.
Original. Reposted with permission.
Related:

Data Science in the Cloud with Dask
Top Python Libraries for Deep Learning, Natural Language Processing & Computer Vision
How to Acquire the Most Wanted Data Science Skills"
https://www.kdnuggets.com/2020/11/jmp-effective-disease-outbreak-alert-system.html,Toward a More Effective Disease Outbreak Alert System: A Symptoms Approach to Biosurveillance [Nov 19 webinar],Learn how the use of more granular symptoms-level data combined with innovative statistical techniques has the potential to identify disease outbreaks faster while limiting false positives.,"Sponsored Post.



Date: Thursday, Nov. 19
Time: 1 - 1:30 p.m. ET
Presenter: Sam Edgemon
Location: Online
Registration: Free
Improving our ability to detect disease outbreaks within a given population as early as possible is a never-ending quest - for obvious reasons.
Whether the cause is bioterrorism, such as an anthrax attack, or a novel virus, such as COVID-19, early detection enables early public health interventions and possibly the fleeting chance to limit morbidity and mortality.
Public health surveillance has traditionally focused on finding evidence of syndromes, i.e., cases with well-defined sets of symptoms that are classified via standard diagnostic codes. Alerts are issued after the number of cases crosses a specified threshold.
In this 30-minute live webinar, Sam Edgemon will demonstrate how the use of more granular symptoms-level data combined with innovative statistical techniques has the potential to identify disease outbreaks faster while limiting false positives.
Using data from emergency departments, ambulance services, poison control centers and social media, Sam will demonstrate:


How unstructured textual data can be combined with a wide breadth of other symptom-level data to spot previously hidden outbreaks.
How to identify potential ""hot spots"" earlier using statistical techniques not usually associated with biological surveillance.
How these techniques could be used to make a difference during the current pandemic, as well as the next one.


Register here"
https://www.kdnuggets.com/2021/03/forget-telling-stories-help-people-navigate.html,Forget Telling Stories; Help People Navigate,"When designing reporting & visualizations, think of them as part of a navigation framework rather than stand-alone information.","By Stan Pugsley, Data Warehouse and Analytics Consultant.
comments
Great reporting and visualizations should create an action path that will guide users through the analysis process and eventual action to improve the business. The goal is not just story telling, but to help user navigate from data to action.
Following is a simple infographic for this framework:

Click image to enlarge
 
Let’s start with the “Current State” batch of KPIs. These are the lead visualizations that you would place on your executive dashboards. They give people a sense for where the key metrics for the business in one place. What are the KPIs that tell the current story of the organization?
The next step is to look for significant signals, changes or events. In our reporting we should use bright colors and attention-grabbing icons to highlight changes in the data.
Every lead KPI and alert needs to be paired with one or more drill-down paths to explore details. These are supplemental visualizations. The path to do drill downs needs to be simple and clearly linked. Ideally each department or business process would have its own drill-down area or folder.
Now we’ve got to move beyond just doing a data dump. People want to be led by the hand to recommended routes to resolve issues. Not every change in every chart will have an automatic recommended best route. But many do. If inventory is low in a particular SKU, there are just a few routes to follow, so why not note them right on the report? If costs are increasing faster than planned in one department, do you have standard ways to address the issue? If so, then list out those standard actions right where the information is needed and useful as the information is consumed.
At the end we need to help users to take action by embedding links to the associated applications right in the reports. If inventory is low, add a link below the KPI to the warehouse management software. If sales are dropping in one region, add a link at the bottom of the KPI to Salesforce or your CRM system to follow up.
We can make our reporting tools into navigation tools by using the five steps in the framework.
 
Bio: Stan Pugsley is a data warehouse and analytics consultant with Eide Bailly Technology Consulting based in Salt Lake City, UT. He is also an adjunct faculty member at the University of Utah Eccles School of Business. You can reach the author via email.
Related:

Telling a Great Data Story: A Visualization Decision Tree
Data Science vs Business Intelligence, Explained
5 Concepts Every Data Scientist Should Know"
https://www.kdnuggets.com/2021/04/sas-viya-cloud-microsoft.html,Shaping the new digital age – with SAS and Microsoft,"Join technology experts, partners and analysts in the industry for this webinar series to see how SAS Viya can help you make the most of AI, analytics and the cloud for faster decisions and trusted results.","Sponsored Post.

Every industry is in a state of transformation. Digital technology is reshaping how organizations manage and act on data, think about innovation and interact with customers.
AI has enabled organizations to automate once time-consuming tasks. Advanced in analytics uncovers insights from massive amounts of complex data. And cloud technology has given organizations the freedom and scalability to realize value faster than ever.
Now SAS and Microsoft are joining forces to help define the future of these powerful and emerging technologies through a unified analytic platform not seen anywhere else in the market. By bringing analytics, AI and cloud computing together, the two companies will help organizations forge a successful path to digital transformation.
As the leader in analytics, SAS has made it easier to access SAS® Viya®, its cloud-native AI, analytic and data management platform, by optimizing it to run seamlessly on the Microsoft Azure cloud platform.
In addition to providing customers access to powerful SAS analytics, it provides container orchestration managed by Azure Kubernetes, IT administration with MS Tools and seamless access to the Azure Data Estate.
This helps customers democratize analytics throughout their organizations, while seamlessly managing analytic workloads and building SAS into a variety of applications.
From data scientists, IT departments, business analysts and developers – SAS users will ultimately benefit from an improved customer experience. And enterprises will benefit from being able to make more trusted decisions.
When SAS and Microsoft bring together what they each do best, the potential for real transformative change for our customers is practically limitless."
https://www.kdnuggets.com/2021/06/coiled-workflow-orchestration-prefect.html,Workflow Orchestration with Prefect and Coiled,"Coiled helps data scientists use Python for ambitious problems, scaling to the cloud for computing power, ease, and speed—all tuned for the needs of teams and enterprises.  In this demo example, see how to spin up a Coiled cluster to execute Prefect jobs during runtime.","Sponsored Post.

Webinar: Workflow Orchestration with Prefect and Coiled
When: Jun 30, 2021, 9 am PDT, 12 pm EDT, 17:00 BST.
Jeremiah Lowin, Founder and CEO of Prefect, along with Kevin Kho, Prefect’s Open Source Community Engineer, will discuss updates about the company and demo a newly released feature called the KV Store.
Prefect is an open-source workflow orchestration tool created to handle the modern data stack. Prefect is built on top of Dask, allowing parallel execution of workflows. Coiled helps data scientists use Python for ambitious problems, scaling to the cloud for computing power, ease, and speed—all tuned for the needs of teams and enterprises. This means Coiled and Prefect have a very strong synergy. In this demo example, we’ll also show how to spin up a Coiled cluster to execute Prefect jobs during runtime. The Coiled cluster will provide parallelism for dynamically mapped tasks in Prefect.
After attending, you’ll leave knowing:

 How to create a simple Prefect Flow
 How to use the new Prefect feature, the KV store
 How to use Coiled and Prefect together
 More knowledge about Prefect as a company

We're looking forward to seeing you there!
Register here."
https://www.kdnuggets.com/2021/07/google-advice-learning-data-science.html,Advice for Learning Data Science from Google’s Director of Research,"Surfing the professional career wave in data science is a hot prospect for many looking to get their start in the world. The digital revolution continues to create many exciting new opportunities. But, jumping in too fast without fully establishing your foundational skills can be detrimental to your success, as is suggested by this advice for data science newbies from Peter Norvig, the Director of Research at Google.","By Benjamin Obi Tayo, Ph.D., DataScienceHub.
comments

Photo by Mitchell Luo on Unsplash.
“In 2021, professionals in the digital market space must be comfortable with data — period. They must know how to manipulate data, understand how it is collected, and analyze and interpret it. The future of decision making is grounded in data science.” — Wendy Moe, Professor of Marketing, University of Maryland
Data science skills have become increasingly more important for jobs that once had little to do with statistics, including marketing and business. Adding data science skills to your portfolio will give you an edge in your current role in the market this year.
If you are interested in adding data science to your portfolio, you no doubt might have pondered over these questions:

How long does it take to learn the fundamentals of data science?
What are some resources for learning data science?

This article discusses some general advice from Peter Norvig to individuals considering data science.
 
Background about Peter Norvig (Director of Research at Google)
 
The motivation for choosing the above title is based on Peter Norvig’s idea of the amount of time it takes to become an expert in programming. If you have not read this article: “Teach Yourself Programming in 10 Years” by Peter Norvig, I encourage you to do so.
The point here is that you don’t need 10 years to learn the basics of data science, but learning data science in a rush is certainly not helpful. It takes time, effort, energy, patience, and commitment to become a data scientist.
Peter Norvig’s suggestion is that learning requires time, patience, and commitment. Beware of articles, books, or websites that tell you that you can learn data science in 4 weeks.

Image by Benjamin O. Tayo.
If you are interested in learning the fundamentals of data science, be prepared to invest the right amount of time and energy. That way, you can master not just the superficial concepts but the in-depth concepts of data science.
It took me 2 years of in-depth studies to master the basics of data science (through self-study), and I continue to challenge myself to learn new things every day. How long it is going to take you to master the fundamentals of data science would depend on your background. Generally, a solid background in an analytical discipline such as mathematics, statistics, computer science, engineering, or economics is advantageous.
 
3 Lessons From Peter Norvig’s “Teach Yourself Programming in Ten Years”
 
1) It takes time, effort, energy, patience, and commitment to master the fundamentals of data science.
Data science is a very multidisciplinary field that requires a solid background in advanced mathematics, statistics, programming, and other related skills in data analysis, data visualization, model building, machine learning, etc. It took me 2 years of dedicated studies to master the fundamentals of data science, and that is because of my solid background in mathematics, physics, and programming. Here are some resources that helped me master the fundamentals of data science.
(i) Professional Certificate in Data Science (HarvardX, through edX)
Includes the following courses, all taught using R (you can audit courses for free or purchase a verified certificate):

Data Science: R Basics
Data Science: Visualization
Data Science: Probability
Data Science: Inference and Modeling
Data Science: Productivity Tools
Data Science: Wrangling
Data Science: Linear Regression
Data Science: Machine Learning
Data Science: Capstone

(ii) Analytics: Essential Tools and Methods (Georgia TechX, through edX)
Includes the following courses, all taught using R, Python, and SQL (you can audit for free or purchase a verified certificate):

Introduction to Analytics Modeling
Introduction to Computing for Data Analysis
Data Analytics for Business

(iii) Applied Data Science with Python Specialization (the University of Michigan, through Coursera)
Includes the following courses, all taught using Python (you can audit most courses for free, some require the purchase of a verified certificate):

Introduction to Data Science in Python
Applied Plotting, Charting & Data Representation in Python
Applied Machine Learning in Python
Applied Text Mining in Python
Applied Social Network Analysis in Python

(iv) Data Science Textbooks
Learning from a textbook provides a more refined and in-depth knowledge beyond what you get from online courses. This book provides a great introduction to data science and machine learning, with code included: “Python Machine Learning” by Sebastian Raschka.

The author explains fundamental concepts in machine learning in a way that is very easy to follow. Also, the code is included, so you can actually use the code provided to practice and build your own models. I have personally found this book to be very useful in my journey as a data scientist. I would recommend this book to any data science aspirant. All that you need is basic linear algebra and programming skills to be able to understand the book.
There are lots of other excellent data science textbooks out there such as “Python for Data Analysis” by Wes McKinney, “Applied Predictive Modeling” by Kuhn & Johnson, and “Data Mining: Practical Machine Learning Tools and Techniques” by Ian H. Witten, Eibe Frank & Mark A. Hall.
(v) Network with other Data Science Aspirants
From my personal experience, I have learned a lot from weekly group conversations on various topics in data science and machine learning by teaming up with other data science aspirants. Network with other data science aspirants, share your code on GitHub, showcase your skills on LinkedIn. This will really help you to learn a lot of new concepts and tools within a short period of time. You also get exposed to new ways of doing things, as well as to new algorithms and technologies.
2) Understanding the theoretical foundations of data science is as important as hands-on data science skills.
Data science is heavily math-intensive and requires knowledge in the following:
(i) Statistics and Probability
(ii) Multi-variable Calculus
(iii) Linear Algebra
(iv) Optimization and Operational Research
Find out more about math topics that you need to focus on from here: Essential Math Skills for Machine Learning.
Even though packages such as Python’s sci-kit learn and R’s Caret package contain several tools for doing data science and building machine learning models, it is extremely important to understand the theoretical foundations of each method.
3) Avoid using machine learning models as blackbox tools.
A solid background in data science would enable a data scientist to build reliable predictive models. For example, before building a model, you may ask yourself:
(i) What are the predictor variables?
(ii) What is the target variable? Is my target variable discrete or continuous?
(iii) Should I use classification or regression analysis?
(iv) How do I handle missing values in my dataset?
(v) Should I use normalization or standardization when bringing variables to the same scale?
(vi) Should I use Principal Component Analysis or not?
(vii) How do I tune hyperparameters in my model?
(viii) How do I evaluate my model to detect biases in the dataset?
(ix) Should I use ensemble methods where I train using different models then perform an ensemble average, e.g., using classifiers such as SVM, KNN, Logistic Regression, then average over 3 models?
(x) How do I select the final model?
What makes the difference between a good and a bad machine learning model depends on one’s ability to understand all the details of the model, including knowledge about different hyperparameters and how these parameters can be tuned in order to obtain the model with the best performance. Using any machine learning model as a black box without fully understanding the intricacies of the model will lead to a falsified model.
In summary, data science is one of the hottest fields nowadays. The digital revolution has created tons upon tons of data. Companies, industries, organizations, and the government are producing tons upon tons of data on a daily basis. The demand for high-skilled data scientists will only continue to grow. This is the right time to invest your time to master the fundamentals of data science. In doing so, beware of articles, books, or websites that tell you that you can learn data science in 4 weeks or in a month. Do not be in a rush. Take your time to master the fundamentals of data science.
Original. Reposted with permission.
 
Related:

A checklist to track your Data Science progress
10 Mistakes You Should Avoid as a Data Science Beginner
Don’t learn Machine Learning in 24 hours"
https://www.kdnuggets.com/2021/04/whats-etl.html,What’s ETL?,"Discover what ETL is, and see in what ways it’s critical for data science.","comments
By Omer Mahmood, Head of Cloud Customer Engineering, CPG & Travel at Google
 
In my last post, I talked about what it means to move machine learning (ML) models into production by introducing the concept of MLOps. This time we’re going to look at the opposite end of the data science steps for ML — data extraction and integration.
 
The TL;DR
 
ETL stands for Extract-Transform-Load, it usually involves moving data from one or more sources, making some changes, and then loading it into a new single destination.

In most companies data tends to be in silos, stored in various formats and is often inaccurate or inconsistent
This situation is far from ideal if we want to be able to easily analyse and get insights from that data or use it for data science

 
🚣🏼 How we got here
 
Most ML algorithms require large amounts of training data in order to produce models that can make accurate predictions. They also require good quality training data, representative of the problem we are trying to solve.
To reinforce this point there is a great example I came across, analogous to ‘Maslow’s hierarchy of needs’ that highlights the importance of data collection and storage as it relates to data science:


Figure 1: The Data Science Hierarchy of Needs Pyramid, SOURCE: “THE AI HIERARCHY OF NEEDS” MONICA ROGATI[1]

 
At the bottom of the pyramid is the basic need to gather the right data, in the right formats and systems, and in the right quantity.


Any application of AI and ML will only be as good as the quality of data collected.


So, let’s say you’ve framed your problem and determined that it’s a good fit for ML. You know what data you need, at least to start experimenting. But unfortunately it’s sitting in different systems and scattered across your organisation.
The next step is to figure out how to bring that data together, transform it as needed, and then land it somewhere as a single integrated dataset. You can only begin to explore the data, carry out feature engineering, and model training once it is accessible — this is where our friendly acronym ETL comes into play!
 
🧪 How does it work?
 
To make it a bit more concrete, let’s use a modern real world ETL example.
Imagine you are an online retailer that uses a Customer Relationship Management (CRM) system such as SalesForce to keep track of your registered customers.
You also use a payment processor such as Stripe to handle and store details of sales transactions made via your e-commerce website.
Suppose your goal is to improve your conversion rate by using data about what your customers purchased historically, to make better product recommendations when they are browsing your website.
You could certainly use an ML model to power a recommendation engine to achieve this goal. But the challenge is that the data you need is sitting in two different systems. The solution in our case is to use an ETL process to extract, transform and combine them into a data warehouse:


Figure 2: The process of moving data from different sources to a warehouse using ETL. Illustration by the author.

 
Let’s break down what’s happening in the diagram above:
1. Extract — this part of the process involves retrieving data from our two sources, SalesForce and Stripe. Once the data has been retrieved, the ETL tool will load it into a staging area in preparation for the next step.
2. Transform — this is a critical step, because it handles the specifics of how our data will be integrated. Any cleansing, reformatting, deduplication, and blending of data happens here before it can move further down the pipeline.
In our case, let’s say in one system a customer record is stored with the name “K. Reeves”, in another system that same customer record is stored against the name “Keanu Reeves”.
Assume we know it’s the same customer (based on their shipping address), but the system still needs to reconcile the two, so we don’t end up with duplicate records.
➡️ ETL frameworks and tools provide us with the logic needed to automate this sort of transformation, and can cater for many other scenarios too.
3. Load — involves successfully inserting the incoming data into the target database, data store, or in our case a data warehouse.
So there you have it, we have collected our data, integrated it using an ETL pipeline and loaded it somewhere that is accessible for data science.
 
📌 Side note 📌
ETL vs. ELT
You might have also come across the term ‘ELT’. Extract, load, and transform (ELT) differs from ETL solely in where the transformation takes place. In the ELT process, the data transformation occurs in the destination data store.
This can simplify the architecture by removing what is sometimes a separate or intermediate staging system that hosts the data transformation. The other advantage is that you can benefit from the additional scale and compute performance usually present in destinations such as cloud data warehouses.
📌 Side note 📌
 
 
🦀 Common challenges
 
OK, all this ETL stuff sounds pretty simple, right? Here are some ‘gotchas’ to look out for:
 
☄️ Scaling
 
The amount of data businesses produce is only expected to grow — 175 Zettabytes by 2025 according to a report by IDC[2]. So you should ensure that the ETL tool you choose has the ability to scale to not just your current but also future needs. You may move data in batches now, but will that always be the case? How many jobs can you run in parallel?
Moving to the cloud is a pretty safe bet if you want to future-proof your ETL processes — by having access to theoretically limitless scalability of storage and compute while also reducing your IT capital expenditure.
 
🧮 Data Accuracy
 
Another big ETL challenge is ensuring that the data you transform is accurate and complete. Manual coding and changes or failure to plan and test before running an ETL job can sometimes introduce errors, including loading duplicates, missing data, and other issues.
An ETL tool will definitely reduce the need for hand-coding and help cut down on errors. Data accuracy testing can help spot inconsistencies and duplicates, and monitoring features can help identify instances where you are dealing with incompatible data types and other data management issues.
 
🍱 Diversity of Data Sources
 
Data is growing in volume. But more importantly, it’s growing in complexity. One enterprise could be handling diverse data from hundreds — or even thousands — of data sources. These can include structured and semi-structured sources, real-time sources, flat files, CSVs, object buckets, streaming sources, and whatever new comes along.
Some of this data is best transformed in batches, while for others, streaming, continuous data transformation works better.
Having a strategy for how you intend to cope with different data sources is key. Some modern ETL tools can offer support for a wide variety, including batch and streaming in one place.
 
👷🏾‍♀️ So how do I get started?
 
At this point you should have a good idea why and when you might need to use ETL in your data science workflow. We also covered common challenges to look out for as you begin thinking about your ETL processes.
I’ll close with a simple methodology for choosing an ETL tool, and some other useful resources.
 
🤷🏽‍♀️ Which ETL tool should I use, and when?
 
So we understand what happens during ETL, but what does it mean in more practical terms?
You will need to design an ETL pipeline that explicitly describes:

What data sources to extract from and how to connect to them
What transformations to carry out on the data once you have it, and finally
Where to load the data once the pipeline is complete

ETL pipelines can be expressed using a code based framework, or a more popular choice these days is to use ETL tools that provide a ‘drag and drop’ user interface that lets you define the steps in your pipeline in a visual way.
Once you’ve implemented your ETL pipeline, it typically needs to run somewhere i.e. using an ETL tool that will execute your pipeline, and an environment that will provide the resources required to temporarily store and transform your data.
I have tried to simplify the decision-making steps for you in the diagram below (click to zoom in):


Figure 3: Which ETL tool to use and when. Illustration by the author.

 
NB. This decision tree is by no means an exhaustive list of either; the decisions you will need to make, frameworks or products available.
Indeed for every intermediate ETL step, there are dozens of open source and proprietary offerings. Ranging from orchestration to scheduling — we’re not going to be able to cover everything here.
The aim of this post was to serve as a springboard into the world of ETL! Good luck on your data integration journey! 😀
 
💡 Useful resources and further reading
 
Links

Data Preparation and Feature Engineering for Machine Learning
Gartner — Data Integration Tools Reviews and Ratings

Books

The Data Warehouse ETL Toolkit: Practical Techniques for Extracting, Cleaning, Conforming, and Delivering Data, Wiley, Authors: Ralph Kimball, Joe Caserta
Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing, O’Reilly, Authors: Tyler Akidau, Slava Chernyak, Reuven Lax

Data Agnostic ETL tools

Fivetran
Stitch

 
📇 References
 
[1] The AI Hierarchy of Needs, Monica Rogati
https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007
[2] 175 Zettabytes By 2025, Forbes, Tom Coughlin
https://www.forbes.com/sites/tomcoughlin/2018/11/27/175-zettabytes-by-2025/?sh=6a5d2e7a5459
 
Bio: Omer Mahmood is Head of Cloud Customer Engineering, CPG & Travel at Google.
Original. Reposted with permission.
Related:

Introducing dbt, the ETL and ELT Disrupter
The Role of the Data Engineer is Changing
Why the Future of ETL Is Not ELT, But EL(T)"
https://www.kdnuggets.com/2020/10/text-mining-r-free-ebook.html,Text Mining with R: The Free eBook,"This freely-available book will show you how to perform text analytics in R, using packages from the tidyverse.","By Matthew Mayo, KDnuggets.
comments
I readily admit that I'm biased toward Python. This isn't intentional — such is the case with many biases — but coming from a computer science background and having been programming since a very young age, I have naturally tended towards general purpose programming languages (Java, C, C++, Python, etc.). This is the major reason that Python books and resources are at the forefront of my radar, recommendations, and reviews. 
Obviously, however, not all data scientists are in this same position, given that there are innumerable paths to data science. Given that, and since R is powerful and popular programming language for a large swath of data scientists, today let's take a look at a book which uses R as a tool to implement solutions to data science problems.
R is designed specifically for statistical computing, in juxtaposition to general purpose languages, the trade-off being that the relative lack of generality means better optimization for specialized scenarios. R's optimization for statistical computing is a big reason why it enjoys such high levels of adoption in data science and analytics.
Text analytics — like all applications and sub-genres of natural language processing — is continually reaching increasing heights of importance for data science, data scientists, and a variety of industries. As R (and its opinionated collection of packages designed for data science, the tidyverse) is an established environment for statistical computing utilized by data scientists, fully capable of performing text analytics, today we will look at Text Mining for R: A Tidy Approach.
 

 
Written by Julia Silge and David Robinson, this book endeavors to cover the following major topics, taken from the outline in the book's preface:


We start by introducing the tidy text format, and some of the ways dplyr, tidyr, and tidytext allow informative analyses of this structure.
Text won’t be tidy at all stages of an analysis, and it is important to be able to convert back and forth between tidy and non-tidy formats.
We conclude with several case studies that bring together multiple tidy text mining approaches we’ve learned.


 
For a more fleshed out list of topics treated within, the book's table of contents are as follows:

The tidy text format
Sentiment analysis with tidy data
Analyzing word and document frequency: tf-idf
Relationships between words: n-grams and correlations
Converting to and from non-tidy formats
Topic modeling
Case study: comparing Twitter archives
Case study: mining NASA metadata
Case study: analyzing usenet text
References


 
Text Mining for R: A Tidy Approach is code-heavy and seems to explain concepts well. The focus is on practical implementation, which should be of no surprise given the book's title, and to an R novice it seems to do a very good job. I have not followed along to the entire book, but I did read the first 2 chapters and feel that I got out of it what was intended.
The book is also very transparent as to what it is not:

This book serves as an introduction to the tidy text mining framework along with a collection of examples, but it is far from a complete exploration of natural language processing. The CRAN Task View on Natural Language Processing provides details on other ways to use R for computational linguistics. There are several areas that you may want to explore in more detail according to your needs.

Clustering, classification, and prediction
Word embedding
More complex tokenization
Languages other than English

 
All in all, this seems to strike a good balance. If you aren't familiar with NLP to any degree, regardless as to your familiarity with the tidyverse, jumping into the deep end with complex tokenization and using word embeddings to solve problems probably isn't a good idea. The starting point really should be what this book lays out, and what it lays out well.
It's at this point I should tell you that this is not actually an eBook; Text Mining with R is an online version of the print book. You can read the book online, and you can also buy physical copies from Amazon.

 
Whether you are interested in applying text mining to your projects and currently reside in the world of R, or you are looking to venture into using R and need some direction in doing so, check out Text Mining for R: A Tidy Approach. I'm certain you will find it beneficial.
 
Related:

Statistics with Julia: The Free eBook
Causal Inference: The Free eBook
Data Mining and Machine Learning: Fundamental Concepts and Algorithms: The Free eBook"
https://www.kdnuggets.com/2021/07/streamlit-tips-tricks-hacks-data-scientists.html,"Streamlit Tips, Tricks, and Hacks for Data Scientists","Today, I am going to talk about a few tips that I learned within more than a year of using Streamlit, that you can also use to unleash your powerful DS/AI/ML (whatever they may be) applications.","comments
By Kaveh Bakhtiyari, PhD Candidate in Artificial Intelligence, Data Scientist at SSENSE
The data science team at SSENSE usually builds very complex tools and dashboards. On the other hand, their maintenance was a challenge for the team. It has been more than a year since the SSENSE data science team has been using Streamlit actively. Before employing Streamlit, we were using Dash, Flask, R Shiny, etc. to build our tools and make them available to stakeholders within the company. In October 2019, we started to evaluate the potential power of Streamlit for our projects by understanding its benefits and how to integrate it into our data science infrastructure. At the end of 2019, we began some pilot projects on Streamlit instead of Flask and Dash.
After the evaluation period of Streamlit, we quickly realized that it had a lot of potentials and that it could increase development pace, and decrease the maintenance effort significantly. Besides all the cool features and being easy to work with, Streamlit does not provide the customized behaviors, events, and UI designs that you could get from other web development libraries such as Flask. And eventually, because of the same limitations, it has been much easier to develop clean apps and maintain them easily in the long term. Its uniform UI was also a positive point from my point of view. Firstly, it is clear, clean, and responsive. Secondly, all team members can build tools with uniform designs. But still, how can we provide such custom elements which we had in our Flask applications? Well, the short answer is that it is not quite possible, but we can use some tricks and tips, which can help you to customize more on what you are designing.
Today, I am going to talk about a few tips that I learned within more than a year of using Streamlit, that you can also use to unleash your powerful DS/AI/ML (whatever they may be) applications.


Streamlit is an active open-source project and the community is providing new updates frequently. I personally have bookmarked their Changelog page to keep track of new updates and features. Some of what we are discussing today are not natively supported in Streamlit (0.82.0), which may not be the case in the future.


 
Page Config
 
This feature was initially introduced in the beta version, and it was moved to the Streamlit namespace in version 0.70.0. This cool feature allows you to set the page title, favicon, page layout mode, and sidebar state.

By default, Streamlit sets the page title as the original python file name, with Streamlit favicon. Having this line of code, you can customize your page title, which is very beneficial if your users bookmark your apps. Then favicon allows them to differentiate the apps if they have many apps open in multiple browser tabs. Setting the layout and initial state of the sidebar can also run your app in the way you desire.
Before introducing this functionality, some of these features could only be possible by injecting CSS into the page. For example, if you wanted to make a widescreen, you could do the following:



This line must be the first Streamlit command on your page, and it can only be set once. Regardless of what you set for the layout (either centered or wide), users have control over them in the settings.


 
Empty component
 
There are multiple occasions when you want to generate new elements on the page, or you want to replace an existing text or element with another. This is possible using st.empty(). This method creates an empty placeholder on your page, and moving forward you can replace it with any object or text that you want.

The above code initially creates a placeholder on your page, then it writes “this is a sample text.” in that same place, and after that, it replaces it with an input number object.
This is very useful to have dynamic objects on the page, or simply showing the progress of some calculations such as progress percentage.
 
Query Strings
 
Setting and retrieving query strings in your Streamlit apps is an experimental feature at the moment. I hope that it will be moved into the main namespace in the future since I personally love this feature. If you have wondered why we need query strings in Streamlit, you are not alone.
When you set your customized inputs in query strings, it makes it possible for the users to share the links with the exact same parameters that they had. Otherwise, they have to enter their parameters as well.
The other use-case that I personally use is to share information between different Streamlit apps. In our team, each data scientist may work on different projects, and we may need to redirect users from one app to another. When we provide the link to the user to navigate to the other Streamlit app, we want to make sure that the user’s experience is as seamless as possible. Therefore, we pass the required parameters to the new app so that it loads with the data and analysis they are looking for.
For example, a few of our tools are related to the products that we have on the website (what a surprise). When they are viewing some analysis on Product 1 on App 1, we want to make sure that once they go to App 2 to get more details or different analysis, it automatically shows Product 1, and the user does not need to reenter the information.

 
Running Streamlit in a Subfolder
 
There are scenarios in data science projects that we may need to have our Streamlit apps in a subfolder. In this case, since Streamlit runs the apps from a subfolder, the app does not have access to the libraries in the parent folders. In order to overcome this problem, we may need to either have our Streamlit main app file in the project root or add the root folder into the system path at the beginning of our Streamlit apps.

 
Sessions
 
Streamlit is a session-based application. It means that once a user comes to the app, Streamlit assigns him/her a session ID, and other consecutive actions and data transfers are associated with that session. Because of that when you have a process, it won’t affect the other simultaneous users unless you use caching. We will discuss Caching later.
By default, you do not have standard access to the Session controls in Streamlit, and it is not documented officially yet, and it is used for internal purposes only. However, you can still access them and make some benefits by using them.
Streamlit apps are developed in a script-like format. It means that every interaction with the app will trigger the whole code to re-run from start to bottom. This makes Streamlit extremely easy to work with, but at the same time, very tricky to control consecutive events since there is no event handling capability for the developers.
Assume that you have a button (st.button) to start a process, and in the resulting screen, you want to give the user some interactive options to work with, for example, another checkbox, radio button, or simply another button. In this case, when you click on the first button (let’s call it button_run) becomes True when it reruns the whole code. There is nothing wrong, and the app runs smoothly.



 
Now, on the resulting page, there is another button (let’s call it button_filter) to filter the results. If you now click on the second button (button_filter), its value becomes True, and Streamlit runs the whole code again. But the problem is that now the first button (button_run) has become False because we did not click on that. In this case, when Streamlit reruns the whole code, there is the assumption that button_run is not clicked, and button_filter is clicked. And it does not remember that button_run was previously clicked. Therefore, button_filter clicked code will never be executed, because button_filter itself was the result of the first button, button_runclick.

 
In such cases, we should register the events, so that Streamlit can remember when a user clicks on the first button, and once the next button is clicked, it can understand that these are two consecutive actions and both buttons should be considered as clicked.
You may think that, well, we can save that information in a DB or temporary text file. It is possible, but how do you differentiate the potential different users?
Streamlit has a built-in undocumented Session object that can store some temporary information for every user. In this case, when a user clicks on button_run, we store the clicked event in the Session, and once button_filter is clicked, we can check if button_run was previously clicked to control the correct flow of data.
Here is the session class that you can include in your app:

Once you have the session class added, you can use the session to store and retrieve the information.

 
SQLAlchemy
 
SQLAlchemy is one of the standard popular libraries to connect to multiple types of databases such as SQLite, MySQL, etc. SQLAlchemy can be used for multiple platforms as desktop apps, web apps, or even mobile apps. If you have used this library before, you have realized that it is pretty simple, but when it comes to web development it may become a bit tricky. The main challenge of using this library for web applications is to control the number of database connections.
For that purpose, we have separate libraries for Flask (sqlalchemy-flask) and Tornado (sqlalchemy-tornado) which developers can use without any worry. But to my knowledge, we do not have any specific library for Streamlit. Since Streamlit is built upon Tornado, maybe we can use the tornado version, but I personally did not test that.
As you remember, Streamlit is session-based, which means that it runs a separate instance for every user. SQLAlchemy here is no exception. If you’re not careful, Streamlit will create a database connection for every user and maybe for every interaction. Depending on your database, your connections may get rejected if there are so many active connections available. As a result, python may end up with some strange error such as “double free or corruption” and crash your application.
In the Streamlit forum, there is a suggestion of caching the connection, which works well on SQLLite, but not very well on MySQL for example. When you cache your database connection, it won’t be open for an unlimited time, so that you may solve that issue with ttl. In this case, you can make sure that your connection object has expired before hitting a wall on the database side because the connection was already killed. Theoretically, this works fine if you have a very limited number of simultaneous users.
The main problem with caching the connection starts when two users run the code which caches the object at the same time. And at the end, the cached connection may not be the right one, but the expired one since there were two connections created at the same time, but only one was cached.
SQLAlchamy has an object called Session, in which we can create our database connections (engines) and execute our SQL queries. This would check if the new connection is already existing in the pool, and if it is existing, it won’t create a new connection to prevent the database connection saturation issue. In this case, you do not need to use Streamlit caching anymore to store your database connection. The following code snippet will help you understand how to use Session to connect to MySQL.

Remember that, prior to using Session in SQLAlchemy, if you were using engine only, you had to return conn = engine.connect() instead of the session, and you could use df = pd.read_sql(query, conn) to run the query. However, these methods are not working on SQLAlchemy Sessions.
 
Caching
 
Streamlit has very thorough, useful documentation on Caching, and honestly, it is one of its most useful features. Not using or misusing it can hugely impact the app performance and load/running time. I do not want to go through the details of caching which is already available in Streamlit documentation but only mentioning a few tips and findings.
 
App Wide Access
 
Unlike Session objects, cached objects are app-wide accessible. It means that once you cache information, it is accessible to all users of the app. So it is important not to cache user-specific settings and data, and instead, we can use Session as we discussed earlier.
 
Caching Parameters
 
Caching mechanism has few parameters which can control how an object must be cached.

ttl <float, None>: This stands for Time-to-Live and sets how long a cached object must be alive. This expiry is set in seconds.
max_entries <int, None>: Once you start calling a function with different parameters, it starts caching all those variations, and in a short time, it can be a huge amount of cached data. This parameter can set how many variations of a function can be cached, and the old ones will be deleted. This controls and limits the amount of memory consumed.
persistent <bool>: It is a boolean parameter to set if the cached data must be stored in a hard drive or memory. Just remember that, once you set it to True, Streamlit is pickling the object and storing it on the hard drive, and not all objects (such as SQLAlchamy database connection) can be pickled. So you may get an error for some persistent caching functions.
allow_output_mutation <bool>: Once the output of a function is cached, if you change the output (mutate), the results will be stored in the cached object and as I mentioned earlier, this is accessible to all users. So the best practice is to avoid changing the cached object. But still, there are some cases where you need to change the cached object directly. In this case, this parameter would allow Streamlit to mutate the cached object.
suppress_st_warning <bool>: Sometimes Streamlit raises some warnings to the user/developer so that they are aware of some consequences of caching. Setting this to False will stop those warnings.
show_spinner <bool>: Each time that Streamlit runs functions that are supposed to be cached, you will see a message on your UI saying “Running function_name”. It may not bother you that much unless you have lots of functions. Then you will see all those kinds of messages on your UI. Setting this parameter to False will prevent showing those messages.


The above code only caches the results for 60 seconds, and it only keeps the last 20 variations of this function. It also does not show any warning, does not show you any message on Streamlit UI when running this function.
Since we set allow_output_mutation to False, the following code is not allowed, and we can not update (mutate) the result of the function.

 
Clearing Cache
 
There are some cases that you may need to clear the cache programmatically. Clearing all cached data is manually possible through the hamburger menu at the top right of the Streamlit apps, but if you want to do it programmatically, you can use the following undocumented method.

 
SQLAlchemy Session / Scoped Session
 
Now that you could successfully connect to the database using SQLAlchemy Session, Scoped Session, and Pooling, you may need to cache your sessions or the functions that are using the database connection. As discussed earlier, since we are using Pool and Scoped Session, we may not need to cache the connection, but we may still need to cache our functions. Below, we are suggesting two recommendations on caching the functions that are using sessions.
The following example would use hash_funcs to identify which parameter of Session must be monitored for hashing.

If the above example is not working, for example in the case of using scoped_session, you can simply ask Streamlit to ignore hashing session as below:

 
UI Hacks
 



The simplicity of Streamlit is because you do not need to deal with UI, and it comes with pre-built-in responsive UI elements which will be placed elegantly on your page. Even though in the recent versions, they have provided new beta updates which enable you to create columns and arrange your elements in them, there is not much customization to do with its UI.
When I deploy my apps, there is a wide range of users in the company to work with them. I heavily use caching mechanisms to control the performance and speed of my apps. Some of my functions take a few minutes to run, and I use a caching mechanism to make sure that other users won’t wait again for the same request and will have a high-performance experience with the app. But, if a user clicks on that hamburger menu button at the top right, and selects “Clear Cache’’, it can hugely impact the performance of the app for the other users, until the function caches the results again. Or for example, some of my apps are designed to be shown the best in the wide mode, and if a user selects the “center” mode, it can affect how my app looks.
Besides all those that can directly affect my app, there are other options in the hamburger menu that a normal user may not need to have access to. For example, access to the Streamlit Github, documentation, etc.
There is a proposed idea on Streamlit Github to limit those hamburger menu options once the app is deployed, but until today, this issue is still open, and we can not manage them directly. Therefore, I came with my CSS solution to solve this issue.



In my proposed solution, you can remove (hide) the Streamlit footer, and control the items in the hamburger menu. You simply need to inject the following CSS into your application using st.markdown and allowing “unsafe” HTML codes.

The numbers mentioned above in li:nth-of-type(n) are referring to the item element in the hamburger menu and their order may change in the future updates of Streamlit.



Also, currently, there is an option in the hamburger menu (3rd item) called “Deploy this app”. This item is shown only if the app is accessed via a loopback local IP address (either localhost or 127.0.0.1). If you access your app through your LAN/WAN IP address, this item will not be shown.
 
Record a Screencast
 
This feature was introduced in version 0.55.0, and I was personally thrilled by this feature which would allow us to record our apps for training and presentation purposes. Soon, we realized that this feature is not working for the other users accessing our Streamlit apps, and they get the following message upon clicking on that option.



Because of the privacy restrictions implemented and imposed by the browsers, this feature works on the following conditions only:

Only on recent versions of Chrome, Firefox, and Edge
Accessing either on localhost or 127.0.0.1
If it is not being accessed locally, it must be behind an SSL certificate (https)

If you are serving your apps behind a proxy — such as Nginx — and you are aiming to use this feature, make sure that it is secured with an SSL certificate. Currently, Streamlit does not natively support SSL, but it can be deployed behind a proxy with an SSL certificate.
 
Components
 
Since the introduction of Streamlit components, developers have started building amazing components which can be served on Streamlit apps. If you would like, you can build your own components using Streamlit Component API. Streamlit has also a component gallery that presents some of the useful and interesting components which are publicly available. Among them, I have selected a few of them that I use to build amazing apps in SSENSE.
 
ACE Editor
 
This editor is providing a color-coded editor for different programming languages. I personally use a lot of JSON data in my apps, and I use this editor to view and edit my JSON content. It is amazing since it can also capture my formatting structures and errors.


https://github.com/okld/streamlit-ace

 
If you are tired of Streamlit standard multi-line text box, this component can be a very good alternative.
 
Ag-Grid
 
Streamlit can handle data frames, and it can show them in a table-based format either using st.write or st.dataframe. However, by default, Streamlit does not provide customized controllers on the presentation of your data frame except sorting by clicking on the column names.
Ag-Grid is a grid component that can be imported into Streamlit. Using this component, not only can you present your data frame, but also include links, images, checkboxes, etc into your grid cells as well as filtering the data, searching, aggregate, and grouping them.


https://github.com/PablocFonseca/streamlit-aggrid

 
If you are dealing with showing data frames a lot, maybe it is time to give Ag-Grid a try to see its huge potential in your applications.
 
Lottie Animations
 
Last, but not least, in my list of components is Lottie Animations. If you check lottiefiles.com, you will see thousands of vector-based animations in multiple formats such as JSON, which can be placed in your apps. This component would allow you to serve those Lottie animations by simply giving its JSON file.


https://lottiefiles.com/968-loading

 
I personally use these animations to show beautifully designed spinners while I am loading or calculating stuff. These animations will give a more vibrant and dynamic look to your next data science project.
 
Final Words
 
Here, I presented some tips and tricks on how to develop Streamlit applications. Some of these tricks may become natively available in the future versions of Streamlit, so that we may not need to do the hacks, or on the other hand, they may come with some updates to prevent our hacks. Who knows, but we can enjoy them for now, and hope for new amazing features in Streamlit.
I would also like to thank the Streamlit community for building such an amazing tool.
 
Bio: Kaveh Bakhtiyari is a PhD Candidate in Artificial Intelligence and a Data Scientist at SSENSE.
Original. Reposted with permission.
Related:

Deploying Streamlit Apps Using Streamlit Sharing
Topic Modeling with Streamlit
Deploying Secure and Scalable Streamlit Apps on AWS with Docker Swarm, Traefik and Keycloak"
https://www.kdnuggets.com/2021/07/build-image-classifier-in-few-lines-of-code-with-flash.html,How to Build An Image Classifier in Few Lines of Code with Flash,Introducing Flash: The high-level deep learning framework for beginners.,"comments
By Irfan Alghani Khalid, Computer Science Student


Photo by Brian Suh on Unsplash

 
Introduction
 
Image classification is a task where we want to predict which class belongs to an image. This task is difficult because of the image representation. If we flatten the image, it will create a long one-dimensional vector. Also, that representation will lose the neighbor information. Therefore, we need deep learning for extracting features and predict the result.
Sometimes, Building a deep learning model can become a difficult task. Although we create a base model for image classification, we need to spend lots of time creating the code. We have to prepare code for preparing the data, training the model, testing the model, and deploy it to the server. And that’s where the Flash comes in!
Flash is a high-level deep learning framework for fast building, training, and testing the deep learning model. Flash is based on the PyTorch framework. So if you know PyTorch, you will be familiar with Flash easily.
In comparison with PyTorch and Lighting, Flash is easy to use but not so flexible as the previous libraries. If you want to build a more complex model, you can use Lightning or straight to the PyTorch.


Created by the author.

 
With Flash, you can build your deep learning model in few lines of code! So, if you are new to deep learning, don’t be afraid. Flash can help you to build a deep learning model without getting confused because of the code.
This article will show you how to build an image classifier using Flash. Without further, let’s get started!
 
Implementation
 
Install the library
 
For installing the library, you can use the pip command like this:

pip install lightning-flash


If the command doesn’t work, you can install the library by using its GitHub repository. The command looks like this:

pip install git+https://github.com/PyTorchLightning/lightning-flash.git


After we can download the package successfully, now let’s load the libraries. We also set the seed with the number 42. Here is the code for doing that:



 
Download the data
 
After we install the library, now let’s get the data. For demonstration, we will use the dataset called Cat and Dog dataset.
This dataset contains images that are divided into two classes. The classes are cat and dog. To access the dataset, you can find this dataset at Kaggle. You can access the dataset here.


Captured by the author.

 
Load the data
 
After we download the data, now let’s load the dataset into an object. We will use the from_folders method for putting our data into the ImageClassification object. Here is the code for doing that:



 
Load the model
 
After we load the data, the next step is to load the model. Because we will not create our own architecture from scratch, we will use the pre-trained model based on existing convolutional neural network architecture.
We will use the ResNet-50 model that has already pretrained. Also, We set the number of classes based on the dataset. Here is the code for doing that:



 
Train the model
 
After we load the model, now let’s train the model. We need to initialize the Trainer object first. We will train the model in 3 epochs. Also, we enable the GPU to train the model. Here is the code for doing that:



After we initialize the object, now let’s train the model. To train the model, we can use a function called finetune. Inside the function, we set the model and the data. Also, we set the training strategy to freeze, where we don’t want to train the feature extractor. In other words, we train the classifier section only.
Here is the code for doing that:



And here is the evaluation result:


Captured by the author.

 
As you can see from the result, our model has achieved around 97% of accuracy. That’s a good one! Now let’s test the model on several new data.
 
Test the model
 
We will use the sample data that have not been trained on the model. Here are the samples that we will test to the model:



To test the model, we can use the predict method from the flash library. Here is the code for doing that:



As you can see from the result above, the model has predicted the samples with correct labels. That’s nice! Now let’s save the model for later use.
 
Save the model
 
Now we have trained and tested the model. Let’s save the model using the save_checkpoint method. Here is the code for doing that:



If you want to load the model on the other code, you can use the load_from_checkpoint method. Here is the code for doing that:



 
Final Remarks
 
Well done! Now you have learned how to build an image classifier using Flash. As I’ve stated from the beginning, it takes only a few lines of code! How cool is that?
I hope this article can help you to build your own deep learning model on your own case. And I hope you can take a step to learn PyTorch if you want to implement a more complex model.
If you are interested in my article, you can follow me on Medium. I will publish articles related to data science and machine learning. Also, if you have any questions or want to say hi, you can connect with me on LinkedIn.
Thank you for reading my article!
 
Bio: Irfan Alghani Khalid is a Computer Science Student @ IPB University, interested in Data Science, Machine Learning, and Open Source.
Original. Reposted with permission.
Related:

High Performance Deep Learning, Part 1
Fine-Tuning Transformer Model for Invoice Recognition
Deep Learning Is Becoming Overused"
https://www.kdnuggets.com/2021/01/mlops-model-monitoring-101.html,MLOps: Model Monitoring 101,Model monitoring using a model metric stack is essential to put a feedback loop from a deployed ML model back to the model building stage so that ML models can constantly improve themselves under different scenarios.,"comments
By Pronojit Saha and Dr. Arnab Bose, Abzooba


Fig 1: ML Workflow (Image from martinfowler.com, 2019)

 
Background
 
ML models are driving some of the most important decisions for businesses. As such it is important that these models remain relevant in the context of the most recent data, once deployed into production. A model may go out of context if there is data skew i.e. data distribution may have changed in production from what was used during training. It may also be that a feature becomes unavailable in production data or that the model may no longer be relevant as the real-world environment might have changed (e.g. Covid19) or further and more simply, the user behavior may have changed. Monitoring the changes in model’s behavior and the characteristics of the most recent data used at inference is thus of utmost importance. This ensures that the model remains relevant and/or true to the desired performance as promised during the model training phase.
An instance of such a model monitoring framework is illustrated in Fig 2 below. The objective is to track models on various metrics, the details of which we will get into the next sections. But first, let us understand the motivation of a model monitoring framework.

 


Fig 2: Model Monitoring Framework Illustrated (Image by author)

 
 
Motivation
 
Feedback loops play an important role in all aspects of life as well as business. Feedback loops are simple to understand: you produce something, measure information on the production, and use that information to improve production. It’s a constant cycle of monitoring and improvement. Anything that has measurable information and room for improvement can incorporate a feedback loop and ML models can certainly benefit from them.
A typical ML workflow includes steps like data ingestion, pre-processing, model building & evaluation, and finally deployment. However, this lacks one key aspect i.e. feedback. The primary motivation of any “model monitoring” framework thus is to create this all-important feedback loop post-deployment back to the model building phase (as depicted in Fig 1). This helps the ML model to constantly improve itself by deciding to either update the model or continue with the existing model. To enable this decision the framework should track & report various model metrics (details in “Metrics” section later) under two possible scenarios described below.

Scenario I: The training data is available and the framework computes the said model metrics both on training data and production (inference) data post-deployment and compares to make a decision.
Scenario II: The training data is not available and the framework computes the said model metrics based only on the data that is available post-deployment.

The following table lists the inputs required by the model monitoring framework to generate the said metrics, under the two scenarios.

 
Based on which of the two scenarios is applicable, metrics highlighted in the next section are computed to decide if a model in production needs an update or some other interventions.
 
Metrics
 
A proposed model monitoring metrics stack is given in Fig 3 below. It defines three broad types of metrics based on the dependency of the metric on data and/or ML model. A monitoring framework should ideally consist of one or two metrics from all three categories, but if there are tradeoff then one may build up from the base i.e. starting with operations metrics and then building up with the maturity of the model. Further, operations metrics should be monitored at a more real time level or at-least daily where stability and performance can be at a weekly or even a larger time frame depending on the domain & business scenario.


Fig 3: Model Monitoring Metrics Stack (Image by author)

 
 
1. Stability Metrics — These metrics help us to capture two types of data distribution shifts:
a) Prior Probability Shift — Captures the distribution shift of the predicted outputs and/or dependent variable between either the training data and production data (scenario I) or various time frames of the production data (scenario II). Examples of these metrics include Population Stability Index (PSI), Divergence Index (Concept Shift), Error Statistic (details & definition to follow in next article of this series)
b) Covariate Shift — Captures the distribution shift of each independent variable between either the training data and production data (scenario I) or various time frames of the production data (scenario II), as applicable. Examples of these metrics include Characteristic Stability Index (CSI) & Novelty Index (details & definition to follow in the next article of this series)
 
2. Performance Metrics — These metrics help us to detect a concept shift in data i.e. identify whether the relation between independent & dependent variables has changed (e.g. post-COVID the way users purchase during festivals may have changed). They do so by examining how good or bad the existing deployed model is performing viz-a-viz when it was trained (scenario I) or during a previous time frame post-deployment (scenario II). Accordingly, a decision can be taken to re-work the deployed model or not. Examples of these metrics include,
a) Project Metrics like RMSE, R-Square, etc for regression and accuracy, AUC-ROC, etc for classification.
b) Gini and KS -Statistics: A statistical measure of how well the predicted probabilities/classes are separated (only for classification models)
 
3. Operations Metrics — These metrics help us to determine how the deployed model is performing from a usage point of view. They are as such independent of model type, data & don’t require any inputs as with the above two metrics. Examples of these metrics include,
a. # of time ML API endpoints called in the past
b. Latency when calling ML API endpoints
c. IO/Memory/CPU usage when performing prediction
d. System uptime
e. Disk utilization
 
Conclusion
 
Model monitoring within the realm of MLOps has become a necessity for mature ML systems. It is quintessential to implement such a framework to ensure consistency and robustness of the ML system, as without it ML systems may lose the “trust” of the end-user, which could be fatal. As such including and planning for it in the overall solution architecture of any ML use case implementation is of utmost importance.
In the next blogs of the series, we will get into more details of the two most important model monitoring metric i.e. Stability & Performance metrics and we will see how we can use them to build our model monitoring framework.
References

D. Sato, A. Wider, C. Windheuser, Continuous Delivery for Machine Learning (2019), martinflower.com
M. Stewart, Understanding Dataset Shift (2019), towardsdatascience.com

 
Pronojit Saha is an AI practitioner with extensive experience in solving business problems, architecting, and building end-to-end ML driven products & solutions by leading and facilitating cross-functional teams. He is currently the Advanced Analytics Practice Lead at Abzooba, wherein apart from project execution he also engages in leading & growing the Practice by nurturing talent, building thought leadership, and enabling scalable processes. Pronojit has worked in the retail, healthcare, and Industry 4.0 domains. Time series analytics and natural language processing are his expertise and he has applied these along with other AI methodologies for use cases like price optimization, readmission prediction, predictive maintenance, aspect-based sentiment analytics, entity recognition, topic modeling, among others.
Dr. Arnab Bose is Chief Scientific Officer at Abzooba, a data analytics company and an adjunct faculty at the University of Chicago where he teaches Machine Learning and Predictive Analytics, Machine Learning Operations, Time Series Analysis and Forecasting, and Health Analytics in the Master of Science in Analytics program. He is a 20-year predictive analytics industry veteran who enjoys using unstructured and structured data to forecast and influence behavioral outcomes in healthcare, retail, finance, and transportation. His current focus areas include health risk stratification and chronic disease management using machine learning, and production deployment and monitoring of machine learning models.
Related:

MLOps – “Why is it required?” and “What it is”?
Model Experiments, Tracking and Registration using MLflow on Databricks
Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes"
https://www.kdnuggets.com/2020/07/understanding-neural-networks-think.html,Understanding How Neural Networks Think,"A couple of years ago, Google published one of the most seminal papers in machine learning interpretability.","By Jesus Rodriguez, Intotheblock.
comments


Source: https://distill.pub/2018/building-blocks/

 

I recently started a new newsletter focus on AI education. TheSequence is a no-BS (meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers and concepts. Please give it a try by subscribing below:

 

 
One of the challenging elements of any deep learning solution is to understand the knowledge and decisions made by deep neural networks. While the interpretation of decisions made by a neural networks has always been difficult, the issue has become a nightmare with the raise of deep learning and the proliferation of large scale neural networks that operate with multi-dimensional datasets. Not surprisingly, the interpretation of neural networks has become one of the most active areas of research in the deep learning ecosystem.
Try to imagine a large neural network with hundreds of millions of neurons that is performing a deep learning task such as image recognition. Typically, you would like to understand how the network arrives to specific decisions. Most of the current research has focused on detecting what neurons in the network have been activated. Knowing that neuron-12345 fired five times is relevant but not incredibly useful in the scale of the entire network. The research about understanding decisions in neural networks has focused on three main areas: feature visualization, attribution and dimensionality reduction. Google, in particular, has done a lot of work in the feature visualization space publishing some remarkable research and tools. Over a year ago, Google researchers published a paper titled “The Building Blocks of Interpretability” that became a seminal paper in the are of machine learning interpretability. The paper proposes some new ideas to understand how deep neural networks make decisions.
The main insight of Google’s research is to not see the different interpretability techniques in isolation but as composable building blocks of larger models that help understand the behavior of neural networks. For instance, feature visualization is a very effective technique to understand the information processed by individual neurons but fails to correlate that insight with the overall decision made by the neural network. Attribution is a more solid technique to explain the relationship between different neurons but not so much when comes to understand the decision made by individual neurons. Combining those building blocks, Google has created an interpretability models that does not only explains what a neural network detects, but it does answer how the network assembles these individual pieces to arrive at later decisions, and why these decisions were made.
How does the new Google model for interpretability works specifically? Well, the main innovation, in my opinion, is that it analyzes the decisions made by different components of a neural network at different levels: individual neurons, connected groups of neurons and complete layers. Google also uses a novel research technique called matrix factorization to analyze the impact that arbitrary groups of neurons can have in the final decision.


Source: https://distill.pub/2018/building-blocks/

 
A good way to think about Google’s blocks of interpretability is as a model that detects insights about the decisions of a neural network at different levels of abstraction from the basic computation graph to the final decision.


Source: https://distill.pub/2018/building-blocks/

 
Google research of deep neural network interpretability is not only a theoretical exercise. The research group accompanied the paper with the release of Lucid, a neural network visualization library that allow developers to make the sort lucid feature visualizations that illustrate the decisions made by individual segments of a neural network. Google also released colab notebooks. These notebooks make it extremely easy to use Lucid to create Lucid visualization in an interactive environment.
 
Original. Reposted with permission.
Related:

Learning by Forgetting: Deep Neural Networks and the Jennifer Aniston Neuron
Uber’s Ludwig is an Open Source Framework for Low-Code Machine Learning
Google Unveils TAPAS, a BERT-Based Neural Network for Querying Tables Using Natural Language"
https://www.kdnuggets.com/2021/07/design-patterns-machine-learning.html,Design patterns in machine learning,Can we abstract best practices to real design patterns yet?,"comments
By Ágoston Török, Director Data Science, AGT International
According to its definition, a design pattern is a reusable solution to a commonly occurring problem. In software engineering, the concept dates back to 1987 when Beck and Cunningham started to apply it to programming. By the 2000s, design patterns — especially the SOLID design principles for OOP — were considered common knowledge to programmers. Fast forward 15 years and we arrive at the era of Software 2.0: machine learning models start to replace classical functions in more and more places of code. Today, we look at software as a fusion of traditional code, machine learning models and the underlying data. This fusion requires a seamless integration of these components, which is often far from trivial given the fields’ often disparate history and evolution.


Today, we look at software as a fusion of traditional code, machine learning models and the underlying data.


Design patterns, however, have not been extended yet to deal with the challenges of this new era. In Software 2.0 common challenges do not appear only at the code level but at the level of problem definition, data representation, training methods, scaling, and on the ethical aspects of the design of AI-enabled systems. This creates a fertile ground for the practice of machine learning antipatterns. Unfortunately, today even blogposts and conferences feature sometimes antipatterns: practices that believed to improve things but in reality they make things worse. Since antipatterns also require skills, they are often not recognized as such by their practitioners. Therefore in the following, I will give two examples of common ML challenges but, instead of starting with the design pattern, I will introduce first their solution antipatterns.
 
The model shows bad performance on the evaluation metrics
 
In the common scenario, after collecting, cleaning, and preparing the data the engineer trains a first model and finds that it shows bad performance on the test data. A common antipattern is to replace the first model with a more complex one (e.g. often gradient boosted trees) and improve the performance by this. A variation of this antipattern may follow this step by combining several models by e.g. model averaging.


Donald Knuth famous quote “premature optimization is the root of all evil” is almost 50 years old and is still true. Image with permission from tddcomics.

 
The problem with these methods is that they look only at part of the problem, i.e. the model, and choose to resolve it by increasing the complexity of the model. This steps forces us to accept the high risk of overfitting and to trade explainability for additional predictive power. While there are efficient practices to mitigate the side effects of this choice (e.g. LIME), we cannot fully eliminate them.
The design pattern is error analysis. This in practice means looking at where our model made errors, either by assessing the model fit on different test sets or by even looking at individual cases where our model was wrong. Although, we all heard the saying “garbage in, garbage out”, still very few people appreciates how much this is true even for little inconsistencies in the data. Maybe the labels are coming from different raters, each having their own, slightly different interpretation of the labelling guidelines. Maybe the way of collecting the data has changed over time. The effect of error analysis is especially strong for small data problems. However, we should also keep in mind that in a significant proportion of big data situations we also deal with long tail events (e.g. identify rare talents from an admission exam).
The true power of error analysis comes from the fact that we do not trade either explainability or risk of overfitting by applying it, in fact solely applying it yields critical knowledge about the distribution of the data. Error analysis, furthermore, enables us to choose both model-centric (e.g. more complex model) and a data-centric (e.g. further cleaning steps) solutions.
 
Performance degradation over time on a deployed model
 
The model goes through extensive validation and is deployed to production. The users are happy and give positive feedback. Then, a month/quarter/year later, reports are coming in that tell about flaws in prediction. This is usually a manifestation of concept drift, the connection that your model learned between input and output has changed over time. There are places where such concept drift is commonly known (word semantics, spam detectors) but ‘concept’ drift can happen in any field. For instance, masks and social distancing regulations challenged many previously deployed computer vision models too.


ML systems without retraining assume no change in the learned relationship between input and output. Image with permission from tddcomics.

 
A common antipattern is to attribute these examples to noise and expect the situation to stabilize with time. This means not only a lack of action but a false attribution too, which should be generally discouraged in a data-driven business. A slightly better antipattern is to react to the reports with quick retraining and deployment of a new model. This is an antipattern even in the case when the team assumes they follow agile software development principles and therefore choose to be quick in reaction to change. The problem is that this solution addresses the symptom but not the flaw in the design of the system.
The design patterns are a continuous evaluation of performance, which means you expect drifts to happen and, hence, design the system to notice it as soon as possible. This is a completely different approach as the focus is not on the speed of reaction but on the speed of detection. This puts the entire system in a much more controlled course giving more room for prioritization of any reaction. Continuous evaluation means establishing processes and tools to continuously generate ground truth for a fraction of the new data. In most cases this involves manual label, often using crowdsourced services. In some instances, though, we can use other more sophisticated but in the deployment setting not feasible models and devices to generate ground truth labels. For example, in the development of self-driving cars the input from one sensor (e.g. LiDAR) can be used to generate the ground truth for another sensor (e.g. camera).
 
The SOLID design principles of machine learning
 
The reason I’m writing about design patterns is that this field has reached the level of maturity where we should not only share our best practices but we should be able to abstract them to real design patterns. Luckily, this work has been started by multiple groups already. In fact, two books have been published recently on the topic [1], [2]. I enjoyed reading them but I was still left with a feeling that although we are going in the right direction we are still few steps away from formulating the SOLID design principles for ML practitioners. I believe that while the underlying knowledge is already available and is used to build the AI-enabled products of today, work on design patterns and antipatterns is an important step towards the era of Software 2.0.


Design patterns are the foundation of the craftsmanship of machine learning. Image with permission from tddcomics.

 
Bio: Ágoston Török is Director of Data Science at AGT International.
Original. Reposted with permission.
Related:

When to Retrain an Machine Learning Model? Run these 5 checks to decide on the schedule
Top 6 Data Science Online Courses in 2021
How Much Memory is your Machine Learning Code Consuming?"
https://www.kdnuggets.com/2021/05/awesome-list-datasets.html,Awesome list of datasets in 100+ categories,"With an estimated 44 zettabytes of data in existence in our digital world today and approximately 2.5 quintillion bytes of new data generated daily, there is a lot of data out there you could tap into for your data science projects. It's pretty hard to curate through such a massive universe of data, but this collection is a great start. Here, you can find data from cancer genomes to UFO reports, as well as years of air quality data to 200,000 jokes. Dive into this ocean of data to explore as you learn how to apply data science techniques or leverage your expertise to discover something new.","comments
By Etienne D. Noumen, Senior Software Engineer.

Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains.
In this blog, we provide links to popular open-source and public data sets, data visualizations, data analytics resources, and data lakes.
 
Table of Contents
 

Latest complete Netflix movie dataset 
Common Crawl
 Dataset on protein prices
 CPOST dataset on suicide attacks over four decades
Credit Card Dataset – Survey of Consumer Finances (SCF) Combined Extract Data 1989-2019
Drone imagery with annotations for small object detection and tracking dataset
NOAA High-Resolution Rapid Refresh (HRRR) Model
Registry of Open Data on AWS
Textbook Question Answering (TQA)
Harmonized Cancer Datasets: Genomic Data Commons Data Portal
The Cancer Genome Atlas
Therapeutically Applicable Research to Generate Effective Treatments (TARGET)
Genome Aggregation Database (gnomAD)
SQuAD (Stanford Question Answering Dataset)
PubMed Diabetes Dataset
Drug-Target Interaction Dataset
Pharmacogenomics Datasets
Pancreatic Cancer Organoid Profiling
Africa Soil Information Service (AfSIS) Soil Chemistry
Dataset for Affective States in E-Environments
NatureServe Explorer Dataset
Flight Records in the US
Worldwide flight data
2019 Crime statistics in the USA
Yahoo Answers DataSets
History of America 1400-2021
Persian words phonetics dataset
Historical Air Quality Dataset
Stack Exchange Dataset
Awesome Public Datasets
Agriculture Datasets
Biology Datasets
Climate and Weather Datasets
Complex Network Datasets 
Computer Network Datasets
CyberSecurity Datasets
Data Challenges Datasets
Earth Science Datasets
Economics Datasets
Education Datasets
Energy Datasets
Entertainment Datasets
Finance Datasets
GIS Datasets
Government Datasets
Healthcare Datasets
Image Processing Datasets
Machine Learning Datasets
Museums Datasets
Natural Language Datasets
Neuroscience Datasets
Physics Datasets
Prostate Cancer Datasets
Psychology and Cognition Datasets
Public Domains Datasets
Search Engines Datasets
Social Networks Datasets
Social Sciences Datasets
Software Datasets
Sports Datasets
Time Series Datasets
Transportation Datasets
eSports Datasets
Complementary Collections
Categorized list of public datasets: Sindre Sorhus /awesome List
Platforms
Programming Languages
Front-End Development
Back-End Development
Computer Science
Big Data
Theory
Books
Editors
Gaming
Development Environment
Entertainment
Databases
Media
Learn
Security
Content Management Systems
Hardware
Business
Work
Networking
Decentralized Systems
Higher Education
Events
Testing
Miscellaneous
Related
US Department of Education CRDC Dataset
Nasa Dataset: sequencing data from bacteria before and after being taken to space
All Trump’s twitter insults from 2015 to 2021 in CSV.
Data is plural
Global terrorism database
The dolphin social network
Dataset of 200,000 jokes
The Million Song Dataset
Cornell University’s eBird dataset
UFO Report Dataset
CDC’s Trend Drug Data
Health and Retirement study: Public Survey data

This is a huge list, so here are here are 100+ more categories
 
Latest complete Netflix movie dataset 
Created from 4 APIs. 11K+ rows and 30+ attributes of Netflix (Ratings, earnings, actors, language, availability, movie trailers, and many more)
Dataset on Kaggle.
Explore this dataset using FlixGem.com (this dataset is powering this webapp)
Dataset on Google Sheets.
 
Common Crawl
A corpus of web crawl data composed of over 50 billion web pages. The Common Crawl corpus contains petabytes of data collected since 2008. It contains raw web page data, extracted metadata and text extractions.
AWS CLI Access (No AWS account required)
aws s3 ls s3://commoncrawl/ --no-sign-request
s3://commoncrawl/crawl-data/CC-MAIN-2021-17 – April 2021
 
 Dataset on protein prices
Data on Primary Commodity Prices are updated monthly based on the IMF’s Primary Commodity Price System.
Excel Database
 
 CPOST dataset on suicide attacks over four decades
The University of Chicago Project on Security and Threats presents the updated and expanded Database on Suicide Attacks (DSAT), which now links to Uppsala Conflict Data Program data on armed conflicts and includes a new dataset measuring the alliance and rivalry relationships among militant groups with connections to suicide attack groups. Access it here.
 
Credit Card Dataset – Survey of Consumer Finances (SCF) Combined Extract Data 1989-2019
You can do a lot of aggregated analysis in a pretty straightforward way there.
 
Drone imagery with annotations for small object detection and tracking dataset
11 TB dataset of drone imagery with annotations for small object detection and tracking

Download and more information are available here
Dataset License: CDLA-Sharing-1.0
Helper scripts for accessing the dataset: DATASET.md
Dataset Exploration: Colab
 
NOAA High-Resolution Rapid Refresh (HRRR) Model
The HRRR is a NOAA real-time 3-km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation. Radar data is assimilated in the HRRR every 15 min over a 1-h period adding further detail to that provided by the hourly data assimilation from the 13km radar-enhanced Rapid Refresh.
 
Registry of Open Data on AWS
This registry exists to help people discover and share datasets that are available via AWS resources. Learn more about sharing data on AWS.
See all usage examples for datasets listed in this registry.
See datasets from Digital Earth Africa, Facebook Data for Good, NASA Space Act Agreement, NIH STRIDES, NOAA Big Data Program, Space Telescope Science Institute, and Amazon Sustainability Data Initiative.

 
Textbook Question Answering (TQA)
1,076 textbook lessons, 26,260 questions, 6229 images
Documentation: https://allenai.org/data/tqa
Download
 
Harmonized Cancer Datasets: Genomic Data Commons Data Portal
The GDC Data Portal is a robust data-driven platform that allows cancer researchers and bioinformaticians to search and download cancer data for analysis.

Genomic Data Commons Data Portal
 
The Cancer Genome Atlas
The Cancer Genome Atlas (TCGA), a collaboration between the National Cancer Institute (NCI) and National Human Genome Research Institute (NHGRI), aims to generate comprehensive, multi-dimensional maps of the key genomic changes in major types and subtypes of cancer.
AWS CLI Access (No AWS account required)
aws s3 ls s3://tcga-2-open/ --no-sign-request
 
Therapeutically Applicable Research to Generate Effective Treatments (TARGET)
The Therapeutically Applicable Research to Generate Effective Treatments (TARGET) program applies a comprehensive genomic approach to determine molecular changes that drive childhood cancers. The goal of the program is to use data to guide the development of effective, less toxic therapies. TARGET is organized into a collaborative network of disease-specific project teams. TARGET projects provide comprehensive molecular characterization to determine the genetic changes that drive the initiation and progression of childhood cancers. The dataset contains open Clinical Supplement, Biospecimen Supplement, RNA-Seq Gene Expression Quantification, miRNA-Seq Isoform Expression Quantification, miRNA-Seq miRNA Expression Quantification data from Genomic Data Commons (GDC), and open data from GDC Legacy Archive. Access it here.
 
Genome Aggregation Database (gnomAD)
The Genome Aggregation Database (gnomAD) is a resource developed by an international coalition of investigators that aggregates and harmonizes both exome and genome data from a wide range of large-scale human sequencing projects. The summary data provided here are released for the benefit of the wider scientific community without restriction on use. Downloads

 
SQuAD (Stanford Question Answering Dataset)
Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. Access it here.
 
PubMed Diabetes Dataset
The Pubmed Diabetes dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words. The README file in the dataset provides more details.
Download Link
 
Drug-Target Interaction Dataset
This dataset contains interactions between drugs and targets collected from DrugBank, KEGG Drug, DCDB, and Matador. It was originally collected by Perlman et al. It contains 315 drugs, 250 targets, 1,306 drug-target interactions, 5 types of drug-drug similarities, and 3 types of target-target similarities. Drug-drug similarities include Chemical-based, Ligand-based, Expression-based, Side-effect-based, and Annotation-based similarities. Target-target similarities include Sequence-based, Protein-protein interaction network-based, and Gene Ontology-based similarities. The original task on the dataset is to predict new interactions between drugs and targets based on different types of similarities in the network. Download link
 
Pharmacogenomics Datasets
PharmGKB data and knowledge is available as downloads. It is often critical to check with their curators at feedback@pharmgkb.org before embarking on a large project using these data, to be sure that the files and data they make available are being interpreted correctly. PharmGKB generally does NOT need to be a co-author on such analyses; They just want to make sure that there is a correct understanding of our data before lots of resources are spent.
 
Pancreatic Cancer Organoid Profiling
The dataset contains open RNA-Seq Gene Expression Quantification data and controlled WGS/WXS/RNA-Seq Aligned Reads, WXS Annotated Somatic Mutation, WXS Raw Somatic Mutation, and RNA-Seq Splice Junction Quantification. Documentation
AWS CLI Access (No AWS account required)
aws s3 ls s3://gdc-organoid-pancreatic-phs001611-2-open/ --no-sign-request
 
Africa Soil Information Service (AfSIS) Soil Chemistry
This dataset contains soil infrared spectral data and paired soil property reference measurements for georeferenced soil samples that were collected through the Africa Soil Information Service (AfSIS) project, which lasted from 2009 through 2018. Documentation
AWS CLI Access (No AWS account required)
aws s3 ls s3://afsis/ --no-sign-request
 
Dataset for Affective States in E-Environments
DAiSEE is the first multi-label video classification dataset comprising of 9068 video snippets captured from 112 users for recognizing the user affective states of boredom, confusion, engagement, and frustration “in the wild”. The dataset has four levels of labels namely – very low, low, high, and very high for each of the affective states, which are crowd annotated and correlated with a gold standard annotation created using a team of expert psychologists. Download it here.
 
NatureServe Explorer Dataset
NatureServe Explorer provides conservation status, taxonomy, distribution, and life history information for more than 95,000 plants and animals in the United States and Canada, and more than 10,000 vegetation communities and ecological systems in the Western Hemisphere.
The data available through NatureServe Explorer represents data managed in the NatureServe Central Databases. These databases are dynamic, being continually enhanced and refined through the input of hundreds of natural heritage program scientists and other collaborators. NatureServe Explorer is updated from these central databases to reflect information from new field surveys, the latest taxonomic treatments and other scientific publications, and new conservation status assessments. Explore Data here
 
Flight Records in the US
Airline On-Time Performance and Causes of Flight Delays – On_Time Data.
This database contains scheduled and actual departure and arrival times, reason of delay. reported by certified U.S. air carriers that account for at least one percent of domestic scheduled passenger revenues. The data is collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS).
FlightAware.com has data but you need to pay for a full dataset.
The anyflights package supplies a set of functions to generate air travel data (and data packages!) similar to nycflights13. With a user-defined year and airport, the anyflights function will grab data on:

flights: all flights that departed a given airport in a given year and month
weather: hourly meterological data for a given airport in a given year and month
airports: airport names, FAA codes, and locations
airlines: translation between two letter carrier (airline) codes and names
planes: construction information about each plane found in flights

Airline On-Time Statistics and Delay Causes
The U.S. Department of Transportation’s (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled and diverted flights appears in DOT’s monthly Air Travel Consumer Report, published about 30 days after the month’s end, as well as in summary tables posted on this website. BTS began collecting details on the causes of flight delays in June 2003. Summary statistics and raw data are made available to the public at the time the Air Travel Consumer Report is released. Access it here
 
Worldwide flight data
Open flights: As of January 2017, the OpenFlights Airports Database contains over 10,000 airports, train stations and ferry terminals spanning the globe

Download: airports.dat (Airports only, high quality)
Download: airports-extended.dat (Airports, train stations and ferry terminals, including user contributions)
Bureau of Transportation:
Flightera.net seems to have a lot of good data for free. It has in-depth data on flights and doesn’t seem limited by date. I can’t speak on the validity of the data though.
flightradar24.com has lots of data, also historically, they might be willing to help you get it in a nice format.
 
2019 Crime statistics in the USA
Dataset with arrest in US by race and separate states. Download Excel here
 
Yahoo Answers DataSets
Yahoo is shutting down in 2021. This is Yahoo Answers datasets (300MB gzip) that is fairly extensive from 2015 with about 1.4m rows. This dataset has the best questions answers, I mean all the answers, including the most insane awful answers and the worst questions people put together. Download it here.
Another option here: According to the tracker, there are 77M done, 20M out(?), and 40M to go:
https://wiki.archiveteam.org/index.php/Yahoo!_Answers
 
History of America 1400-2021

Sources:
https://os-connect.com/pop/p2an.asp
https://ourworldindata.org/
http://www.ggdc.net/maddison/oriindex.htm
https://www.globalfirepower.com/countries-comparison.asp
 
Persian words phonetics dataset
This is a dataset of about 55K Persian words with their phonetics. Each word is in a line and separated from its phonetic by a tab.
 
Historical Air Quality Dataset
Air Quality Data Collected at Outdoor Monitors Across the US. This is a BigQuery Dataset. There are no files to download, but you can query it through Kernels using the BigQuery API. The AQS Data Mart is a database containing all of the information from AQS. It has every measured value the EPA has collected via the national ambient air monitoring program. It also includes the associated aggregate values calculated by EPA (8-hour, daily, annual, etc.). The AQS Data Mart is a copy of AQS made once per week and made accessible to the public through web-based applications. The intended users of the Data Mart are air quality data analysts in the regulatory, academic, and health research communities. It is intended for those who need to download large volumes of detailed technical data stored at EPA and does not provide any interactive analytical tools. It serves as the back-end database for several Agency interactive tools that could not fully function without it: AirData, AirCompare, The Remote Sensing Information Gateway, the Map Monitoring Sites KML page, etc.
 
Stack Exchange Dataset

https://data.stackexchange.com/
 
Awesome Public Datasets
This list of a topic-centric public data sources in high quality. They are collected and tidied from blogs, answers, and user responses. Most of the data sets listed below are free, however, some are not.
 
Agriculture

The global dataset of historical yields for major crops 1981–2016 – The […]
Hyperspectral benchmark dataset on soil moisture – This dataset was […]
Lemons quality control dataset – Lemon dataset has been prepared to […]
Optimized Soil Adjusted Vegetation Index – The IDB is a tool for working […]
U.S. Department of Agriculture’s Nutrient Database
U.S. Department of Agriculture’s PLANTS Database – The Complete PLANTS […]

 
Biology

1000 Genomes – The 1000 Genomes Project ran between 2008 and 2015, […]
American Gut (Microbiome Project) – The American Gut project is the […]
Broad Bioimage Benchmark Collection (BBBC) – The Broad Bioimage Benchmark […]
Broad Cancer Cell Line Encyclopedia (CCLE)
Cell Image Library – This library is a public and easily accessible […]
Complete Genomics Public Data – A diverse data set of whole human genomes […]
EBI ArrayExpress – ArrayExpress Archive of Functional Genomics Data […]
EBI Protein Data Bank in Europe – The Electron Microscopy Data Bank […]
ENCODE project – The Encyclopedia of DNA Elements (ENCODE) Consortium is […]
Electron Microscopy Pilot Image Archive (EMPIAR) – EMPIAR, the Electron […]
Ensembl Genomes
Gene Expression Omnibus (GEO) – GEO is a public functional genomics data […]
Gene Ontology (GO) – GO annotation files
Global Biotic Interactions (GloBI)
Harvard Medical School (HMS) LINCS Project – The Harvard Medical School […]
Human Genome Diversity Project – A group of scientists at Stanford […]
Human Microbiome Project (HMP) – The HMP sequenced over 2000 reference […]
ICOS PSP Benchmark – The ICOS PSP benchmarks repository contains an […]
International HapMap Project
Journal of Cell Biology DataViewer [fixme]
KEGG – KEGG is a database resource for understanding high-level functions […]
MIT Cancer Genomics Data
NCBI Proteins
NCBI Taxonomy – The NCBI Taxonomy database is a curated set of names and […]
NCI Genomic Data Commons – The GDC Data Portal is a robust data-driven […]
OpenSNP genotypes data – openSNP allows customers of direct-to-customer […]
Palmer Penguins – The goal of palmerpenguins is to provide a great […]
Pathguid – Protein-Protein Interactions Catalog
Protein Data Bank – This resource is powered by the Protein Data Bank […]
Psychiatric Genomics Consortium – The purpose of the Psychiatric Genomics […]
PubChem Project – PubChem is the world’s largest collection of freely […]
PubGene (now Coremine Medical) – COREMINE™ is a family of tools developed […]
Sanger Catalogue of Somatic Mutations in Cancer (COSMIC) – COSMIC, the […]
Sanger Genomics of Drug Sensitivity in Cancer Project (GDSC)
Sequence Read Archive(SRA) – The Sequence Read Archive (SRA) stores raw […]
Stanford Microarray Data
Stowers Institute Original Data Repository
Systems Science of Biological Dynamics (SSBD) Database – Systems Science […]
The Cancer Genome Atlas (TCGA), available via Broad GDAC
The Catalogue of Life – The Catalogue of Life is a quality-assured […]
The Personal Genome Project – The Personal Genome Project, initiated in […]
UCSC Public Data
UniGene
Universal Protein Resource (UnitProt) – The Universal Protein Resource […]
Rfam – The Rfam database is a collection of RNA families, each […]

 
Climate and Climate and Weather

Actuaries Climate Index
Australian Weather
Aviation Weather Center – Consistent, timely and accurate weather […]
Brazilian Weather – Historical data (In Portuguese) – Data related to […]
Canadian Meteorological Centre
Climate Data from UEA (updated monthly)
Dutch Weather – The KNMI Data Center (KDC) portal provides access to KNMI […]
European Climate Assessment & Dataset
German Climate Data Center
Global Climate Data Since 1929
Charting The Global Climate Change News Narrative 2009-2020 – These four […]
NASA Global Imagery Browse Services
NOAA Bering Sea Climate [fixme]
NOAA Climate Datasets
NOAA Realtime Weather Models
NOAA SURFRAD Meteorology and Radiation Datasets
The World Bank Open Data Resources for Climate Change
UEA Climatic Research Unit
WU Historical Weather Worldwide
Wahington Post Climate Change – To analyze warming temperatures in the […]
WorldClim – Global Climate Data

 
Complex Complex Network

AMiner Citation Network Dataset
CrossRef DOI URLs
DBLP Citation dataset
DIMACS Road Networks Collection
NBER Patent Citations
NIST complex networks data collection
Network Repository with Interactive Exploratory Analysis Tools [fixme]
Protein-protein interaction network
PyPI and Maven Dependency Network
Scopus Citation Database
Small Network Data
Stanford GraphBase
Stanford Large Network Dataset Collection
Stanford Longitudinal Network Data Sources [fixme]
The Koblenz Network Collection
The Laboratory for Web Algorithmics (UNIMI)
UCI Network Data Repository
UFL sparse matrix collection
WSU Graph Database [fixme]
Community Resource for Archiving Wireless Data At Dartmouth – Contains […]

 
Computer Network

3.5B Web Pages from CommonCrawl 2012
53.5B Web clicks of 100K users in Indiana Univ.
CAIDA Internet Datasets
CRAWDAD Wireless datasets from Dartmouth Univ. [fixme]
ClueWeb09 – 1B web pages
ClueWeb12 – 733M web pages
CommonCrawl Web Data over 7 years
Criteo click-through data
Internet-Wide Scan Data Repository
MIRAGE-2019 – MIRAGE-2019 is a human-generated dataset for mobile traffic […]
OONI: Open Observatory of Network Interference – Internet censorship data
Open Mobile Data by MobiPerf
The Peer-to-Peer Trace Archive – Real-world measurements play a key role […]
Rapid7 Sonar Internet Scans
UCSD Network Telescope, IPv4 /8 net

 
CyberSecurity

CCCS-CIC-AndMal-2020 – The dataset includes 200K benign and 200K malware […]
Traffic and Log Data Captured During a Cyber Defense Exercise – This […]

 
Data Challenges

Bruteforce Database
Challenges in Machine Learning
CrowdANALYTIX dataX [fixme]
D4D Challenge of Orange [fixme]
DrivenData Competitions for Social Good
ICWSM Data Challenge (since 2009)
KDD Cup by Tencent 2012
Kaggle Competition Data
Localytics Data Visualization Challenge
Netflix Prize
Space Apps Challenge
Telecom Italia Big Data Challenge [fixme]
TravisTorrent Dataset – MSR’2017 Mining Challenge
TunedIT – Data mining & machine learning data sets, algorithms, challenges [fixme]
Yelp Dataset Challenge [fixme]

 
Earth Science Datasets

38-Cloud (Cloud Detection) – Contains 38 Landsat 8 scene images and their […]
AQUASTAT – Global water resources and uses
BODC – marine data of ~22K vars
EOSDIS – NASA’s earth observing system data
Earth Models [fixme]
Global Wind Atlas – The Global Wind Atlas is a free, web-based […]
Integrated Marine Observing System (IMOS) – roughly 30TB of ocean measurements
Marinexplore – Open Oceanographic Data
Alabama Real-Time Coastal Observing System
National Estuarine Research Reserves System-Wide Monitoring Program – […]
Oil and Gas Authority Open Data – The dataset covers 12,500 offshore […]
Smithsonian Institution Global Volcano and Eruption Database
USGS Earthquake Archives

 
Economics Datasets

American Economic Association (AEA)
EconData from UMD
Economic Freedom of the World Data
Historical MacroEconomic Statistics
INFORUM – Interindustry Forecasting at the University of Maryland
DBnomics – the world’s economic database – Aggregates hundreds of […]
International Trade Statistics
Internet Product Code Database
Joint External Debt Data Hub
Jon Haveman International Trade Data Links
Long-Term Productivity Database – The Long-Term Productivity database was […]
OpenCorporates Database of Companies in the World
Our World in Data
SciencesPo World Trade Gravity Datasets [fixme]
The Atlas of Economic Complexity
The Center for International Data
The Observatory of Economic Complexity [fixme]
UN Commodity Trade Statistics
UN Human Development Reports

 
Education Datasets

College Scorecard Data
New York State Education Department Data – The New York State Education […]
Student Data from Free Code Camp

 
Energy Datasets

AMPds – The Almanac of Minutely Power dataset
BLUEd – Building-Level fully labeled Electricity Disaggregation dataset
COMBED
DBFC – Direct Borohydride Fuel Cell (DBFC) Dataset
DEL – Domestic Electrical Load study datasets for South Africa (1994 – 2014)
ECO – The ECO data set is a comprehensive data set for non-intrusive load […]
EIA
Global Power Plant Database – The Global Power Plant Database is a […]
HES – Household Electricity Study, UK
HFED
PEM1 – Proton Exchange Membrane (PEM) Fuel Cell Dataset
PLAID – The Plug Load Appliance Identification Dataset [fixme]
The Public Utility Data Liberation Project (PUDL) – PUDL makes US energy […]
REDD
SYND – A synthetic energy dataset for non-intrusive load monitoring – […]
Smart Meter Data Portal – The Smart Meter Data Portal is part of the […]
Tracebase
Ukraine Energy Centre Datasets
UK-DALE – UK Domestic Appliance-Level Electricity
WHITED
iAWE

 
Entertainment Datasets

Top Streamers on Twitch – This contains data of Top 1000 Streamers from […]

 
Finance Datasets

BIS Statistics – BIS statistics, compiled in cooperation with central […]
Blockmodo Coin Registry – A registry of JSON formatted information files […]
CBOE Futures Exchange
Complete FAANG Stock data – This data set contains all the stock data of […]
Google Finance
Google Trends
NASDAQ [fixme]
OANDA
OSU Financial data [fixme]
Quandl
St Louis Federal
Yahoo Finance

 
GIS Datasets

Awesome 3D Semantic City Models – Collection of open 3D semantic city and […]
ArcGIS Open Data portal
Cambridge, MA, US, GIS data on GitHub
Database of all continents, countries, States/Subdivisions/Provinces and […]
Factual Global Location Data
IEEE Geoscience and Remote Sensing Society DASE Website
Geo Maps – High Quality GeoJSON maps programmatically generated
Geo Spatial Data from ASU
Geo Wiki Project – Citizen-driven Environmental Monitoring
GeoFabrik – OSM data extracted to a variety of formats and areas
GeoNames Worldwide
Global Administrative Areas Database (GADM) – Geospatial data organized […]
Homeland Infrastructure Foundation-Level Data
Landsat 8 on AWS
List of all countries in all languages
National Weather Service GIS Data Portal
Natural Earth – vectors and rasters of the world [fixme]
OpenAddresses
OpenStreetMap (OSM)
Pleiades – Gazetteer and graph of ancient places
Reverse Geocoder using OSM data
Robin Wilson – Free GIS Datasets
TIGER/Line – U.S. boundaries and roads
TZ Timezones shapefile
TwoFishes – Foursquare’s coarse geocoder
UN Environmental Data
World boundaries from the U.S. Department of State
World countries in multiple formats

 
Government Datasets

Alberta, Province of Canada
Antwerp, Belgium
Argentina (non official) [fixme]
Datos Argentina – Portal de datos abiertos de la República Argentina. […]
Austin, TX, US
Australia (abs.gov.au)
Australia (data.gov.au)
Austria (data.gv.at)
Baton Rouge, LA, US
Beersheba, Israel – Open Data Portal (Smart7 OpenData)
Belgium
City of Berkeley Open Data
Brazil
Buenos Aires, Argentina
Calgary, AB, Canada
Cambridge, MA, US
Canada
Chicago
Chile
China [fixme]
Dallas Open Data
DataBC – data from the Province of British Columbia
Debt to the Penny – The Debt to the Penny dataset provides information […]
Denver Open Data
Durham, NC Open Data
Edmonton, AB, Canada
England LGInform
EuroStat
EveryPolitician – Ongoing project collating and sharing data on every […]
Federal Committee on Statistical Methodology (FCSM) (formerly FedStats)
Finland
France
Fredericton, NB, Canada
Gatineau, QC, Canada
Germany
Ghent, Belgium
Glasgow, Scotland, UK [fixme]
Greece
Guardian world governments
Halifax, NS, Canada
Helsinki Region, Finland
Hong Kong, China
Houston, TX, US [fixme]
Indian Government Data
Indonesian Data Portal
Iowa – Welcome to the State of Iowa’s data portal. Please explore data […]
Ireland’s Open Data Portal
Israel’s Open Data Portal
Istanbul Municipality Open Data Portal
Italy – Il Portale dati.gov.it è il catalogo nazionale dei metadati […]
Jail deaths in America – The U.S. government does not release jail by […]
Japan
Laval, QC, Canada
Lexington, KY
London Datastore, UK
London, ON, Canada [fixme]
Los Angeles Open Data
Luxembourg – Luxembourgish Open Data Portal
MassGIS, Massachusetts, U.S.
Metropolitan Transportation Commission (MTC), California, US
Mexico [fixme]
Mississauga, ON, Canada
Moldova
Moncton, NB, Canada
Montreal, QC, Canada
Mountain View, California, US (GIS)
NYC Open Data [fixme]
NYC betanyc
Netherlands
New York Department of Sanitation Monthly Tonnage – DSNY Monthly Tonnage […]
New Zealand
OECD
Oakland, California, US [fixme]
Oklahoma
Open Data for Africa
Open Government Data (OGD) Platform India
OpenDataSoft’s list of 1,600 open data
Oregon
Ottawa, ON, Canada
Palo Alto, California, US
OpenDataPhilly – OpenDataPhilly is a catalog of open data in the […]
Portland, Oregon
Portugal – Pordata organization
Puerto Rico Government
Quebec City, QC, Canada [fixme]
Quebec Province of Canada
Regina SK, Canada
Rio de Janeiro, Brazil
Romania
Russia [fixme]
San Diego, CA
San Antonio, TX – Community Information Now – CI:Now is a nonprofit […] [fixme]
San Francisco Data sets
San Jose, California, US
San Mateo County, California, US
Saskatchewan, Province of Canada
Seattle
Singapore Government Data
South Africa Trade Statistics
South Africa
State of Utah, US
Switzerland
Taiwan gov
Taiwan
Tel-Aviv Open Data
Texas Open Data
The World Bank [fixme]
Toronto, ON, Canada [fixme]
Tunisia [fixme]
U.K. Government Data
U.S. American Community Survey
U.S. CDC Public Health datasets
U.S. Census Bureau
U.S. Department of Housing and Urban Development (HUD)
U.S. Federal Government Agencies
U.S. Federal Government Data Catalog
U.S. Food and Drug Administration (FDA)
U.S. National Center for Education Statistics (NCES)
U.S. Open Government
UK 2011 Census Open Atlas Project
US Counties – This is a repository of various data, broken down by US […]
U.S. Patent and Trademark Office (USPTO) Bulk Data Products
Uganda Bureau of Statistics [fixme]
Ukraine
United Nations
Uruguay
Valley Transportation Authority (VTA), California, US
Vancouver, BC Open Data Catalog [fixme]
Victoria, BC, Canada
Vienna, Austria
Statistics from the General Statistics Office of Vietnam – Data in […] [fixme]
U.S. Congressional Research Service (CRS) Reports

 
Healthcare Datasets

AWS COVID-19 Datasets – We’re working with organizations who make […]
COVID-19 Case Surveillance Public Use Data – The COVID-19 case […]
2019 Novel Coronavirus COVID-19 Data Repository by Johns Hopkins CSSE – […]
Coronavirus (Covid-19) Data in the United States – The New York Times is […]
COVID-19 Reported Patient Impact and Hospital Capacity by Facility – The […]
Composition of Foods Raw, Processed, Prepared USDA National Nutrient Database for Standard […]
The COVID Tracking Project – The COVID Tracking Project collects and […]
EHDP Large Health Data Sets
GDC – GDC supports several cancer genome programs for CCG, TCGA, TARGET etc.
Gapminder World demographic databases
MeSH, the vocabulary thesaurus used for indexing articles for PubMed
MeDAL – A large medical text dataset curated for abbreviation […]
Medicare Coverage Database (MCD), U.S.
Medicare Data Engine of medicare.gov Data
Medicare Data File
Number of Ebola Cases and Deaths in Affected Countries (2014)
Open-ODS (structure of the UK NHS)
OpenPaymentsData, Healthcare financial relationship data
PhysioBank Databases – A large and growing archive of physiological data.
The Cancer Imaging Archive (TCIA)
The Cancer Genome Atlas project (TCGA)
World Health Organization Global Health Observatory
Yahoo Knowledge Graph COVID-19 Datasets – The Yahoo Knowledge Graph team […]
Informatics for Integrating Biology & the Bedside [fixme]

 
Image Processing Datasets

10k US Adult Faces Database
2GB of Photos of Cats
Audience Unfiltered faces for gender and age classification
Affective Image Classification
Animals with attributes
CADDY Underwater Stereo-Vision Dataset of divers’ hand gestures – […]
Cytology Dataset – CCAgT: Images of Cervical Cells with AgNOR Stain […]
Caltech Pedestrian Detection Benchmark
Chars74K dataset – Character Recognition in Natural Images (both English […]
Cube++ – 4890 raw 18-megapixel images, each containing a SpyderCube color […]
Danbooru Tagged Anime Illustration Dataset – A large-scale anime image […]
DukeMTMC Data Set – DukeMTMC aims to accelerate advances in multi-target […] [fixme]
ETH Entomological Collection (ETHEC) Fine Grained Butterfly (Lepidoptra) Images
Face Recognition Benchmark
Flickr: 32 Class Brand Logos [fixme]
GDXray – X-ray images for X-ray testing and Computer Vision
HumanEva Dataset – The HumanEva-I dataset contains 7 calibrated video […]
ImageNet (in WordNet hierarchy)
Indoor Scene Recognition
International Affective Picture System, UFL
KITTI Vision Benchmark Suite
Labeled Information Library of Alexandria – Biology and Conservation – […]
MNIST database of handwritten digits, near 1 million examples [fixme]
Multi-View Region of Interest Prediction Dataset for Autonomous Driving – […]
Massive Visual Memory Stimuli, MIT
Newspaper Navigator – This dataset consists of extracted visual content […]
Open Images From Google – Pictures with segmentation masks for 2.8 […]
RuFa – Contains images of text written in one of two Arabic fonts (Ruqaa […]
SUN database, MIT
SVIRO Synthetic Vehicle Interior Rear Seat Occupancy – 25.000 synthetic […]
Several Shape-from-Silhouette Datasets [fixme]
Stanford Dogs Dataset
The Action Similarity Labeling (ASLAN) Challenge
The Oxford-IIIT Pet Dataset
Violent-Flows – Crowd Violence / Non-violence Database and benchmark
Visual genome
YouTube Faces Database

 
Machine Learning Datasets

All-Age-Faces Dataset – Contains 13’322 Asian face images distributed […]
Audi Autonomous Driving Dataset – We have published the Audi Autonomous […]
Context-aware data sets from five domains
Delve Datasets for classification and regression
Discogs Monthly Data
Free Music Archive
IMDb Database
Iranis – A Large-scale Dataset of Farsi/Arabic License Plate Characters
Keel Repository for classification, regression and time series
Labeled Faces in the Wild (LFW)
Lending Club Loan Data
Machine Learning Data Set Repository [fixme]
Million Song Dataset
More Song Datasets
MovieLens Data Sets
New Yorker caption contest ratings
RDataMining – “R and Data Mining” ebook data
Registered Meteorites on Earth [fixme]
Restaurants Health Score Data in San Francisco
TikTok Dataset – More than 300 dance videos that capture a single person […]
UCI Machine Learning Repository
Yahoo! Ratings and Classification Data
YouTube-BoundingBoxes
Youtube 8m
eBay Online Auctions (2012)

 
Museums Datasets 

Canada Science and Technology Museums Corporation’s Open Data
Cooper-Hewitt’s Collection Database
Metropolitan Museum of Art Collection API
Minneapolis Institute of Arts metadata
Natural History Museum (London) Data Portal
Rijksmuseum Historical Art Collection
Tate Collection metadata
The Getty vocabularies

 
Natural Language Datasets

Automatic Keyphrase Extraction
The Big Bad NLP Database
Blizzard Challenge Speech – The speech + text data comes from […]
Blogger Corpus
CLiPS Stylometry Investigation Corpus [fixme]
ClueWeb09 FACC
ClueWeb12 FACC
DBpedia – 4.58M things with 583M facts
Dirty Words – With millions of images in our library and billions of […]
Flickr Personal Taxonomies
Freebase of people, places, and things [fixme]
German Political Speeches Corpus – Collection of political speeches from […]
Google Books Ngrams (2.2TB)
Google MC-AFP – Generated based on the public available Gigaword dataset […]
Google Web 5gram (1TB, 2006)
Gutenberg eBooks List [fixme]
Hansards text chunks of Canadian Parliament
LJ Speech – Speech dataset consisting of 13,100 short audio clips of a […]
M-AILabs Speech – The M-AILABS Speech Dataset is the first large dataset […] [fixme]
Microsoft MAchine Reading COmprehension Dataset (or MS MARCO)
Machine Comprehension Test (MCTest) of text from Microsoft Research
Machine Translation of European languages
Making Sense of Microposts 2013 – Concept Extraction [fixme]
Making Sense of Microposts 2016 – Named Entity rEcognition and Linking
Multi-Domain Sentiment Dataset (version 2.0)
Noisy speech database for training speech enhancement algorithms and TTS […] [fixme]
Open Multilingual Wordnet
POS/NER/Chunk annotated data
Personae Corpus [fixme]
SMS Spam Collection in English
SaudiNewsNet Collection of Saudi Newspaper Articles (Arabic, 30K articles)
Stanford Question Answering Dataset (SQuAD)
USENET postings corpus of 2005~2011
Universal Dependencies
Webhose – News/Blogs in multiple languages
Wikidata – Wikipedia databases
Wikipedia Links data – 40 Million Entities in Context
WordNet databases and tools
WorldTree Corpus of Explanation Graphs for Elementary Science Questions – […]

 
Neuroscience Datasets

Allen Institute Datasets
Brain Catalogue
Brainomics
CodeNeuro Datasets [fixme]
Collaborative Research in Computational Neuroscience (CRCNS)
FCP-INDI
Human Connectome Project
NDAR
NIMH Data Archive
NeuroData
NeuroMorpho – NeuroMorpho.Org is a centrally curated inventory of […]
Neuroelectro
OASIS
OpenNEURO
OpenfMRI
Study Forrest

 
Physics Datasets

CERN Open Data Portal
Crystallography Open Database
IceCube – South Pole Neutrino Observatory
Ligo Open Science Center (LOSC) – Gravitational wave data from the LIGO […]
NASA Exoplanet Archive
NSSDC (NASA) data of 550 space spacecraft
Sloan Digital Sky Survey (SDSS) – Mapping the Universe

 
Prostate Cancer Datasets

EOPC-DE-Early-Onset-Prostate-Cancer-Germany – Early Onset Prostate Cancer […]
GENIE – Data from the Genomics Evidence Neoplasia Information Exchange […]
Genomic-Hallmarks-Prostate-Adenocarcinoma-CPC-GENE – Comprehensive […]
MSK-IMPACT-Clinical-Sequencing-Cohort-MSKCC-Prostate-Cancer – Targeted […]
Metastatic-Prostate-Adenocarcinoma-MCTP – Comprehensive profiling of 61 […]
Metastatic-Prostate-Cancer-SU2CPCF-Dream-Team – Comprehensive analysis of […]
NPCR-2001-2015 – Database from CDC’s National Program of Cancer […]
NPCR-2005-2015 – Database from CDC’s National Program of Cancer […]
NaF-Prostate – NaF Prostate is a collection of F-18 NaF positron emission […]
Neuroendocrine-Prostate-Cancer – Whole exome and RNA Seq data of […]
PLCO-Prostate-Diagnostic-Procedures – The Prostate Diagnostic Procedures […]
PLCO-Prostate-Medical-Complications – The Prostate Medical Complications […]
PLCO-Prostate-Screening-Abnormalities – The Prostate Screening […]
PLCO-Prostate-Screening – The Prostate Screening dataset (177,315 […]
PLCO-Prostate-Treatments – The Prostate Treatments dataset (13,409 […]
PLCO-Prostate – The Prostate dataset is a comprehensive dataset that […]
PRAD-CA-Prostate-Adenocarcinoma-Canada – Prostate Adenocarcinoma – […]
PRAD-FR-Prostate-Adenocarcinoma-France – Prostate Adenocarcinoma – […]
PRAD-UK-Prostate-Adenocarcinoma-United-Kingdom – Prostate Adenocarcinoma […]
PROSTATEx-Challenge – Retrospective set of prostate MR studies. All […]
Prostate-3T – The Prostate-3T project provided imaging data to TCIA as […]
Prostate-Adenocarcinoma-Broad-Cornell-2012 – Comprehensive profiling of […]
Prostate-Adenocarcinoma-Broad-Cornell-2013 – Comprehensive profiling of […]
Prostate-Adenocarcinoma-CNA-study-MSKCC – Copy-number profiling of 103 […]
Prostate-Adenocarcinoma-Fred-Hutchinson-CRC – Comprehensive profiling of […]
Prostate Adenocarcinoma (MSKCC/DFCI) – Whole Exome Sequencing of 1013 […]
Prostate-Adenocarcinoma-MSKCC – MSKCC Prostate Oncogenome Project. 181 […]
Prostate-Adenocarcinoma-Organoids-MSKCC – Exome profiling of prostate […]
Prostate-Adenocarcinoma-Sun-Lab – Whole-genome and Transcriptome […]
Prostate-Adenocarcinoma-TCGA-PanCancer-Atlas – Comprehensive TCGA […]
Prostate-Adenocarcinoma-TCGA – Integrated profiling of 333 primary […]
Prostate-Diagnosis – PCa T1- and T2-weighted magnetic resonance images […]
Prostate-Fused-MRI-Pathology – The Prostate Fused-MRI-Pathology […]
Prostate-MRI – The Prostate-MRI collection of prostate Magnetic Resonance […]
Prostate-R – The R package ‘ElemStatLearn’ contains a prostate cancer […]
QIN-PROSTATE-Repeatability – The QIN-PROSTATE-Repeatability dataset is a […]
QIN-PROSTATE – The QIN PROSTATE collection of the Quantitative Imaging […]
SEER-YR1973_2015.SEER9 – The SEER November 2017 Research Data files from […]
SEER-YR1992_2015.SJ_LA_RG_AK – The SEER November 2017 Research Data files […]
SEER-YR2000_2015.CA_KY_LO_NJ_GA – The SEER November 2017 Research Data […]
SEER-YR2000_2015.CA_KY_LO_NJ_GA – The July – December 2005 diagnoses for […]
TCGA-PRAD-US – TCGA Prostate Adenocarcinoma (499 samples).

 
Psychology and Cognition Datasets

OSU Cognitive Modeling Repository Datasets [fixme]

 
Public Domains Datasets

Ably Open Realtime Data
Amazon
Archive.org Datasets
Archive-it from Internet Archive
CMU JASA data archive
CMU StatLab collections
Data.World
Data360 [fixme]
Enigma Public
Google
Grand Comics Database – The Grand Comics Database (GCD) is a nonprofit, […]
Infochimps [fixme]
KDNuggets Data Collections
Microsoft Azure Data Market Free DataSets [fixme]
Microsoft Data Science for Research
Microsoft Research Open Data
Open Library Data Dumps
Reddit Datasets
RevolutionAnalytics Collection [fixme]
Sample R data sets
StatSci.org
Stats4Stem R data sets (archived)
The Washington Post List
UCLA SOCR data collection
UFO Reports
Wikileaks 911 pager intercepts
Yahoo Webscope

 
Search Engines Datasets

Academic Torrents of data sharing from UMB
Datahub.io
Domains Project – Sorted list of Internet domains
Harvard Dataverse Network of scientific data
ICPSR (UMICH)
Institute of Education Sciences
National Technical Reports Library
Open Data Certificates (beta)
OpenDataNetwork – A search engine of all Socrata powered data portals
Statista.com – statistics and Studies
Zenodo – An open dependable home for the long-tail of science

 
Social Networks Datasets

2021 Portuguese Elections Twitter Dataset – 57M+ tweets, 1M+ users – This […]
72 hours #gamergate Twitter Scrape
CMU Enron Email of 150 users
Cheng-Caverlee-Lee September 2009 – January 2010 Twitter Scrape
China Biographical Database – The China Biographical Database is a freely […]
A Twitter Dataset of 40+ million tweets related to COVID-19 – Due to the […]
43k+ Donald Trump Twitter Screenshots – This archive contains screenshots […]
EDRM Enron EMail of 151 users, hosted on S3
Facebook Data Scrape (2005)
Facebook Social Connectedness Index – We use an anonymized snapshot of […]
Facebook Social Networks from LAW (since 2007)
Foursquare from UMN/Sarwat (2013)
GitHub Collaboration Archive
Google Scholar citation relations
High-Resolution Contact Networks from Wearable Sensors
Indie Map: social graph and crawl of top IndieWeb sites
Mobile Social Networks from UMASS
Network Twitter Data
Reddit Comments
Skytrax’ Air Travel Reviews Dataset
Social Twitter Data
SourceForge.net Research Data
Twitch Top Streamer’s Data
Twitter Data for Online Reputation Management
Twitter Data for Sentiment Analysis
Twitter Graph of entire Twitter site
Twitter Scrape Calufa May 2011 [fixme]
UNIMI/LAW Social Network Datasets
United States Congress Twitter Data – Daily datasets with tweets of 1100+ […]
Yahoo! Graph and Social Data
Youtube Video Social Graph in 2007,2008

 
Social Sciences Datasets

ACLED (Armed Conflict Location & Event Data Project)
Authoritarian Ruling Elites Database – The Authoritarian Ruling Elites […]
Canadian Legal Information Institute
Center for Systemic Peace Datasets – Conflict Trends, Polities, State Fragility, etc [fixme]
Correlates of War Project
Cryptome Conspiracy Theory Items
Datacards [fixme]
European Social Survey
FBI Hate Crime 2013 – aggregated data
Fragile States Index [fixme]
GDELT Global Events Database
General Social Survey (GSS) since 1972
German Social Survey
Global Religious Futures Project
Gun Violence Data – A comprehensive, accessible database that contains […]
Humanitarian Data Exchange
INFORM Index for Risk Management
Institute for Demographic Studies
International Networks Archive
International Social Survey Program ISSP
International Studies Compendium Project
James McGuire Cross National Data
MIT Reality Mining Dataset
MacroData Guide by Norsk samfunnsvitenskapelig datatjeneste
Mass Mobilization Data Project – The Mass Mobilization (MM) data are an […]
Microsoft Academic Knowledge Graph – The Microsoft Academic Knowledge […]
Minnesota Population Center
Notre Dame Global Adaptation Index (ND-GAIN)
Open Crime and Policing Data in England, Wales and Northern Ireland
OpenSanctions – A global database of persons and companies of political, […]
Paul Hensel General International Data Page
PewResearch Internet Survey Project
PewResearch Society Data Collection
Political Polarity Data [fixme]
StackExchange Data Explorer
Terrorism Research and Analysis Consortium
Texas Inmates Executed Since 1984
Titanic Survival Data Set
UCB’s Archive of Social Science Data (D-Lab) [fixme]
UCLA Social Sciences Data Archive
UN Civil Society Database
UPJOHN for Labor Employment Research
Universities Worldwide
Uppsala Conflict Data Program
World Bank Open Data
World Inequality Database – The World Inequality Database (WID.world) […]
WorldPop project – Worldwide human population distributions

 
Software Datasets

FLOSSmole data about free, libre, and open source software development
GHTorrent – Scalable, queryable, offline mirror of data offered through […]
Libraries.io Open Source Repository and Dependency Metadata
Public Git Archive – a Big Code dataset for all – dataset of 182,014 top- […]
Code duplicates – 2k Java file and 600 Java function pairs labeled as […]
Commit messages – 1.3 billion GitHub commit messages till March 2019
Pull Request review comments – 25.3 million GitHub PR review comments […]
Source Code Identifiers – 41.7 million distinct splittable identifiers […]

 
Sports Datasets

American Ninja Warrior Obstacles – Contains every obstacle in the history […]
Betfair Historical Exchange Data
Cricsheet Matches (cricket)
Equity in Athletics – The Equity in Athletics Data Analysis Cutting Tool […]
Ergast Formula 1, from 1950 up to date (API)
Football/Soccer resources (data and APIs)
Lahman’s Baseball Database
NFL play-by-play data – NFL play-by-play data sourced from: […]
Pinhooker: Thoroughbred Bloodstock Sale Data
Pro Kabadi season 1 to 7 – Pro Kabadi League is a professional-level […]
Retrosheet Baseball Statistics
Tennis database of rankings, results, and stats for ATP
Tennis database of rankings, results, and stats for WTA
USA Soccer Teams and Locations – USA soccer teams and locations. MLS, […]

 
Time Series Datasets

3W dataset – To the best of its authors’ knowledge, this is the first […]
Databanks International Cross National Time Series Data Archive
Hard Drive Failure Rates
Heart Rate Time Series from MIT
Time Series Data Library (TSDL) from MU
Turing Change Point Dataset – Contains 42 annotated time series collected […]
UC Riverside Time Series Dataset

 
Transportation

Airlines OD Data 1987-2008
Ford GoBike Data (formerly Bay Area Bike Share Data) [fixme]
Bike Share Systems (BSS) collection
Dutch Traffic Information
GeoLife GPS Trajectory from Microsoft Research
German train system by Deutsche Bahn
Hubway Million Rides in MA [fixme]
Montreal BIXI Bike Share
NYC Taxi Trip Data 2009-
NYC Taxi Trip Data 2013 (FOIA/FOILed)
NYC Uber trip data April 2014 to September 2014
Open Traffic collection
OpenFlights – airport, airline and route data
Philadelphia Bike Share Stations (JSON)
Plane Crash Database, since 1920
RITA Airline On-Time Performance data [fixme]
RITA/BTS transport data collection (TranStat) [fixme]
Renfe (Spanish National Railway Network) dataset
Toronto Bike Share Stations (JSON and GBFS files)
Transport for London (TFL)
Travel Tracker Survey (TTS) for Chicago [fixme]
U.S. Bureau of Transportation Statistics (BTS)
U.S. Domestic Flights 1990 to 2009
U.S. Freight Analysis Framework since 2007

 
eSports Datasets

CS:GO Competitive Matchmaking Data – In this data set we have data about […]
FIFA-2021 Complete Player Dataset
OpenDota data dump

 
Complementary Collections

Data Packaged Core Datasets
Database of Scientific Code Contributions
A growing collection of public datasets: CoolDatasets.
DataWrangling: Some Datasets Available on the Web
Inside-r: Finding Data on the Internet
OpenDataMonitor: An overview of available open data resources in Europe
Quora: Where can I find large datasets open to the public?
RS.io: 100+ Interesting Data Sets for Statistics
StaTrek: Leveraging open data to understand urban lives
CV Papers: CV Datasets on the web
CVonline: Image Databases

 
Categorized list of public datasets: Sindre Sorhus /awesome List
 
Platforms

Node.js – Async non-blocking event-driven JavaScript runtime built on Chrome’s V8 JavaScript engine.

Cross-Platform – Writing cross-platform code on Node.js.


Frontend Development
iOS – Mobile operating system for Apple phones and tablets.
Android – Mobile operating system developed by Google.
IoT & Hybrid Apps
Electron – Cross-platform native desktop apps using JavaScript/HTML/CSS.
Cordova – JavaScript API for hybrid apps.
React Native – JavaScript framework for writing natively rendering mobile apps for iOS and Android.
Xamarin – Mobile app development IDE, testing, and distribution.
Linux

Containers
eBPF – Virtual machine that allows you to write more efficient and powerful tracing and monitoring for Linux systems.
Arch-based Projects – Linux distributions and projects based on Arch Linux.


macOS – Operating system for Apple’s Mac computers.

Command-Line
Screensavers
Apps
Open Source Apps


watchOS – Operating system for the Apple Watch.
JVM
Salesforce
Amazon Web Services
Windows
IPFS – P2P hypermedia protocol.
Fuse – Mobile development tools.
Heroku – Cloud platform as a service.
Raspberry Pi – Credit card-sized computer aimed at teaching kids programming, but capable of a lot more.
Qt – Cross-platform GUI app framework.
WebExtensions – Cross-browser extension system.
RubyMotion – Write cross-platform native apps for iOS, Android, macOS, tvOS, and watchOS in Ruby.
Smart TV – Create apps for different TV platforms.
GNOME – Simple and distraction-free desktop environment for Linux.
KDE – A free software community dedicated to creating an open and user-friendly computing experience.
.NET

Core
Roslyn – Open-source compilers and code analysis APIs for C# and VB.NET languages.


Amazon Alexa – Virtual home assistant.
DigitalOcean – Cloud computing platform designed for developers.
Flutter – Google’s mobile SDK for building native iOS and Android apps from a single codebase written in Dart.
Home Assistant – Open source home automation that puts local control and privacy first.
IBM Cloud – Cloud platform for developers and companies.
Firebase – App development platform built on Google Cloud Platform.
Robot Operating System 2.0 – Set of software libraries and tools that help you build robot apps.
Adafruit IO – Visualize and store data from any device.
Cloudflare – CDN, DNS, DDoS protection, and security for your site.
Actions on Google – Developer platform for Google Assistant.
ESP – Low-cost microcontrollers with WiFi and broad IoT applications.
Deno – A secure runtime for JavaScript and TypeScript that uses V8 and is built in Rust.
DOS – Operating system for x86-based personal computers that was popular during the 1980s and early 1990s.
Nix – Package manager for Linux and other Unix systems that makes package management reliable and reproducible.

 
Programming Languages

JavaScript

Promises
Standard Style – Style guide and linter.
Must Watch Talks
Tips
Network Layer
Micro npm Packages
Mad Science npm Packages – Impossible sounding projects that exist.
Maintenance Modules – For npm packages.
npm – Package manager.
AVA – Test runner.
ESLint – Linter.
Functional Programming
Observables
npm scripts – Task runner.
30 Seconds of Code – Code snippets you can understand in 30 seconds.
Ponyfills – Like polyfills but without overriding native APIs.


Swift – Apple’s compiled programming language that is secure, modern, programmer-friendly, and fast.

Education
Playgrounds


Python – General-purpose programming language designed for readability.

Asyncio – Asynchronous I/O in Python 3.
Scientific Audio – Scientific research in audio/music.
CircuitPython – A version of Python for microcontrollers.
Data Science – Data analysis and machine learning.
Typing – Optional static typing for Python.
MicroPython – A lean and efficient implementation of Python 3 for microcontrollers.


Rust
Haskell
PureScript
Go
Scala

Scala Native – Optimizing ahead-of-time compiler for Scala based on LLVM.


Ruby
Clojure
ClojureScript
Elixir
Elm
Erlang
Julia – High-level dynamic programming language designed to address the needs of high-performance numerical analysis and computational science.
Lua
C
C/C++ – General-purpose language with a bias toward system programming and embedded, resource-constrained software.
R – Functional programming language and environment for statistical computing and graphics.

Learning


D
Common Lisp – Powerful dynamic multiparadigm language that facilitates iterative and interactive development.

Learning


Perl
Groovy
Dart
Java – Popular secure object-oriented language designed for flexibility to “write once, run anywhere”.

RxJava


Kotlin
OCaml
ColdFusion
Fortran
PHP – Server-side scripting language.

Composer – Package manager.


Pascal
AutoHotkey
AutoIt
Crystal
Frege – Haskell for the JVM.
CMake – Build, test, and package software.
ActionScript 3 – Object-oriented language targeting Adobe AIR.
Eta – Functional programming language for the JVM.
Idris – General purpose pure functional programming language with dependent types influenced by Haskell and ML.
Ada/SPARK – Modern programming language designed for large, long-lived apps where reliability and efficiency are essential.
Q# – Domain-specific programming language used for expressing quantum algorithms.
Imba – Programming language inspired by Ruby and Python and compiles to performant JavaScript.
Vala – Programming language designed to take full advantage of the GLib and GNOME ecosystems, while preserving the speed of C code.
Coq – Formal language and environment for programming and specification which facilitates interactive development of machine-checked proofs.
V – Simple, fast, safe, compiled language for developing maintainable software.

 
Front-End Development

ES6 Tools
Web Performance Optimization
Web Tools
CSS – Style sheet language that specifies how HTML elements are displayed on screen.

Critical-Path Tools
Scalability
Must-Watch Talks
Protips
Frameworks


React – App framework.

Relay – Framework for building data-driven React apps.
React Hooks – A new feature that lets you use state and other React features without writing a class.


Web Components
Polymer – JavaScript library to develop Web Components.
Angular – App framework.
Backbone – App framework.
HTML5 – Markup language used for websites & web apps.
SVG – XML-based vector image format.
Canvas
KnockoutJS – JavaScript library.
Dojo Toolkit – JavaScript toolkit.
Inspiration
Ember – App framework.
Android UI
iOS UI
Meteor
BEM
Flexbox
Web Typography
Web Accessibility
Material Design
D3 – Library for producing dynamic, interactive data visualizations.
Emails
jQuery – Easy to use JavaScript library for DOM manipulation.

Tips


Web Audio
Offline-First
Static Website Services
Cycle.js – Functional and reactive JavaScript framework.
Text Editing
Motion UI Design
Vue.js – App framework.
Marionette.js – App framework.
Aurelia – App framework.
Charting
Ionic Framework 2
Chrome DevTools
PostCSS – CSS tool.
Draft.js – Rich text editor framework for React.
Service Workers
Progressive Web Apps
choo – App framework.
Redux – State container for JavaScript apps.
webpack – Module bundler.
Browserify – Module bundler.
Sass – CSS preprocessor.
Ant Design – Enterprise-class UI design language.
Less – CSS preprocessor.
WebGL – JavaScript API for rendering 3D graphics.
Preact – App framework.
Progressive Enhancement
Next.js – Framework for server-rendered React apps.
lit-html – HTML templating library for JavaScript.
JAMstack – Modern web development architecture based on client-side JavaScript, reusable APIs, and prebuilt markup.
WordPress-Gatsby – Web development technology stack with WordPress as a back end and Gatsby as a front end.
Mobile Web Development – Creating a great mobile web experience.
Storybook – Development environment for UI components.
Blazor – .NET web framework using C#/Razor and HTML that runs in the browser with WebAssembly.
PageSpeed Metrics – Metrics to help understand page speed and user experience.
Tailwind CSS – Utility-first CSS framework for rapid UI development.
Seed – Rust framework for creating web apps running in WebAssembly.
Web Performance Budget – Techniques to ensure certain performance metrics for a website.
Web Animation – Animations in the browser with JavaScript, CSS, SVG, etc.
Yew – Rust framework inspired by Elm and React for creating multi-threaded frontend web apps with WebAssembly.
Material-UI – Material Design React components for faster and easier web development.
Building Blocks for Web Apps – Standalone features to be integrated into web apps.
Svelte – App framework.
Design systems – Collection of reusable components, guided by rules that ensure consistency and speed.

 
Back-End Development

Flask – Python framework.
Docker
Vagrant – Automation virtual machine environment.
Pyramid – Python framework.
Play1 Framework
CakePHP – PHP framework.
Symfony – PHP framework.

Education


Laravel – PHP framework.

Education
TALL Stack – Full-stack development solution featuring libraries built by the Laravel community.


Rails – Web app framework for Ruby.

Gems – Packages.


Phalcon – PHP framework.
Useful .htaccess Snippets
nginx – Web server.
Dropwizard – Java framework.
Kubernetes – Open-source platform that automates Linux container operations.
Lumen – PHP micro-framework.
Serverless Framework – Serverless computing and serverless architectures.
Apache Wicket – Java web app framework.
Vert.x – Toolkit for building reactive apps on the JVM.
Terraform – Tool for building, changing, and versioning infrastructure.
Vapor – Server-side development in Swift.
Dash – Python web app framework.
FastAPI – Python web app framework.
CDK – Open-source software development framework for defining cloud infrastructure in code.
IAM – User accounts, authentication and authorization.
Chalice – Python framework for serverless app development on AWS Lambda.

 
Computer Science

University Courses
Data Science

Tutorials


Machine Learning

Tutorials
ML with Ruby – Learning, implementing, and applying Machine Learning using Ruby.
Core ML Models – Models for Apple’s machine learning framework.
H3O – Open source distributed machine learning platform written in Java with APIs in R, Python, and Scala.
Software Engineering for Machine Learning – From experiment to production-level machine learning.
AI in Finance – Solving problems in finance with machine learning.
JAX – Automatic differentiation and XLA compilation brought together for high-performance machine learning research.


Speech and Natural Language Processing

Spanish
NLP with Ruby
Question Answering – The science of asking and answering in natural language with a machine.
Natural Language Generation – Generation of text used in data to text, conversational agents, and narrative generation applications.


Linguistics
Cryptography

Papers – Theory basics for using cryptography by non-cryptographers.


Computer Vision
Deep Learning – Neural networks.

TensorFlow – Library for machine intelligence.
TensorFlow.js – WebGL-accelerated machine learning JavaScript library for training and deploying models.
TensorFlow Lite – Framework that optimizes TensorFlow models for on-device machine learning.
Papers – The most cited deep learning papers.
Education


Deep Vision
Open Source Society University
Functional Programming
Empirical Software Engineering – Evidence-based research on software systems.
Static Analysis & Code Quality
Information Retrieval – Learn to develop your own search engine.
Quantum Computing – Computing which utilizes quantum mechanics and qubits on quantum computers.

 
Big Data

Big Data
Public Datasets
Hadoop – Framework for distributed storage and processing of very large data sets.
Data Engineering
Streaming
Apache Spark – Unified engine for large-scale data processing.
Qlik – Business intelligence platform for data visualization, analytics, and reporting apps.
Splunk – Platform for searching, monitoring, and analyzing structured and unstructured machine-generated big data in real-time.

 
Theory

Papers We Love
Talks
Algorithms

Education – Learning and practicing.


Algorithm Visualizations
Artificial Intelligence
Search Engine Optimization
Competitive Programming
Math
Recursion Schemes – Traversing nested data structures.

 
Books

Free Programming Books
Go Books
R Books
Mind Expanding Books
Book Authoring
Elixir Books

 
Editors

Sublime Text
Vim
Emacs
Atom – Open-source and hackable text editor.
Visual Studio Code – Cross-platform open-source text editor.

 
Gaming

Game Development
Game Talks
Godot – Game engine.
Open Source Games
Unity – Game engine.
Chess
LÖVE – Game engine.
PICO-8 – Fantasy console.
Game Boy Development
Construct 2 – Game engine.
Gideros – Game engine.
Minecraft – Sandbox video game.
Game Datasets – Materials and datasets for Artificial Intelligence in games.
Haxe Game Development – A high-level strongly typed programming language used to produce cross-platform native code.
libGDX – Java game framework.
PlayCanvas – Game engine.
Game Remakes – Actively maintained open-source game remakes.
Flame – Game engine for Flutter.
Discord Communities – Chat with friends and communities.
CHIP-8 – Virtual computer game machine from the 70s.
Games of Coding – Learn a programming language by making games.

 
Development Environment

Quick Look Plugins – For macOS.
Dev Env
Dotfiles
Shell
Fish – User-friendly shell.
Command-Line Apps
ZSH Plugins
GitHub – Hosting service for Git repositories.

Browser Extensions
Cheat Sheet
Pinned Gists – Dynamic pinned gists for your GitHub profile.


Git Cheat Sheet & Git Flow
Git Tips
Git Add-ons – Enhance the git CLI.
Git Hooks – Scripts for automating tasks during git workflows.
SSH
FOSS for Developers
Hyper – Cross-platform terminal app built on web technologies.
PowerShell – Cross-platform object-oriented shell.
Alfred Workflows – Productivity app for macOS.
Terminals Are Sexy
GitHub Actions – Create tasks to automate your workflow and share them with others on GitHub.

 
Entertainment

Science Fiction – Scifi.
Fantasy
Podcasts
Email Newsletters
IT Quotes

 
Databases

Database
MySQL
SQLAlchemy
InfluxDB
Neo4j
MongoDB – NoSQL database.
RethinkDB
TinkerPop – Graph computing framework.
PostgreSQL – Object-relational database.
CouchDB – Document-oriented NoSQL database.
HBase – Distributed, scalable, big data store.
NoSQL Guides – Help on using non-relational, distributed, open-source, and horizontally scalable databases.
Contexture – Abstracts queries/filters and results/aggregations from different backing data stores like ElasticSearch and MongoDB.
Database Tools – Everything that makes working with databases easier.
Grakn – Logical database to organize large and complex networks of data as one body of knowledge.

 
Media

Creative Commons Media
Fonts
Codeface – Text editor fonts.
Stock Resources
GIF – Image format known for animated images.
Music
Open Source Documents
Audio Visualization
Broadcasting
Pixel Art – Pixel-level digital art.
FFmpeg – Cross-platform solution to record, convert and stream audio and video.
Icons – Downloadable SVG/PNG/font icon projects.
Audiovisual – Lighting, audio and video in professional environments.

 
Learn

CLI Workshoppers – Interactive tutorials.
Learn to Program
Speaking
Tech Videos
Dive into Machine Learning
Computer History
Programming for Kids
Educational Games – Learn while playing.
JavaScript Learning
CSS Learning – Mainly about CSS – the language and the modules.
Product Management – Learn how to be a better product manager.
Roadmaps – Gives you a clear route to improve your knowledge and skills.
YouTubers – Watch video tutorials from YouTubers that teach you about technology.

 
Security

Application Security
Security
CTF – Capture The Flag.
Malware Analysis
Android Security
Hacking
Honeypots – Deception trap, designed to entice an attacker into attempting to compromise the information systems in an organization.
Incident Response
Vehicle Security and Car Hacking
Web Security – Security of web apps & services.
Lockpicking – The art of unlocking a lock by manipulating its components without the key.
Cybersecurity Blue Team – Groups of individuals who identify security flaws in information technology systems.
Fuzzing – Automated software testing technique that involves feeding pseudo-randomly generated input data.
Embedded and IoT Security
GDPR – Regulation on data protection and privacy for all individuals within EU.
DevSecOps – Integration of security practices into DevOps.

 
Content Management Systems

Umbraco
Refinery CMS – Ruby on Rails CMS.
Wagtail – Django CMS focused on flexibility and user experience.
Textpattern – Lightweight PHP-based CMS.
Drupal – Extensible PHP-based CMS.
Craft CMS – Content-first CMS.
Sitecore – .NET digital marketing platform that combines CMS with tools for managing multiple websites.
Silverstripe CMS – PHP MVC framework that serves as a classic or headless CMS.

 
Hardware

Robotics
Internet of Things
Electronics – For electronic engineers and hobbyists.
Bluetooth Beacons
Electric Guitar Specifications – Checklist for building your own electric guitar.
Plotters – Computer-controlled drawing machines and other visual art robots.
Robotic Tooling – Free and open tools for professional robotic development.
LIDAR – Sensor for measuring distances by illuminating the target with laser light.

 
Business

Open Companies
Places to Post Your Startup
OKR Methodology – Goal setting & communication best practices.
Leading and Managing – Leading people and being a manager in a technology company/environment.
Indie – Independent developer businesses.
Tools of the Trade – Tools used by companies on Hacker News.
Clean Tech – Fighting climate change with technology.
Wardley Maps – Provides high situational awareness to help improve strategic planning and decision making.
Social Enterprise – Building an organization primarily focused on social impact that is at least partially self-funded.
Engineering Team Management – How to transition from software development to engineering management.
Developer-First Products – Products that target developers as the user.

 
Work

Slack – Team collaboration.

Communities


Remote Jobs
Productivity
Niche Job Boards
Programming Interviews
Code Review – Reviewing code.
Creative Technology – Businesses & groups that specialize in combining computing, design, art, and user experience.

 
Networking

Software-Defined Networking
Network Analysis
PCAPTools
Real-Time Communications – Network protocols for near simultaneous exchange of media and data.

 
Decentralized Systems

Bitcoin – Bitcoin services and tools for software developers.
Ripple – Open source distributed settlement network.
Non-Financial Blockchain – Non-financial blockchain applications.
Mastodon – Open source decentralized microblogging network.
Ethereum – Distributed computing platform for smart contract development.
Blockchain AI – Blockchain projects for artificial intelligence and machine learning.
EOSIO – A decentralized operating system supporting industrial-scale apps.
Corda – Open source blockchain platform designed for business.
Waves – Open source blockchain platform and development toolset for Web 3.0 apps and decentralized solutions.
Substrate – Framework for writing scalable, upgradeable blockchains in Rust.

 
Higher Education

Computational Neuroscience – A multidisciplinary science which uses computational approaches to study the nervous system.
Digital History – Computer-aided scientific investigation of history.
Scientific Writing – Distraction-free scientific writing with Markdown, reStructuredText and Jupyter notebooks.

 
Events

Creative Tech Events – Events around the globe for creative coding, tech, design, music, arts and cool stuff.
Events in Italy – Tech-related events in Italy.
Events in the Netherlands – Tech-related events in the Netherlands.

 
Testing

Testing – Software testing.
Visual Regression Testing – Ensures changes did not break the functionality or style.
Selenium – Open-source browser automation framework and ecosystem.
Appium – Test automation tool for apps.
TAP – Test Anything Protocol.
JMeter – Load testing and performance measurement tool.
k6 – Open-source, developer-centric performance monitoring and load testing solution.
Playwright – Node.js library to automate Chromium, Firefox and WebKit with a single API.
Quality Assurance Roadmap – How to start & build a career in software testing.

 
Miscellaneous

JSON – Text based data interchange format.

GeoJSON
Datasets


CSV – A text file format that stores tabular data and uses a comma to separate values.
Discounts for Student Developers
Radio
Awesome – Recursion illustrated.
Analytics
REST
Continuous Integration and Continuous Delivery
Services Engineering
Free for Developers
Answers – Stack Overflow, Quora, etc.
Sketch – Design app for macOS.
Boilerplate Projects
Readme
Design and Development Guides
Software Engineering Blogs
Self Hosted
FOSS Production Apps
Gulp – Task runner.
AMA – Ask Me Anything.

Answers


Open Source Photography
OpenGL – Cross-platform API for rendering 2D and 3D graphics.
GraphQL
Transit
Research Tools
Data Visualization
Social Media Share Links
Microservices
Unicode – Unicode standards, quirks, packages and resources.

Code Points


Beginner-Friendly Projects
Katas
Tools for Activism
Citizen Science – For community-based and non-institutional scientists.
MQTT – “Internet of Things” connectivity protocol.
Hacking Spots
For Girls
Vorpal – Node.js CLI framework.
Vulkan – Low-overhead, cross-platform 3D graphics and compute API.
LaTeX – Typesetting language.
Economics – An economist’s starter kit.
Funny Markov Chains
Bioinformatics
Cheminformatics – Informatics techniques applied to problems in chemistry.
Colorful – Choose your next color scheme.
Steam – Digital distribution platform.
Bots – Building bots.
Site Reliability Engineering
Empathy in Engineering – Building and promoting more compassionate engineering cultures.
DTrace – Dynamic tracing framework.
Userscripts – Enhance your browsing experience.
Pokémon – Pokémon and Pokémon GO.
ChatOps – Managing technical and business operations through a chat.
Falsehood – Falsehoods programmers believe in.
Domain-Driven Design – Software development approach for complex needs by connecting the implementation to an evolving model.
Quantified Self – Self-tracking through technology.
SaltStack – Python-based config management system.
Web Design – For digital designers.
Creative Coding – Programming something expressive instead of something functional.
No-Login Web Apps – Web apps that work without login.
Free Software – Free as in freedom.
Framer – Prototyping interactive UI designs.
Markdown – Markup language.
Dev Fun – Funny developer projects.
Healthcare – Open source healthcare software for facilities, providers, developers, policy experts, and researchers.
Magento 2 – Open Source eCommerce built with PHP.
TikZ – Graph drawing packages for TeX/LaTeX/ConTeXt.
Neuroscience – Study of the nervous system and brain.
Ad-Free – Ad-free alternatives.
Esolangs – Programming languages designed for experimentation or as jokes rather than actual use.
Prometheus – Open-source monitoring system.
Homematic – Smart home devices.
Ledger – Double-entry accounting on the command-line.
Web Monetization – A free open web standard service that allows you to send money directly in your browser.
Uncopyright – Public domain works.
Crypto Currency Tools & Algorithms – Digital currency where encryption is used to regulate the generation of units and verify transfers.
Diversity – Creating a more inclusive and diverse tech community.
Open Source Supporters – Companies that offer their tools and services for free to open source projects.
Design Principles – Create better and more consistent designs and experiences.
Theravada – Teachings from the Theravada Buddhist tradition.
inspectIT – Open source Java app performance management tool.
Open Source Maintainers – The experience of being an open source maintainer.
Calculators – Calculators for every platform.
Captcha – A type of challenge–response test used in computing to determine whether or not the user is human.
Jupyter – Create and share documents that contain code, equations, visualizations and narrative text.
FIRST Robotics Competition – International high school robotics championship.
Humane Technology – Open source projects that help improve society.
Speakers – Conference and meetup speakers in the programming and design community.
Board Games – Table-top gaming fun for all.
Software Patreons – Fund individual programmers or the development of open source projects.
Parasite – Parasites and host-pathogen interactions.
Food – Food-related projects on GitHub.
Mental Health – Mental health awareness and self-care in the software industry.
Bitcoin Payment Processors – Start accepting Bitcoin.
Scientific Computing – Solving complex scientific problems using computers.
Amazon Sellers
Agriculture – Open source technology for farming and gardening.
Product Design – Design a product from the initial concept to production.
Prisma – Turn your database into a GraphQL API.
Software Architecture – The discipline of designing and building software.
Connectivity Data and Reports – Better understand who has access to telecommunication and internet infrastructure and on what terms.
Stacks – Tech stacks for building different apps and features.
Cytodata – Image-based profiling of biological phenotypes for computational biologists.
IRC – Open source messaging protocol.
Advertising – Advertising and programmatic media for websites.
Earth – Find ways to resolve the climate crisis.
Naming – Naming things in computer science done right.
Biomedical Information Extraction – How to extract information from unstructured biomedical data and text.
Web Archiving – An effort to preserve the Web for future generations.
WP-CLI – Command-line interface for WordPress.
Credit Modeling – Methods for classifying credit applicants into risk classes.
Ansible – A Python-based, open source IT configuration management and automation platform.
Biological Visualizations – Interactive visualization of biological data on the web.
QR Code – A type of matrix barcode that can be used to store and share a small amount of information.
Veganism – Making the plant-based lifestyle easy and accessible.
Translations – The transfer of the meaning of a text from one language to another.

 
Related

All Awesome Lists – All the Awesome lists on GitHub.
Awesome Indexed – Search the Awesome dataset.
Awesome Search – Quick search for Awesome lists.
StumbleUponAwesome – Discover random pages from the Awesome dataset using a browser extension.
Awesome CLI – A simple command-line tool to dive into Awesome lists.
Awesome Viewer – A visualizer for all of the above Awesome lists.

 
US Department of Education CRDC Dataset
The US Department of Ed has a dataset called the CRDC that collects data from all the public schools in the US and has demographic, academic, financial and all sorts of other fun data points. They also have corollary datasets that use the same identifier—an expansion pack if you may. It comes out every 2-3 years. Access it here.
 
Nasa Dataset: sequencing data from bacteria before and after being taken to space
NASA has some sequencing data from bacteria before and after being taken to space, to look at genetic differences caused by lack of gravity, radiation and others. Very fun if you want to try your hand at some bio data science. Access it here.
 
All Trump’s twitter insults from 2015 to 2021 in CSV.
Extracted from the NYT story: here
 
Data is plural
Data is Plural is a really good newsletter published by Jeremy Singer-Vine. The datasets are very random, but super interesting. Access it here.
 
Global terrorism database
Huge list of terrorism incidents from inside the US and abroad. Each entry has date and location of the incident, motivations, whether people or property were lost, the size of the attack, type of attack, etc. Access it here.
Terrorist Attacks Dataset: This dataset consists of 1293 terrorist attacks each assigned one of 6 labels indicating the type of the attack. Each attack is described by a 0/1-valued vector of attributes whose entries indicate the absence/presence of a feature. There are a total of 106 distinct features. The files in the dataset can be used to create two distinct graphs. The README file in the dataset provides more details. Download Link
Terrorists: This dataset contains information about terrorists and their relationships. This dataset was designed for classification experiments aimed at classifying the relationships among terrorists. The dataset contains 851 relationships, each described by a 0/1-valued vector of attributes where each entry indicates the absence/presence of a feature. There are a total of 1224 distinct features. Each relationship can be assigned one or more labels out of a maximum of four labels making this dataset suitable for multi-label classification tasks. The README file provides more details. Download Link
 
The dolphin social network
This network dataset is in the category of Social Networks. A social network of bottlenose dolphins. The dataset contains a list of all of links, where a link represents frequent associations between dolphins. Access it here
 
Dataset of 200,000 jokes
There are about 208 000 jokes in this database scraped from three sources.
Access it here
 
The Million Song Dataset
The Million Song Dataset is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.
Its purposes are:

To encourage research on algorithms that scale to commercial sizes
To provide a reference dataset for evaluating research
As a shortcut alternative to creating a large dataset with APIs (e.g. The Echo Nest’s)
To help new researchers get started in the MIR field

 
Cornell University’s eBird dataset
Decades of observations of birds all around the world, truly an impressive way to leverage citizen science. Access it here.
 
UFO Report Dataset
NUFORC geolocated and time standardized ufo reports for close to a century of data. 80,000 plus reports. Access it here
 
CDC’s Trend Drug Data
The CDC has a public database called NAMCS/NHAMCS that allows you to trend drug data. It has a lot of other data points so it can be used for a variety of other reasons. Access it here.
 
Health and Retirement study: Public Survey data
A listing of publicly available biennial, off-year, and cross-year data products.
Example: COVID-19 Data




Year
Product




2020
2020 HRS COVID-19 Project




 
Original. Reposted with permission.
 
Related:

Introducing The NLP Index
8 Places for Data Professionals to Find Datasets
3 Best Sites to Find Datasets for your Data Science Projects"
